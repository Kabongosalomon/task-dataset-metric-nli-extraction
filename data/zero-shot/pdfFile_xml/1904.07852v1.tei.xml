<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matrix and tensor decompositions for training binary neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian.bulat@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
							<email>j.kossaifi@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>georgios.t@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<email>maja.pantic@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Matrix and tensor decompositions for training binary neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is on improving the training of binary neural networks in which both activations and weights are binary. While prior methods for neural network binarization binarize each filter independently, we propose to instead parametrize the weight tensor of each layer using matrix or tensor decomposition. The binarization process is then performed using this latent parametrization, via a quantization function (e.g. sign function) applied to the reconstructed weights. A key feature of our method is that while the reconstruction is binarized, the computation in the latent factorized space is done in the real domain. This has several advantages: (i) the latent factorization enforces a coupling of the filters before binarization, which significantly improves the accuracy of the trained models. (ii) while at training time, the binary weights of each convolutional layer are parametrized using real-valued matrix or tensor decomposition, during inference we simply use the reconstructed (binary) weights. As a result, our method does not sacrifice any advantage of binary networks in terms of model compression and speeding-up inference. As a further contribution, instead of computing the binary weight scaling factors analytically, as in prior work, we propose to learn them discriminatively via back-propagation. Finally, we show that our approach significantly outperforms existing methods when tested on the challenging tasks of (a) human pose estimation (more than 4% improvements) and (b) ImageNet classification (up to 5% performance gains).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One key aspect of the performance of deep neural networks is the availability of abundant computational resources (i.e high-end GPUs) during both training and inference. However, often, such models need to be deployed on devices with limited resources such as smartphones, FP-GAs or embedded boards. To this end, there is a plethora of works that attempt to miniaturize the models and speed-up inference with popular directions including matrix and tensor decomposition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19]</ref>, weights pruning <ref type="bibr" target="#b10">[11]</ref> or network quantization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>. Of particular interest in this work is the extreme case of quantization -binarization, where all the weights and features are restricted to 2 states only. Such networks can achieve a compression rate of up to 32? and an even higher order speed-up that can go up to 58? <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref>. Despite these attractive properties, training binary networks to a comparable accuracy to that of their real-valued counterparts is still an open problem. For example, there is ? 20% accuracy drop between real and binary networtks on ImageNet <ref type="bibr" target="#b30">[31]</ref>, and ? 9% difference for human pose estimation on MPII <ref type="bibr" target="#b3">[4]</ref>.</p><p>Most current works that attempt to improve the accuracy of binary network fall into two broad categories: a) methodological changes and b) architectural improvements. The authors of <ref type="bibr" target="#b6">[7]</ref> propose to binarize the weights using the sign(x) function, with encouraging results on a few selected datasets. Because the representational power of binary networks is very limited, the authors of <ref type="bibr" target="#b30">[31]</ref> propose to add a scaling factor to the weights and channels of each convolutional layer, showing for the first time competitive results on ImageNet. From an architectural point of view, the method of <ref type="bibr" target="#b3">[4]</ref> proposes a novel residual module specially designed for binary networks, while in <ref type="bibr" target="#b37">[38]</ref>, the authors incorporate densenet-like connections into the U-Net architecture.</p><p>In this work, we propose a simple method to improve the accuracy of binary networks by introducing a linear or multi-linear re-parametrization of the weight tensor during training. Let's consider a 4-dimensional weight tensor W ? R O?C?w?h . A common limitation in prior work is that each filter W i ? R C?w?h (a slice of W) of a given convolutional layer is binarized independently as follows:</p><formula xml:id="formula_0">B i = sign(W i ).</formula><p>In contrast, our key idea in this work is to model the filters jointly by re-parametrizing them in a shared subspace using a matrix or tensor decomposition, and then binarizing the weights. A simplified version of our idea can be described 1 arXiv:1904.07852v1 [cs.CV] <ref type="bibr" target="#b15">16</ref> Apr 2019 as follows:</p><formula xml:id="formula_1">W = UV B i = sign(W i ).</formula><p>This allows us to introduce an inter-dependency between the to-be-binarized weights through the shared factor U either at a layer level or even more globally at a network level.</p><p>A key feature of our method is that the decomposition factors (i.e U, V) are kept real during training. This allows us to introduce additional redundancy which, as we will show facilitates learning.</p><p>Note that this latent parametrization is used only during training. During inference, our method only uses the reconstructed weights, which have been binarized using the sign function (the decomposition factors are neither used nor stored). Hence, our method does not sacrifice any of the advantages of binary networks in terms of model compression and inference speed-up.</p><p>In summary, we make the following contributions:</p><p>? We are the first to propose parameterizing the binarized weights of a neural networks using a real-valued linear and multi-linear decomposition (at training time).</p><p>In other words, we enforce a shared subspace between the filters of the convolutions, as opposed to prior work that model and binarize each filter independently. This novel approach allows us to further improve the accuracy of binary networks without sacrificing any of their advantage in terms of model compression and speeded-up inference. (Section 4.2).</p><p>? We revise the convolutional approximation proposed in <ref type="bibr" target="#b30">[31]</ref>: (I * W = (sign(I) * sign(W))?, where the scaling factor ? is computed analytically from ? = 1 n W l1 and propose to instead learn it discriminatively via back-propagation at train time (Section 4.3).</p><p>? We explore several types of decomposition (SVD and Tucker) applied either layer-by-layer or jointly to the entire network as a whole (Section 4). We perform in-depth ablation studies that help shed light on the advantages of the newly proposed method.</p><p>? We show that our method significantly advances the state-of-the-art for two important computer vision tasks: human pose estimation on MPII and large-scale image classification on ImageNet (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review the related work, in terms of neural network architectures (2.1), network binarization (2.2) and tensor methods (2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Efficient neural network architectures</head><p>Despite the remarkable accuracy of deep neural networks on a large variety of tasks, deploying such networks on devices with low computational resources is highly impractical. Recently, a series of works have attempted to alleviate this issue via architectural changes applied either at the block or architecture level.</p><p>Block-level optimization. In <ref type="bibr" target="#b11">[12]</ref> He et al. proposes the so-called bottleneck block that attempts to reduce the number of 3 ? 3 filters using 2 convolutional layers with a 1 ? 1 kernel that project the features into a lower dimensional subspace and back. The authors from <ref type="bibr" target="#b41">[42]</ref> introduce a new convolutional block that splits the module into a series of parallel sub-blocks with the same topology. The resulting block has a smaller footprint and higher representational power. In a similar fashion, MobileNet <ref type="bibr" target="#b13">[14]</ref> and its improvement <ref type="bibr" target="#b33">[34]</ref> make use of depth-wise convolutional layers with the later proposing an inverted bottleneck module. In <ref type="bibr" target="#b43">[44]</ref>, the authors combine point-wise group convolution and channel shuffle incorporating them in the bottleneck structure.</p><p>Note, that in this work we do not attempt to improve the architecture itself and simply use the already well-established basic block with pre-activation introduced in [13] (see <ref type="figure">Fig. 1</ref>).</p><p>Network-level optimization. The Dense-Net architecture <ref type="bibr" target="#b15">[16]</ref> proposes to inter-connect each layer to every other layer in a feed-forward fashion. This results in a better gradient flow and higher performance per number of parameters ratio. Variations of it were later adopted for other tasks, such as human pose estimation <ref type="bibr" target="#b37">[38]</ref>. In <ref type="bibr" target="#b31">[32]</ref> and its followup <ref type="bibr" target="#b32">[33]</ref> the authors introduce the so-called YOLO architecture which proposes a new framework for object detection and an optimized architecture for the network backbone that can run real-time on a high-end GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network binarization</head><p>Another direction for speeding-up neural networks is network quantization. This process reduces the number of possible states that the weights and/or the features can take and has become increasingly popular with the advent of low-precision computational hardware.</p><p>While normally CNNs operate using float-32 values, the work of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref> proposes to use 16-and 8-bit quantization showing in the process insignificant performance drop on a series of small datasets (MNIST, CIFAR10). Zhou et al. <ref type="bibr" target="#b45">[46]</ref> proposes to allocate a different numbers of bits for the network parameters (1 bit), activations (2 bits) and gradients (6 bits), the values of which are selected based on their sensitivity to numerical inaccuracies. <ref type="bibr" target="#b40">[41]</ref> propose a twostep n-bits quantization (n ? 2), where the first step consists of learning a low-bit code and the second in learning a transformation function. In <ref type="bibr" target="#b9">[10]</ref>, the authors propose to learn a 1-2 bit quantization for the weights and 2-8 for activations by learning a symmetric codebook for each particular weights subgroup. While such methods can lead to significant space and speed gains, the most interesting case is that of binarized neural networks. Such networks have their features and weights quantized to two states only. In <ref type="bibr" target="#b34">[35]</ref> the authors propose to binarize the weights using the sign function. Follow-up work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> further improve these results, by binarizing both the activations and the weights. In such type of networks the multiplications inside the convolutional layer can be replaced with XOR bitwise operations. The current state-of-the-art binarization technique is the XNOR-Net method <ref type="bibr" target="#b30">[31]</ref> that proposes a real-valued scaling factor for the weights and inputs. The proposed XNOR-Net method <ref type="bibr" target="#b30">[31]</ref> is the first to report good results on a large scale dataset (ImageNet). In <ref type="bibr" target="#b3">[4]</ref>, the authors propose a new module specifically designed for binary networks. The work of <ref type="bibr" target="#b27">[28]</ref> explores ways of increase the quantized network accuracy by increasing its width (i.e number of channels) motivated by the idea that often the activations are taking most of the memory during training. In a similar fashion, in <ref type="bibr" target="#b26">[27]</ref> the authors use up to 5 parallel binary convolutional layers to approximate a real one, as such increasing the size and computational requirements of the network up to 5?. <ref type="bibr" target="#b44">[45]</ref> proposes a loss-aware binarization method that jointly regularizes the weights approximation error and the accompanying loss, however this method quantizes the weights while leaving the features real. <ref type="bibr" target="#b14">[15]</ref> proposes a semi-binary decomposition of the binary weight tensor into two binary matrices and a diagonal real-valued one which are used (instead of the actual binary weights) during test time. As mentioned by the authors the proposed binaryto-(semi-)binary decomposition is a difficult optimization problem and hence harder to train. More importantly, and in contrary to our method, in this approach, the activations are kept real.</p><p>In this work, we propose to improve the binarization process itself, introducing a novel approach that increases the representation power and flexibility of binary weights at train time via matrix and tensor re-parametrization while maintaining the same structure and very large speed gains during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Tensor methods</head><p>Tensor methods offer a natural extension of the more traditional algebraic methods to higher orders that naturally arise in convolutional networks. As such, this family of methods is actively deployed, both for compressing and speeding-up the networks via re-parametrization <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref>, or by taking advantage directly of the higher order dimensionality present in the data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Separable convolutions, recently popularized in <ref type="bibr" target="#b4">[5]</ref>, are one such example that can be obtained by applying a CP decomposition to the layer weights. In <ref type="bibr" target="#b24">[25]</ref>, the weights of each convolutional layer are decomposed into a sum of rank-1 tensors using a CP decomposition in an attempt to speed-up the convolutional modules. At inference time this is achieved by replacing the original layers with a set of smaller ones where the weights of each newly introduced layer represent the factors themselves. Similarly, in <ref type="bibr" target="#b17">[18]</ref> the authors re-parametrize the layer weights using a Tucker decomposition. At test time, the resulting module resembles a bottleneck <ref type="bibr" target="#b11">[12]</ref> block. <ref type="bibr" target="#b35">[36]</ref> propose to decrease the redundancy typically present in large neural networks by expressing each layer as the composition of two convolutional layers with less parameters. Each 2D filter is approximated by a sum of rank-1 tensors. However, this can be applied only for convolutional layer which have a kernel size larger than 1. While most of the works mentioned above are applied to convolutional layers other types can be parameterized too. In all the aforementioned works, tensor decompositions are applied to individual convolutional layers. More recently, the work of <ref type="bibr" target="#b20">[21]</ref> proposed a simple method for whole network tensorization using a single high-order tensor.</p><p>To our knowledge, none of the above methods have been applied to binary networks. By doing so, our approach allows us to combine the best of both words: take advantage of the very high compression rate and speed-up typically offered by binarized networks while maintaining the increased representational power offered by the tensor re-parametrization methods. A crucial aspect of this reparametrization is that it enables us to enforce an interdependency between the binary filters, which were previously treated independently by prior work on binarization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Human pose estimation</head><p>While a complete review of recent work on human pose estimation goes beyond the scope of this paper, the current state-of-the-art on single person human pose estimation is based on the so-called "Hourglass"(HG) architecture <ref type="bibr" target="#b28">[29]</ref> and its variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43]</ref>. Most of this prior work focuses on achieving the highest performance without imposing any computational restrictions. Only recently, the work in <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b3">[4]</ref> study this problem in the context of quantized neural networks. In <ref type="bibr" target="#b37">[38]</ref> the authors propose an improved HG architecture that makes use of dense connections <ref type="bibr" target="#b15">[16]</ref> while <ref type="bibr" target="#b3">[4]</ref> introduces a novel residual block specially tailored to binarized neural networks. In contrast with the aforementioned methods, in this work, instead of improving the network architecture itself, we propose a novel, improved binarization technique that is independent of the network and task at hand.  <ref type="figure">Figure 1</ref>: The original residual basic block proposed in <ref type="bibr" target="#b11">[12]</ref> with the changes introduced <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Let W ? R O?C?w?h and I ? R C?win?hin denote the weights and respectively, the input of the L-th convolutional layer, where O represents the number of output channels, C the number of input channels and (w, h) the width and height of the convolutional kernel. w in ? w and h in ? h represent the spatial dimension of the input features I. In its simplest form the binarization process can be achieved by taking the sign of the weights and respectively, of the input features where:</p><formula xml:id="formula_2">sign(x) = ?1, if x ? 0 1, if x &gt; 0<label>(1)</label></formula><p>However, such approach leads to sub par performance on the more challenging datasets. In <ref type="bibr" target="#b30">[31]</ref>, Rastegari et al. proposes to alleviate this by introducing a real-valued scaling factor that boosts the representational power of such networks:</p><formula xml:id="formula_3">I * W = (sign(I) * sign(W)) ?,<label>(2)</label></formula><p>where ? i = 1 n W(i, : , : , : ) 1 , i = {1, 2, ? ? ? , O} and n = C ? w ? h.We denotes as * the real-valued convolutional operation and * its binary counterpart, implemented using XNOR bitwise operations. Note, that while in <ref type="bibr" target="#b30">[31]</ref> a scaling factor is proposed for both input features and weights, in this work we use only the later since removing the first significantly speeds-up the network at a negligible drop in accuracy <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this section, we present our novel binarization method that aims to increase the representational power of the binary networks by enforcing for the first time an interdependency between the binary filters via a linear or multilinear over-parametrization of the weights. We start by introducing some necessary notation (Section 4.1). We then continue by describing the main algorithm and its variations (Section 4.2). Finally, in Section 4.3 we describe how to further improve the proposed binarization technique by optimizing the scaling factor with respect to the target loss function via back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Notation</head><p>We denote vectors (1 st order tensors) as v, matrices (2 nd order tensors) as M and tensors of order ? 3, as X . We denote element (i 0 , i 1 , ? ? ? , i N ) of a tensor as X i0,i1,??? ,i N or X (i 0 , i 1 , ? ? ? , i N ). A colon is used to denote all elements of a mode, e.g. the mode-1 elements of X are denoted as X (:, i 2 , i 3 , ? ? ? , i N ). Tensor contraction: we define the n-mode product (contraction between a tensor and a matrix), for a tensor X ? R D0?D1?????D N and a matrix M ? R R?Dn , as the tensor</p><formula xml:id="formula_4">T = X ? n M ? R D0?????Dn?1?R?Dn+1?????D N , with: T i0,i1,??? ,in = Dn k=0 M in,k X i0,i1,??? ,in .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Matrix and tensor re-parametrizations for training binary CNNs</head><p>A key limitation of prior work on training binary networks is that each of the filters of the weight tensor in each convolutional layer is binarized independently, without imposing any relation between the filters explicitly. To alleviate this, we propose to increase the representational power of the binary network via re-parametrization. During training, we propose to express the to-be-binarized weights W of each convolutional layer using a linear or multi-linear real-valued decomposition:</p><formula xml:id="formula_5">W = ReconstructWeights(? 0 , ? 1 , ..., ? N ),<label>(4)</label></formula><p>where the function ReconstructWeights(;) is specific to the decomposition used, there exists at least one decomposition factor ? k which is shared among all filters in W, and the set of all decomposition factors ? i , i = 1, . . . , N are all realvalued. Using a real-valued decomposition is a key feature of the proposed approach as it allows us to introduce additional redundancy which as we show facilitates learning.</p><p>Note that when training is done, our method simply uses the reconstructed weights which are converted to binary numbers using the sign function. Hence, during inference the factors ? i , i = 1, . . . , N are neither used nor need to be stored, only the reconstructed binarized weights are used. Hence, our method does not sacrifice any advantage of binary networks in terms of model compression and speedingup inference.</p><p>In the context of this work, we explore two different decompositions: SVD and Tucker. We apply these decompositions in two different ways: layer-wise and holistically. Layer-wise decomposition refers to modeling the weight tensor of each convolutional layer separately, i.e. performing a different decomposition for each convolutional layer (e.g. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36]</ref>). We note that this is the standard way in literature that SVD and Tucker decomposition have been applied for neural network re-parametrization. More recently, the work of <ref type="bibr" target="#b20">[21]</ref> proposed a single method for whole network tensorization using a single high-order tensor. We refer to this tensorization approach as holistic.</p><p>Note, that unlike other binarization methods where two set of weights are explicitly stored in memory and swapped at each iteration <ref type="bibr" target="#b30">[31]</ref> our method can deal with this implicitly without a secondary copy. This is due to the fact that the factors are always real-valued and are reconstructed and binarized on-demand during training.</p><p>The entire proposed method for binarization is described in Algorithm 1:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Layer-wise SVD decomposition</head><p>Let W ? R O?(Cwh) be the reshaped version of weight W of the L-th layer. By applying an SVD decomposition we can express W as follows:</p><formula xml:id="formula_6">sign(W) = sign(U?V T ),<label>(5)</label></formula><p>where <ref type="figure">Cwh)</ref> is a diagonal matrix and U ? R O?K , V ? R K?(Cwh) . By substituting eq. (5) in (2) we obtain:</p><formula xml:id="formula_7">? ? R M ?M , M ? min(O,</formula><formula xml:id="formula_8">I * W = (sign(I) * sign(U?V T )) ?,<label>(6)</label></formula><p>where U and V are learned via backpropagation. When evaluated on the validation set of MPII, reparametrizing the weights layer-wise using SVD improves the performance by 0.3% (see <ref type="table">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Layer-wise Tucker decomposition</head><p>While the SVD decomposition shows some benefits on the MPII dataset, one of its core limitation is that it requires reshaping the weight to a 2D matrix, those losing the important spatial structure information present in them.</p><p>To alleviate this, we propose using the Tucker decomposition, a natural extension of SVD for higher order tensors. Using the Tucker decomposition we can express the binary weights as follow: <ref type="bibr" target="#b6">(7)</ref> where G ? R O?C?w?h is a full-rank core and (U (0) , U <ref type="bibr" target="#b0">(1)</ref> , U <ref type="bibr" target="#b1">(2)</ref> , U (3) ) a set of factors.</p><formula xml:id="formula_9">sign(W) = sign(G ? 0 U (0) ? 1 U (1) ? 2 U (2) ? 3 U (3) ),</formula><p>The results from <ref type="table">Table 1</ref> and 2 confirm the proposed hypothesis, showing an improvement of more than 0.7% on top of the gains offered by the SVD decomposition.</p><p>Algorithm 1 Training an L-layer CNN with binary weights via matrix or tensor decomposition. The rows colored in blue are the changes introduced by our method when compared against the approach proposed in <ref type="bibr" target="#b30">[31]</ref>.</p><p>Input: A minibatch of inputs and targets (I, Y), cost function C(Y,?), current set of matrices from which the weights can be reconstructed ? 0 t , ? 1 t , ? ? ? , ? N t (obtained using one of the methods described in Sections 4.2.1-4.2.3) and current learning rate ? t . Optionally, if the scaling factor is computed using the method described in Section 4.3, the current weights scales ? t . Output: updated factors ? 0 t+1 , ? 1 t+1 , ? ? ? , ? N t+1 and updated learning rate ? t+1 . If ? is computed using the method from Section 4.3, also return the updated ? t+1 . 1: Binarizing weight filters: 2: for l = 1 to L do 3:</p><formula xml:id="formula_10">W l = ReconstructWeights(? l 0 t , ? l 1 t , ..., ? l N t )</formula><p>// Using Eqs. 5 or 7 or 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>if ? is defined then 5: </p><formula xml:id="formula_11">B l = sign(W t l ) 6: W l = ? l B l // If</formula><formula xml:id="formula_12">? t+1 i = UpdateParameters(? t i , ?C ??i , ? t ) 17: if ? is defined then 18: ? t+1 = UpdateParameters(? t , ?C ?? , ? t ) 19: ? t+1 = UpdateLearningRate(? t , t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Holistic Tucker decomposition</head><p>Motivated by the method proposed in <ref type="bibr" target="#b20">[21]</ref> and our finding from Section 4.2.2, where we re-parametrized the weights using a layer-wise Tucker decomposition, herein we go one step further and propose to group together identically shaped weights inside the network in a higher-order tensor in order to exploit the inter relation between them holistically.</p><p>For ResNet-18 <ref type="bibr" target="#b11">[12]</ref> used for ImageNet classification, we create 3 groups of convolutional layers based on the macromodule structure characterizing the architecture. Each of <ref type="figure">Figure 2</ref>: Distribution of the scaling factor ? for various layers from the bottom to the top of the network (left to right) on a ResNet-18 trained until convergence on ImageNet. First row: ? is computed using the analytical form proposed in <ref type="bibr" target="#b30">[31]</ref>; Second row: ? is computed using our proposed method (see Section 4.3). Notice that our method allows for a more spread out distribution, that can take both positive and negative values, with significantly higher values that lead both to faster and more stable training.  <ref type="table">Table 1</ref>: PCK-h based results on the validation set of MPII for different variations of the proposed binarization method. Notice that the proposed holistic approach significantly outperforms the baseline.</p><p>these groups is then parameterized with a single 5-th order tensor W ? R N ?O?C?w?h obtained by concatenating the weights of the N convolutional layers in this group. The resulting decomposition is then defined as:</p><formula xml:id="formula_13">sign(W ) = sign(G ? 0 U (0) ? 1 U (1) ? ? ? ? ? 4 U (4) ). (8)</formula><p>The individual weights of a given layer l can be obtained from W = W (l, :, :, :, :).</p><p>For the hourglass network used in our experiments for human pose estimation, we follow <ref type="bibr" target="#b20">[21]</ref> to derive a single 7-th order tensor W, the modes of which correspond to the number of HGs, the depth of each HG, the three signal pathways, the number of convolutional blocks, the number of input features, the number of output features, and finally the height and width of each of the convolutional kernels. The remaining few layers in the architecture are decomposed using a layer-wise Tucker decomposition.</p><p>When tested on MPII, the proposed representation improves the performance with more than 3% in terms of absolute error against the baseline and more than 1% when compared with its layer-wise version (see <ref type="table">Table 1</ref>). Similar results are observed on ImageNet (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learnable scaling factors</head><p>One of the key ingredients of the recent success of binarized neural network was the introduction of the ? weight scaling factor in <ref type="bibr" target="#b30">[31]</ref> (see Eq. 2), computed analytically as the average of absolute weight values. While this estimation generally performs well, it attempts to minimize the difference between the real weights and the binary ones W ? ?sign(W) and does not explicitly decrease the overall network loss. In contrast, in this work we propose to learn <ref type="figure">Figure 3</ref>: Distribution of the weights before binarizing them using the sign function for various layers from the bottom to the top of the network (left to right) on a HourGlass trained on MPII. First row: the weights are obtained using no parameterization method (i.e using the method from <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4]</ref>) Second row: the weights are computed using our proposed method and reconstructed using a holistic Tucker parameterization (see Section 4.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hourglass module</head><p>Heatmaps <ref type="figure">Figure 4</ref>: The hourglass architecture as introduced in <ref type="bibr" target="#b28">[29]</ref> using binarized basic blocks as building modules. the scaling factor by minimizing its value with respect to the networks cost function, learning it discriminatively via back-propagation. <ref type="figure">Fig. 2</ref> shows the difference between the scaling factors learned using our proposed method vs the ones computed using the analytic solution from <ref type="bibr" target="#b30">[31]</ref>. Note that our method leads to (a) a more spread out distribution that can take both positive and negative values, (b) has significantly higher magnitude, thus leading to a faster and more stable training. <ref type="table">Table 1</ref> and 2 show that the newly proposed method for learning the scale factor offers consistent gains across all decompositions and tasks (both human pose estimation and image recognition), with the largest gain observed for the MPII dataset (more than 1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental evaluation</head><p>This section firstly presents the experimental setup, network architecture and training procedure. We then empirically demonstrate the advantage of our approach on single person human pose estimation and large-scale image recognition where we surpass the state-of-the-art by more than 4% (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Human pose estimation</head><p>Datasets. MPII <ref type="bibr" target="#b0">[1]</ref> is one of the most challenging human pose estimation datasets to-date consisting of over 40,000 people, each annotated with up to 16 keypoints and visibility labels. The images were extracted from various youtube videos. For training/validation split, we used the same partitioning as introduced in <ref type="bibr" target="#b39">[40]</ref>. The results are reported in terms of PCKh <ref type="bibr" target="#b0">[1]</ref>.</p><p>Network architecture. The Hourglass (HG) <ref type="bibr" target="#b28">[29]</ref> and its variants represent the current state-of-the-art on human pose estimation. As such, in this work, we used an hourglass-like architecture <ref type="figure">(Fig. 4</ref>) constructed using the basic blocks introduced in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> (see also <ref type="figure">Fig. 1</ref>). The HG network as a whole follows an encoder-decoder structure with skip connections between each corresponding level of the decoder and encoder part. The basic block used has 128 channels. Training. During training, we followed the best practices and randomly augmented the data with rotation (between ?30 ? and 30 ? degrees), flipping and scale jittering (between 0.7 and 1.3). All models were trained until convergence (typically 120 epochs max). During this time, the learning rate was dropped multiple times from 2.5e ? 4 to 5e ? 6. We used no weight decay.</p><p>All of our models were trained using pytorch <ref type="bibr" target="#b29">[30]</ref> and RMSProp <ref type="bibr" target="#b38">[39]</ref>. The tensor operations were implemented using tensorly <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Large-scale image classification</head><p>Datasets ImageNet <ref type="bibr" target="#b8">[9]</ref> is a large scale image recognition dataset consisting of more than 1.2M images for training distributed over 1000 object classes and 50,000 images for validation. Network architecture. Following <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref>, we used a Resnet-18 <ref type="bibr" target="#b11">[12]</ref> architecture for our experiments on Ima-geNet. The ResNet-18 consists of 18 convolutional layers distributed across 4 macro-modules that are linked via a skip-connection. At the beginning of each macro-module the resolution is dropped using a convolutional layer with a stride &gt; 1. The final predictions are obtained by using an average pooling layer followed by a fully connected one. Training. During training, we resized the input images to 256 ? 256px and then a random 224 ? 224px crop was selected for training. At test time, instead of random cropping the images, a center crop was applied. The network was trained using Adam <ref type="bibr" target="#b19">[20]</ref> for 90 epochs with a learning rate of 1e ? 3 that was gradually reduced (dropped every 30 epochs) to 1e ? 6. The weight decay was set to 1e ? 7 for the entire duration of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with state-of-the-art</head><p>Method #parameters PCKh HBC <ref type="bibr" target="#b3">[4]</ref> 6.2M 78.1% Ours 6.0M 82.5% Real valued 6.0M 85.8% <ref type="table">Table 3</ref>: Comparison with the state-of-the-art method of <ref type="bibr" target="#b3">[4]</ref> on the validation set of the MPII dataset. Our method improves upon the state-of-the-art approach of <ref type="bibr" target="#b3">[4]</ref> by mote than 3% further bridging the gap between the real and binary domain.</p><p>In this section, we report the performance of our method on the challenging and diverse tasks of human pose estimation (on MPII) and large scale-image recognition (on Imagenet), and compare it with that of published state-of-the-art methods that use fully binarized neural networks (i.e both the weights and the features are binary).</p><p>On human pose estimation, the only other work that trains fully binarized networks is that of <ref type="bibr" target="#b3">[4]</ref>. As the results from <ref type="table">Table 3</ref> show, our method offers an improvement of more than 4% on the MPII dataset when compared against the state-of-the-art method of <ref type="bibr" target="#b3">[4]</ref>. Qualitative results are shown in <ref type="figure" target="#fig_2">figure 5</ref>.</p><p>As <ref type="table">Table 4</ref> shows, for ImageNet classification, our method improves upon the results from <ref type="bibr" target="#b30">[31]</ref> by up to 5% in terms of absolute error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 accuracy Top-5 accuracy BNN <ref type="bibr" target="#b7">[8]</ref> 42.2% 69.2% XNOR-Net <ref type="bibr" target="#b30">[31]</ref> 51.2% 73.2% Ours 55.6% 78.5% Real valued <ref type="bibr" target="#b11">[12]</ref> 69.3% 89.2% <ref type="table">Table 4</ref>: Top-1 and Top-5 accuracy on ImageNet using a ResNet-18 binarized architecture. Notice that out methods surpass the current state-of-the-art by a large margin, up to 5% improvement in terms of absolute error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a novel binarization method in which the weight tensor of each layer or group of layers is parametrized using matrix or tensor decomposition. The binarization process is then performed using this latent parametrization, via a quantization function (e.g. sign function) applied to the reconstructed weights.</p><p>This simple idea enforces a coupling of the filters before binarization which is shown to significantly improve the accuracy of the trained models. Additionally, instead of computing the weight scaling factor analytically we propose to learn them via backpropagation. When evaluated on single person human pose estimation (on MPII) and large scale image recognition (Imagenet) our method surpasses the state-of-the-art by 4%, and respectively 5% while retaining the speed-up (up to 58?) and space saving (up to 32?) typically offered by binary networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>using method proposed in Section 4 for k th filter in l th layer do 9 :W</head><label>49</label><figDesc>lk = ? lk B lk 12:? = ForwardPass(I, W) // standard forward propagation where the convolutional operations use the reconstructed binarized weights W 13: ?C ?? = BackwardPass( ?C ?? , W) // standard backward propagation where gradients are computed using the reconstructed binary weights with respect to the factors 14: Update the matrices using an update rule (i.e ADAM, RMSprop): 15: for i = 0 to N do 16:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative examples produced by our binary method on the validation set of MPII. Notice that our method can cover a large variety of poses and across a large number of different activities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>).</figDesc><table><row><cell cols="3">Decomposition Holistic Learn. alpha Top-1</cell><cell>Top-5</cell></row><row><cell>None</cell><cell>-</cell><cell cols="2">52.3% 74.1%</cell></row><row><cell>None</cell><cell>-</cell><cell cols="2">53.0% 74.7%</cell></row><row><cell>SVD</cell><cell></cell><cell cols="2">52.5% 74.2%</cell></row><row><cell>Tucker</cell><cell></cell><cell cols="2">54.0% 76.9%</cell></row><row><cell>Tucker</cell><cell></cell><cell cols="2">54.7% 77.4%</cell></row><row><cell>Tucker</cell><cell></cell><cell cols="2">55.2% 78.2%</cell></row><row><cell>Tucker</cell><cell></cell><cell cols="2">55.6% 78.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top-1 and Top-5 accuracy on the ImageNet dataset for different variations of the proposed binarization method.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cp-decomposition with tensor power method for convolutional neural networks compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1701.07148</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training deep neural networks with low precision multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Syq: Learning symmetric quantization for efficient deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Faraone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4300" to="4309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training binary weight networks via semi-binary decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Densely connected convolutional networks. arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06530</idno>
		<title level="m">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parametrizing fully convolutional nets with a single highorder tensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR submission</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tensor contraction layers for parsimonious deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1940" to="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tensor regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>abs/1707.08308</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09555</idno>
		<title level="m">Tensorly: Tensor learning in python</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Speeding-up convolutional neural networks using fine-tuned cp-decomposition. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6553</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Annapureddy</surname></persName>
		</author>
		<title level="m">Fixed point quantization of deep convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards accurate binary convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="345" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wrpn: Training and inference using wide reduced-precision networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03079</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06067</idno>
		<title level="m">Convolutional neural networks with low-rank regularization</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2012. 7</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-step quantization for low-bit neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4376" to="4384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Aggregated residual transformations for deep neural networks. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Explicit losserror-aware quantization for low-bit deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<title level="m">Dorefanet: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
