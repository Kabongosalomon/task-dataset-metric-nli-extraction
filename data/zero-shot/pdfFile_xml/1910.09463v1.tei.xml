<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Meyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<addrLine>3 Mila</addrLine>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>USING SPEECH SYNTHESIS TO TRAIN END-TO-END SPOKEN LANGUAGE UNDERSTANDING MODELS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end models are an attractive new approach to spoken language understanding (SLU) in which the meaning of an utterance is inferred directly from the raw audio without employing the standard pipeline composed of a separately trained speech recognizer and natural language understanding module. The downside of end-to-end SLU is that in-domain speech data must be recorded to train the model. In this paper, we propose a strategy for overcoming this requirement in which speech synthesis is used to generate a large synthetic training dataset from several artificial speakers. Experiments on two open-source SLU datasets confirm the effectiveness of our approach, both as a sole source of training data and as a form of data augmentation.</p><p>Index Termsspoken language understanding, speech synthesis, speech recognition, end-to-end spoken language understanding, backtranslation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The use of end-to-end models for spoken language understanding (SLU) is beginning to be given more serious consideration <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Whereas conventional SLU uses an automatic speech recognition (ASR) component to transcribe the audio into text and a natural language understanding (NLU) component to map the text to semantics, an end-to-end model maps the audio directly to the semantics <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. End-to-end models have several advantages over the conventional SLU setup: they have reduced computational requirements and software implementation complexity, avoid downstream errors due to incorrect transcripts, can have the entire set of model parameters optimized for the ultimate performance criterion (semantic accuracy) as opposed to a surrogate criterion (word error rate), and can take advantage of information present in the speech signal but not in the transcript, such as prosody.</p><p>But because the input to an end-to-end model is speech and not text, end-to-end models cannot learn from text data. This means that new audio data must be recorded to train the model for every new SLU domain or application. In contrast, the conventional ASR-NLU pipeline can be trained just once on a generic speech corpus to learn the mapping from speech to text, and subsequently only on text data. Thus, end-to-end SLU can be more difficult to implement in practice than conventional SLU because audio is more expensive and timeconsuming to obtain than text data.</p><p>In this paper, we propose a method for reducing, or avoiding entirely, the need to record audio data to train an end-toend SLU model. Given a dataset of semantically labeled text data, we use a generic speech synthesizer, or text-to-speech (TTS), to read out these texts, thus generating an audio dataset that can be used for training the model. The ability to use synthetic data greatly lowers the barrier to entry for people who want to develop an SLU model for a new application: even if the accuracy of a model trained on synthetic speech is not satisfactory for end users, it may be good enough to allow fast prototyping of voice interfaces without waiting on the slow, expensive process of recording real speakers. Our method is useful not only when no real data is available: it also acts as data augmentation by exposing the model to more speaking styles and more ways of pronouncing the same phrases.</p><p>Our main contributions in this paper are as follows: 1</p><p>? We show that it is possible to train an end-to-end SLU model using only synthetic speech and achieve high accuracy on a test set of real speech, even when the speech synthesizer has imperfections.</p><p>? We run experiments using synthetic speech to augment an existing dataset of real speech and show that this augmentation can significantly improve accuracy, especially when few real speakers are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Our method is closely related to the idea of using speech synthesis to generate training data for end-to-end ASR <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. In end-to-end ASR, instead of using a separate acoustic model, language model, and pronunciation model, a single sequenceto-sequence model predicts the transcript from the audio <ref type="bibr" target="#b9">[10]</ref>. Because the language model in end-to-end ASR is only implicit and not decoupled from the rest of the model, it is difficult to train on standalone text data, so it does not easily handle certain types of utterances that are not well represented in the training audio, such as numeric sequences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. To help the model recognize these domain-specific types of utterances, they can be synthesized and added to the training set.</p><p>Outside of speech recognition, backtranslation is another technique in a similar vein often used for data augmentation in machine translation <ref type="bibr" target="#b12">[13]</ref>. In backtranslation, given three languages A, B, and C and paired data for (A, B) and (B, C), synthetic paired data for (A, C) is generated by translating the B text in (B, C) data into language A using a model trained on (A, B) data, and vice versa. If we think of the three modalities of audio, text, and semantics as three "languages", then our proposed technique is just backtranslation from semantically labeled text into audio.</p><p>Another related idea is "sim2real" transfer in robotics <ref type="bibr" target="#b13">[14]</ref>. In sim2real transfer, a policy is learned in a simulated environment, avoiding the risks involved in physically operating a robot, such as breaking the robot or harming humans in the environment. The speed of simulation can also give the robot more experiences than would be possible in a limited amount of time in the real world. Likewise, fast speech synthesis can allow generating more audio than would be possible with a human speaker, due to time constraints or fatigue for the speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head><p>The method proposed in this paper is simple. Two ingredients are required: 1) a text dataset, where each example consists of a transcript (e.g., "turn it up a couple notches") and corresponding semantic label (e.g., {"intent": "ChangeVolume", "slots": [{"action": "increase"}, {"amount": "two"}]}), and 2) a TTS for the language in which the transcripts are written. The TTS is used to synthesize each transcript. The label assigned to the synthesized audio is the label for the transcript used to synthesize the audio. If the TTS has multiple speakers, each speaker is used to synthesize the transcript, so that multiple training examples per transcript are generated. A subset of the available speakers can be used for a given transcript if it is too expensive to use all speakers. If spoken training examples from real speakers are available, the real and synthetic datasets can be concatenated to form a single larger dataset. An end-to-end SLU model can then be trained using the generated dataset.</p><p>We have identified three criteria that are important for choosing the TTS:</p><p>1. Multi-speaker: In the past, we have found that having multiple speakers in the training set is crucial to achieving high test accuracy in end-to-end SLU. We anticpated that this would also be the case when using synthetic speakers.</p><p>2. "Everyday" voices: Commercial TTS voices typically speak in refined "actor speech", which is pleasant for the listener. But this type of speech sounds very different from the casual speech in which most people naturally speak to voice interfaces. To avoid this mismatch, casual, everyday voices should be used to synthesize training data.</p><p>3. Open-source: Like most researchers, we have a limited budget and want to perform research that is easy to reproduce, so we avoid commercial services like Google's Cloud TTS.</p><p>For the experiments in this paper, the TTS that best met these criteria was Facebook's VoiceLoop <ref type="bibr" target="#b14">[15]</ref>. We used the pre-trained US English model included with the VoiceLoop repo, which has 22 synthetic speakers trained using the VCTK dataset <ref type="bibr" target="#b15">[16]</ref>. We have listened to some of the synthesized audios selected at random and found the VoiceLoop speech to sound fairly natural. However, the synthesized speech does have some flaws: it contains audible vocoder artifacts, punctuation is ignored, and in some instances the model did not correctly pronounce the input text. Despite these imperfections, the synthesized speech works quite well for training, as we will show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>To test our method, we run a number of experiments on two open-source SLU datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For the main set of experiments, we use the Fluent Speech Commands dataset <ref type="bibr" target="#b16">[17]</ref>. Fluent Speech Commands is a dataset of 30,043 English audios with 77 speakers, each labeled with "action", "object", and "location" slots. There are 248 distinct sentences, each spoken by multiple speakers in both the training set and validation/test sets.</p><p>We also use the Snips SLU Dataset <ref type="bibr" target="#b17">[18]</ref>, more specifically the "smart lights" near-field subset of the dataset. This dataset is smaller and more challenging than Fluent Speech Commands: it contains numbers and has only 1,660 audios, each corresponding to a different sentence, so the model is tested entirely on sentences it has never heard before and must generalize to them to achieve high accuracy. Also, the number of slots varies across sentences: for example, the sentence "Could you turn the lights on please?" has the label {"intent": "SwitchLightOn", "slots": []} with no slots, but the sentence "Turn the flat light to twelve" has the label {"intent": "SetLight-Brightness", "slots": [{"entity": "houseroom unique", "slot name": "room", "text": "flat"}, {"entity": snips/number", "slotname": "brightness", "text": "twelve"}]} with two slots. The dataset is intended to be split into five folds for cross-validation and has multiple speakers, but the splits and speaker identities are not included in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Models</head><p>We use encoder-decoder models in our experiments. The encoder is a deep neural network with multiple convolutional layers and recurrent layers, with max-pooling in some layers to reduce the sequence length. The encoder is pre-trained using the LibriSpeech ASR dataset <ref type="bibr" target="#b18">[19]</ref>; more details on how the pre-training is done are given in <ref type="bibr" target="#b16">[17]</ref>. The decoder for Fluent Speech Commands is a linear classifier applied to the output of the encoder at each timestep separately, followed by global max-pooling to convert the variable-length sequence of vectors of slot scores into a single vector ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. For simplicity, we use the same hyperparameters and transfer learning methodology as were used in the best performing model in <ref type="bibr" target="#b16">[17]</ref> across all experiments.</p><p>For the Snips SLU Dataset, since the number of slots varies across utterances, it is not possible to use the simple max-pooling decoder with a fixed-length output. Instead, we use an attention-based autoregressive decoder <ref type="bibr" target="#b19">[20]</ref>, as was proposed for SLU in <ref type="bibr" target="#b1">[2]</ref>  <ref type="figure" target="#fig_1">(Fig. 2)</ref>. The decoder uses two gated recurrent unit (GRU) layers of 256 hidden units each <ref type="bibr" target="#b20">[21]</ref>, with key-value attention <ref type="bibr" target="#b21">[22]</ref>, and sequentially predicts the semantic label string, character by character, using a beam search. We trained autoregressive models on Fluent Speech Commands and used the test accuracy to determine the hyperparameters used in the models for the Snips SLU Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results for purely synthetic training sets</head><p>We first present results for models trained using only synthetic speakers. We used all 22 synthetic VoiceLoop speakers to synthesize all sentences in Fluent Speech Commands 2 . To quantify how many speakers are needed to achieve good accuracy, we train models using the data from one speaker, two speakers, and so on, and report the resulting accuracy. The accuracy is measured on the test set of real speakers in Fluent Speech Commands; if any slot is incorrectly predicted, the entire utterance is deemed incorrect.</p><p>Not every speaker is equally high-quality or useful for training, so the randomly chosen subset of speakers can have a big impact on test accuracy, in addition to other sources of stochasticity, like the initial model weights and the order in which training examples are presented. To reduce the variance of the results, we run each experiment five times using different random seeds, and record the mean and standard deviation of the results. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the test accuracy as a function of the number of speakers. The accuracy increases sharply up to about 15 speakers, and plateaus afterwards, with a very slight upward trend. The conclusion we draw is that one should use all available synthetic speakers if possible, but if synthesis is expensive, or if the resulting dataset is too large to train on exhaustively, it may make sense to incrementally add new synthetic speakers and stop when adding more speakers is not very helpful. In subsequent experiments when using synthetic speakers, we use all 22 available synthetic speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results combining real and synthetic speech</head><p>We next present results for when the model is trained using real speech and augmented with synthetic speech. We simulate the scenario where only a few real speakers are available <ref type="bibr" target="#b1">2</ref> The synthesized dataset can be downloaded here: https://zenodo.org/record/3509828   by selecting a random subset of speakers from the full training set. The experiments here take longer to run since there are more speakers, so we run each experiment just three times instead of five times. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the results, presented alongside the accuracy when all 22 synthetic speakers are used (green bar on bottom) and the accuracy when all 77 real speakers are used (grey bar on top). Unsurprisingly, real speech is more useful than synthesized speech. The model trained using only real speech is about 4% more accurate than the model trained using only synthetic speech (99.1% ? 0.1% versus 94.9% ? 0.2%). Also, with only three real speakers and no synthetic speakers, the model already performs better than when using all 22 synthetic speakers.</p><p>Up to about 40 real speakers, there is unambiguous improvement from including the synthetic speakers in the training set. When using more than 40 real speakers, it is less clear from our experiments if including synthetic speakers is helpful. We measured the difference in accuracy across the number of real speakers with more than 40 real speakers; the accuracy was 0.07% higher on average when synthetic speakers were included. The difference is not significant, but it at least suggests that it is not harmful to include synthetic speakers even when a large number of real speakers is available.</p><p>Finally, we present results for the smaller, more challenging Snips SLU Dataset. Again, we synthesize each sentence using all 22 speakers, which boosts the size of the training set for each fold from 1,328 audios to 30,544 audios. The autoregressive model used for this dataset requires many more steps of stochastic gradient descent (SGD) to fit a dataset than the simpler max-pooling model, so we upsample the real-only dataset so that an equivalent number of SGD steps are taken each epoch for that dataset as for the dataset with synthetic speakers. The model is able to overfit the dataset without synthetic speakers; we therefore record the best test accuracy achieved over the course of training for each fold, instead of the final test accuracy. We also record the best loss, i.e. the average negative log likelihood of the correct semantic label sequence when teacher forcing is used in the decoder. <ref type="table" target="#tab_0">Table 1</ref> reports these results: both the best accuracy and best loss are significantly better when synthetic speakers are included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we have shown that it is possible to use synthetic speech to train an end-to-end SLU model. Including synthesized speech in the training set improves accuracy across a variety of settings, in some cases by a large amount. Our results strongly suggest that practitioners should try our method to augment their datasets.</p><p>There is still a gap between the performance of a model trained solely on synthetic speech and a model trained on a comparable amount of real speech. In the future, we hope to find ways to reduce this gap, which might include trying other forms of TTS, (automatically) removing badly synthesized utterances from the training set, and combining synthesized speech with other traditional methods for data augmentation, like additive noise and speed perturbation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Model with max-pooling decoder. The portion of the model shaded in blue is pre-trained using an ASR task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Model with autoregressive decoder used for the Snips SLU Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Test accuracy on Fluent Speech Commands as a function of the number of synthetic speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Test accuracy on Fluent Speech Commands as a function of the number of real speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Cross-validation results for Snips SLU Dataset. Real 65.5% ? 2.9% 2.81 ? 0.42 Real + synthetic 71.4% ? 1.4% 1.67 ? 0.16</figDesc><table><row><cell>Data type</cell><cell>Best accuracy</cell><cell>Best loss</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The PyTorch code for our experiments is available online at https://github.com/lorenlugosch/end-to-end-SLU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Santi Pascual and Kyle Kastner for helpful discussions with us about multi-speaker TTS, and thanks to Alice Coucke for help with using the Snips SLU Dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent advances in end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Caubriere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Esteve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="44" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From Audio to Semantics: Approaches to end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end spoken language understanding: Bootstrapping in low resource scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil Kumar</forename><surname>Sri Harsha Dumpala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopparapu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Curriculum-based transfer learning for an effective end-to-end spoken language understanding and domain portability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Caubri?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Est?ve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rutuja</forename><surname>Ubale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Vikram Ramanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lange</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Training neural speech recognition systems with synthetic speech augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Lavrukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00707</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Speech recognition with augmented synthesized speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11699</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards end-toend speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving performance of end-to-end ASR on numeric sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cal</forename><surname>Peyser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigating backtranslation in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Poncelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitar</forename><surname>Shterionov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Association for Machine Translation (EAMT)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Gideon Maillette de Buy Wenniger, and Peyman Passban</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noise and the reality gap: The use of simulation in evolutionary robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Jakobi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inman</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Artificial Life</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="704" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">VoiceLoop: Voice fitting and synthesis via a phonological loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. The Centre for Speech Technology Research (CSTR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikrant</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma?l</forename><surname>Primet</surname></persName>
		</author>
		<title level="m">NeurIPS Workshop on Energy Efficient Machine Learning and Cognitive Computing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Spoken language understanding on the edge</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LibriSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
