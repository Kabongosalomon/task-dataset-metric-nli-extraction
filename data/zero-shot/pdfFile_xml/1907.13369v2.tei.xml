<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<email>hedongliang01@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
							<email>tanxiao01@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
							<email>shifeng.chen@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
							<email>wenshilei@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt handcrafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-ofthe-art results on YouTube Birds and YouTube Cars.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, video recognition has attracted great research interest in the computer vision community, due to its importance in real-world applications such as video surveillance, <ref type="bibr">Figure 1</ref>. A demonstration of our approach. At each time step, each agent relies on context information among nearby agents to take action for adjusting its sampling location. When all agents stop, the prediction is emitted by the classification decision. video search, and video recommendation. A video contains a sequence of frames, both spatial information, and temporal relation are important for accurate recognition. Compared to recognizing well-trimmed clips, untrimmed videos pose a more critical challenge since not all the frames consistently respond to the specified ground-truth label. Picking out the most informative frames can be an effective method for recognition.</p><p>Existing research efforts <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref> mainly focus on building effective and efficient video modeling networks. Generally speaking, they can be divided into two directions, namely, (1) two-stage solutions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> which extract spatial feature vectors from video frames and then integrate the obtained local feature sequence into one compact video descriptor for recognition; <ref type="bibr" target="#b1">(2)</ref> the 2D <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref> or 3D convolution based <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b4">5]</ref> end-to-end video classification methods. Though great progress has been achieved by these methods, limited attention is paid to the aforementioned variation of frame-level salience among different frames. The mainstream methods all propose to sample input frames by hand-crafted strategies, for instance, evenly sampling N frames/clips from the original video is done in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>, sampling N successive frames from a video is adopted by <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref>, and directly feeding all video frames in the test phase is chosen in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35]</ref>. Sampling N frames/clips either evenly or successively from an untrimmed video cannot guarantee optimum. Meanwhile, feeding all the frames is a sort of bruteforce and introduces a much unnecessary computation burden. Therefore, in this paper, we concentrate on how to pick out the most discriminative frames from untrimmed videos to achieve better classification performance.</p><p>Intuitively, an effective algorithm for humans to pick out N representative frames from an untrimmed video can be as follows: we first observe N scenes of the whole video, according to the initial observation, then we infer where to check next time until we find out the satisfying frames round by round. Motivated by this, we proposed to formulate frame selection as N parallel Markov decision processes. As is known, a Markov decision process can be naturally modeled by the reinforcement learning framework <ref type="bibr" target="#b37">[38]</ref>. Inspired by the success of reinforcement learning in solving sequential decision-making problems, we propose a multi-agent reinforcement learning (MARL) framework to select multiple discriminative frames or video clips from an untrimmed video to improve the recognition performance. The workflow of our system is illustrated in <ref type="figure">Figure 1</ref>.</p><p>Specifically, there are N agents in our framework and each of them is responsible for selecting one informative frame/clip from an untrimmed video. They initially sample N frames/clips evenly from the entire video and iteratively decide that each of their samples should come from temporally preceding or later location until encountering a STOP action at T stop -th step or the maximum step number T max is reached. To both combine nearby context and track previous status information for better decision making, we design a shared RNN-based context-aware observation network to model the local environment and its nearby context information as well as historical states to generate a status vector for each agent. Conditioned on the status vector, the policy network estimates the probability distribution over the predefined action space, according to which each agent takes action to adjust its sampling location. A carefully designed reward function is proposed and the MARL framework is optimized following REINFORCE <ref type="bibr" target="#b48">[49]</ref> by maximizing the expected reward.</p><p>To verify the effectiveness of our proposed multi-agent reinforcement learning framework for frame sampling, extensive experiments are conducted on several popular untrimmed video classification datasets, including Activi-tyNet v1.2 and v1.3 <ref type="bibr" target="#b1">[2]</ref>, YouTube Birds and YouTube Cars <ref type="bibr" target="#b54">[55]</ref>. Results show that the proposed scheme achieves remarkable improvement over 2D/3D CNN baseline solutions which are equipped with different hand-crafted sampling strategies. In more detail, a new state-of-the-art on YouTube Birds and YouTube Cars is achieved and our single RGB model reaches a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion. To sum up, we make the following contributions.</p><p>? We focus on a previously overlooked point, i.e., the frame sampling strategy, in improving untrimmed video classification performance and intuitively formulate it as a Markov decision process.</p><p>? Multi-agent reinforcement learning is adopted to solve the formulated sequential decision problems. A novel framework that takes both context information and historical environment states into consideration for decision making is designed.</p><p>? The proposed method can be effectively applied to various existing untrimmed video recognition models to improve the performance, which is well witnessed by the excellent experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Action Recognition</head><p>Our paper is closely related to works on deep-learning based action recognition, including end-to-end convolutional classification networks and two-stage recognition solutions. Karpathy et al. firstly introduces CNN for video classification in <ref type="bibr" target="#b23">[24]</ref>. Then two-stream ConvNet <ref type="bibr" target="#b35">[36]</ref> is proposed to merge the predicted scores from RGB and optical flow modalities, and the performance is improved by a large margin. ST-ResNet <ref type="bibr" target="#b9">[10]</ref> further introduces residual connections between the two streams of <ref type="bibr" target="#b35">[36]</ref> and shows great advantages in results. Currently, TSN <ref type="bibr" target="#b44">[45]</ref> and C3D <ref type="bibr" target="#b40">[41]</ref> are two well-known baseline methods for video recognition. The former is a 2D CNN based approach while the latter is based on 3D CNN. There are numerous followup studies to improve the aforementioned two baselines. For example, TLE <ref type="bibr" target="#b6">[7]</ref>, ShuttleNet <ref type="bibr" target="#b34">[35]</ref>, AttentionClusters <ref type="bibr" target="#b28">[29]</ref> and NetVlad <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> are proposed for better local feature integration instead of directly AVG-Pooling as used in TSN. OFF <ref type="bibr" target="#b36">[37]</ref> and motion feature network <ref type="bibr" target="#b25">[26]</ref> are proposed for integrating motion information modeling into a spatial CNN network, instead of using two streams. I3D <ref type="bibr" target="#b2">[3]</ref> inflates deeper networks than C3D for spatial-temporal modeling. Given the heavy computation overhead of I3D, a series of works <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20]</ref> are done to strike good effectiveness-efficiency trade-off. Meanwhile, nonlocal network <ref type="bibr" target="#b46">[47]</ref>, compact generalized nonlocal network <ref type="bibr" target="#b53">[54]</ref> and Nonlocal + GCN <ref type="bibr" target="#b47">[48]</ref> bring great performance gain by leveraging extra network modules to capture feature-point-wise or ROI-wise spatial-temporal relations. The frame sampling strategies of all the aforementioned state-of-the-arts are hand-crafted. Instead, we propose to use the learning-based strategy to improve recognition performance for untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Reinforcement Learning</head><p>Our work is largely inspired by those on leveraging reinforcement learning to solve vision problems. <ref type="bibr" target="#b30">[31]</ref> is a classical work that uses RL for the spatial attention policy in image recognition, as well as in <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr" target="#b26">[27]</ref> proposes an aesthetic aware reinforcement learning framework to learn the policy for image cropping and achieves promising results. RL is also adopted for object detection in <ref type="bibr" target="#b31">[32]</ref>, for video object segmentation in <ref type="bibr" target="#b17">[18]</ref> and object tracking in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>As for semantic level video understanding, RL has also played an important role. For instance, it is utilized for action detection in <ref type="bibr" target="#b52">[53]</ref>, for action anticipation in <ref type="bibr" target="#b13">[14]</ref>, for natural language grounding in <ref type="bibr" target="#b18">[19]</ref> and for video description in <ref type="bibr" target="#b45">[46]</ref>. Our work is most closely related to the ones that use reinforcement learning for action recognition. In <ref type="bibr" target="#b3">[4]</ref>, the part activation policy of human body parts is learned with RL for action prediction. <ref type="bibr" target="#b39">[40]</ref> focuses on skeletonbased action recognition and RL is adopted to distinguish discriminative poses. Following <ref type="bibr" target="#b52">[53]</ref>, the Fast-Forward algorithm is proposed in <ref type="bibr" target="#b8">[9]</ref> to reduce the computation burden for untrimmed video classification. In this work, RL is utilized for both frames skipping planning and early stop decision making. In <ref type="bibr" target="#b50">[51]</ref>, the authors also focus on fast video prediction by adaptively selecting relevant frames on a pervideo bias using reinforcement learning. Closely related work mainly focuses on improving prediction efficiency and a single agent is used for decision making. In contrast, we emphasize improving untrimmed video classification baselines and our model adopts multiple agents to cooperatively select multiple frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this paper, we formulate frames selection as multiple sequential decision-making problems. Therefore, it naturally fits into the reinforcement learning framework. <ref type="figure">Figure  2</ref> illustrates the multi-agent reinforcement learning framework of our proposed model. The model can be seen as multiple agents that interact with a video sequence of F frames/clips over time. Each agent picks a specific frame out of F frames and employs a context-aware observation network to encode the explored environment into a vector, h a t which is then fed into the following policy network to generate a proper action u a t from the action space A. This action adjusts the frame that the agent to pick at the next time step. N agents are identified by a ? A ? {1, 2, ..., N }. All agents are initialized to be evenly distributed over the temporal space. When all agents decide to pick the current frame (to take the STOP action), a classification network emits the prediction based on the selected frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>Context-aware observation network The contextaware observation network is composed of a basic observation network f o parameterized by ? o , followed by a context network. The basic observation network is used to encode the video information at the frame/clip selected by the agent (namely, the local environment) into a feature vector o a t providing it as input to the context network. CNN based networks, both 2D CNN and 3D CNN are good at capturing features in frames or frame clips, and hence either of them can be adopted as the basic observation network. Unlike the single-agent system, for the multi-agent system, the action selected by each agent not only depends on its local environment state but is also impacted by its context information. Hence, we design a context-aware module f h upon the basic observation network, parameterized by ? h , to maintain a joint internal state of agents which summarizes history context information by a recurrent neural network. To make it work effectively, each agent only accesses context information from its 2M neighboring agents but not from all agents. More formally, at the time step t, the agent a observes a concatenated state</p><formula xml:id="formula_0">s a t = [o a?M t , ..., o a t , o a+M t ]</formula><p>and its previous hidden states h a t?1 as inputs of the context module, then produces its current hidden states h a t :</p><formula xml:id="formula_1">h a t = f h (h a t?1 , s a t ; ? h ).<label>(1)</label></formula><p>Especially, if M is set to 1, then the first agent loses the rear information while the last agent loses the front information. We use the beginning and ending frame/clip of the video to compensate for the information needed for the two cases respectively. For the multi-agent model, all of these agents share the context-aware observation network.</p><p>Policy network We adopt a fully connected layer followed by a softmax function as the policy network parameterized by ? u . In every time-step t, each agent a selects an action u a t ? U to execute, according to the probability distribution ?(u a t |h a t ; ? u ) generated by the policy network. The action space consists of three predefined actions, namely, moving ahead, moving back and staying. The moving stride is set to be ? frames/clips. When all agents choose staying, it means a STOP action is encountered. Note that we still make parameters of the policy network shared among all agents so that our model is readily applicable for testing at arbitrary N .</p><p>Classification network The classification network f p is parameterized by ? p . At each time step, these selected frames go through the classification network to produce its </p><formula xml:id="formula_2">? " # ? " $ ? " % ? ? ? [ " # , " $ , " % ] Figure 2.</formula><p>The proposed multi-agent reinforcement learning framework is composed of a context-aware observation network for capturing environment state h n t , a policy network for estimating probabilistic distribution over action space and a classification network for videolevel prediction. There are N (which is 3 for illustration) agents and the environment is the F sampled frames/clips in our system. The agent interacts with the environment by taking action u n t to adjust sampling locations iteratively. The context-aware observation network is designed to allow context information communication among nearby agents. GRU, which integrates preceding and current states, is used to model the sequential decision making property of our system. corresponding prediction logit l a t ? R C , where C is the number of classes. At the last time step, these prediction scores l Tstop = {l 1</p><p>Tstop , l 2 Tstop , ..., l N Tstop } before Softmax normalization are aggregated with average pooling to yield the final video-level prediction. In our proposed method, the classification network can be easily replaced with any video classification module, such as Two Stream <ref type="bibr" target="#b35">[36]</ref>, TSN <ref type="bibr" target="#b44">[45]</ref> and 3D CNNs <ref type="bibr" target="#b40">[41]</ref>. For the simplicity in design, the classification network shares the parameters of layers before the last classifier layer with the observation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Objectives</head><p>The overall objective of the MARL-framework is to simultaneously maximize the expected reward of the frame sampling network and minimizing the classification loss. We use standard back-propagation to train the classification network parameterized by ? p , and REINFORCE <ref type="bibr" target="#b48">[49]</ref> to optimize the parameters of the basic observation network, the context module and the policy network ? ? = {? o , ? h , ? u }. Hence, our loss function consists of the MARL loss L M ARL (? ? ) and the classification loss L Cls (? p ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">MARL Objective</head><p>Reward function The reward function reflects how good the actions taken by the agents. When all the agents take actions, each agent gets its own reward r a t based on the classification probability p a t = Sof tmax(l a t ) of the t-th time step. The reward is given to encourage the agent to find a more informative frame which can improve the probability of correct prediction step by step. Hence, we design a simple reward function that encourages the model to increase its confidence. Specifically, for the t-th (t &gt; 1) time step, agent a receives a reward follows:</p><formula xml:id="formula_3">r a t = p a t,gt ? p a t?1,gt ,<label>(2)</label></formula><p>where p a t,c represents the probability of predicting the video as class c at the t-th time step for agent a, and gt is the ground-truth label of the video. All agents share the same reward function. Considering the sequential decisionmaking scenario, it is more appropriate to consider a cumulative discounted rewards, where rewards obtained in the more distant future contribute less to the current step. Specifically, at the time step t, the discounted return for agent a is</p><formula xml:id="formula_4">R a t = Tstop?t k=0 ? k r a k+t ,<label>(3)</label></formula><p>where ? ? (0, 1] is a constant discount factor that controls the importance of future rewards.</p><p>Policy gradient Given U, a space of action sequences. Following REINFORCE <ref type="bibr" target="#b48">[49]</ref>, our objective can be ex-pressed as</p><formula xml:id="formula_5">J(? ? ) = N a=1 u?U ?(u|s; ? ? )R a .<label>(4)</label></formula><p>In our case we wish to learn network parameters ? ? that maximize the equation <ref type="bibr" target="#b3">(4)</ref>. The gradient of J(? ? ) is ? ?? J(? ? ) = N a=1 u?U ?(u|s; ? ? )? ?? log ?(a|s; ? ? )R a .</p><p>(5) This leads to a non-trivial optimization problem due to the high dimension of the action sequence space. REINFORCE addresses this by using Monte Carlo sampling to obtain K interaction sequences to approximate the policy gradients:</p><formula xml:id="formula_6">? ?? J(? ? ) ? 1 K K k=1 N a=1</formula><p>Tstop t=0 ? ?? log ?(u a t,k |s a t,k ; ? ? )R a t .</p><p>(6) Then we can use stochastic gradient descent to minimize the loss function:</p><formula xml:id="formula_7">L J (? ? ) = ? 1 K K k=1 N a=1</formula><p>Tstop t=0 log ?(u a t,k |s a t,k ; ? ? )R a t . <ref type="formula">(7)</ref> Maximum entropy To prevent policies from becoming deterministic too quickly, researchers use entropy regularization <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b29">30]</ref>. Its success has sometimes been attributed to the fact that it "encourages exploration" <ref type="bibr" target="#b29">[30]</ref>. The greater the entropy, the more ability of exploration an agent will have. Therefore, we follow the practice of using the entropy of policy to increase the ability to explore by:</p><formula xml:id="formula_8">L H (? ? ) = ? N a=1</formula><p>Tstop t=0 u?A ?(u a t |s a t ; ? ? ) log ?(u a t |s a t ; ? ? ).</p><p>(8) Hence, the overall loss for MARL Objective is a combination of the two losses:</p><formula xml:id="formula_9">L M ARL (? ? ) = L J (? ? ) + ? 1 L H (? ? ),<label>(9)</label></formula><p>where ? 1 is a constant scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Classification Objective</head><p>A cross-entropy loss is applied to minimize the KL divergence between the ground truth distribution y and prediction p:</p><formula xml:id="formula_10">L Cls (? p ) = ? C c=1 y c log p c .<label>(10)</label></formula><p>Finally, we minimize a hybrid loss that combines all the losses:</p><formula xml:id="formula_11">Loss = L Cls (? p ) + ? 2 L M ARL (? ? ),<label>(11)</label></formula><p>where ? 2 is a constant scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>ActivityNet is a large-scale video benchmark for human activity understanding <ref type="bibr" target="#b1">[2]</ref>. The first version of this dataset (termed as ActivityNet v1.2) has 100 classes of human activities. and its second version (termed as ActivityNet v1.3) contains 19,994 videos from 200 activity categories. Moreover, the ActivityNet dataset has temporal annotations of action instances for training data. It is also worth noting that the labels of the test set are not publicly available and thus the performances on ActivityNet dataset are all reported on the validation set. We decode videos at 1fps and use RGB frames in our experiments. Following the official evaluation script, the evaluation metric is based on mean average precision (mAP) for action recognition on ActivityNet.</p><p>YouTube Birds and YouTube Cars are two challenging video datasets for fine-grained video classification which consist of 200 different bird species and 196 different car models respectively <ref type="bibr" target="#b54">[55]</ref>. We experimented with the RGB frames of the two datasets. Videos in YouTube Birds and YouTube Cars were downsampled to 2fps and 4fps respectively. We employ top-1 precision as the evaluation metric for the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>One of the major differences in current video architectures is whether the convolutional operators use 2D (imagebased) or 3D (video-based) kernels. Hence, we choose two successful video classification methods for feature extraction in our method, namely temporal segment network <ref type="bibr" target="#b44">[45]</ref> and C3D <ref type="bibr" target="#b40">[41]</ref>. The temporal segment network is equipped with segmental modeling (5 segments) to capture longrange temporal information and C3D uses 3D ConvNets to extract the temporal and spatial information of a video clip.</p><p>Training When using 2D ConvNets as our backbone, we first pre-train classification network f p with the method introduced in <ref type="bibr" target="#b44">[45]</ref>. For 3D ConvNets, we use the C3D <ref type="bibr" target="#b40">[41]</ref> features provided by ActivityNet's <ref type="bibr" target="#b1">[2]</ref> website which are extracted every 8 frames with a temporal resolution of 16 frames. We use the pre-trained weights to initialize f p , then jointly train the network with extra components. Adam with the initial learning rate of 0.0001 is adopted. The parameters used in the experiments are set as follows. We set F to 120, 100 and 100 for ActivityNet, YouTube Birds and YouTube Cars respectively. For the videos shorter than F frames, we cyclically repeat their frames to derive a video of F frames. In the policy network, we use a gated recurrent unit (GRU) cell with the hidden size of 1024 to model the sequential decision process and T max is set to 10. ? is empirically set to 0.9 and ? 1 , ? 2 are set to 1. We set N to 5 during training.</p><p>Testing Many state-of-the-art methods rely on some so- phisticated testing strategy or post-processing techniques, such as 10-crop testing, to boost performance. However, our framework does not demand these testing strategies. We simply sample 25 frames (N is set to 25) uniformly per video as the initial temporal location, and single-center cropping is applied to the selected frames from MARL to make the final prediction directly. During evaluation, we use maximum a posterior estimation to choose the action for each agent according to ?(u a t |s a t ; ? ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Improvements over 2D/3D CNN Baselines</head><p>MARL improves various backbones Here we examine our framework with several recent network architectures on the ActivityNet dataset using the RGB modality. Specifically, we equip MARL to five deep architectures: BN-Inception <ref type="bibr" target="#b22">[23]</ref>, Inception-V3 <ref type="bibr" target="#b38">[39]</ref>, ResNet-101 <ref type="bibr" target="#b20">[21]</ref>, ResNet-152 <ref type="bibr" target="#b20">[21]</ref>, and C3D <ref type="bibr" target="#b40">[41]</ref>. For each architecture, we evaluate three hand-crafted frame sampling strategies as baselines: randomly sampling 25 frames (R25), uniformly sampling (U25) and using all frames (All). For random sampling, we evaluate three times and report the average results. Since the average duration of videos in Ac-tivityNet is 117s and we decode videos at one fps, we set F to be 120, which indicates that using all frames means using 120 frames. For 2D ConvNets, we follow the TSN <ref type="bibr" target="#b44">[45]</ref> framework and train the network on annotated action instances, all these 2D ConvNets are initialized with Ima-geNet pre-trained weights <ref type="bibr" target="#b5">[6]</ref>. For C3D, we also carry out experiments with TSN-style training strategy by sampling 5 clips and predicting labels based on consensus. An average pooling on logits predicted from each sampled frame/clip is followed to produce video-level predictions for different sampling strategies in these experiments.</p><p>The results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. We observe that deeper models with higher accuracy (on ImageNet dataset <ref type="bibr" target="#b5">[6]</ref>) result in better performance in the video classification task, and our method consistently obtains robust improvements over various 2D/3D architectures. For each architecture, our method achieves the best performance and improved over them by a large margin on both datasets comparing with randomly sampling, uniformly sampling and using all frames.</p><p>We can see that even with a very powerful ResNet-152 backbone, MARL can largely boost recognition performance, specifically, the gain over U25 and All is 1.99% and 1.63% on ActivityNet v1.2, and as is 1.73% and 1.28% on ActivityNet v1.3. This verifies the effectiveness of our method, regardless of shallower or deeper baseline models.</p><p>Instance level v.s. video level supervision Annotations of action instances in untrimmed videos are expensive and hardly available under most circumstances. Experiments are carried out to show that our method consistently improves over the baseline method. Besides using these annotated activity instances when training the network, we train our model when only the video-level labels are used. In this experiment, the powerful ResNet-152 is utilized as our backbone network. We present the experimental results of our proposed MARL with annotated instances information (Instance) and video-level information (Video) in <ref type="table">Table 2</ref>. From the results of ActivityNet v1.3 val set, we observe that our method achieves the best performance in comparison with the three hand-crafted baseline strategies, whether instance-level supervision is available or not. It is also worthy of noting that, our method with video-level information achieves even better performance than the model trained with instance-level supervision and tested with all frames. It further confirms that MARL is effective in picking discriminant frames in untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-arts</head><p>Comparison with other frame selection methods We make comparisons under the scenario of observing the same number of total frames and results demonstrate our method outperforms other methods as shown in <ref type="figure">Figure 3</ref>. The re-  <ref type="figure">Figure 3</ref>.</p><p>Mean average precision vs. number of observed frames. Comparisons of our method with AdaFrame <ref type="bibr" target="#b50">[51]</ref>, FrameGlimpse <ref type="bibr" target="#b52">[53]</ref>, Fast-Forward <ref type="bibr" target="#b8">[9]</ref>, and uniformly sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone sults of these methods are directly cited from Adaframe <ref type="bibr" target="#b50">[51]</ref> whose authors reproduce FrameGlimpse <ref type="bibr" target="#b52">[53]</ref> and Fast-Forward <ref type="bibr" target="#b8">[9]</ref>. To be fair, we use the same preprocessing method, backbone network (i.e. ResNet-101) and training strategy of backbone as Adaframe <ref type="bibr" target="#b50">[51]</ref>. Note that the performances of our solution in <ref type="figure">Figure 3</ref> are lower than those reported in <ref type="table" target="#tab_0">Table 1</ref>, this is because the backbone is not trained as what TSN <ref type="bibr" target="#b44">[45]</ref> does for a fair comparison. In this experiment, the number of observed frames is calculated by averaging the total number of frames observed for each video at all steps of MARL.</p><p>Comparison with other SOTAs We also compare our method with other state-of-the-art methods on these challenging datasets. v1.3, where the results of state-of-the-art-methods all come from published paper or tech reports. We compare with several well known video recognition methods, which once achieved the state-of-the-art performance, including improved trajectories (iDT) <ref type="bibr" target="#b42">[43]</ref>, 3D convolutional networks (C3D) <ref type="bibr" target="#b40">[41]</ref>, temporal segment networks (TSN) <ref type="bibr" target="#b44">[45]</ref>, Pseudo-3d residual network (P3D) <ref type="bibr" target="#b32">[33]</ref>, and Redundancy Reduction Attention (RRA) <ref type="bibr" target="#b54">[55]</ref>. We see that our method with ResNet-152 outperforms all these previous methods on the ActivityNet v1.3. Moreover, as shown in <ref type="table">Table 3</ref>, it is beneficial to pre-train a model using Kinetics dataset <ref type="bibr" target="#b24">[25]</ref> and then transfer it to the video classification task on ActivityNet dataset. "SEResNeXt152" pre-trained on Kinetics and finetuned on ActivityNet1.3 is the current SOTA model which won the champion of Action Proposal task in ActivityNet2018 <ref type="bibr" target="#b14">[15]</ref>. We conducted experiments with RGB modality based on "SEResNeXt152", it achieves SOTA mAP of 87.95% (U25), 88.15% (All) and 90.05% (MARL). For readers' reference, here we list the results of the 2016 champion submission as well as ours with SE-ResNeXt-152 <ref type="bibr" target="#b21">[22]</ref> pre-trained on Kinetics dataset. To be noted, our results on ActivityNet v1.3 are obtained by only using 1fps RGB frames during the training phase and a single-center cropping strategy in validation. Even with this simple train and test setup, our single RGB model still achieves comparable results with the 2016 champion submission which fuses multiple powerful models and integrates RGB, optical flow and audio modalities. <ref type="table">Table 4</ref> shows results on YouTube Birds and YouTube Cars. To make a fair comparison, the Inception-V3 <ref type="bibr" target="#b38">[39]</ref> with the same architecture as RRA <ref type="bibr" target="#b54">[55]</ref> is used as the backbone. Our method surpasses RRA on all these two datasets since categories in fine-grained tasks often share a similar appearance in general and hence require to focus more on the informative frames to distinguish from each other. Our best performance is 5.805% above that of other methods on the YouTube Birds and 2.145% on the YouTube Cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Number of agents Our model is trained with N of 5 and we evaluate how the testing performance of ActivityNet 1.3 val set varies when N changes. Results can be found in Playing drums Painting furniture <ref type="figure">Figure 4</ref>. Visualization of the selected frame with different strategies on ActivityNet. The first row show frames from uniformly sampling, the second row depicts frames from our method without context-aware observation while the last row contains frames from our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N=5 N=15 N=25 N=120</head><p>All ResNet-152 80.19 82.99 83.81 83.72 82.53 <ref type="table">Table 5</ref>. Impact of N on ActivityNet v1.3 val set in terms of mAP. <ref type="table">Table 5</ref>. It can be observed when N increases, the performance gradually improves at first. There is a very interesting phenomenon. When N reaches the total number of frames (which is 120), the performance slightly drops to 83.72% but is still better than feeding all frames directly (82.53%). It is because that some agents might select the same frame to avoid less informative frames, but not all the irrelevant frames are skipped by the agents. In this case, though performance drops, it is still better than 82.53%. Such results evident the variation of frame-level saliency in the untrimmed video and support our motivation.</p><p>Context range We explore the impact of parameter M in context-aware observation network by ablation study. We carry out experiments with MARL frameworks with various settings of M . The experimental results are listed in <ref type="table" target="#tab_4">Table 6</ref>. Basically, our method still works for untrimmed videos when M = 0. It outperforms uniformly sampling by 0.91% and is even better than using all frames. The use of the context-aware observation network improves the noncontext model with clear margins. However, no obvious gain is obtained by increasing M . Larger M means larger network size and cost, so we empirically set M to 1.</p><p>Policy network transferring We show that the learned policy network can still be effective when it is transferred directly. (1) With Resnet-152, performances of directly applying sampling networks trained on ActivityNet1.2/Youtube-Cars(Birds) to ActivityNet1.3 are shown in <ref type="table" target="#tab_5">Table 7</ref>, see Cars'S, Birds'S and ANet1.2'S. <ref type="bibr" target="#b1">(2)</ref> Our policy network trained for ResNet-101 classifier still works for ResNet-152 classifier on ActivityNet1.3, which is indicated as "cross-cls" in <ref type="table" target="#tab_5">Table 7</ref>.</p><p>Qualitative results We also visualize some examples of  selected frames with different strategies on the validation data of ActivityNet in <ref type="figure">Figure 4</ref>. From top to bottom, frames picked by uniformly sampling, M = 0 and M = 1 are depicted. We see that our method is able to automatically select important frames according to surroundings and to avoid irrelevant frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented a multi-agent reinforcement learning method for untrimmed video recognition, which can be effectively applied to existing video recognition frameworks to select the informative frames/clips from the untrimmed video. Experiments demonstrate that our method outperforms state-of-the-art baseline methods by a substantial margin, which verifies the effectiveness of our method. In the future, we plan to distillate our MARL based recurrent frame sampling network into a smaller feedforward CNN to achieve more efficient untrimmed video classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Each agent executes the action to adjust the sampling location according to the policy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of different ConvNet architectures on the ActivityNet dataset. For different architectures, randomly sampling 25 frames (R25), uniformly sampling 25 frames (U25), using all frames (All) and using our method to sample 25 frames (Ours) are evaluated. All architectures are based on ImageNet<ref type="bibr" target="#b5">[6]</ref> pre-trained model, except C3D.</figDesc><table><row><cell>Architecture</cell><cell>R25</cell><cell cols="2">ActivityNet v1.2 U25 All</cell><cell>Ours</cell><cell>R25</cell><cell cols="2">ActivityNet v1.3 U25 All</cell><cell>Ours</cell></row><row><cell>C3D</cell><cell>62.06</cell><cell>62.89</cell><cell>63.00</cell><cell>64.13</cell><cell>59.73</cell><cell>60.68</cell><cell>60.83</cell><cell>62.00</cell></row><row><cell>BN-Inception</cell><cell>78.76</cell><cell>80.02</cell><cell>80.50</cell><cell>81.99</cell><cell>75.08</cell><cell>76.48</cell><cell>77.33</cell><cell>78.32</cell></row><row><cell>ResNet-101</cell><cell>80.73</cell><cell>81.94</cell><cell>82.26</cell><cell>83.76</cell><cell>78.69</cell><cell>79.96</cell><cell>80.64</cell><cell>81.54</cell></row><row><cell>Inception-V3</cell><cell>81.90</cell><cell>82.66</cell><cell>83.25</cell><cell>85.01</cell><cell>79.27</cell><cell>80.33</cell><cell>80.86</cell><cell>82.34</cell></row><row><cell>ResNet-152</cell><cell>82.72</cell><cell>83.71</cell><cell>84.07</cell><cell>85.70</cell><cell>80.69</cell><cell>82.08</cell><cell>82.53</cell><cell>83.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 Table 4 .</head><label>34</label><figDesc>Comparing with methods on YouTube Birds and YouTube Cars. * indicates the results of the method come from the latest project page of these datasets.</figDesc><table><row><cell>shows results on ActivityNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>82.53 82.99 83.81 83.80 83.72 Evaluation of context range on ActivityNet v1.3 val set using ResNet-152. U25 and All are hand-crafted strategies.</figDesc><table><row><cell></cell><cell>U25</cell><cell>All</cell><cell>M=0</cell><cell>M=1</cell><cell>M=2</cell><cell>M=4</cell></row><row><cell cols="2">mAP 82.08 U25 All</cell><cell>MARL</cell><cell>Birds'S</cell><cell>Cars'S</cell><cell>ANet1.2'S</cell><cell>cross-cls</cell></row><row><cell>82.08</cell><cell>82.53</cell><cell>83.81</cell><cell>82.70</cell><cell>82.66</cell><cell>83.41</cell><cell>83.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>The performance of different settings on ActivityNet v1.3 val set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The work was funded by National Natural Science Foundation of China (U1713203) and Shenzhen Science and Technology Innovation Commission (Project KQJSCX20180330170238897).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Part-activated deep reinforcement learning for action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Watching a small portion could be as good as watching all: Towards efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal residual networks for dynamic scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Red: Reinforced encoderdecoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04818</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khrisna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03766</idno>
		<title level="m">The activitynet large-scale activity recognition challenge 2018 summary</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual-agent deep reinforcement learning for deformable face tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement cutting-agent learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Read, watch, and move: Reinforcement learning for temporally grounding natural language descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stnet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A2-RL: Aesthetics aware reinforcement learning for image cropping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Localizing by describing: Attribute-guided attention localization for finegrained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning of region proposal networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collaborative deep reinforcement learning for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies for action recognition with a biologically-inspired deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: a fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Appearance-andrelation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video captioning via hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Function optimization using connectionist reinforcement learning algorithms. Connection Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fine-grained video categorization with redundancy reduction attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
