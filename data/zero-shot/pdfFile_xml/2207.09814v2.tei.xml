<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
							<email>jianfw@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
							<email>xiaowei.hu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
							<email>fangyj@ss.pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<email>nanduan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present NUWA-Infinity, a generative model for infinite visual synthesis, which is defined as the task of generating arbitrarily-sized high-resolution images or long-duration videos. An autoregressive over autoregressive generation mechanism is proposed to deal with this variable-size generation task, where a global patch-level autoregressive model considers the dependencies between patches, and a local token-level autoregressive model considers dependencies between visual tokens within each patch. A Nearby Context Pool (NCP) is introduced to cache-related patches already generated as the context for the current patch being generated, which can significantly save computation costs without sacrificing patch-level dependency modeling. An Arbitrary Direction Controller (ADC) is used to decide suitable generation orders for different visual synthesis tasks and learn order-aware positional embeddings. Compared to DALL?E, Imagen and Parti, NUWA-Infinity can generate high-resolution images with arbitrary sizes and support long-duration video generation additionally. Compared to NUWA, which also covers images and videos, NUWA-Infinity has superior visual synthesis capabilities in terms of resolution and variable-size generation. The GitHub link is https://github.com/microsoft/NUWA. The homepage link is https://nuwa-infinity.microsoft.com.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outpainting of Garden at Sainte-Adresse by Claude Monet</head><p>Input: a field with a house and a cloudy sky Input: a large lake surrounded by green vegetation Input: a cliff with a large canyon Input: a desert landscape with trees and mountains in the background  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ongoing convergence of vision-language representation and modeling techniques brings new opportunities to visual synthesis research. By learning visual knowledge and patterns from a largescale visual and multimodal corpus, recent visual synthesis models can generate images or videos based on given text, visual, or multimodal inputs and support various visual content creation tasks, such as text-to-image or video generation, image inpainting or outpainting, video prediction, etc. We also witness a notable trend in this area, where more and more work begin to explore how to generate images with higher resolutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>, or generate videos with longer durations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. This is because high-resolution images and long-duration videos can provide better visual effects to practical applications, such as design, advertisement, presentation, entertainment, etc.</p><p>However, generating arbitrarily-sized high-quality images (in resolution) or videos (in both resolution and duration) is not a trivial task. First, compared to text generation in the NLP field, the research of generating variable-size visual content is still in its early stage, and therefore is not well studied. Some existing works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> try to solve this problem with a divide-and-conquer strategy, which divides images or videos into patches, trains the model to generate patches separately without considering their dependencies, and composes the generated patches to form the final image or video. As such methods do not model the dependencies between the generated patches explicitly, they struggle to guarantee the consistency of generated contents, especially when generating highresolution images or long-duration videos. Second, different from text generation that usually follows a fixed order (e.g., left-to-right), images have two dimensions (i.e., width and height), and videos have three (i.e., width, height, and duration). These suggest that visual synthesis models should consider and model different generation orders and directions for different types of tasks.</p><p>In this paper, we use Infinite Visual Synthesis to denote the task of generating arbitrarily-sized high-quality images or videos, and propose NUWA-Infinity as a general visual synthesis model that can solve the two challenges of this task mentioned before. First, NUWA-Infinity is based on an autoregressive over autoregressive generation mechanism, where a global patch-level autoregressive model considers the dependencies between patches, and a local token-level autoregressive model considers the dependencies between visual tokens within each patch. Compared to diffusion-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> that are only able to generate images with a fixed size, the autoregressive formulation naturally considers different levels of dependencies and perfectly deals with the variablesize generation task. We also introduce a Nearby Context Pool (NCP) to cache-related patches already generated as the context for the current patch being generated, which can significantly save the computation cost without sacrificing the patch-level dependency modeling. Second, we propose an Arbitrary Direction Controller (ADC) to decide suitable generation orders and learn order-aware position embeddings, which is extremely useful for image outpainting.</p><p>We evaluate NUWA-Infinity on five high-resolution visual synthesis tasks, including Unconditional Image Generation HD , Text-to-Image HD , Text-to-Video HD , Image Animation HD and Image Outpainting HD . Compared to DALL?E <ref type="bibr" target="#b18">[19]</ref>, Imagen <ref type="bibr" target="#b19">[20]</ref> and Parti <ref type="bibr" target="#b29">[30]</ref>, which generate images with a fixed resolution (i.e., 1024 ? 1024), NUWA-Infinity can generate high-resolution images with arbitrary sizes and support long-duration video generation additionally. Compared to NUWA <ref type="bibr" target="#b27">[28]</ref>, which also supports image and video synthesis at the same time, the generation quality of NUWA-Infinity has been improved significantly. We also show the huge application potential of NUWA-Infinity on creative visual synthesis tasks, such as image outpainting and cartoon creation from natural language descriptions. We hope this technique can help visual content creators to save time, cut costs and improve their productivity and creativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Autoregressive Methods DALL?E <ref type="bibr" target="#b18">[19]</ref> tokenizes each image into discrete visual tokens and trains an autoregressive model to generate visual tokens from the corresponding text. The output image is reconstructed by the VQVAE decoder, which takes the visual tokens generated by the autoregressive model as inputs. Parti <ref type="bibr" target="#b29">[30]</ref> follows the same architecture of DALL?E, but uses ViT-VQGAN <ref type="bibr" target="#b28">[29]</ref> to discretize and reconstruct images, which is an improved version of VQGAN from both architecture and codebook learning aspects. NUWA <ref type="bibr" target="#b27">[28]</ref> is the first autoregressive visual synthesis pre-trained model to support both image and video generation tasks. Compared to these previous works, NUWA-Infinity introduces the autoregressive over autoregressive mechanism into the generation procedure, which enables the capability of generating variable-size images and videos. <ref type="table" target="#tab_2">1  6  11  16  21   2  7  12  17  22   3  8  13  18  23   4  9  14  19  24   5  10  15</ref>   Diffusion Methods DALL?E 2 <ref type="bibr" target="#b17">[18]</ref> generates image embedding from an input text based on either an autoregressive or a diffusion model, and uses a diffusion model to produce the output image. Imagen <ref type="bibr" target="#b19">[20]</ref> uses a frozen large-scale pre-trained language model T5-XXL <ref type="bibr" target="#b16">[17]</ref> to encode each input text, and uses two diffusion models to generate high-resolution images based on the text embeddings. Both of these two diffusion-based text-to-image generation methods cannot support arbitrarily-sized image generation, as the size of the output images is pre-defined before training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mountain reflects on the lake.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearby Context Pool (NCP) &lt;bos&gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Attention</head><p>Infinite Visual Synthesis To support infinite visual synthesis, most existing works follow the divide-and-conquer strategy to first divide a large image into several patches, and then train them in an independent way. GAN-based models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> attempt to divide large images into patches and optimize each of them from global or coordinated latent space independently. Since different patches have no explicit dependency, these models struggle to merge different patches during inference, and can easily lead to inconsistent results. To address this issue, autoregressive models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> incorporate a sliding window to enforce dependencies between different patches during inference. Recently, Mask-Predict <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref> also uses the sliding window approach, but incorporates a progressively maskand-predict strategy to model dependencies between patches during the window sliding in inference. However, both autoregressive models and Mask-Predict models bring a huge gap between training and inference, since different patches are still trained independently but inferred in a dependent way. By introducing the autoregressive over autoregressive mechanism with Nearby Context Pool and Arbitrary Direction Controller, NUWA-Infinity can enable variable-size image and video generation, and save the computation cost without losing global dependency and consistency modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Given an input y, which can be a text or an image, the infinite visual synthesis task aims to generate an image or a video x ? R W ?H?C?F with a user-specified resolution and duration, i.e., P(x|y), where W , H and C denote the width, height and channel of each image or video frame, respectively, F denotes the frame number. x denotes an image if F = 1 and denotes a video if F &gt; 1.</p><p>In general, NUWA-Infinity follows an autoregressive over autoregressive model to solve this task:</p><formula xml:id="formula_0">P (x|y) = N n=1 P (p n |p &lt;n , y) = N n=1 M m=1 P p (m) n |p &lt;n , p (&lt;m) n , y<label>(1)</label></formula><p>N n=1 P (p n |p &lt;n , y) denotes the global autoregressive generation procedure, where p n is the n th patch being generated, p &lt;n is the previous n ? 1 patches already generated, N is the total number of patches. where p (m) n is the m th visual token being generated in p n , p (&lt;m) n is the previous m ? 1 visual tokens already generated, M is the total number of visual tokens in each patch. Each p n is reconstructed by a pre-trained VQGAN decoder <ref type="bibr" target="#b5">[6]</ref>, which takes as input the visual token sequence {p</p><formula xml:id="formula_1">(1) n , ..., p (M ) n }.</formula><p>The final image or video is formed by composing all generated patches {p 1 , ..., p N } based on the specified resolution (i.e., W ? H) and duration (i.e., F ).</p><p>NUWA-Infinity uses an encoder-decoder architecture to model the above generation procedure. In this paper, we mainly focus on five types of high-definition (HD) visual synthesis tasks, including Unconditional Image Generation HD , Image Outpainting HD , Image Animation HD , Text-to-Image HD and Text-to-Video HD . In the 1 st task, the output image is generated by the vision decoder without any input. In the 2 nd and 3 rd tasks, the input image is fed into the vision decoder directly as the prefix to generate the output image or video. In the 4 th and 5 th tasks, the input text is encoded by the text encoder, and the output image or video is generated by the vision decoder.</p><p>One problem is that, different from text generation that follows the left-to-right order, images have two dimensions (i.e., width and height), and videos have three (i.e., width, height, and duration). These suggest the model should consider and handle different patch generation orders for different visual synthesis tasks. Motivated by this, we propose an Arbitrary Direction Controller (ADC) (Section 3.1) that can plan proper patch generation orders and learn order-aware positional embeddings.</p><p>Another problem is that, the length (i.e., N ? M ) of the visual tokens to be generated could be extremely long, which is challenging for most existing sequence generation models. To alleviate this issue, we propose a Nearby Context Pool (NCP) (Section 3.2) to cache-related patches already generated as the context for the current patch being generated, which can significantly save the computation cost without sacrificing the patch-level dependency modeling.</p><p>We train NUWA-Infinity (Section 3.3) using high-quality image-text pairs crawled from the web, and image-video pairs extracted from high-quality videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Arbitrary Direction Controller (ADC)</head><p>In this subsection, we introduce Arbitrary Direction Controller (ADC), which provides two functions: Split, which splits images/videos and decides the patch generation order for training and inference procedures; Emb, which assigns order-aware positional embeddings based on the current context.</p><p>? Split. This function takes the shape of an existing or to-be-generated image or video x as input and returns an ordered patch sequence:  shows how Split(?) works in the training stage. We define four basic generation orders and represent them using four Greek letters, respectively, according to their writing orders: ?-order (??), ? * -order (??), ?-order (??), ? * -order(??), where * denotes the reversed writing order. When choosing the ?-order in training, Split(?) will return patches in order of the red numerical sequence (top-left example in <ref type="figure" target="#fig_4">Fig. 6</ref>). Similarly, the other three options will let NUWA-Infinity learn how to generate patches based on the corresponding orders. Note that there are more orders such as (??) or a snake-like order, but the above four basic orders and their compositions are enough to generate images in arbitrary resolutions or shapes. The right part of <ref type="figure" target="#fig_4">Fig. 6</ref> shows how Split(?) works in the inference stage for Image Outpainting HD . Given a small image of a volcanic vent as input and its relative position in the targeted image with a specified larger resolution, the goal is to synthesize this targeted image by generating all the surrounding patches of the input. In order to leverage as many contextual patches as possible when generating new patches, Split(?) selects a patch generation order illustrated as a numerical sequence from 1 to 52.</p><formula xml:id="formula_2">p 1:N = ADC.Split(x)<label>(2)</label></formula><p>? Emb. This function assigns positional embeddings to the patch p n being generated and the patches in c n that are already generated and selected as the context of p n :</p><formula xml:id="formula_3">e n = ADC.Emb([p n ; c n ])<label>(3)</label></formula><p>It is crucial to design a proper positional embedding for the order-aware patch generation procedure, since the absolute positional embedding <ref type="bibr" target="#b24">[25]</ref> is unable to consider and model all relative positions in image and video generation tasks. Motivated by the recent work on relative positional embedding <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, we propose a dynamic positional embedding in ADC, where dynamic means the positional embeddings could change according to different situations. <ref type="figure" target="#fig_5">Fig. 7</ref> shows 18 dynamic relative embeddings from "a" to "r". The center patch being generated (in green color) is always labeled as "n" and the relative positions of previously generated patches based on "n" are labeled by other symbols (in orange color). As a result, an embedding matrix of the size of 18 ? d is formed. The right part shows how the embeddings are dynamically assigned to different patches when generating a specific patch. For example, when generating the last (i.e., 27 th ) patch, the positional embedding "n" is assigned to the current patch and the positional embeddings of "a","d","b","e","j","m","k" are assigned to patch 14, 15, 17, 18, 23, 24 and 26 in c 27 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Nearby Context Pool (NCP)</head><p>An image or a video could be extremely large, thus the previous patches p &lt;n in Eq. 1 could have a large size. A natural idea is to only consider nearby patches as contexts. However, simply ignoring distant patches will lose long-term memory and thus harm the global consistency of the generated image or video. To address this issue, we propose a Nearby Context Pool (NCP) with three functions illustrated in <ref type="figure" target="#fig_7">Fig. 8</ref>: Select, which dynamically selects nearby patches as the context to promote infinite generation; Add, which saves multi-layer hidden states of previously generated patches to help long-term memory; Remove, which removes expired caches for self-cleaning. ? Add. This function adds the cache a n of the patch p n already generated into NCP:</p><p>NCP.Add(a n )</p><p>In NCP, the cache a n of the patch p n is defined as all the resulting multi-layer hidden states from the generation of p n . Since NUWA-Infinity will not retain all generation history, this cache mechanism ensures the transmission of the necessary information in the whole generation procedure. ? Select. This function selects the context c n for the patch p n to be generated:</p><formula xml:id="formula_5">c n = NCP.Select(p n )<label>(5)</label></formula><p>In NCP, the context c n of p n is defined as the caches of nearby patches already generated within a pre-defined 3D extent (e w , e h , e f ), denoting the width extent, height extent, and frame extent. For example, when NUWA-Infinity generates the 14 th patch (as p n ) in <ref type="figure" target="#fig_7">Fig. 8</ref> and the maximum extent is set as (1, 1, 1), the context will include the patches from 1 to 13. ? Remove. This function removes the caches of those patches in NCP that no longer have any effect on the generation of future patches:</p><formula xml:id="formula_6">NCP.Remove()<label>(6)</label></formula><p>In NCP, a cache is only cleaned when it cannot serve as the context for any patch to be generated. The Remove(?) function will be invoked after each Add(?) function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference Strategy</head><p>This section will introduce the training and inference strategies of NUWA-Infinity in Algorithm 1 and Algorithm 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Training Strategy</head><p>Given each input-output pair y, x ? R W ?H?C?F in the pre-training corpus, we first split the visual data x into patches and then randomly select one patch generation order p 1:N = [p 1 , ..., p N ] from the four orders {?, ? * , ?, ? * } described in Section 3. n ]. y is encoded by a text encoder as y , which denotes a sequence of token embeddings.</p><p>We train NUWA-Infinity based on each ordered patch sequence r. For the n th patch p n , we first select its context c n ? R N c ?L?M ?d based on the NCP described in Section 3.2:</p><formula xml:id="formula_7">c n = NCP.Select(p n )<label>(7)</label></formula><p>where N c denotes the number of patches in q, L denotes the layer number of the vision decoder, M denotes the visual token number in each context patch, and d denotes the dimension of each visual token embedding. Note that N c can be changed during training, as different patches may have different numbers of context patches in q. The positional embeddings e n ? R (1+N c )?d of p n and its context c n are dynamically assigned by the ADC operation described in Section 3.1: for all n from 1 to N do c n ? NCP.Select(p n ); e n ? ADC.Emb([p n ; c n ]); a n ,p n ? NUWA(p n , c n , e n , y); NCP.Add(a n ); NCP.Remove(); L n = CrossEntropy(p n ,p n ); optimize L n ; end Then, an L-layer vision decoder takes as input p n = [p <ref type="bibr" target="#b0">(1)</ref> n , ..., p (M ) n ] ? R M ?d and c n . In the 1 st layer, p n and the 1 st layer hidden states c <ref type="bibr" target="#b0">(1)</ref> n ? R N c ?M ?d of all patches in c n are fed into a self-attention module, enhanced by the positional embeddings e n :</p><formula xml:id="formula_8">e n = ADC.Emb([p n ; c n ])<label>(8)</label></formula><formula xml:id="formula_9">Q s = p n W q K s = [p n ; c (1) n ]W k + e n V s = [p n ; c (1) n ]W ? Q s = SelfAtt(Q s , K s , V s ) (9) Q s ? R M ?d , K s ? R (1+N c )?M ?d , V s ? R (1+N c )</formula><p>?M ?d are queries, keys and values, respectively, W q , W k , W v ? R d?d are parameters to be learned,Q s denotes the attended results.</p><p>For the Text-to-Image HD task,Q s and y are further fed into a cross-attention module as shown in Eq. <ref type="bibr" target="#b9">(10)</ref>.</p><formula xml:id="formula_10">Q c =Q s W q , K c = y W k , V c = y W v Q c = CrossAtt(Q c , K c , V c )<label>(10)</label></formula><p>whereQ c ? R M ?d , K c ? R T ?d , V c ? R T ?d are queries, keys and values, respectively, W q , W k , W v ? R d?d are parameters to be learned, T is the number of token embeddings in y ,Q c denotes the attended results.</p><p>By feedingQ s (for tasks without text input) orQ c (for tasks with text input) into a feed forward network, the output of the 1 st layerp <ref type="bibr" target="#b0">(1)</ref> n ? R M ?d is obtained:</p><formula xml:id="formula_11">p (1) n = FFN(Q c )<label>(11)</label></formula><p>By iteratively stacking Eq. (9)?(11) into L layers, we obtainp</p><formula xml:id="formula_12">(1) n ,p<label>(2)</label></formula><p>n , ...,p (L) n ? R M ?d . p n and the previous L ? 1 layer outputs are concatenated to obtain a L-layer cache of the n th patch p n : a n = [p n ;p <ref type="bibr" target="#b0">(1)</ref> n ;p (2) n ; ..;p (L?1)</p><formula xml:id="formula_13">n ]<label>(12)</label></formula><p>where a n ? R L?M ?d . For simplicity, the procedure from Eq. (9) to Eq. <ref type="formula" target="#formula_0">(12)</ref> is defined as NUWA:</p><p>p n , a n = NUWA(p n , c n , e n , y)</p><p>wherep n =p (L) n denotes the output embeddings. Then, NCP will collect the cache of p n to help the prediction of the next patches and conduct a self-cleaning to remove useless patches, as shown in Eq. <ref type="bibr" target="#b13">(14)</ref>.</p><p>NCP.Add(a n ) NCP.Remove()</p><p>Finally, the cross-entropy loss is used to optimize model parameters based onp n and the ground-truth p n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Inference Strategy</head><p>Input: a text y or an image h Output: generated image/video x Initial Arbitrary Direction Controller ADC; Initial Nearby Context Pool NCP ? ? ; q 1:K ? ADC.Split(h); for all k from 1 to K do c k ? NCP.Select(q k ); e k ? ADC.Emb([q k ; c k ]); a k ,q k ? NUWA(q k , c k , e k , y); NCP.Add(a k ); NCP.Remove(); end p 1:N ? ADC.Split(x); for all n from 1 to N do c n ? NCP.Select(p n ); e n ? ADC.Emb([p n ; c n ]); a n ,p n ? NUWA(?, c n , e n , y); NCP.Add(a n );</p><formula xml:id="formula_16">NCP.Remove(); x ? [x ;p n ]; end if target x is an image then return VQGANDecoder(x ) else return PixelGuidedVQGANDecoder(x ) end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Condition</head><p>Pre-caching Pixel-Guided VQGAN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Inference Strategy</head><p>NUWA-Infinity can support various visual synthesis scenarios, and we focus on five tasks in this paper: Unconditional Image Generation HD , Image Outpainting HD , Image Animation HD , Text-to-Image HD and Text-to-Video HD . There are two specific designs for the last four tasks: Image Condition Pre-caching and Pixel-Guided VQGAN. Image Condition Pre-caching For Image Outpainting HD and Image Animation HD , the input is an image condition h and the output is a spatial extended image or a temporal extended video. A VQGAN encoder is used to encode h into a list of patches with corresponding visual tokens, where K denotes the number of conditional patches. Then, these patches and visual tokens are fed into the vision decoder (Eq. 8?14) as a prefix to initialize NCP, which will be used next to generate the extended image or following video frames.</p><p>Frame Frame <ref type="figure">Figure 9</ref>: Pixel-Guided VQGAN.</p><p>Pixel-Guided VQGAN For Image Animation HD and Text-to-Video HD , the output are videos. Since traditional VQVAE is trained only on images, simply decoding a video frame-by-frame will lead to inconsistency between frames. To solve this issue, we propose a Pixel-Guided VQGAN (PG-VQGAN), as shown in <ref type="figure">Fig. 9</ref>. Different from traditional VQGAN which is trained on images independently, we sample 2 consecutive frames n ? 1 and n as a training pair and use the pixel-level information of the n ? 1 frame to enhance the decoder of frame n. In detail, the frame n ? 1 is encoded with the same number of layers as the traditional VQGAN decoder, and the output of each encoder layer is fused with the corresponding output layer of the VQGAN decoder. We simply use an element-sum operation as the fusion strategy and observe promising results (see <ref type="figure">Fig. 9</ref>). When decoding the first frame during inference, Image Animation HD has a ground-truth n ? 1 frame. However, For Text-to-Video HD , since there are no ground-truth frames, we instead use traditional VQGAN to decode the first frame and Pixel-Guided VQGAN to decode the following frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearby Context Pool (NCP)</head><p>Mountain reflects on the lake.   <ref type="figure" target="#fig_6">Figure 10</ref>: Inference pipeline of NUWA-Infinity for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arbitrary Direction Controller (ADC)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Different from most visual synthesis works, NUWA-Infinity focuses on generating images and videos with high resolutions and long durations. As a result, most existing datasets cannot be used in training or evaluation. To evaluate the ability of NUWA-Infinity on the five tasks mentioned before, we build four datasets with high resolutions ( 1024 2 ) in the following:</p><p>? RQF. We build a new dataset, Riverside of Qingming Festival (RQF), based on the version of Along the River During the Qingming Festival drawn by Qiu Ying 3 . This version is 30.5 centimeters high and 987 centimeters wide in reality, and we download a digital print with 4200 ? 135912 pixels. We resize the whole image to 2048 ? 66270 resolution and split it into several overlapped 2048 ? 2048 patches instead of non-overlapping ones. We end with a dataset of 128 images. We train NUWA-Infinity with the &lt; ?, image &gt; pairs on the dataset with 2048 ? 2048 resolution but qualitatively evaluate Unconditional Image Generation HD at higher resolution 2048 ? 38912.</p><p>? LHQC. We build a new dataset, Landscape High Quality with Captions (LHQC), based on the publicly available dataset LHQ <ref type="bibr" target="#b21">[22]</ref>. The original LHQ dataset consists of 90K high-resolution ( 1024 2 ) nature landscapes. To support the text prompt, we use the image captioning model from <ref type="bibr" target="#b26">[27]</ref> to generate the captions for the dataset first and manually fix some errors in the generated results. We finally obtain a dataset with 90K text-image pairs. We spilt the dataset into two splits: train (85K) and test (5K). We train NUWA-Infinity with the &lt; text, image &gt; pairs on the train split and evaluate Text-to-Image HD and Image Outpainting HD on the test split.</p><p>? LHQ-V. We build a new dataset, Landscape High Quality for Videos (LHQ-V), based on the videos scrapped from the www.pexels.com website. We first build a query set with 100 landscape related keywords (e.g., "sky", "forest", "cloud"). Then, we query the website using these keywords and obtain 85K high-resolution videos. To further make the dataset cleaner, we use Mask R-CNN <ref type="bibr" target="#b7">[8]</ref> to detect objects from these videos and remove videos containing objects that are not related to landscape (i.e., "table", "human", "computer"). Finally, we obtain a dataset with 40K videos. We split the dataset into two splits: train (38K) and test (2K). We train NUWA-Infinity with the &lt; ?, video &gt; pairs on the train split and evaluate Image Animation HD on the test split.</p><p>? PeppaPig. We build a new dataset, PeppaPig, based on the famous cartoon PeppaPig. We collect Season 1-4 videos of PeppaPig and split the videos into multiple clips based on the timeline of subtitles. We then ask 20 trained annotators to annotate the captions for these videos. If the clip is not smooth, the annotators are asked to provide a "N/A" caption. We also ask a meta annotator to check these captions. Finally, we remove videos with the "N/A" caption and obtain 10K text-video pairs. We split the dataset into two splits: train (9K) and test (1K). We train NUWA-Infinity with the &lt; text, video &gt; pairs on the train split and evaluate Text-to-Video HD on the test split.  <ref type="table" target="#tab_2">Table 1</ref>: Implementation details of model training for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The training of NUWA-Infinity can be split into two stages. In the first stage, all images are cropped into 1024 ? 1024 and videos are cut into 1024 ? 1024 ? 5 with 5 fps. Then, they are encoded into discrete visual tokens using the VQGAN model with a compression ratio of 16. In the second stage, we train the model using Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with a learning rate of 1 ? 10 ?4 , a batch size of 256, and warm-up 5% of total 50 epochs. We train four models on four datasets and inference on five tasks. More settings for different tasks can be found in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Metrics</head><p>? FID/Block-FID. Fr?chet Inception Distance (FID) <ref type="bibr" target="#b8">[9]</ref> is used to calculate the quality of the generated image. In this work, we also propose Block-FID, which splits a large image into several blocks and calculates the average Fr?chet Inception Distance of all blocks.</p><p>? IS. Inception Score (IS) <ref type="bibr" target="#b20">[21]</ref> is a common metric to calculate the diversity of the generated results. A higher IS indicates the model generates more diversified results.</p><p>? FVD. Fr?chet Video Distance (FVD) <ref type="bibr" target="#b23">[24]</ref> is widely used to calculate the quality of generated videos. It measures the distance between the ground-truth video and the generated video. A lower FVD denotes a higher similarity.</p><p>? CLIP-SIM. Recently, the CLIP Similarity Score (CLIP-SIM) <ref type="bibr" target="#b15">[16]</ref> is used to measure the semantic consistency between images and text. We use the CLIP model to calculate the similarity score between the generated image and given text to judge its semantic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Unconditional Image Generation HD</head><p>Unconditional Image Generation HD aims to generate images without conditions. <ref type="figure" target="#fig_0">Fig. 11 and 12</ref> show a single long image generated by the model trained on the RQF dataset for Unconditional Image Generation HD . The generated image has an extremely high resolution of 38912 ? 2048. To better fit the page, we split the complete image into 6 splits, each having a resolution of 6485 ? 2048. The generated results demonstrate the following abilities of NUWA-Infinity:</p><p>? Infinity ability. NUWA-Infinity can generate visual content with large size. Although trained on 2048 ? 2048 patches as illustrated in Tab. 1, NUWA-Infinity can generate images 19 times longer than each training instance. This is attributed to our proposed NCP module, which makes the computation grow linearly with the output size, as only a small number of previously generated patches in the context are used during inference. ? Creation ability. By comparing the generated results with the original painting, we find that NUWA-Infinity has a strong creative ability. For example, for the gate wall of the second split in <ref type="figure" target="#fig_6">Fig. 11</ref>, it is a composition of multiple walls in different directions. Also, for the first split in <ref type="figure" target="#fig_0">Fig. 12</ref>, NUWA-Infinity generates a lot of pedestrians and houses. Many different people walk together, which leads to a crowded situation in this split. Note that these scenes do not appear in the original painting, and they are fully created by NUWA-Infinity.  ? Local details and global consistency. The generated results also show decent local details and global consistency. In NUWA-Infinity, the local autoregression generates visual tokens one by one, thus the human faces, human gestures, leaves of trees, roof tiles, and many other details are clearly painted. The global autoregression generates patches one-by-one. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, the human figures gradually dwindle, and then the picture transitions to the mountains. The smooth transitions make the picture look natural globally even though it is extremely long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Text-to-Image HD</head><p>Text-to-Image HD aims to generate an image based on the input text. For a fair comparison, all models are trained from scratch on the LHQC dataset. As shown in Tab. 2, when generating images with 1024 ? 1024 resolution, NUWA-Infinity outperforms AR-based models Taming Transformer <ref type="bibr" target="#b5">[6]</ref> and Mask-Predict model MaskGIT <ref type="bibr" target="#b0">[1]</ref>, in visual quality (Block-FID), semantic consistency (CLIP-SIM), and diversity (IS). When generating images of size 4096 ? 1024 which is 4 times as long as the training images, the performance of MaskGIT <ref type="bibr" target="#b0">[1]</ref> decreases rapidly but NUWA-Infinity still maintains excellent visual quality with Block-FID of 15.65. As also shown in <ref type="figure" target="#fig_1">Fig. 13</ref>, NUWA-Infinity generates significantly better results, and the reflection of the hill can be clearly seen. Note that we did not compare with DALL?E 2 <ref type="bibr" target="#b17">[18]</ref>, Imagen <ref type="bibr" target="#b19">[20]</ref>, or Parti <ref type="bibr" target="#b29">[30]</ref> directly because of three reasons: (i) all of them do not support arbitrary-large visual synthesis (i.e, generating images with 4096 ? 1024 resolution); (ii) they did not make their pre-trained models public; and (iii) NUWA-Infinity focuses on enabling infinite visual synthesis and is not pre-trained on large-scale datasets, thus it is hard to make a direct comparison.   <ref type="table">Table 3</ref>: Comparisons on LHQ-V dataset for Image Animation HD task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Image Animation HD</head><p>Image Animation HD aims to generate a video based on an input image. We build two baseline models as a comparison shown in Tab. 3. All three models are globally autoregressive, but they use different local generative methods (shown in round brackets). NUWA-Infinity (AR) is our default autoregressive over autoregressive model with a local autoregressive mechanism to generate a patch. NUWA-Infinity (NAR) can be viewed as an autoregressive over non-autoregressive model, as the tokens inside a patch are generated locally in parallel instead of autoregressively. NUWA-Infinity (P-NAR) can be viewed as an autoregressive over progressive non-autoregressive model, as locally, the tokens inside a patch are generated by a Mask-Predict method <ref type="bibr" target="#b0">[1]</ref> introduced in Sec. 2. NUWA-Infinity achieves significantly better performance than baselines, with an FVD score of 62.57. <ref type="figure" target="#fig_2">Fig. 14</ref> provides a qualitative comparison between the generated 60-frame videos from the same input image of 1024 ? 1024 resolution. We find that NUWA-Infinity with a default local AR mechanism can generate more realistic images. However, we do find a speed-performance trade-off as AR generation takes more time compared with NAR and P-NAR. We will discuss it in Sec. 4.5.3.  <ref type="figure" target="#fig_2">Figure 14</ref>: Samples on LHQ-V dataset for Image Animation HD task. We compare local NAR, P-NAR, and AR by generating 60 frames with a high resolution of 1024 ? 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Image Outpainting HD</head><p>Image Outpainting HD aims to generate an out-painted image based on the input image. We do not train NUWA-Infinity for the Image Outpainting HD task, but used the model trained for Text-to-Image HD directly. To evaluate the model's ability of outpainting in all directions, we set up four settings: Right Extend ?, Left Extend ?, Down Extend ? and Up Extend ?. For example, in the Right Extend ? setting, we input the image with the left half in the LHQC dataset and ask the model to predict the right half. Since for the output image in this task, half is ground-truth and half is extended, we calculate Block-FID between the extended area and the same area in the ground-truth test set of LHQC. As shown in Tab. 4, NUWA-Infinity significantly outperforms Taming <ref type="bibr" target="#b5">[6]</ref> and MaskGIT <ref type="bibr" target="#b0">[1]</ref> by a large margin in all four directions. Since the model trained by Text-to-Image HD also supports a text prompt, we also try to outpaint the image by adding text controlling, and we find better performance on Down Extend ? and Up Extend ?, while similar performance on Right Extend ? and Left Extend ? compared with one without text. We hypothesize that it is because the upper or lower half of the image contains less information, while the left or right half has more visual semantic hints. Note that for a fair comparison, Taming and MaskGIT use text prompt by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUWA-Infinity(Ours)</head><p>MaskGIT Taming Input a fenced area with a cloudy sky above it a large rock formation in the desert with a cloudy sky a road that is going down a hill a tree is reflected in the water of a river <ref type="figure" target="#fig_3">Figure 15</ref>: Samples for Image Outpainting HD task on LHQC dataset. The input image has a resolution of half of 1024 ? 1024.</p><p>In <ref type="figure" target="#fig_3">Fig. 15</ref>, we provide four input images illustrating four directions. Taming Transformer only successfully generates the input image in the third column, as it predicts the lower half of the hill based on the upper half. This is because Taming Transformer only trains token-by-token in ? ? order, and this order only fits the down extension. MaskGIT successfully predicts another half in all directions benefited by its bidirectional masked language model. Compared with MaskGIT, NUWA-Infinity can generate more realistic results. For example, in the fourth column, when inputting the reflection of a tree in the lake, NUWA-Infinity successfully generates the most consistent tree on the shore.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>We conduct detailed ablations on three components of our model, ADC, NCP, and Vision Decoder.  <ref type="table">Table 5</ref>: Impact of feed position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Ablation Study on ADC</head><p>Feed position of ADC.Emb As introduced in Sec. 3.1, ADC dynamically provides relative positional embeddings during training and inference stages. In Eq. 9, the relative positional embedding e n is added to the key of self-attention. We call it pre-feeding, as the positional information is fed before the computation of attention. This is different from traditional post-feeding in Transformers, where the positional information is fed after the computation of attention. Tab. 5 shows that pre-feeding brings better performance than post-feeding. This is because pre-feeding controls which contexts to attend to with positional information while post-feeding only adjusts the attention distributions after the attention.  <ref type="table">Table 6</ref>: Ablation results in caches in NCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Ablation Study on NCP</head><p>Caches in NCP As introduced in Sec. 3.2, the Add operation saves multi-layer hidden states of previously generated patches as "caches". This allows information transmission between patches during training and inference. In other words, even though distant patches are removed from NCP, their information can be still captured in the hidden states of the nearby patches in NCP. To verify the effectiveness of this design, we train another model without using the caches in NCP and show its result of Text-to-Image HD in Tab. 6. The NCP design with information transmission significantly outperforms the one without information transmission.  Extents in NCP As introduced in Sec. 3.2, NCP saves caches in a 3D extent (e w , e h , e f ). <ref type="figure" target="#fig_5">Fig. 17a</ref> shows the comparisons between different extent size of (e w , e h , e f ) for Text-to-Image HD . Although larger extent size brings better Block-FID scores, the performance becomes limited when e h , e w &gt; 2 while computation improves significantly. For videos, <ref type="figure" target="#fig_5">Fig. 17b</ref> shows the impact of temporal extent on Image Animation HD . We find the FVD improvements become limited if the e f increases to 3. As a result, we choose (e w , e h , e f ) = (2, 2, 3) as our default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Ablation Study on Vision Decoder</head><p>Tab. 7a shows ablations on different vision decoders. NUWA-Infinity is an autoregressive over autoregressive model and it follows the autoregressive (AR) formulation in the vision decoder. We also try another two vision decoders based on a non-autoregressive (NAR) formulation and a progressive autoregressive (P-NAR) formulation. For these two models, the global autoregression is still maintained, while the local autoregression is changed into NAR and P-NAR, respectively. We find AR-based vision decoder achieves the best performance and NAR-based vision decoder achieves the fastest speed.</p><p>Tab. 7b shows two settings of NUWA-Infinity: Base and Large. We find that the base model can also achieve acceptable performance compared with the large model. This is due to the limited training samples in the dataset. In this paper, we focus on the effectiveness of NUWA-Infinity architecture, instead of large-scale pre-training. We will pre-train NUWA-Infinity with more data in the future.</p><p>Tab. 7c shows when to optimize the loss between the predicted patch of vision decoder and the ground-truth patches. During training, as long as a patch is predicted, NUWA-Infinity calculates the patch loss and optimizes it immediately instead of accumulating the gradients of all patches. We simply call this mechanism "patch loss" and the accumulated one "accumulated loss". We find that patch loss accelerates convergence from 70 epochs to 50 epochs and improves the Block-FID score compared with the accumulated loss. This is because patch loss will share optimized parameters between each patch, which helps the model learn large images or videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head><p>Training Data The model for infinite visual synthesis requires high-resolution images and videos as the training data. Such high-quality visual data are harder to collect duo to quality and license issues. In the future, we will collect large-scale datasets satisfying the quality and license criteria for the development of this research direction.</p><p>Evaluation Metric Compared to text generation tasks, such as machine translation and text summarization, visual synthesis is more difficult to evaluate as the number of ground-truths of a model's output could be unlimited. Currently, we follow the traditions (i.e., using FID, FVD, IS, and CLIP-SIM scores) to measure the quality of generated images and videos. In the future, we will explore better evaluation metrics for visual synthesis tasks.</p><p>Inference Speed Autoregressive models can deal with dependencies between generated contents well. However, the training and inference efficiency of this generation mechanism is still a blocking issue for deploying such models for practical usage. In the future, we will explore ways to combine the advantages of autoregressive models and non-autoregressive models (such as the diffusion model) to achieve both generation quality and inference (or training) efficiency.</p><p>Pre-trained Version In this paper, we train NUWA-Infinity for different downstream tasks directly, due to the lack of large-scale high-quality visual data. In the future, we will pre-train the next version of NUWA-Infinity with more collected visual data and report its generalization capabilities on open-domain inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>NUWA-Infinity is a visual synthesis framework that can be trained to generate high-quality images and videos from the given text or image input. Different from DALL?E, DALL?E 2, Imagen and Parti, an autoregressive over autoregressive mechanism is proposed to support variable-size visual content generation tasks, such as image outpainting, image animation, text-to-image generation, and text-to-video generation. We hope such models help visual content creators save time, cut costs, and increase productivity and creativity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Image Outpainting HD examples. Four examples in the upper part (2048 ? 1024) show the outpainting of four different directions. The example in the lower part (3328 ? 2048) shows outpainting in all directions of the famous painting of Monet, Garden at Sainte-Adresse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Text-to-Image HD examples in the resolution of 4096 ? 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Image Animation HD examples in the resolution of 2560 ? 1536 with 20 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>An overview of the proposed NUWA-Infinity model during the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>nFigure 6 :</head><label>6</label><figDesc>|p &lt;n , p (&lt;m) n , y denotes the local autoregressive generation procedure, Illustration of patch order control in NUWA-Infinity. The left part shows four basic patch generation orders (?, ? * , ?, ? * ) during training. The right part shows how NUWA-Infinity performs the image outpainting task by composing these four orders. Arabic numerals indicate the order of global autoregression, arrows indicate the order of local autoregression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of dynamic position control in NUWA-Infinity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>p 1 :</head><label>1</label><figDesc>N = [p 1 , ..., p N ] denotes the ordered patch sequence. For simplicity, we useFig. 6to explain how this function splits an image into ordered patches in the training and inference stages. It is straightforward to extend from images to videos by considering the temporal dimension. The left part ofFig. 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Illustration of NCP in ?-order with a context extent of (1,1,1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Part I of a huge image (38912 ? 2048) synthesized in Unconditional Image Generation HD task on RQF dataset. Splits are connectable by row, each has a resolution of 6485 ? 2048.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Part II of the huge image (38912 ? 2048) illustrated inFig. 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 17 :</head><label>17</label><figDesc>Ablation results on extents in NCP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. A pre-trained VQGAN encoder [6] transforms all images in x into visual tokens [p ], and each patch p n ? R M ?d is represented by its corresponding visual tokens [p</figDesc><table><row><cell>(1) 1 , ..., p</cell><cell>(M ) 1</cell><cell>, ..., p</cell><cell cols="2">(1) N , ..., p</cell><cell>(M )</cell></row><row><cell></cell><cell></cell><cell cols="2">(1) n , ..., p</cell><cell>(M )</cell></row></table><note>N</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Training Strategy Input: images or videos x, optional text y Output: optimized NUWA-Infinity model Initial Arbitrary Direction Controller ADC; Initial Nearby Context Pool NCP ? ? ; p 1:N ? ADC.Split(x);</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on LHQC dataset for Text-to-Image HD task.</figDesc><table><row><cell>a field of yellow flowers with a mountain in the background</cell><cell>a lake with a mountain in the background</cell></row><row><cell>NUWA-Infinity(Ours) Taming</cell><cell></cell></row></table><note>MaskGIT Figure 13: Samples on LHQC dataset for Text-to-Image HD . Left: 1024 ? 1024, Right: 1024 ? 4096. For a fair comparison, we did not provide input sketch for Taming Transformer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on LHQC dataset for Image Outpainting HD task.</figDesc><table><row><cell></cell><cell>40 45</cell><cell>1? 38.80</cell><cell></cell><cell cols="2">Block-FID FLOPs</cell><cell>FLOPs (x4) Block-FID (?4)</cell><cell></cell><cell>160 180 ?</cell><cell>1?</cell><cell>783.15</cell><cell>FVD</cell><cell>FLOPs</cell></row><row><cell></cell><cell>35</cell><cell>36.61 1?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>140</cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>120</cell><cell></cell><cell></cell></row><row><cell>Score</cell><cell>20 25</cell><cell></cell><cell>4.1? 23.62</cell><cell cols="2">17.78</cell><cell>16.97</cell><cell>Score</cell><cell>80 100</cell><cell></cell><cell>73.66</cell><cell>71.62</cell><cell>69.91</cell><cell>69.86</cell></row><row><cell></cell><cell>15</cell><cell></cell><cell></cell><cell>6.7?</cell><cell></cell><cell>10.3?</cell><cell></cell><cell>60</cell><cell></cell><cell>1.63?</cell><cell>2.16?</cell><cell>2.61?</cell><cell>3.05?</cell></row><row><cell></cell><cell>5 10</cell><cell></cell><cell>10.21 2.1?</cell><cell cols="2">10.05 3.1?</cell><cell>10.13 4.2?</cell><cell></cell><cell>20 40</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell cols="2">Spatial extension size</cell><cell>and</cell><cell cols="2">in nearby context</cell><cell></cell><cell></cell><cell cols="3">Temporal extension size</cell><cell>in nearby context</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) Spatial extension.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Temporal extension.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Patch size of ADC.Split As introduced in Sec. 3.3, each patch representation has a size of p n ? R M ?d . The horizontal axis ofFig. 16shows different patch sizes M , and the vertical axis shows FID scores for Text-to-Image HD . The blue line shows the FID score of generating image resolution of 1024 ? 1024. The red line shows the Block-FID score of generating image resolution of 1024 ? 4096. A smaller patch size harms the FID score of the generated image, but a larger patch size requires larger GPU memory. When patch size M = 256, a good balance between performance and memory can be achieved.</figDesc><table><row><cell></cell><cell>30 25</cell><cell>24.21</cell><cell>23.46</cell><cell></cell><cell>Block-FID Memory</cell><cell>Block-FID (?4)</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell>17.78</cell><cell>17.69</cell></row><row><cell>Score</cell><cell>15</cell><cell>11.83</cell><cell>10.91</cell><cell></cell><cell>10.05</cell><cell>10.12</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.3 x</cell></row><row><cell></cell><cell>5</cell><cell>1 x</cell><cell>1.4 x</cell><cell></cell><cell>3.2 x</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>Patch size</cell><cell>256</cell><cell>1024</cell></row><row><cell></cell><cell></cell><cell cols="4">Figure 16: Impact of patch size.</cell></row><row><cell></cell><cell></cell><cell cols="5">RPE Block-FID? Block-FID(?4) ?</cell></row><row><cell></cell><cell></cell><cell>Pre</cell><cell cols="2">10.05</cell><cell cols="2">17.78</cell></row><row><cell></cell><cell></cell><cell>Post</cell><cell cols="2">10.47</cell><cell cols="2">18.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Ablation experiments with text-to-image generation on LHQC. We use a base model with 16 layers and dim of 768 except (b).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.comuseum.com/painting/masters/qiu-ying/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We'd like to thank Minheng Ni, Xiaodong Wang, and Bei Li for the figure and table formats of this paper. We'd also like to thank Yu Liu, Jieyu Xiao, Scarlett Li, and Jane Ma for the discussion of potential application scenarios. We'd also like to thank Yang Ou and Bella Guo for the design of the homepage, and Tiantian Xue and Daisy Hou for the implementation of the homepage. We'd also like to thank Ting Song, Yan Xia, and Shiyou Ren for the help with the dataset construction. We'd also like to thank Yan Fan and Quanlu Zhang for their system support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Masked generative image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maskgit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11315" to="11325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8785" to="8805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adversarial video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cogview: Mastering text-to-image generation via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vector quantized diffusion model for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="10696" to="10706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improve Transformer Models with Better Relative Position Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3327" to="3335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">InfinityGAN: Towards Infinite-Pixel Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Infinite Nature: Perpetual View Generation of Natural Scenes From a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14458" to="14467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3202" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-Shot Text-to-Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
		<title level="m">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
		<editor>Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, and Rapha Gontijo Lopes</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aligning latent and image spaces to connect the unconnectable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorii</forename><surname>Sotnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14144" to="14153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">LocoGAN -Locally convolutional GAN. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Struski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Knop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemys?aw</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiktor</forename><surname>Daniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Tabor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-08" />
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page">103462</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkanath</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14100</idno>
		<title level="m">GIT: A Generative Image-to-text Transformer for Vision and Language</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UWA: Visual Synthesis Pre-training for Neural visUal World creAtion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Duan</forename><surname>N\</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vector-quantized Image Modeling with Improved VQGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><forename type="middle">Yu</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10789</idno>
		<title level="m">Yinfei Yang, and Burcu Karagol Ayan. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">M6-ufc: Unifying multi-modal controls for conditional image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14211</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
