<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
							<email>ccxiang@hdu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<email>zhaozhou@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qi.tian@utsa.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff3">
								<orgName type="department">SIT, FEIT</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual grounding aims to localize an object in an image referred to by a textual query phrase. Various visual grounding approaches have been proposed, and the problem can be modularized into a general framework: proposal generation, multimodal feature representation, and proposal ranking. Of these three modules, most existing approaches focus on the latter two, with the importance of proposal generation generally neglected. In this paper, we rethink the problem of what properties make a good proposal generator. We introduce the diversity and discrimination simultaneously when generating proposals, and in doing so propose Diversified and Discriminative Proposal Networks model (DDPN). Based on the proposals generated by DDPN, we propose a high performance baseline model for visual grounding and evaluate it on four benchmark datasets. Experimental results demonstrate that our model delivers significant improvements on all the tested data-sets (e.g., 18.8% improvement on ReferItGame and 8.2% improvement on Flickr30k Entities over the existing stateof-the-arts respectively).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in deep neural networks have helped to solve many challenges in computer vision and natural language processing. These advances have also stimulated high-level research into the connections that exist between vision and language such as visual captioning <ref type="bibr" target="#b0">[Donahue et al., 2015;</ref>, visual question answering <ref type="bibr" target="#b0">[Fukui et al., 2016;</ref><ref type="bibr" target="#b3">Yu et al., 2017b;</ref><ref type="bibr" target="#b3">Yu et al., 2018]</ref> and visual grounding . * Jun Yu is the corresponding author Visual grounding (a.k.a., referring expressions) aims to localize an object in an image referred to by a textual query phrase. It is a challenge task that requires a finegrained understanding of the semantics of the image and the query phrase and the ability to predict the location of the object in the image. The visual grounding task is a natural extension of object detection <ref type="bibr">[Ren et al., 2015]</ref>. While object detection aims to localize all possible pre-defined objects, visual grounding introduces a textual query phrase to localize only one best matching object from an open vocabulary.</p><p>Most existing visual grounding approaches <ref type="bibr" target="#b0">[Rohrbach et al., 2016;</ref><ref type="bibr" target="#b0">Fukui et al., 2016;</ref><ref type="bibr" target="#b1">Hu et al., 2016]</ref> can be modularized into the general framework shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Given an input image, a fixed number of object proposals are first generated using a proposal generation model and visual features for each proposal are then extracted. Meanwhile, the input query is encoded by a language model (e.g., <ref type="bibr">LSTM [Hochreiter and Schmidhuber, 1997]</ref>) that outputs a textual feature vector. The textual and visual features are then integrated by a multi-modal fusion model and fed into a proposal ranking model to output the location of the proposal with the highest ranking score.</p><p>Of the three modules shown in <ref type="figure" target="#fig_0">Figure 1</ref>, most existing visual grounding approaches focus on designing robust multimodal feature fusion models. SCRC <ref type="bibr" target="#b1">[Hu et al., 2016]</ref> and phrase-based CCA <ref type="bibr" target="#b2">[Plummer et al., 2015]</ref> learn the multi-modal common space via canonical correlation analysis (CCA) and a recurrent neural network (RNN), respectively. MCB learns a compact bilinear pooling model to fuse the multi-modal features, to obtain a more discriminative fused feature <ref type="bibr" target="#b0">[Fukui et al., 2016]</ref>. Further, some approaches have explored designing different loss functions to help improve the accuracy of the final object localization. Rohrbach et al. use an auxiliary reconstruction loss function to regularize the model training . Wu et al. propose a model that gradually refine the predicted bounding box via reinforcement learning <ref type="bibr" target="#b3">[Wu et al., 2017]</ref>.</p><p>Compared with the aforementioned two modules, proposal generation has been less thoroughly investigated. Most visual grounding approaches usually use class-agnostic models (e.g., selective search <ref type="bibr" target="#b2">[Uijlings et al., 2013]</ref> or datadriven region proposal networks (RPNs) <ref type="bibr">[Ren et al., 2015]</ref> trained on specific datasets) to generate object proposals and extract visual features for each proposal <ref type="bibr" target="#b0">[Fukui et al., 2016;</ref><ref type="bibr" target="#b1">Hu et al., 2016;</ref>. Although many visual grounding approaches have been proposed, what makes a good proposal generator for visual grounding remains uncertain.</p><p>Choosing the optimal number of generated proposals for visual grounding is difficult. If the number is small, recall of the true objects is limited, which seriously influencing visual grounding performance. If the number is large, recall is satisfactory but it does not necessarily improve visual grounding performance, as it increases the difficulty for accurate prediction of the proposal ranking model. Here, we rethink this problem and suggest that the ideal generated proposal set should contain as many different objects as possible but be of a relatively small size. To achieve this goal, we introduce diversity and discrimination simultaneously when generating proposals, and in doing so propose Diversified and Discriminative Proposal Networks (DDPN).</p><p>Based on the proposals generated by DDPN, we propose a high performance baseline model to verify the effectiveness of the DDPN for visual grounding. Our model uses simple feature concatenation as the multi-modal fusion model, and trained with two novel loss functions: Kullback-Leibler Divergence (KLD) loss with soft labels to penalize the proposals while capturing contextual information of the generated proposals, and smoothed L 1 regression loss to refine the proposal bounding boxes.</p><p>The main contributions of this study are as follows: 1) we analyze the limitations of existing proposal generators for visual grounding and propose DDPN to generate highquality proposals; 2) based on DDPN, we propose a high performance baseline model trained with two novel losses; and 3) by way of extensive ablation studies, we evaluate our models on four benchmark datasets. Our experimental results demonstrate that our model delivers a significant improvement on all the tested datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Diversified and Discriminative</head><p>Proposal Networks (DDPN)</p><p>For visual grounding, we propose that the ideal generated proposals should be diversified and discriminative simulta- neously: 1) the proposals of all images should be diversified to detect objects from open-vocabulary classes (see <ref type="figure" target="#fig_1">Figure  2</ref>(a)). As the objects to be localized during visual grounding may vary significantly, proposals only covering objects from a small set of classes will be a bottleneck for the model that follows and limit final visual grounding performance; 2) the proposals of an individual image should be discriminative to guarantee that the proposals and visual features accurately represent the true semantic (see <ref type="figure" target="#fig_1">Figure 2</ref>(b)).</p><p>In practice, learning a proposal generator that fully meets these two properties is challenging. The diversified property requires a class-agnostic object detector, while the discriminative property requires a class-aware detector. Since classaware object detectors are usually trained on datasets with a small set of object classes (e.g., 20 classes for PASCAL VOC <ref type="bibr" target="#b0">[Everingham et al., 2010]</ref> or 80 classes for COCO <ref type="bibr" target="#b2">[Lin et al., 2014]</ref>), directly using the output bounding boxes of the detection model as the proposals for visual grounding may compromise diversity. As a trade-off, most existing visual grounding approaches only consider diversity and ignore the discrimination during proposal generation. Specifically, they use the class-agnostic object detector (e.g., Selective Search <ref type="bibr" target="#b2">[Uijlings et al., 2013]</ref>) to generate proposals and then extract the visual features for the proposals using a pre-trained model <ref type="bibr" target="#b1">Hu et al., 2016;</ref><ref type="bibr" target="#b2">Lin et al., 2014;</ref><ref type="bibr" target="#b0">Fukui et al., 2016]</ref> or a fine-tuned model . However, when there is only a limited number of proposals, they may have the following limitations: 1) they may not be accurate enough to localize the corresponding objects; 2) they may fail to localize small objects; and 3) they may contain some noisy information (i.e., meaningless proposals).</p><p>To overcome the drawbacks inherent in the existing approaches, we relax the diversity constraint to a relatively large set of object classes (e.g., more than 1k classes) and propose a simple solution by training a class-aware object detector that can detect a large number of object classes as an approximation. We choose the commonly used class-aware model Faster R-CNN <ref type="bibr">[Ren et al., 2015]</ref> as the object detector, and train the model on the Visual Genome dataset which contains a large number of object classes and extra attribute labels <ref type="bibr" target="#b1">[Krishna et al., 2017]</ref>. We call our scheme Diversified and Discriminative Proposal Networks (DDPN).</p><p>Following <ref type="bibr" target="#b0">[Anderson et al., 2017]</ref>, we clean and filter of the training data in Visual Genome by truncating the low frequency classes and attributes. Our final training set contains 1,600 object classes and 400 attribute classes. To train this model, we initialize Faster R-CNN with the CNN model (e.g., <ref type="bibr">VGG-16 [Simonyan and Zisserman, 2014]</ref> or ResNet-101 ) pre-trained on ImageNet. We then train Faster R-CNN on the processed Visual Genome datasets. Slightly different from the standard Faster R-CNN model with four losses, we add an additional loss to the last fully-connected layer to predict attribute classes in addition to object classes. Specifically, we concatenate the fullyconnected feature with a learned embedding of the groundtruth object class and feed this into an additional output layer defining a softmax distribution over the attribute classes.</p><p>Note that although DDPN is similar to <ref type="bibr" target="#b0">[Anderson et al., 2017]</ref>, we have a different motivation. In their approach, Faster R-CNN is used to build bottom-up attention to provide high-level image understanding. In this paper, we use Faster R-CNN to learn diversified and discriminative proposals for visual grounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Model for Visual Grounding</head><p>We first introduce a high performance baseline model for visual grounding based on DDPN. Given an image-query pair, our model is trained to predict the bounding boxes of the referred object in the image. The flowchart of our model is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><formula xml:id="formula_0">q = LSTM(Embed(p))</formula><p>(1) For an input image, we first use the DDPN model to extract the top-N proposals with diversity and discrimination.</p><p>Each proposal is represented as a visual feature (the last fully-connected layer after RoI pooling) v vis ? R dv and a 5-D spatial feature v spat = [x tl /W, y tl /H, x br /W, y br /H, wh/W H] proposed in <ref type="bibr" target="#b3">[Yu et al., 2016]</ref>. We concatenate v vis (with L 2 normalization) and v spat to obtain the visual feature v ? R dv+5 for each region proposal. For N region proposals, the visual features are defined as</p><formula xml:id="formula_1">V = [v 1 , ..., v N ].</formula><p>For an input textual query p = [w 1 , w 2 , ..., w t ] describing the object in the image, t is the query length, and each word w i is represented as a one-hot feature vector referring to the index of the word in a large vocabulary. As the length varies for different queries, we use the LSTM network to encode variable-length queries <ref type="bibr" target="#b1">[Hochreiter and Schmidhuber, 1997</ref>]. Specifically, we first learn a word embedding layer to embed each word into a d e -dimensional latent space. The sequence of the embedded word features is then fed to a one-layer LSTM network with d q hidden units. We extract the feature of the last word from the LSTM network as the output feature q ? R dq for the query phrase p. The encoding procedure for the query phrases is represented as follows.</p><p>Once we have obtained the query feature q ? R dq and visual features V = [v 1 , ..., v N ] ? R dv?N , we can fuse the multi-modal features with a feature fusion model and output the integrated features F = [f 1 , ..., f N ].</p><p>To better demonstrate the capacity of the visual features, we do not introduce a complex feature fusion model (e.g., bilinear pooling) and only use simple feature concatenation followed by a fully-connected layer for fusion. For each pair {q, v}, the output feature f is obtained as follows. where W f ? R (dv+dq)?do and b f ? R do are the weights and bias respectively, || denotes concatenation of vectors, f ? R do is the fused feature. ?(?) is the ReLU activation function.</p><formula xml:id="formula_2">f = ?(W T f (q||v) + b f )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Objective</head><p>Given the fused feature f ? F , we use an additional fullyconnected layer to predict the score s ? R for feature f .</p><formula xml:id="formula_3">s = W T s f + b s (3)</formula><p>where W s ? R o and b s ? R are the weights and bias, respectively. For N proposals, we can obtain their scores S = [s 1 , ..., s N ] ? R N using Eq.</p><p>(3). We apply softmax normalization to S to make it satisfy a score distribution.</p><p>The problem now becomes how to define the loss function to make the predicted ranking scores S consistent with their ground-truth ranking scores S * . In most existing visual grounding approaches <ref type="bibr" target="#b0">[Fukui et al., 2016;</ref><ref type="bibr" target="#b1">Hu et al., 2016;</ref>, the ground-truth ranking scores S * = [s * 1 , ..., s * N ] ? {0, 1} N are defined as a one-hot vector in which the only element is set to 1 when the corresponding proposal most overlaps with the ground-truth bounding box (i.e., the largest IoU score) and 0 otherwise. Based on the one-hot single label, softmax cross-entropy loss is used to learning the ranking model.</p><p>Slightly different to their strategy, we use the soft label distribution as the ground-truth. We calculate the IoU scores of all proposals w.r.t. the ground-truth bounding box and assign the IoU score of the i-th proposal to s * i if the IoU score is larger than a threshold ? and 0 otherwise. In this way, we obtain the real-value label vector S * = [s * 1 , ..., s * N ] ? R N . To make S * satisfy a probability distribution, S * is L 1 -normalized to guarantee N i s * i = 1. Accordingly, we use Kullback-Leibler Divergence (KLD) as our ranking function for model optimization and to make the predicted score distribution S and ground-truth label distribution S * as close as possible.</p><formula xml:id="formula_4">L rank = 1 N N i s * i log( s * i s i )<label>(4)</label></formula><p>The benefits of using soft labels are three-fold: 1) except for the max-overlapping proposal, other proposals may also contain useful information, that provides contextual knowledge of the ground-truth; 2) the soft label can be seen as a model regularization strategy, as introduced in [Szegedy et al., 2016] as label smoothing; and 3) during testing, the predicted bounding box is considered to be correct when its IoU score with the ground-truth is larger than a threshold ?. Therefore, optimizing the model with soft labels applies consistency to training and testing and may improve visual grounding performance.</p><p>Although using the KLD loss in Eq.(4) can capture the proposals' contextual information, the accuracies of the region proposals themselves could be a performance bottleneck for visual grounding. In the case that all the generated region proposals do not overlap with the ground-truth bounding box, the grounding accuracy will be zero. Inspired by the strategy used in Faster R-CNN <ref type="bibr">[Ren et al., 2015]</ref>, we append an additional fully-connected layer on top of fused feature f and add a regression layer to refine the proposal coordinates.</p><formula xml:id="formula_5">t = W T t f + b t (5)</formula><p>where t ? R 4 corresponds to the coordinates of the refined bounding box, and W t ? R do?4 and b t ? R 4 are the weights and bias respectively. Accordingly, the smoothed L 1 regression loss is defined as follows to penalize the difference between t and its ground-truth bounding box coordinates t * for all the proposals.</p><formula xml:id="formula_6">L reg = 1 N N i smooth L1 (t * i , t i )<label>(6)</label></formula><p>where smooth L1 (x, y) is the smoothed L 1 loss function for input features x and y <ref type="bibr">[Ren et al., 2015]</ref>. It is worth noting that although the proposals generated by DDPN has been refined by a regression layer in the Faster R-CNN model, the regression layer here has a different meaning: it acts as a domain adaptation function to align the semantics of the source domain (i.e., the Visual Genome dataset) and the target domain (i.e., the specific visual grounding dataset). The overall loss for our model is defined as:</p><formula xml:id="formula_7">L = L rank + ?L reg<label>(7)</label></formula><p>where ? is a hyper-parameter to balance the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our approach on four benchmark datasets: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We use the same hyper-parameters as in <ref type="bibr" target="#b0">[Anderson et al., 2017]</ref> to train the DDPN model. We train DDPN with different CNN backbones (VGG-16 and ResNet-101). Both models are trained for up to 30 epochs, which takes two GPUs 2?3 weeks to finish. Based on the optimized DDPN model, we train our visual grounding model with the hyper-parameters listed as follows. The loss weight ? is set to 1 for all experiments. The dimensionality of the visual feature d v is 2048 (for ResNet-101) or 4096 (for VGG-16), the dimensionality of the word embedding feature d e is 300, the dimensionality of the output feature of the LSTM network d q is 1024, the dimensionality of the fused feature d o is 512, the threshold of IoU scores ? is 0.5, and the number of proposals N is 100. During training, all the weight parameters, including the word embedding layer, the LSTM network and the multi-modal fusion model are initialized using the xavier method. We use the Adam solver to train the model with ? 1 = 0.9, ? 2 = 0.99. The base learning rate is set to 0.001 with an exponential decay rate of 0.1. The mini-batch size is set to 64. All the models are trained up to 10k iterations. During testing, we feed forward the network and output the scores for all the proposals before picking the proposal with the highest score and using its refined bounding box as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>Flickr30K Entities Flickr30K Entities contains 32k images, and 275k bounding boxes and 360k query phrases. Some phrases in the dataset may correspond to multiple boxes. In such cases, we merge the boxes and use their union as the ground-truth . We use the standard split in our setting, i.e., 1k images for validation, 1k for testing, and 30k for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReferItGame</head><p>ReferItGame contains over 99k bounding boxes and 130k query phrases from 20k images. Each bounding box is associated with 1-3 query phrases. We use the same data split as in , namely 10k images for testing, 9k for training and 1k for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RefCOCO and RefCOCO+</head><p>Both the RefCOCO and RefCOCO+ datasets utilize images from the COCO dataset <ref type="bibr" target="#b2">[Lin et al., 2014]</ref>. Both of them consist of 142k query phrases and 50k bounding boxes from 20k images. The difference between the two datasets is that the query phrases in RefCOCO+ does not contain any location word, which is more difficult to understand the query intent. The datasets are split into four sets: train, validation, testA and testB. The images in testA contain multiple people, while images in testB contain objects in other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup Evaluation Metric</head><p>For all datasets, we adopt accuracy as the evaluation metric, which is defined as the percentage in which the predicted bounding box overlaps with the ground-truth of IoU &gt; 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods</head><p>We compare our approach with state-of-the-art visual ground- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We also perform the following ablation experiments on Flickr30k Entities to verify the efficacy of DDPN, and the loss functions to train our baseline visual grounding model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Diversity and Discrimination in Proposal Generation</head><p>We define the discrimination score (S DIS. ) and the diversity score (S DIV. ) in Eq.(8) to evaluate the proposal generators.</p><formula xml:id="formula_8">S DIS. = 1 M M i max j ?(t j i , t * i ) S DIV. = 1/ 1 M M i N j ?(t j i , t * i )<label>(8)</label></formula><p>where M is the number of query-image samples in the test set, N is the number of proposals per sample, and t j i indicates the j-th proposal in the i-th sample. For the i-th sample, the indicator function ?(t j i , t * i ) = 1 if a proposal t j i covers its corresponding ground-truth bounding box t * i with IoU &gt; 0.5 and 0 otherwise. S DIS. is the discrimination score which is defined as the percentage of samples with at least one proposal covering its ground-truth objects. A larger discrimination score leads to more accurate proposals. S DIV. is the diversity score defined as the reciprocal of the average number of covered proposals per ground-truth object. A larger diversity score leads to fewer redundant proposals and a more balanced proposal distribution. A good proposal generator should have high S DIV. and S DIS. simultaneously.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we report the S DIV. and S DIS. , as well as the visual grounding accuracies of different models by replacing DDPN (shown in <ref type="figure" target="#fig_2">Figure 3</ref>) with other proposal generators. The results show that: 1) SS and RPN (w/o ft) models have similar discriminative scores, while the diversity score of RPN (w/o ft) is smaller than that of SS. This reflects the fact that SS is more diversified compared to the data-driven RPN model pre-trained on PASCAL VOC 07 <ref type="bibr" target="#b0">[Everingham et al., 2010]</ref>; 2) after fine-tuning on Flickr30k Entities, the RPN (w/ ft) model obtains a better discriminative score than RPN (w/o ft). However, the diversity score is reduced, indicating that RPN (w/ ft) fits Flickr30k Entities at the expense of model generalizability; 3) DDPN significantly outperforms other approaches for both discriminative and diversity scores. To further demonstrate the effect of DDPN, we illustrate proposals for an image using the aforementioned four models in <ref type="figure" target="#fig_4">Figure 4</ref>, and the visualized results show that DDPN generates proposals of higher quality than other approaches; 4) DDPN significantly improve the visual grounding performance compared to other proposal generation models, which proves our hypothesis that both diversity and discrimination  are of great significance for visual grounding performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Different Losses on Visual Grounding</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we report the performance of our model variants trained with different losses. It can be seen that: 1) training with the KLD loss leads to a 1.2?1.7-point improvement over the models with classical single-label softmax loss. This verifies our hypothesis that using soft labels captures contextual information and enhances the model's capacity. Compared to the context modeling strategy proposed by , which exploits the ground-truth bounding boxes within an image from multiple queries, our strategy is more general and easier to implement; 2) training with an additional regression loss to refine the proposals leads to a 4.7?5.2-point improvement. We believe this will become a commonly-used strategy in future visual grounding studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons with State-of-the-Art</head><p>We next compare our models to current state-of-the-art approaches. We report the results of our approach with DDPN of two backbones, namely <ref type="bibr">VGG-16 [Simonyan and Zisserman, 2014]</ref> and ResNet-101 .   Tables 3, 4 and 5 show the comparative results on Re-fCOCO, RefCOCO+, Flickr30k Entities and ReferItGame, respectively. We note the following: 1) with the same CNN backbone (i.e., VGG-16), our model achieves an absolute improvement of 3.2 points on RefCOCO (testA), 6.3 points on RefCOCO+ (testA), 4.9 points on Flickr30k Entities and 16.1 points on ReferItGame, respectively. The improvement is primarily due to the high-quality proposals generated by DDPN, and the loss functions we use for our visual grounding model; 2) by replacing VGG-16 with ResNet-101 as the backbone for DDPN, all results improve by 3?4 points, illustrating that the representation capacity of DDPN significantly influences the visual grounding performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Results</head><p>We visualize some visual grounding results of our model with ResNet-101 backbone model on Flickr30k Entities (the first row) and ReferItGame (the second row) in <ref type="figure" target="#fig_5">Figure 5</ref>. It can be seen that our approach achieves good visual grounding performance, and is able to handle the small and fine-grained objects. Moreover, the bounding box regression helps refine the results, rectifying some inaccurate proposals. Finally, our approach still has some limitations, especially when faced with complex queries or confused visual objects. These  <ref type="bibr">et al., 2016]</ref> 28.5 MCB <ref type="bibr" target="#b0">[Fukui et al., 2016]</ref> 28.9 Wu et al. <ref type="bibr" target="#b3">[Wu et al., 2017]</ref> 36.8 QRC  44.1 Li et al.  44.2</p><p>Ours (VGG-16) 60.3 Ours (ResNet-101) 63.0</p><p>observations are useful to guide further improvements for visual grounding in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we interrogate the proposal generation for visual grounding and in doing so propose Diversified and Discriminative Proposal Networks (DDPN) to produce highquality proposals. Based on the proposals and visual features extracted from the DDPN model, we design a high performance baseline for visual grounding trained with two novel losses: KLD loss to capture the contextual information of the proposals and regression loss to refine the proposals. We conduct extensive experiments on four benchmark datasets and achieve significantly better results on all datasets. Since the models studied here represent the baseline, there remains significant room for improvement, for example by introducing a more advanced backbone model for DDPN or introducing a more powerful multi-modal feature fusion model such as bilinear pooling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The general framework of the visual grounding task. Given an arbitrary image and an open-ended query phrase as the inputs, the visual grounding model outputs the predicted bounding box of the referred object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples of diversified proposals and discriminative proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The flowchart of our visual grounding model during the training stage. DDPN corresponds to the model described in section 2. The parameters in our model (within the dashed box) is optimized via back-propagation with the regression and ranking losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ing methods: SCRC [Hu et al., 2016], GroundeR [Rohrbach et al., 2016], MCB [Fukui et al., 2016], QRC [Chen et al., 2017], and the approaches of Li et al. [Li et al., 2017], Wu et al. [Wu et al., 2017] and Yu et al. [Yu et al., 2017a].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The generated region proposals with different proposal generators on Flickr30k: (a) selective search (SS); (b) region proposal network (RPN) trained on PASCAL VOC 07; (c) the RPN model in (b) with fine-tuning on Flickr30k Entities; (d) our diversified and discriminative proposal networks (DDPN). For better visualization, we only show the top-20 proposals for each model w.r.t. their proposal scores in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?Q:Figure 5 :</head><label>5</label><figDesc>the cup near the glass ? The examples in Flickr30k Entities (the 1st row) and ReferItGame (the 2nd row) datasets. The ground-truth (green), the top-ranked predicted proposal (yellow) and the refined final prediction (red) are visualized respectively. The last three columns shows the examples with incorrect predictions (IoU ? 0.5). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The discrimination scores (SDIS.), the diversity scores (SDIV.) and the visual grounding accuracies of different proposal generators evaluated on the test set of Flickr30k Entities.</figDesc><table><row><cell>All the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The visual grounding accuracies of the variants with different loss functions on Flickr30k Entities. All the variants use the same DDPN model with VGG-16 backbone.</figDesc><table><row><cell>Different losses Lsoftmax Lkld Lreg</cell><cell cols="2">Accuracy (%) Improv. (%)</cell></row><row><cell></cell><cell>63.6</cell><cell>-</cell></row><row><cell></cell><cell>64.8</cell><cell>1.2</cell></row><row><cell></cell><cell>68.3</cell><cell>4.7</cell></row><row><cell></cell><cell>70.0</cell><cell>6.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The accuracies of different methods on RefCOCO and RefCOCO+ datasets</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RefCOCO</cell><cell></cell><cell></cell><cell></cell><cell>RefCOCO+</cell></row><row><cell></cell><cell cols="2">Methods</cell><cell></cell><cell cols="7">TestA (%) TestB (%) Validation (%) TestA (%) TestB (%) Validation (%)</cell></row><row><cell></cell><cell cols="3">SCRC [Hu et al., 2016]</cell><cell cols="2">18.5</cell><cell>20.2</cell><cell cols="2">19.0</cell><cell>14.4</cell><cell>13.3</cell><cell>13.7</cell></row><row><cell></cell><cell cols="3">Wu et al. [Wu et al., 2017]</cell><cell cols="2">54.8</cell><cell>41.6</cell><cell cols="2">48.2</cell><cell>40.4</cell><cell>22.8</cell><cell>31.9</cell></row><row><cell></cell><cell cols="3">Li et al. [Li et al., 2017]</cell><cell cols="2">55.2</cell><cell>46.2</cell><cell cols="2">52.4</cell><cell>45.2</cell><cell>32.2</cell><cell>40.2</cell></row><row><cell></cell><cell cols="3">Yu et al. [Yu et al., 2017a]</cell><cell cols="2">73.7</cell><cell>65.0</cell><cell cols="2">69.5</cell><cell>60.7</cell><cell>48.8</cell><cell>55.7</cell></row><row><cell></cell><cell cols="3">Ours (VGG-16)</cell><cell cols="2">76.9</cell><cell>67.5</cell><cell cols="2">73.4</cell><cell>67.0</cell><cell>50.2</cell><cell>60.1</cell></row><row><cell></cell><cell cols="3">Ours (ResNet-101)</cell><cell cols="2">80.1</cell><cell>72.4</cell><cell cols="2">76.8</cell><cell>70.5</cell><cell>54.1</cell><cell>64.8</cell></row><row><cell>P</cell><cell>Q: little girl</cell><cell>P</cell><cell>Q: a collie</cell><cell></cell><cell>P</cell><cell>Q: red hat</cell><cell>?</cell><cell>Q: the passenger</cell><cell cols="2">Q: the businessman ?</cell><cell>Q: a red knit scarf</cell></row><row><cell cols="2">Q: 2nd to right jewel P</cell><cell cols="3">Q: hanging above bed P P</cell><cell>P</cell><cell>Q: apple far left</cell><cell>?</cell><cell>Q: the table</cell><cell>?</cell><cell>Q: the tile roof</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The accuracies of different methods on Flickr30k Entities</figDesc><table><row><cell>Methods</cell><cell>Accuracy (%)</cell></row><row><cell>SCRC [Hu et al., 2016]</cell><cell>27.8</cell></row><row><cell>GroundeR [Rohrbach et al., 2016]</cell><cell>47.8</cell></row><row><cell>MCB [Fukui et al., 2016]</cell><cell>48.7</cell></row><row><cell>QRC [Chen et al., 2017]</cell><cell>65.1</cell></row><row><cell>Ours (VGG-16)</cell><cell>70.0</cell></row><row><cell>Ours (ResNet-101)</cell><cell>73.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The accuracies of different methods on ReferItGame</figDesc><table><row><cell>Methods</cell><cell>Accuracy (%)</cell></row><row><cell>SCRC [Hu et al., 2016]</cell><cell>17.9</cell></row><row><cell>GroundeR [Rohrbach</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07998</idno>
		<idno>arXiv:1606.01847</idno>
	</analytic>
	<monogr>
		<title level="m">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<meeting><address><addrLine>Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The pascal visual object classes (voc) challenge. Multimodal compact bilinear pooling for visual question answering and visual grounding</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
	</analytic>
	<monogr>
		<title level="m">Connecting language and vision using crowdsourced dense image annotations. IJCV</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="32" to="73" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Ronghang Hu, Trevor Darrell, and Bernt Schiele</title>
		<meeting><address><addrLine>Anna Rohrbach, Marcus Rohrbach; Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR. Uijlings et al., 2013. Selective search for object recognition. IJCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<idno type="arXiv">arXiv:1703.07579</idno>
	</analytic>
	<monogr>
		<title level="m">An end-to-end approach to natural language object retrieval via context-aware deep reinforcement learning</title>
		<imprint>
			<publisher>Zhou Yu</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Chenchao Xiang, Jianping Fan, and Dacheng Tao. Beyond bilinear</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalized multi-modal factorized high-order pooling for visual question answering</title>
		<idno type="DOI">10.1109/TNNLS.2018.2817340</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
