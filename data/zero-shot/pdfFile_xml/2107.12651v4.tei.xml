<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Greedy Gradient Ensemble for Robust Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhe</forename><surname>Han</surname></persName>
							<email>hanxinzhe17@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
							<email>wangshuhui@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
							<email>suchi@kingsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Kingsoft Cloud</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff4">
								<orgName type="department">Cloud BU</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Greedy Gradient Ensemble for Robust Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language bias is a critical issue in Visual Question Answering (VQA), where models often exploit dataset biases for the final decision without considering the image information. As a result, they suffer from performance drop on out-of-distribution data and inadequate visual explanation. Based on experimental analysis for existing robust VQA methods, we stress the language bias in VQA that comes from two aspects, i.e., distribution bias and shortcut bias. We further propose a new de-bias framework, Greedy Gradient Ensemble (GGE), which combines multiple biased models for unbiased base model learning. With the greedy strategy, GGE forces the biased models to over-fit the biased data distribution in priority, thus makes the base model pay more attention to examples that are hard to solve by biased models. The experiments demonstrate that our method makes better use of visual information and achieves state-of-the-art performance on diagnosing dataset VQA-CP without using extra annotations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual Question Answering (VQA) is a challenging task that requires both language-aware reasoning and image understanding. With advances in deep learning, neural networks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref> that model the correlations between vision and language have shown remarkable results on large-scale benchmark datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>However, recent studies have demonstrated that most VQA methods tend to rely on existing idiosyncratic biases in the datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48]</ref>. They often leverage superficial correlations between questions and answers to train the model without considering exact vision information. For example, a model may blindly answer "tennis" for the question "What sports ..." just based on the most common textual QA pairs in the train set. Unfortunately, models exploiting  statistical shortcuts during training often show poor generalization ability to out-of-domain data, and hardly provide proper visual evidence for a certain answer.</p><p>Currently, the prevailing solutions for this problem can be categorized into ensemble-based <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>, groundingbased <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23]</ref> and counterfactual-based <ref type="bibr" target="#b7">[8]</ref>. Similar to re-weighting and re-sampling strategies in traditional longtailed classification <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>, ensemble-based methods re-weight the samples by the question-only branch. Grounding-based models stress a better use of image information according to human-annotated visual explanation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>. Newly proposed counterfactual-based meth-ods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34]</ref> further combine these two lines of work and achieve better performance.</p><p>Nevertheless, it has been shown that existing methods have not fully leveraged both vision and language information. For example, Shrestha et al. <ref type="bibr" target="#b44">[45]</ref> argue that improved accuracy in grounding-based methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref> does not actually emerge from proper visual grounding but some unknown regularization effects. Similar to <ref type="bibr" target="#b44">[45]</ref>, we further analyse all the three categories of existing work by control experiments in Section 3.2. We found that language bias in VQA is actually two-fold: (a) the statistical distribution gap between train and test, i.e., distribution bias shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, and (b) the semantic correlation between specific QA pairs, i.e., shortcut bias shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Although long-tailed distribution in train set is usually considered to be one of the factors that increase shortcut bias, we experimentally demonstrate that they are actually two aspects of the language bias. Grounding supervision in <ref type="bibr" target="#b43">[44]</ref> or ensemble regularization in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> does not necessarily force the model to focus on visual information as expected. To encourage the model to pay attention to the images, we need to explicitly model bothbiases and reduce them step by step.</p><p>Inspired by our empirical findings, we propose Greedy Gradient Ensemble (GGE), a model-agnostic debias framework that ensembles biased models and the base model like gradient descent in functional space. The key idea of our method is to make use of the over-fitting phenomenon in deep learning. The biased part of data is greedily over-fitted by biased features, as a result, the expected base model can be learned with more ideal data distribution and focus on examples that are hard to solve with biased models.</p><p>In the experiments, variants of GGE models are provided in ablation study, which demonstrates the generalization ability of our method and further supports our claim that distribution bias and shortcut bias are complementary in VQA. To verify if a model can really use visual information for the answer decision, we further study the language bias in VQA from a visual modelling perspective. Quantitative and qualitative evaluations show that GGE can provide better visual evidence accompanied with predictions.</p><p>The major contributions are:</p><p>? We provide analysis for the language bias in VQA task and decompose the language bias into distribution bias and shortcut bias.</p><p>? We propose a new model-agnostic de-bias framework Greedy Gradient Ensemble (GGE), which sequentially ensembles biased models for robust VQA.</p><p>? On VQA-CP, our method makes better use of visual information and achieves state-of-the-art performance, with 17.34% gain against simple UpDn baseline without extra annotations. Code is available at https: //github.com/GeraldHan/GGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. De-bias with dataset construction</head><p>The most straightforward way to remove the dataset bias is to construct a balanced dataset. For instance, Zhang et al. <ref type="bibr" target="#b47">[48]</ref> collect complementary abstract scenes with opposite answers for all binary questions. Similarly, VQA v2 <ref type="bibr" target="#b14">[15]</ref> is introduced to weaken language priors in the VQA v1 dataset <ref type="bibr" target="#b2">[3]</ref> by adding similar images with different answers for each question. Agrawal et al. <ref type="bibr" target="#b0">[1]</ref> introduce a diagnosing VQA dataset under Changing Prior (VQA-CP) constructed with different answer distributions between the train and test splits. Most of the models that perform well on VQA v2 significantly drop on VQA-CP in Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">De-bias with model design</head><p>Collecting new large-scale datasets is costly. It is crucial to develop models that are robust to biases. Along with VQA-CP dataset <ref type="bibr" target="#b0">[1]</ref>, Agrawal et al. propose GVQA model that disentangles the visual concept recognition from the answer space prediction. LDP <ref type="bibr" target="#b22">[23]</ref> and GVQE <ref type="bibr" target="#b27">[28]</ref> exploit different information in questions for better question representation. These models require a pre-defined question parser, making them hard to implement.</p><p>Another line of work starts from visual grounding. Early works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref> directly apply human grounding <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> as supervision to attention maps, but the improvement is limited. HINT <ref type="bibr" target="#b43">[44]</ref> and SCR <ref type="bibr" target="#b46">[47]</ref> change supervised attention maps to Grad-CAM, which directly encourages the contribution of each object to be consistent with human annotations. Recent work <ref type="bibr" target="#b44">[45]</ref> experimentally challenges the effectiveness of visual grounding in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref>, but does not find a good way to test if systems are actually visually grounded.</p><p>The most effective solution so far is ensemble-based, which formulates a question-only branch as explicit modelling for language bias. Ramakrishnan et al. <ref type="bibr" target="#b38">[39]</ref> introduce an adversarial regularization to remove answer discriminative feature from the questions. RUBi <ref type="bibr" target="#b6">[7]</ref>, LMH <ref type="bibr" target="#b9">[10]</ref> and PoE <ref type="bibr" target="#b28">[29]</ref> re-weight samples based on the question-only prediction. Niu et al. <ref type="bibr" target="#b33">[34]</ref> further improve ensemble strategies from a causal-effect perspective. CSS <ref type="bibr" target="#b7">[8]</ref> combines grounding-based and ensemble-based methods with counterfactual samples synthesizing. Gat et al. <ref type="bibr" target="#b13">[14]</ref> introduce a regularization by maximizing functional entropies (MFE), which forces the model to use multiple sources of information in multi-modal tasks. Nam et al. <ref type="bibr" target="#b32">[33]</ref> propose a general framework LfF, which trains the de-biased classifier from a biased classifier. Compared to our work, they mainly focus on single-modality classification problems and their General Cross-Entropy (GCE) re-weighting strategy is less flexible, which relies on hyper-parameter in GCE and can only handle one pair of attributes in de-bias learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting Language Bias in VQA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>For base model, we consider the common formulation of VQA task as a multi-class classification problem. Given a dataset D = {v i , q i , a i } N i=1 consisting of an image v i ? V, a question q i ? Q and a labelled answer a i ? A, we need to optimize a mapping f V Q : V ? Q ? R C which produces a distribution over the C answer candidates. Without loss of generality, the function is composed as following:</p><formula xml:id="formula_0">a i = f ? (v i , q i ) = c (m (e v (v i ), e q (q i ))) ,<label>(1)</label></formula><p>where e v : V ? R nv?dv is an image encoder, e q : Q ? R nq?dq is a question encoder, m(.) denotes the multi-modal fusion or reasoning module, and c(.) is the multi-layer perception classifier. The output is a vector? ? R C indicating the probability belonging to each answer candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Analysis for Language Bias</head><p>In recent work, Shrestha et al. <ref type="bibr" target="#b44">[45]</ref> experimentally challenge the way grounding-based methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44]</ref> work on VQA-CP <ref type="bibr" target="#b0">[1]</ref>. However, they did not provide insights into the language bias itself. In this section, from a new de-bias method perspective, we provide three control experiments for baseline model UpDn <ref type="bibr" target="#b1">[2]</ref>, grounding-based method HINT <ref type="bibr" target="#b43">[44]</ref>, ensemble-based method RUBi <ref type="bibr" target="#b6">[7]</ref> LMH <ref type="bibr" target="#b9">[10]</ref> and counterfactual-based method CSS <ref type="bibr" target="#b7">[8]</ref> on VQA-CP and VQA v2 to discuss the language bias in VQA.</p><p>Inverse Grounding Annotation. To analyse the contribution of visual-grounding, we first experiment with HINT and CSS-V that use human attention as extra information. Following <ref type="bibr" target="#b44">[45]</ref>, we change human-annotated region importance scores <ref type="bibr" target="#b10">[11]</ref> S h to irrelevant grounding S h = 1 ? S h . As shown in <ref type="table" target="#tab_1">Table 1</ref>, the performance of HINT inv and CSS-V inv is almost the same as the original models. This indicates that the Accuracy gains are not necessarily from looking at relevant regions <ref type="bibr" target="#b3">[4]</ref>. Although the models correctly answer some hard questions, they still make predictions based on language information regardless of images. We refer to this unexpected solution as "inverse language bias".</p><p>Vision-only Model. The second experiment aims to analyse the function of the ensemble branch in RUBi and LMH. For the base model, we only feed the vision feature without multi-modal fusion to the answer classifier:</p><formula xml:id="formula_1">a i = c (e v (v i )) .<label>(2)</label></formula><p>There is no question information for classification in base model, and thus obviously no shortcut between QA pairs to reduce. As shown in <ref type="table" target="#tab_1">Table 1</ref>, RUBi vo degrades a lot, but LMH vo still surpasses UpDn vo by a large margin in Accuracy. Apart from restraining shortcuts between questionanswer pairs, we think the improved Accuracy in LMH mainly comes from penalizing the most common answers in the train set, which leads to a more balanced classifier according to inverse distribution. This means the distribution bias in LMH plays a different role compared with the question shortcut in RUBi. Inverse Supervision for Balanced Classifier. To directly verify if such "inverse distribution bias" can improve Accuracy, inspired by the two-round training in CSS <ref type="bibr" target="#b7">[8]</ref>, we design a simple "inverse supervision" strategy. For each iteration, the parameters are updated two rounds with different supervisions. In the first round, we train the model supervised by ground-truth label A and get the prediction P (a). The top-N answers with the highest predicted probabilities are selected as a + . In the second-round training, the label is defined as? = {a i |a i ? A, a i / ? a + }. This strategy is actually a simplified version of CSS <ref type="bibr" target="#b7">[8]</ref> without object/question masks. In this way, the model continuously penalizes the most confident answers in the first round training, thus formulates a more balanced classifier according to inverse distribution bias. The Accuracy improvement in UpDn vo,is reveals the existence of distribution bias. The result of RUBi is further indicates that distribution bias and shortcut bias are complementary. LMH is is even comparable to CSS that uses extra annotations. However, this method leads to catastrophic degradation on the in-distribution dataset VQA v2 as shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>According to the above experiments, we obtain the following insights: 1) Good Accuracy can not guarantee that the system is really visually grounded for answer classification. Grounding supervision or question-only regularization may encourage models to make use of inverse language bias rather than better visual information for higher Accuracy. 2) Distribution bias and shortcut bias are complementary aspects of language bias in VQA. A single ensemble branch is unable to model such two types of biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Based on the above findings, we propose GGE, a new model-agnostic de-bias learning paradigm, which removes distribution bias and shortcut bias step by step, thus forces the model to focus on images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Greedy Gradient Ensemble</head><p>Let (X, Y ) denote the train set, where X is the space of observations, and Y is the space of labels. Following previous VQA methods, we mainly consider the classification problem with binary cross-entropy (BCE) loss</p><formula xml:id="formula_2">L(P, Y ) = ? C i=1 y i log(p i ) + (1 ? y i ) log(1 ? p i ), (3)</formula><p>where C denotes the number of classes. p i = ?(z i ) where z i is the predicted logit for class i and ?(.) is the sigmoid function. Baseline methods directly minimize the loss between the prediction f (X; ?) and label Y</p><formula xml:id="formula_3">min ? L (?(f (X; ?)), Y ) .<label>(4)</label></formula><p>Since f (.) is over-parametrized DNNs, the model is easy to over-fit the dataset biases and suffers from poor generalization ability. For our method, we make use of this kind of over-fitting in deep learning. Assume B = {B 1 , B 2 , . . . , B M } to be a set of bias features that can be extracted based on prior knowledge. This time we fit the ensemble of bias models and base model to label Y</p><formula xml:id="formula_4">min ?,? L ? f (X; ?) + M i=1 h i (B i ; ? i ) , Y ,<label>(5)</label></formula><p>where h i (.) is a biased model for certain biased feature. Ideally, we hope the biased part of data is only over-fitted by the bias models, thus the base model can be learned with unbiased data distribution. To achieve this goal, we propose GGE in which biased models have a higher priority to over-fit the dataset with greedy strategy.</p><p>Viewing in the functional space, suppose we have</p><formula xml:id="formula_5">H m = m i=1 h i (B i ) and we wish to find h m+1 (B m+1 ) added to H m so that the loss L (?(H m + h m+1 (B m+1 )), Y ) de- creases. In theory, the desired direction of h m+1 is the neg- ative derivative of L at H m , where ??L(H m,i ) := ?L (?(H m ), Y ) ?H m,i = 2y m,i ? (?2y m,i H m,i ) .</formula><p>(6) For a classification problem, we only care about the probability for class i: ?(f i (x)) ? {0, 1}. Therefore, we treat the negative gradients as pseudo labels for classification and optimize the new model h m+1 (B m+1 ) with BCE loss:</p><formula xml:id="formula_6">L m+1 = L (?(h m+1 (B m+1 ; ? m+1 )), ??L(H m )) . (7)</formula><p>After integrating all biased models, the expected base model f is optimized with</p><formula xml:id="formula_7">L b (?) = L (?(f (X; ?)), ??L(H M )) .<label>(8)</label></formula><p>In the test stage, we only use the base model for predictions. More intuitively, for a sample that is easy to fit by biased models, the negative gradient of its loss ??L(H M ) (i.e., the pseudo label for the base model) will become relatively small. f (X; ?) will pay more attention to samples that are hard to solve by previous ensemble biased classifiers H M .</p><p>In order to make the above paradigm adaptive to Batch Stochastic Gradient Decent (Batch SGD), we implement two optimization schedules GGE-iteration and GGEtogether, as shown in Algorithm 1 and Algorithm 2 in Supplementary. GGE-tog jointly optimizes biased models and the base model with</p><formula xml:id="formula_8">L(?) = L b (?) + M m=1 L m (? m ).<label>(9)</label></formula><p>For GGE-iter, each model is iteratively updated within a certain data-batch iteration. More details for GGE are provided in Section A in Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">GGE Implementation for Robust VQA</head><p>Following analysis in Section 3, we define two biased features for VQA, i.e., distribution bias and shortcut bias.</p><p>Distribution Bias. We define the distribution bias as answer distribution in the train set conditioned on question types</p><formula xml:id="formula_9">B i d = p(a i |t i ),<label>(10)</label></formula><p>where t i denotes the type of question q i . The reason for counting samples conditioned on question types is to maintain type information when reducing distribution bias. Question type information can only be obtained from the questions rather than the images, which does not belong to the language bias to be reduced. Shortcut Bias. Shortcut bias is the semantic correlation between specific QA pairs. Similar to <ref type="bibr" target="#b6">[7]</ref>, we compose the question shortcut bias as a question-only branch</p><formula xml:id="formula_10">B i q = c q (e q (q i )) ,<label>(11)</label></formula><p>where c q : Q ? R C .</p><p>To verify our claim that distribution bias and shortcut bias are complementary, we design three versions of GGE for ensembles of different language biases.</p><p>GGE-D only models distribution bias for ensemble, shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b). The loss for the base model is</p><formula xml:id="formula_11">L = L(?(?), ??L(B d , A)),<label>(12)</label></formula><p>where? is the predictions, and A is the labelled answers. GGE-Q only uses a question-only branch for shortcut bias. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(c), we first optimize the question-only branch with labelled answers</p><formula xml:id="formula_12">V Q A A A L VQA Model V Q VQA Model V Q (a) Baseline V VQA Model V Q VQA Model V Q Q ( ) , d d L A B B ? ? ? d B L A (b) GGE-D V VQA Model V Q VQA Model V Q Q QA Classifier QA Classifier Q q B ( ) , q q L A B B ? ? ? 1 L 2 L A A (c) GGE-Q V VQA Model V Q Q QA Classifier QA Classifier Q ( ) , d d L A B B ? ? ? ( ) ( ) , d q d q L A B B B B ? + ? ? + q B 1 L 2 L d B A (d) GGE-DQ</formula><formula xml:id="formula_13">L 1 = L(?(B q ), A).<label>(13)</label></formula><p>The loss for base model is</p><formula xml:id="formula_14">L 2 = L(?(?), ??L(?(B q ), A)).<label>(14)</label></formula><p>GGE-DQ uses both distribution bias and question shortcut bias. As shown in <ref type="figure" target="#fig_1">Figure 2(d)</ref>, the loss for B q is</p><formula xml:id="formula_15">L 1 = L(?(B q ), ??L(B d , A)).<label>(15)</label></formula><p>The loss for base model is</p><formula xml:id="formula_16">L 2 = L(?(?), ??L(?(B q ) + B d , A)).<label>(16)</label></formula><p>We test both GGE-iter or GGE-tog for L 1 and L 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Connection to Boosting</head><p>Boosting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b8">9]</ref> is a widely used ensemble strategy for classification problems. The key idea of boosting is to combine multiple weak classifiers with high bias but low variance to produce a strong classifier with low bias and low variance. Each base learner has to be weak enough, otherwise, the first few classifiers will easily over-fit to the training data <ref type="bibr" target="#b4">[5]</ref>. However, the neural networks' fitting ability is too strong to be "high bias" and "low variance" for boosting strategy, making it hard to use deep models as weak learners. In this paper, our method exploits this over-fitting phenomenon, making biased weak features to over-fit the bias distribution. In the test stage, we only use the base model trained with the gradient of biased models, thus removing language bias in VQA.</p><p>On the other hand, the idea of approximating negative gradients is very similar to Gradient Boost <ref type="bibr" target="#b30">[31]</ref>. However, Gradient Boost has to greedily learn weak learners in turn.</p><p>This will be costly for complicated neural networks via back-propagation. We design two strategies, GGE-iteration and GGE-together, in which the learners are updated along with Batch SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The experiments are conducted on both language-bias sensitive VQA-CP v2 <ref type="bibr" target="#b0">[1]</ref> and standard VQA v2 <ref type="bibr" target="#b14">[15]</ref>. Considering there is no validation set for VQA-CP, we simply choose the model in the last training epoch for comparison in consequent experiments. More implementation details can be found in Section C in the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metrics</head><p>For each model, we compare Accuracy, the standard VQA evaluation metric <ref type="bibr" target="#b2">[3]</ref>. Moreover, a robust VQA model is expected to leverage both visual and language information, but good Accuracy is not enough to indicate the system is well visually grounded according to analysis in Sec. 3.</p><p>In <ref type="bibr" target="#b44">[45]</ref>, a new metric Correctly Predicted but Improperly Grounded (CPIG) is proposed to quantitatively assess visual grounding in VQA. An instance is regarded as correctly grounded if the ground-truth regions for the right answer (e.g., HAT <ref type="bibr" target="#b21">[22]</ref>) are within the model's top-N most sensitive visual regions. For convenience, we define 1 ? CP IG as CGR (Correct Grounding for Right prediction):</p><formula xml:id="formula_17">%CGR = N rg,rp N rp ? 100%,<label>(17)</label></formula><p>where N rp is the total number of right predictions, N rg,rp is the number of instances that are correctly answered with correct visual grounding. However, similar to results in <ref type="bibr" target="#b44">[45]</ref>, we find that CGR is not very discriminative across different methods as shown in <ref type="table" target="#tab_2">Table 2</ref> in Supplementary. The model with high CGR (e.g., UpDn) may not actually use enough visual information for classification. If a model locates the right object but still produces a wrong answer, it is a safe bet that it heavily relies on language bias instead of images for prediction. To quantitatively assess whether a model uses visual information for answer decision, we introduce CGW (Correct Grounding but Wrong prediction):</p><formula xml:id="formula_18">%CGW = N rg, wp N wp ? 100%,<label>(18)</label></formula><p>where N wp is the number of wrong predictions, and N rg,wp is the number of instances for which the model provides the right visual evidences but wrong prediction. Bad cases like example 2 and 3 from UpDn in <ref type="figure">Fig. 4</ref> are ignored by CGR but can be identified by high CGW.</p><p>For clearer comparison, we denote the difference of CGR and CGW as CGD (Correct Grounding Difference):</p><formula xml:id="formula_19">%CGD = %CGR ? %CGW.<label>(19)</label></formula><p>CGD only evaluates whether the visual information is taken in answer decision, which is parallel with Accuracy. The key idea for CGD is that a model actually makes use of visual information should not only provide the right predictions based on the correct visual-groundings but also a wrong answer due to improper visual evidence as well. Detailed CGR and GCD for all experiments are provided in <ref type="table" target="#tab_2">Table 2</ref> in Supplementary. It shows that UpDn, HINT inv and CSS-V inv achieve comparable performance on Accuracy but significantly degrade on CGD. This meets our intuitive analysis that these methods do not fully exploit visual information for the answer decision. Although the visual-grounding annotations are not so reliable for some instances 1 , CGD can offer statistically better distinction from the whole dataset level. More details for CGD are provided in Section B in the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-of-the-art Methods</head><p>We compare our best performed model GGE-DQ with existing state-of-the-art bias reduction techniques, including visual grounding-based methods HINT <ref type="bibr" target="#b43">[44]</ref>, SCR <ref type="bibr" target="#b46">[47]</ref>, ensemble-based methods AdvReg. <ref type="bibr" target="#b38">[39]</ref>, RUBi <ref type="bibr" target="#b6">[7]</ref>, LM (LMH) <ref type="bibr" target="#b9">[10]</ref>, MFE <ref type="bibr" target="#b13">[14]</ref>, new question encoding-based methods GVQE <ref type="bibr" target="#b27">[28]</ref>, DLP <ref type="bibr" target="#b33">[34]</ref>, counterfactual-based methods CF-VQA <ref type="bibr" target="#b33">[34]</ref>, CSS <ref type="bibr" target="#b7">[8]</ref> , and recent proposed regularization method MFE <ref type="bibr" target="#b13">[14]</ref>.</p><p>Experiments on VQA-CP test set aim to evaluate whether VQA models effectively reduce language bias. As shown in <ref type="table" target="#tab_2">Table 2</ref>, GGE-DQ achieves state-of-the-art performance without extra annotation. It outperforms the baseline model UpDn by 17% higher in Accuracy and 13% higher in CGD, which verifies the effectiveness of GGE on both answer classification and visual-grounding ability. Under the same base model UpDn, our method achieves the best performance in both Accuracy and CGD, with ? 5% gain comparing to all other methods, even competitive with methods that use stronger base models.</p><p>For the comparison of question-type-wise results, incor-  porating GGE reduces the biases and improves the performance for all the question-types, especially the more challenging "other" question type <ref type="bibr" target="#b45">[46]</ref>. CF-VQA <ref type="bibr" target="#b33">[34]</ref> performs the best in Y/N, but worse than our methods in all other metrics. LMH <ref type="bibr" target="#b9">[10]</ref>, LMH-MFE <ref type="bibr" target="#b13">[14]</ref> and LMH-CSS <ref type="bibr" target="#b7">[8]</ref> surpass other methods in Num., and LMH-CSS even slightly outperforms GGE-DQ in overall Accuracy due to high performance in Num. (40.73%). Comparing LM and LMH, it is obvious that the performance gains in Num. are due to the additional regularization for entropy. However, methods with entropy regularization drop nearly 10% on VQA v2. This indicates that these models may over-correct the bias and largely use "inverse language bias".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>In this section, we design various ablations to verify the effectiveness of greedy ensemble and our claim that distribution bias and question shortcut bias are two aspects of language bias. More results on VQA v2 are provided in Section D in the Supplementary.</p><p>The first group of ablations is to verify if the greedy ensemble can guarantee biased data is learned with biased models. We compare with other two ensemble strategies. SUM-DQ directly sums up the outputs of biased models and the base model. LMH+RUBi combines LMH <ref type="bibr" target="#b9">[10]</ref> and RUBi <ref type="bibr" target="#b6">[7]</ref>. It reduces distribution bias with LMH and shortcut bias with RUBi. The implementation details for these two ablations are provided in Section C in Supplementary.</p><p>As shown in <ref type="table">Table 5</ref>, SUM-DQ performs even worse than baseline. Meanwhile, the Accuracy of LMH+RUBi is just similar to that of LMH, and about 6% worse than GGE-DQ. This shows that GGE can really force the biased data to be sequentially learned with biased models. Instances that are easy to predict based on distribution or shortcut bias will be well fitted by the corresponding biased model. As a result, the base model has to pay more attention to hard examples and consider more visual information for final decision.</p><p>In the second group of experiments, we experimentally compare distribution bias and shortcut bias. The case analysis in <ref type="figure" target="#fig_3">Figure 3</ref> shows that GGE-D only uniforms predic- tions, which mainly improves Y/N as shown in <ref type="table">Table 5</ref>. B q works like "hard example mining" but will also introduce some noise (e.g. "mirror" and "no" in this example) due to inverse distribution bias. Reducing B d at the first stage can further encourage the discovery of the hard examples and force the base model to capture visual information. In <ref type="figure" target="#fig_3">Figure 3</ref>, the correct answer has higher confidence and the top predictions are all based on the image. As shown in Table 5, GGE-DQ surpasses single-bias versions by ?10%. This well verifies our claim that distribution bias and shortcut bias are two complementary aspects of language bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Generalization of GGE</head><p>Self-Ensemble. The performance of GGE largely depends on the predefined biased features, which requires prior knowledge of the task or dataset.In order to further discuss the generalization of GGE, we test a more flexible Self-Ensemble fashion (GGE-SF) on VQA-CP. GGE-SF takes the joint representation r i = m (e v (v i ), e q (q i )) itself as the biased feature instead of predefined question-only branch, the biased prediction is</p><formula xml:id="formula_20">B si = c s (r i ) ,<label>(20)</label></formula><p>where c s : r ? R C is the classifier of the biased model. The training process is the same as GGE-Q. As shown in <ref type="table" target="#tab_4">Table 4</ref>, GGE-SF still surpasses the baseline even without predefined biased features. This means that the base model itself can also be regarded as a biased model, as long as the tasks or datasets are biased enough. Moreover, if we first remove distribution bias with GGE-D before Self-Ensemble, the performance of GGE-D-SF is also comparable to existing state-of-the-art methods.</p><p>Generalization for Loss Function. For a fair comparison with previous work, we adopt Sigmoid+BCE loss for the above experiments. Actually, GGE is agnostic for classification losses. We provide extra experiments for Soft-max+CE loss in <ref type="table" target="#tab_4">Table 4</ref>. The implementation for GGE sxce is provided in Section A in the Supplementary.</p><p>Generalization for Base Model. GGE is also agnostic for base model choices. We provide extra experiments with BAN <ref type="bibr" target="#b26">[27]</ref> and S-MRL <ref type="bibr" target="#b6">[7]</ref> as base model. The results are provided in Section D in the Supplementary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>UpDn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GGE-DQ-iter</head><p>What is the object in the water? <ref type="figure">Figure 4</ref>. Qualitative Evaluation for GGE-DQ. We provide a comparison between UpDn and GGE-DQ on the visualization of the most sensitive regions and confidence of the top-5 answers. Red answers denote the ground-truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Evaluation</head><p>Examples in <ref type="figure">Figure 4</ref> illustrate how GGE-DQ makes of visual information for inference. From top to bottom, we provide three representative failure cases from baseline UpDn. The first example is about shortcut bias. Despite offering the right answer "yes", the prediction from UpDn is not based on the right visual grounding. On the contrary, GGE correctly grounds the giraffe that is eating leaves. The second example is about distribution bias. UpDn correctly grounds the curtain but still answers the question based on distribution bias ("flowers" is the most common answer for "what pattern..." in the train set). The last example is a case for reducing language prior apart from Yes/No questions. UpDn answers "boat" just based on the language context "in the water", while GGE-DQ provides correct answers "tv" and "television" with more salient visual grounding. These examples qualitatively verify our improvement in both Accuracy and visual explanation for the predictions. More examples and failure cases can be found in Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we experimentally analyse several methods for robust VQA and propose a new framework to reduce the language bias in VQA. We demonstrate that the language bias in VQA can be decomposed into distribution bias and shortcut bias and then propose a Greedy Gradient Ensemble strategy to removes such two kinds of preferences step by step. Experimental results demonstrate the rationality of our bias decomposition and the effectiveness of GGE. We believe the idea behind GGE is valuable and has the potential to become a generic method for dataset bias problems. In the future, we will extend GGE to solve bias problems for other tasks, provide a more rigorous analysis to guarantee model convergence, and learn to automatically detect different kinds of bias features without prior knowledge.</p><p>-Section A introduces A.1 GGE for Sigmoid+BCE loss (Section 4.1); A.2 GGE for Softmax+CE loss (Section 5.4); A.3 algorithm for GGE-iter and GGE-tog (Section 4.1).</p><p>-Section B provides more detailed settings for CGR, CGW, and CGD (Section 5.1).</p><p>-Section C provides C.1 implementation details for the base model; C.2 and ablations for ensemble strategy, SUM-DQ and LMH+RUBi (Section 5.3).</p><p>-Section D provides D.1 ablation studies for base model S-MRL and BAN (Section 5.3); D.2 comparison between Self-Ensemble GGE and RUBi; D.3 additional experimental results (Section 3.2 and 5.1), including Accuracy on VQA v2 and CGR/CGD for all implemented methods.</p><p>-Section E provides more quantitative examples and failure cases from GGE-DQ (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details for GGE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Sigmoid+BCE</head><p>For classification problem with BCE loss, the negative gradient is shown in Eq. 7 in the main paper</p><formula xml:id="formula_21">? ?L(H m,i ) = 2y m,i ? (?2y m,i H m,i ) .<label>(1)</label></formula><p>If y m,i = 0, the gradient will always be 0. If the label y m,i = 1, we plot the change of negative gradient versus prediction H m,i .</p><p>As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, the gradient will continuously decrease when biased models can predict the right answers with higher confidence. This means the base model will pay more attention to samples that are hard to solve by biased models.</p><p>In practice, we clip ?(H m,i ) &gt; 0 with Sigmoid function, to make the range of ??L(H m,i ) consistent with the label space [0,1] of BCE loss. B d is a statistic answer distribution of the training set, which satisfies B d &gt; 0. Therefore, we do not need to add Sigmoid function on distribution bias in Eq.11-15 in the main paper.</p><p>However, clipping the gradient does not directly increase the scale of hard samples but only lowers the scale of easy ones, resulting in performance degradation on VQA v2. Actually, for the hard samples, the gradient can be up to 2.0 without clip operation. If we can design a new classification loss with label space [0, 2] in place of BCE, it may be an alternative approach to deal with this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Softmax+CE</head><p>We provide GGE optimized with Softmax+CE loss in Section 5.4. The loss function can be written as </p><formula xml:id="formula_22">L(Z, Y ) = ? C i=1 y i log(? i ),<label>(2)</label></formula><formula xml:id="formula_23">with ? i = e zi C j=1 z j ,<label>(3)</label></formula><p>where Z = {z i } C i=1 is the predicted logits, C is the number of classes, and y i ? [0, 1] is the ground truth labels. The negative gradient of loss function is</p><formula xml:id="formula_24">? ?L(z i ) = y i ? ? i .<label>(4)</label></formula><p>Similar to implementation of Sigmoid+BCE, we directly clip the ?L(z i ) to the label space [0,1]</p><formula xml:id="formula_25">? ?L(z i ) = y i ? ? i y i &gt; 0 0 y i = 0 .<label>(5)</label></formula><p>As a result, if y i = 0 the pseudo labelL(z i ) will still be 0, otherwise, it will decrease when biased models can predict the right answer with higher confidence. The optimization process is the same with that in Sigmoid+BCE. Additionally, since the statistical distribution B d ? (0, 1), we treat ? i = B di when calculate the gradient in GGE-D and GGE-DQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. GGE-Iter and GGE-tog</head><p>In Section 4.1 we provide two optimization schemes GGE-iteration and GGE-together. The detailed implemen-</p><formula xml:id="formula_26">Algorithm 1: GGE-iteration Input: Observations X, Lables Y , Biased features Observations B = {B m } M m=1 , Base function f (.|?) : X ? R |Y | , Bias functions {h m (.|? i ) : B i ? R |Y | } M m=1 Initialize: H 0 = 0 ; for Batch t = 1 . . . T do for m = 1 . . . M do L m (? m ) ? L (h m (B m ; ? m ), ??L(H m?1 , Y )) Update ? m ? ? m ? ?? ?m L m (? m ) end L M +1 (?) ? L (f (X; ?), ??L(H M , Y )) Update ? ? ? ? ?? ? L M +1 (?) end return Y = f (X; ?) Algorithm 2: GGE-together Input: Observations X, Lables Y , Biased features Observations B = {B m } M m=1 , Base function f (.|?) : X ? R |Y | , Bias functions {h m (.|? i ) : B i ? R |Y | } M m=1 Initialize: H 0 = 0 ; for Batch t = 1 . . . T do for m = 1 . . . M do L m (? m ) ? L (h m (B m ; ? m ), ??L(H m?1 , Y )) end L M +1 (?) ? L (f (X; ?), ??L(H M , Y )) L(?) ? M +1 m=1 L m Update ? ? ? ? ?? ? L(?) end return Y = f (X; ?)</formula><p>tation is shown in Algorithm 1 and 2. Two variants of implementation do not show an obvious performance gap in most experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details for CGD</head><p>First, we should stress that CGD only evaluates whether the visual information is taken for answer prediction, which is parallel with Accuracy and different from metrics in Referring Expression and Visual Grounding tasks. It is proposed to help quantitatively evaluate models' grounding ability.</p><p>CGD considers the top-N most sensitive visual region. In this paper, we evaluate the sensitivity via attention. In <ref type="figure">Figure 6</ref>, we plot change of CGR, CGW and CGD with different threshold for prevailing methods UpDn <ref type="bibr" target="#b1">[2]</ref>, RUBi <ref type="bibr" target="#b6">[7]</ref>, LMH <ref type="bibr" target="#b9">[10]</ref> CSS <ref type="bibr" target="#b7">[8]</ref> and CSS-V inv?hat . We set attention threshold t ? {0.1, 0.2, 0.3, 0.4}, which indicates that top-N is no more than {9, 4, 3, 2}.</p><p>We choose to consider top-4 (t = 0.2) objects for CGD, since many questions need to consider multiple objects and t = 0.2 is the most discriminative threshold as shown in <ref type="figure">Figure 6</ref>(c). Apart from attention, Grad-CAM <ref type="bibr" target="#b42">[43]</ref> can be an alternative for grounding evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details for Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Base Model</head><p>We use the publicly available reimplementation of UpDn 2 <ref type="bibr" target="#b1">[2]</ref> for our baseline architecture, data preprocess and optimization.</p><p>Image Encoder. Following the popular bottom-up attention mechanism <ref type="bibr" target="#b1">[2]</ref>, we use a Faster R-CNN <ref type="bibr" target="#b39">[40]</ref> based framework to extract visual features. We select the top-36 region proposals for each image v ? R 36?2048 .</p><p>Question Encoder. Each word is first initialized by 300dim GloVe word embeddings <ref type="bibr" target="#b35">[36]</ref>, then fed into a GRU with 1024-d hidden vector. The question representation is the last state of GRU h T ? R 1024 .</p><p>Multi-modal Fusion. We use traditional linear attention between h T and v for visual representation. and the final representation for classification is the Hadamard product of vision and question representation.</p><p>Question-only Classifier. The question-only classifier is implemented as two fully-connected layers with ReLU activations. The input question representation is shared with that in VQA base model.</p><p>Question types. We use 65 question types annotated in VQA v2 and VQA-CP, according to the first few words of the question (e.g., "What color is"). To save the training time, we simply use statistic answer distribution conditioned by question type in the train set as the prediction of distribution bias.</p><p>Optimization. Following UpDn <ref type="bibr" target="#b1">[2]</ref>, all the experiments are conducted with the Adamax optimizer for 20 epochs with learning rate initialized as 0.001. We train all models on a single RTX 3090 GUP with PyTorch 1.7 <ref type="bibr" target="#b34">[35]</ref> and batch size 512.</p><p>Data Preprocessing. Following previous works, we filter the answers that appear less than 9 times in the train set. For each instance with 10 annotated answers, we set the scores for labels that appear 1/2/3 times as 0.3/0.6/0.9, more than 3 times as 1.0. model is</p><formula xml:id="formula_27">L = L(B d + ?(B q ) + ?(?), A).<label>(6)</label></formula><p>LMH+RUBi. LMH <ref type="bibr" target="#b9">[10]</ref> and RUBi <ref type="bibr" target="#b6">[7]</ref> are methods that can only reduce a single type of bias. LMH+RUBi is a direct combination of LMH and RUBi. It reduces distribution bias with LMH and shortcut bias with RUBi step by step. The loss for RUBi is written as <ref type="bibr" target="#b6">(7)</ref> where G q = g(e q (q i )), g(.)Q ? R C . Combining with LMH, we compose A as</p><formula xml:id="formula_28">L rubi (?, A) = L(? ?(G q ),?) + L(c q (G q ), A),</formula><formula xml:id="formula_29">F (A, B, M ) = log A + g(M ) log B,<label>(8)</label></formula><p>where M and B are the fused feature and the bias in LMH.</p><p>The combined loss function is</p><formula xml:id="formula_30">L = L rubi (F (A, B, M ),?) + wH(g(M ) log B),<label>(9)</label></formula><p>where H(.) is the entropy and w is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Supplementary Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Ablations of Base Models</head><p>We do experiments on other base models BAN <ref type="bibr" target="#b26">[27]</ref> and S-MRL <ref type="bibr" target="#b6">[7]</ref>. The models are re-implemented based on officially released codes. For BAN, we set the number of Bilinear Attention blocks as 3. We choose the last bi-linear attention map of BAN and sum up along the question axis, which is referred to as the object attention for CGR and CGW. Although Accuracy of our reproduced S-MRL is a litter lower than that in <ref type="bibr" target="#b6">[7]</ref>, GGE-DQ can improve the Accuracy over 10% and surpass most of the existing methods. As shown in the table, GGE is a model-agnostic de-bias method, which can improve all three base models UpDn <ref type="bibr" target="#b1">[2]</ref>, S-MRL <ref type="bibr" target="#b6">[7]</ref> and BAN <ref type="bibr" target="#b26">[27]</ref> by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Self-Ensemble Comparison</head><p>We provide an additional experiment for RUBi <ref type="bibr" target="#b6">[7]</ref> with Self-Ensemble fashion. The input of the question-only branch is replaced by the joint representation from the base model. As shown in <ref type="table">Table 6</ref>, RUBi-SF is even worse than baseline UpDn on both VQA-CP v2 and VQA v2. On the contrary, Accuracy of GGE-SF is comparable to GGE-Q, which further demonstrates the generalization of GGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Additional Experimental Results</head><p>We provide detailed CGR, CGW, and results on VQA-CP and VQA v2 for all re-implemented methods in Section 3 and Section 5.</p><p>As shown <ref type="table">Table 6</ref>, GGE-DQ largely improves more challenging "Others" question type <ref type="bibr" target="#b45">[46]</ref>. This means that GGE-DQ really focuses on images largely rather than only relying on "inverse language bias" for higher Accuracy. Moreover, Inverse-Supervision strategy does not improve GGE-DQ-tog (GGE-DQ-tog is in <ref type="table">Table 6</ref>), which also demonstrates that GGD-DQ better reduces distribution bias compared with other methods.</p><p>There are still some issues about language bias that deserves further consideration. First, both GGE-D sxce and GGE-Q sxce are robust on VQA v2 but GGE-DQ sxce drops a lot. We think the softmax function will amplify the gradient of biased models and over-estimate the dataset biases. Second, LMH+RUBi performs much better than both LMH and RUBi on VQA v2. This can bring further research into the relationship between distribution bias and shortcut bias. Third, UpDn is does not degrade a lot in VQA v2, which indicates some entanglement between entropy regularization and Inverse-Supervision strategy.</p><p>Moreover, we find that GGE also suffers from degradation on in-distribution data (VQA v2) similar to previous ensemble-based methods. This indicates that the model may over-estimate the bias for some instances. We speculate that it is due to too small scale of the gradient for some samples easy to fit by distribution bias or shortcut bias. How to control the over-fitting "degree" and scale up pseudo labels are potential research directions in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Qualitative Results</head><p>In this section, we provide more examples from GGE-DQ in <ref type="figure">Figure 7</ref> and some failure cases in <ref type="figure">Figure 8</ref>.  . The model is still weak in counting problem and questions that hardly appear in the train set (upper row). Some failure case are due to missing annotation in the dataset, since "outside" and "decoration" can also be regarded as the right answers (middle row). The last row shows that answers for failure cases are still consistent with visual explanations rather than language bias, which is identified by low CGW and indicates GGE-DQ really has better visual-grounding ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Two aspects of language bias in VQA. (a) Distribution Bias: The answer distribution for certain question type is significantly long-tailed. (b) Shortcut Bias: The correct answers produced by the model may rely on the question-answer shortcut rather than proper visual grounding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Different versions of GGE. V, Q and? denote image, question, and answer prediction respectively. A is the human-annotated labels. B d and Bq indicate the prediction from distribution bias and question shortcut bias respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Q?What object is the focal point</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Predicted distribution for three variants of GGE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>??L(H m,i ) vs. H m,i ??L(H m,i ) vs. ?(H m,i ) Negative Gradient versus Predictions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>C. 2 .Figure 6 .</head><label>26</label><figDesc>Ablations for Ensemble SUM-DQ. SUM-DQ ablation is to verify if GGE can learn biased data with biased models. The loss for the whole 2 https://github.com/hengyuan-hu/bottom-up-attention-vqa CGR, CGW and CGD versus attention threshold for prevailing methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>More examples from GGE-DQ. The model can successfully provide the right prediction with right evidences. Q?How many bananas are there? Failure Cases. Most of the failure cases still match their visual explanations (Wrong predictions with corresponding wrong evidences)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experimental analysis for representative methods on VQA-CP and VQA v2. Footnote inv stands for Inverse Grounding Annotation, vo for Vision-only, and is for Inverse-Supervision.</figDesc><table><row><cell>Method</cell><cell>VQA-CP</cell><cell>VQA 2.0</cell></row><row><cell>UpDn [2]</cell><cell>39.89</cell><cell>63.79</cell></row><row><cell>HINT [44]</cell><cell>47.50</cell><cell>63.38</cell></row><row><cell>RUBi [7]</cell><cell>45.42</cell><cell>58.19</cell></row><row><cell>LMH [10]</cell><cell>52.73</cell><cell>56.35</cell></row><row><cell>CSS [8]</cell><cell>58.11</cell><cell>53.15</cell></row><row><cell>HINT inv</cell><cell>47.20</cell><cell>60.33</cell></row><row><cell>CSS-V inv</cell><cell>58.05</cell><cell>54.39</cell></row><row><cell>UpDn vo</cell><cell>33.18</cell><cell>45.67</cell></row><row><cell>RUBi vo</cell><cell>23.53</cell><cell>46.11</cell></row><row><cell>LMH vo</cell><cell>43.68</cell><cell>27.18</cell></row><row><cell>UpDn vo,is</cell><cell>39.44</cell><cell>40.03</cell></row><row><cell>UpDn is</cell><cell>42.12</cell><cell>60.85</cell></row><row><cell>RUBi is</cell><cell>48.42</cell><cell>59.10</cell></row><row><cell>LMH is</cell><cell>58.12</cell><cell>43.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Experimental results on VQA-CP v2 test set and VQA v2 val set of state-of-the-art methods. Best and second performance are highlighted in each column. Methods with * use extra annotations (e.g., human attention (HAT), explanations (VQA-X), or object label information). Methods with CGD are our reimplementation using released codes. Other results are reported in the original papers.</figDesc><table><row><cell>Method</cell><cell>Base</cell><cell>All</cell><cell>Y/N</cell><cell>VQA-CP test Num.</cell><cell>Others</cell><cell>CGD</cell><cell>All</cell><cell cols="2">VQA v2 val Y/N Num.</cell><cell>Others</cell></row><row><cell>GVQA [1]</cell><cell>-</cell><cell>31.30</cell><cell>57.99</cell><cell>13.68</cell><cell>22.14</cell><cell>-</cell><cell>48.24</cell><cell>72.03</cell><cell>31.17</cell><cell>34.65</cell></row><row><cell>UpDn [2]</cell><cell>-</cell><cell>39.89</cell><cell>43.01</cell><cell>12.07</cell><cell>45.82</cell><cell>3.91</cell><cell>63.79</cell><cell>80.94</cell><cell>42.51</cell><cell>55.78</cell></row><row><cell>S-MRL [7]</cell><cell>-</cell><cell>38.46</cell><cell>42.85</cell><cell>12.81</cell><cell>43.20</cell><cell>-</cell><cell>63.10</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HINT* [44]</cell><cell>UpDn</cell><cell>47.50</cell><cell>67.21</cell><cell>10.67</cell><cell>46.80</cell><cell>10.34</cell><cell>63.38</cell><cell>81.18</cell><cell>42.14</cell><cell>55.66</cell></row><row><cell>SCR* [47]</cell><cell>UpDn</cell><cell>49.45</cell><cell>72.36</cell><cell>10.93</cell><cell>48.02</cell><cell>-</cell><cell>62.2</cell><cell>78.8</cell><cell>41.6</cell><cell>54.4</cell></row><row><cell>AdvReg. [39]</cell><cell>UpDn</cell><cell>41.17</cell><cell>65.49</cell><cell>15.48</cell><cell>35.48</cell><cell>-</cell><cell>62.75</cell><cell>79.84</cell><cell>42.35</cell><cell>55.16</cell></row><row><cell>RUBi [7]</cell><cell>UpDn</cell><cell>45.42</cell><cell>63.03</cell><cell>11.91</cell><cell>44.33</cell><cell>6.27</cell><cell>58.19</cell><cell>63.04</cell><cell>41.00</cell><cell>54.43</cell></row><row><cell>LM [10]</cell><cell>UpDn</cell><cell>48.78</cell><cell>70.37</cell><cell>14.24</cell><cell>46.42</cell><cell>11.33</cell><cell>63.26</cell><cell>81.16</cell><cell>42.22</cell><cell>55.22</cell></row><row><cell>LMH [10]</cell><cell>UpDn</cell><cell>52.73</cell><cell>72.95</cell><cell>31.90</cell><cell>47.79</cell><cell>10.60</cell><cell>56.35</cell><cell>65.06</cell><cell>37.63</cell><cell>54.69</cell></row><row><cell>DLP [23]</cell><cell>UpDn</cell><cell>48.87</cell><cell>70.99</cell><cell>18.72</cell><cell>45.57</cell><cell>-</cell><cell>57.96</cell><cell>76.82</cell><cell>39.33</cell><cell>48.54</cell></row><row><cell>GVQE* [28]</cell><cell>UpDn</cell><cell>48.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.04</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CSS* [8]</cell><cell>UpDn</cell><cell>41.16</cell><cell>43.96</cell><cell>12.78</cell><cell>47.48</cell><cell>8.23</cell><cell>59.21</cell><cell>72.97</cell><cell>40.00</cell><cell>55.13</cell></row><row><cell>CF-VQA(Sum) [34]</cell><cell>UpDn</cell><cell>53.69</cell><cell>91.25</cell><cell>12.80</cell><cell>45.23</cell><cell>-</cell><cell>63.65</cell><cell>82.63</cell><cell>44.01</cell><cell>54.38</cell></row><row><cell>GGE-DQ-iter (Ours)</cell><cell>UpDn</cell><cell>57.12</cell><cell>87.35</cell><cell>26.16</cell><cell>49.77</cell><cell>16.44</cell><cell>59.30</cell><cell>73.63</cell><cell>40.30</cell><cell>54.29</cell></row><row><cell>GGE-DQ-tog (Ours)</cell><cell>UpDn</cell><cell>57.32</cell><cell>87.04</cell><cell>27.75</cell><cell>49.59</cell><cell>15.27</cell><cell>59.11</cell><cell>73.27</cell><cell>39.99</cell><cell>54.39</cell></row><row><cell>RUBi [7]</cell><cell>S-MRL</cell><cell>47.11</cell><cell>68.65</cell><cell>20.28</cell><cell>43.18</cell><cell>-</cell><cell>61.16</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GVQE* [28]</cell><cell>S-MRL</cell><cell>50.11</cell><cell>66.35</cell><cell>27.08</cell><cell>46.77</cell><cell>-</cell><cell>63.18</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CF-VQA(Sum) [34]</cell><cell>S-MRL</cell><cell>54.95</cell><cell>90.56</cell><cell>21.88</cell><cell>45.36</cell><cell>-</cell><cell>60.76</cell><cell>81.11</cell><cell>43.48</cell><cell>49.58</cell></row><row><cell>MFE [14]</cell><cell>LMH</cell><cell>54.55</cell><cell>74.03</cell><cell>49.16</cell><cell>45.82</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CSS* [8]</cell><cell>LMH</cell><cell>58.21</cell><cell>83.65</cell><cell>40.73</cell><cell>48.14</cell><cell>8.81</cell><cell>53.15</cell><cell>61.20</cell><cell>37.65</cell><cell>53.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for different versions of GGE on VQA-CP v2 test set. Best results are highlighted in the columns. DQ-iter 57.12 87.35 49.77 26.16 16.44 GGE-DQ-tog 57.32 87.04 49.59 27.75 15.27</figDesc><table><row><cell>Method</cell><cell>All</cell><cell>Y/N Others Num. CGD</cell></row><row><cell>Baseline</cell><cell cols="2">39.89 43.01 45.80 11.88 3.91</cell></row><row><cell>SUM-DQ</cell><cell cols="2">35.46 42.66 38.01 12.38 3.10</cell></row><row><cell>LMH+RUBi</cell><cell cols="2">51.54 74.55 47.41 22.65 6.12</cell></row><row><cell>GGE-D</cell><cell cols="2">48.27 70.75 47.53 13.42 14.31</cell></row><row><cell>GGE-Q-iter</cell><cell cols="2">43.72 48.17 48.78 14.24 6.70</cell></row><row><cell>GGE-Q-tog</cell><cell cols="2">44.62 47.64 48.89 14.34 6.63</cell></row><row><cell>GGE-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Variants of GGE on VQA-CP v2. SF stands for Self-Ensemble, sxce denotes models trained with softmax+CE loss.</figDesc><table><row><cell>Method</cell><cell>All</cell><cell>Y/N</cell><cell cols="2">Others Num.</cell></row><row><cell>UpDn</cell><cell cols="2">39.89 43.01</cell><cell>45.80</cell><cell>11.88</cell></row><row><cell>UpDn sxce</cell><cell cols="2">41.37 45.96</cell><cell>46.90</cell><cell>12.46</cell></row><row><cell>GGE-SF-iter</cell><cell cols="2">44.53 50.98</cell><cell>48.90</cell><cell>18.24</cell></row><row><cell>GGE-SF-tog</cell><cell cols="2">43.10 49.90</cell><cell>47.33</cell><cell>17.74</cell></row><row><cell>GGE-D-SF-iter</cell><cell cols="2">56.33 86.43</cell><cell>49.32</cell><cell>24.37</cell></row><row><cell>GGE-D-SF-tog</cell><cell cols="2">52.86 76.25</cell><cell>49.46</cell><cell>20.56</cell></row><row><cell>GGE sxce -D</cell><cell cols="2">53.98 86.06</cell><cell>47.85</cell><cell>15.09</cell></row><row><cell>GGE sxce -Q-iter</cell><cell cols="2">52.98 82.27</cell><cell>48.06</cell><cell>14.97</cell></row><row><cell>GGE sxce -Q-tog</cell><cell cols="2">52.99 81.86</cell><cell>47.97</cell><cell>16.11</cell></row><row><cell cols="3">GGE sxce -DQ-iter 56.25 85.08</cell><cell>48.56</cell><cell>24.78</cell></row><row><cell cols="3">GGE sxce -DQ-tog 55.84 84.47</cell><cell>48.76</cell><cell>26.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Ablations of base model BAN and S-MRL. Extra experimental results for Section 3 and Section 5. DQ-iter 56.25 85.08 24.78 48.56 43.13 29.52 13.61 52.38 54.51 39.93 54.07 GGEsxce-DQ-tog 55.84 84.47 26.96 48.76 41.41 31.02 10.39 52.17 54.17 40.10 53.85</figDesc><table><row><cell>Method</cell><cell></cell><cell>All</cell><cell>Y/N</cell><cell></cell><cell>Num.</cell><cell cols="2">VQA-CP test Others</cell><cell>?CGR</cell><cell>?CGW</cell><cell>?CGD</cell></row><row><cell>S-MRL [7]</cell><cell></cell><cell>37.90</cell><cell>43.68</cell><cell></cell><cell>12.04</cell><cell>41.97</cell><cell></cell><cell>41.94</cell><cell>27.32</cell><cell>14.62</cell></row><row><cell cols="2">+GGE-DQ-tog</cell><cell>54.62</cell><cell>76.11</cell><cell></cell><cell>18.04</cell><cell>47.70</cell><cell></cell><cell>35.61</cell><cell>18.17</cell><cell>17.44</cell></row><row><cell cols="2">+GGE-DQ-iter</cell><cell>54.03</cell><cell>79.66</cell><cell></cell><cell>20.77</cell><cell>46.72</cell><cell></cell><cell>38.10</cell><cell>22.42</cell><cell>15.68</cell></row><row><cell>BAN [27]</cell><cell></cell><cell>35.94</cell><cell>40.39</cell><cell></cell><cell>12.24</cell><cell>40.51</cell><cell></cell><cell>5.33</cell><cell>5.19</cell><cell>0.14</cell></row><row><cell cols="2">+GGE-DQ-tog</cell><cell>51.91</cell><cell>81.37</cell><cell></cell><cell>21.85</cell><cell>45.46</cell><cell></cell><cell>36.93</cell><cell>27.10</cell><cell>9.83</cell></row><row><cell cols="2">+GGE-DQ-iter</cell><cell>50.75</cell><cell>74.56</cell><cell></cell><cell>20.59</cell><cell>46.54</cell><cell></cell><cell>20.87</cell><cell>16.85</cell><cell>4.98</cell></row><row><cell>Method</cell><cell>All</cell><cell>Y/N</cell><cell cols="3">VQA-CP test Num. Others CGR</cell><cell cols="3">CGW CGD</cell><cell>All</cell><cell>VQA v2 val Y/N Num. Others</cell></row><row><cell>UpDn [2]</cell><cell cols="3">39.89 43.01 12.07</cell><cell>45.82</cell><cell cols="2">44.27 40.63</cell><cell cols="2">3.91</cell><cell>63.79 80.94 42.51</cell><cell>55.78</cell></row><row><cell>HINT [44]</cell><cell cols="3">47.50 67.21 10.67</cell><cell>46.80</cell><cell cols="4">45.21 34.87 10.34</cell><cell>63.38 81.18 42.14</cell><cell>55.66</cell></row><row><cell>RUBi [7]</cell><cell cols="3">45.42 63.03 11.91</cell><cell>44.33</cell><cell cols="2">39.60 33.33</cell><cell cols="2">6.27</cell><cell>55.19 61.04 41.00</cell><cell>54.43</cell></row><row><cell>LM [10]</cell><cell cols="3">48.78 70.37 14.24</cell><cell>46.42</cell><cell cols="4">47.30 35.97 11.33</cell><cell>63.26 81.16 42.22</cell><cell>55.22</cell></row><row><cell>LMH [10]</cell><cell cols="3">52.73 72.95 31.90</cell><cell>47.79</cell><cell cols="4">46.44 35.84 10.60</cell><cell>56.35 65.06 37.63</cell><cell>54.69</cell></row><row><cell>CSS-V [8]</cell><cell cols="3">57.91 80.36 50.45</cell><cell>47.83</cell><cell cols="4">42.72 31.28 11.44</cell><cell>53.94 57.48 55.37</cell><cell>38.39</cell></row><row><cell>CSS [8]</cell><cell cols="3">58.11 83.65 40.73</cell><cell>48.14</cell><cell cols="2">46.70 37.89</cell><cell cols="2">8.81</cell><cell>53.15 61.20 37.65</cell><cell>53.36</cell></row><row><cell>HINTinv</cell><cell cols="3">47.20 67.23 13.21</cell><cell>46.15</cell><cell cols="2">42.01 39.11</cell><cell cols="2">2.90</cell><cell>60.33 74.36 40.31</cell><cell>55.12</cell></row><row><cell>CSS-Vinv</cell><cell cols="3">58.05 79.84 52.24</cell><cell>47.23</cell><cell cols="2">41.38 34.93</cell><cell cols="2">6.45</cell><cell>54.39 58.73 38.81</cell><cell>55.23</cell></row><row><cell>UpDnis</cell><cell cols="3">42.12 45.81 12.98</cell><cell>47.02</cell><cell cols="2">44.52 39.59</cell><cell cols="2">4.93</cell><cell>62.85 80.34 42.00</cell><cell>55.08</cell></row><row><cell>RUBiis</cell><cell cols="3">48.16 72.34 12.69</cell><cell>45.22</cell><cell cols="4">47.55 33.73 13.83</cell><cell>59.10 76.67 41.09</cell><cell>50.50</cell></row><row><cell>LMHis</cell><cell cols="3">58.12 79.73 53.41</cell><cell>48.01</cell><cell cols="2">39.51 30.82</cell><cell cols="2">8.69</cell><cell>43.29 33.22 34.14</cell><cell>53.40</cell></row><row><cell>GGE-DQ-togis</cell><cell cols="3">54.64 85.47 23.43</cell><cell>47.64</cell><cell cols="4">40.47 25.81 14.66</cell><cell>57.16 70.43 38.00</cell><cell>52.13</cell></row><row><cell>SUM-DQ</cell><cell cols="3">35.46 42.66 12.38</cell><cell>38.01</cell><cell cols="2">41.28 38.18</cell><cell cols="2">3.91</cell><cell>56.85 81.09 38.55</cell><cell>43.25</cell></row><row><cell>LMH+RUBi</cell><cell cols="3">51.54 74.55 22.65</cell><cell>47.41</cell><cell cols="2">46.67 40.55</cell><cell cols="2">6.12</cell><cell>60.68 77.91 39.10</cell><cell>53.15</cell></row><row><cell>GGE-D</cell><cell cols="3">48.27 70.75 13.42</cell><cell>47.53</cell><cell cols="4">38.79 24.48 14.31</cell><cell>62.79 79.24 42.31</cell><cell>55.71</cell></row><row><cell>GGE-Q-iter</cell><cell cols="3">43.72 48.17 14.24</cell><cell>48.78</cell><cell cols="2">43.74 37.04</cell><cell cols="2">6.70</cell><cell>61.23 78.28 41.42</cell><cell>53.50</cell></row><row><cell>GGE-Q-tog</cell><cell cols="3">44.62 47.64 14.34</cell><cell>48.89</cell><cell cols="2">45.19 38.56</cell><cell cols="2">6.63</cell><cell>62.14 78.64 40.72</cell><cell>54.21</cell></row><row><cell>GGE-DQ-iter</cell><cell cols="3">57.12 87.35 26.16</cell><cell>49.77</cell><cell cols="4">44.35 27.91 16.44</cell><cell>59.30 73.63 40.30</cell><cell>54.29</cell></row><row><cell>GGE-DQ-tog</cell><cell cols="3">57.32 87.04 27.75</cell><cell>49.59</cell><cell cols="4">42.74 27.47 15.27</cell><cell>59.11 73.27 39.99</cell><cell>54.39</cell></row><row><cell>RUBi-SF</cell><cell cols="3">37.53 43.27 14.11</cell><cell>41.07</cell><cell cols="2">39.30 32.66</cell><cell cols="2">7.14</cell><cell>55.06 70.85 30.97</cell><cell>49.44</cell></row><row><cell>GGE-SF-iter</cell><cell cols="3">44.53 50.98 18.24</cell><cell>48.90</cell><cell cols="2">45.07 38.99</cell><cell cols="2">6.08</cell><cell>60.66 74.93 41.14</cell><cell>52.95</cell></row><row><cell>GGE-SF-tog</cell><cell cols="3">43.10 49.90 17.74</cell><cell>47.33</cell><cell cols="2">42.40 35.85</cell><cell cols="2">6.55</cell><cell>59.00 73.71 41.14</cell><cell>52.54</cell></row><row><cell>GGE-D-SF-iter</cell><cell cols="3">56.33 86.43 23.37</cell><cell>49.32</cell><cell cols="4">43.77 29.30 14.47</cell><cell>62.03 80.73 41.79</cell><cell>53.14</cell></row><row><cell>GGE-D-SF-tog</cell><cell cols="3">52.86 76.25 20.56</cell><cell>49.46</cell><cell cols="4">42.48 30.25 12.23</cell><cell>59.00 73.71 41.14</cell><cell>52.54</cell></row><row><cell>UpDnsxce</cell><cell cols="3">41.37 45.96 12.46</cell><cell>46.90</cell><cell cols="2">42.81 40.90</cell><cell cols="2">1.91</cell><cell>63.38 81.26 43.13</cell><cell>55.14</cell></row><row><cell>GGEsxce-D</cell><cell cols="3">53.98 86.06 15.09</cell><cell>47.85</cell><cell cols="2">37.45 30.52</cell><cell cols="2">6.93</cell><cell>62.34 79.17 41.50</cell><cell>55.06</cell></row><row><cell>GGEsxce-Q-iter</cell><cell cols="3">52.98 82.27 14.97</cell><cell>48.06</cell><cell cols="2">40.64 31.55</cell><cell cols="2">9.09</cell><cell>61.76 78.57 42.01</cell><cell>54.20</cell></row><row><cell>GGEsxce-Q-tog</cell><cell cols="3">52.99 81.86 16.11</cell><cell>47.97</cell><cell cols="2">41.01 32.62</cell><cell cols="2">8.39</cell><cell>61.38 77.53 42.30</cell><cell>54.14</cell></row><row><cell>GGEsxce-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Not all examples in VQA v2 are annotated in VQAX<ref type="bibr" target="#b10">[11]</ref>. Moreover, visual grounding for some instances are hard to evaluate (e.g., questions that require global image information or without referring objects)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05656,2020.3</idno>
		<title level="m">Why is attention not so attentive</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Some theory for generalized boosting algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Ya&amp;apos;acov Ritov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="705" to="732" />
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reducing unimodal biases for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterfactual samples synthesizing for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="90" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boosting a weak learning algorithm by majority. Information and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="256" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-modality latent interaction network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (2)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretable visual reasoning via probabilistic formulation under natural supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="553" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explainable neural computation via stack neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language-conditioned graph networks for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10294" to="10303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal explanations: Justifying decisions and pointing to the evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overcoming language priors in vqa via decomposed linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11181" to="11188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analysis of visual question answering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1965" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for long-tailed recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reducing language biases in visual question answering with visually-grounded question encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Gouthaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06198</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end bias mitigation by modelling biases in corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Rabeeh Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Boosting algorithms as gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llew</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus R Frean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="512" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from failure: De-biasing classifier from biased classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuntak</forename><surname>Jun Hyun Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Conference on Neural Information Processing Systems (NeurIPS) 2020. Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04315</idno>
		<title level="m">Counterfactual vqa: A cause-effect look at language bias</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exploring human-like attention supervision in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06308</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Overcoming language priors in visual question answering with adversarial regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1541" to="1551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The strength of weak learnability. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schapire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="197" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Taking a hint: Leveraging explanations to make vision and language models more grounded</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A negative case analysis of visual grounding methods for VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robik</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On the value of out-of-distribution testing: An example of goodhart&apos;s law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robik</forename><surname>Shrestha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09241</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ehsan Abbasnejad, Christopher Kanan, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-critical reasoning for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8604" to="8614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5014" to="5022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interpretable visual question answering by visual grounding from attention supervision mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
