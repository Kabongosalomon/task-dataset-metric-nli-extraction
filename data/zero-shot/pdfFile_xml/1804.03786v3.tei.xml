<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonlinear 3D Face Morphable Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
							<email>tranluan@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nonlinear 3D Face Morphable Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D Morphable Model (3DMM) is a statistical model of 3D facial shape and texture in a space where there are explicit correspondences <ref type="bibr" target="#b3">[4]</ref>. The morphable model framework provides two key benefits: first, a point-to-point correspondence between the reconstruction and all other models, enabling morphing, and second, modeling underlying transformations between types of faces (male to female, neutral to smile, etc.). 3DMM has been widely applied in numerous areas, such as computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44]</ref>, graphics <ref type="bibr" target="#b0">[1]</ref>, human behavioral analysis <ref type="bibr" target="#b1">[2]</ref> and craniofacial surgery <ref type="bibr" target="#b33">[34]</ref>.</p><p>3DMM is learnt through supervision by performing dimension reduction, normally Principal Component Anal- <ref type="bibr" target="#b0">1</ref> Project page: http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html shape/texture, which are trained with 3D face scans and associated controlled 2D images. We propose a nonlinear 3DMM to model shape/texture via deep neural networks (DNNs). It can be trained from in-the-wild face images without 3D scans, and also better reconstructs the original images due to the inherent nonlinearity. ysis (PCA), on a training set of face images/scans. To model highly variable 3D face shapes, a large amount of high-quality 3D face scans is required. However, this requirement is expensive to fulfill. The first 3DMM <ref type="bibr" target="#b3">[4]</ref> was built from scans of 200 subjects with a similar ethnicity/age group. They were also captured in well-controlled conditions, with only neutral expressions. Hence, it is fragile to large variances in the face identity. The widely used Basel Face Model (BFM) <ref type="bibr" target="#b25">[26]</ref> is also built with only 200 subjects in neutral expressions. Lack of expression can be compensated using expression bases from FaceWarehouse <ref type="bibr" target="#b8">[9]</ref> or BD-3FE <ref type="bibr" target="#b42">[43]</ref>. After more than a decade, almost all models use less than 300 training scans. Such a small training set is far from adequate to describe the full variability of human faces <ref type="bibr" target="#b7">[8]</ref>. Only recently, Booth et al. <ref type="bibr" target="#b7">[8]</ref> spent a significant effort to build 3DMM from scans of ?10, 000 subjects. Second, the texture model of 3DMM is normally built with a small number of 2D face images co-captured with 3D scans, under well-controlled conditions. Therefore, such a model is only learnt to represent the facial texture in similar conditions, rather than in-the-wild environments. This substantially limits the application scenarios of 3DMM.</p><p>Finally, the representation power of 3DMM is limited by not only the size of training set but also its formulation. The facial variations are nonlinear in nature. E.g., the variations in different facial expressions or poses are nonlinear, which violates the linear assumption of PCA-based models. Thus, a PCA model is unable to interpret facial variations well. Given the barrier of 3DMM in its data, supervision and linear bases, this paper aims to revolutionize the paradigm of learning 3DMM by answering a fundamental question:</p><p>Whether and how can we learn a nonlinear 3D Morphable Model of face shape and texture from a set of unconstrained 2D face images, without collecting 3D face scans?</p><p>If the answer were yes, this would be in sharp contrast to the conventional 3DMM approach, and remedy all aforementioned limitations. Fortunately, we have developed approaches that offer positive answers to this question. Therefore, the core of this paper is regarding how to learn this new 3DMM, what is the representation power of the model, and what is the benefit of the model to facial analysis.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, starting with an observation that the linear 3DMM formulation is equivalent to a single layer network, using a deep network architecture naturally increases the model capacity. Hence, we utilize two network decoders, instead of two PCA spaces, as the shape and texture model components, respectively. With careful consideration of each component, we design different networks for shape and texture: the multi-layer perceptron (MLP) for shape and convolutional neural network (CNN) for texture. Each decoder will take a shape or texture representation as input and output the dense 3D face or a face texture. These two decoders are essentially the nonlinear 3DMM.</p><p>Further, we learn the fitting algorithm to our nonlinear 3DMM, which is formulated as a CNN encoder. The encoder takes a 2D face image as input and generates the shape and texture parameters, from which two decoders estimate the 3D face and texture. The 3D face and texture would perfectly reconstruct the input face, if the fitting algorithm and 3DMM are well learnt. Therefore, we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face, texture, and the camera projection parameters estimated by the encoder. Finally, the endto-end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face. Jointly learning the 3DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans. We show significantly improved shape and texture representation power over the linear 3DMM. Consequently, this also benefits other tasks such as 2D face alignment and 3D reconstruction.</p><p>In this paper, we make the following contributions:</p><p>1) We learn a nonlinear 3DMM model that has greater representation power than its traditional linear counterpart.</p><p>2) We jointly learn the model and the model fitting algorithm via weak supervision, by leveraging a large collection of 2D images without 3D scans. The novel rendering layer enables the end-to-end training.</p><p>3) The new 3DMM further improves performance in related tasks: face alignment and face reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>Linear 3DMM. Since the original work by Blanz and Vetter <ref type="bibr" target="#b3">[4]</ref>, there has been a large amount of effort trying to improve 3DMM modeling mechanism. Paysan et al. <ref type="bibr" target="#b25">[26]</ref> use a Nonrigid Iterative Closest Point <ref type="bibr" target="#b2">[3]</ref> to directly align 3D scans as an alternative to the UV space alignment method in <ref type="bibr" target="#b3">[4]</ref>. Vlasic et al. <ref type="bibr" target="#b39">[40]</ref> use a multilinear model to model the combined effect of identity and expression variation on the facial shape. Later, Bolkart and Wuhrer <ref type="bibr" target="#b5">[6]</ref> show how such a multilinear model can be estimated directly from the 3D scans using a joint optimization over the model parameters and groupwise registration of 3D scans. Improving Linear 3DMM. With PCA bases, the statistical distribution underlying 3DMM is Gaussian. Koppen et al. <ref type="bibr" target="#b19">[20]</ref> argue that single-mode Gaussian can't represent real-world distribution. They introduce the Gaussian Mixture 3DMM that models the global population as a mixture of Gaussian subpopulations, each with its own mean, but shared covariance. Booth el al. <ref type="bibr" target="#b6">[7]</ref> aim to improve texture of 3DMM to go beyond controlled settings by learning inthe-wild feature-based texture model. However, both works are still based on statistical PCA bases. Duong et al. <ref type="bibr" target="#b24">[25]</ref> address the problem of linearity in face modeling by using Deep Boltzmann Machines. However, they only work with 2D face and sparse landmarks; and hence cannot handle faces with large-pose variations or occlusion well. 2D Face Alignment.</p><p>2D Face Alignment can be cast as a regression problem where 2D landmark locations are regressed directly <ref type="bibr" target="#b11">[12]</ref>. For large-pose or occluded faces, strong priors of 3DMM face shape have been shown to be beneficial. Hence, there is increasing attention in conducting face alignment by fitting a 3D face model to a single 2D image <ref type="bibr">[16-19, 21, 24, 46]</ref>. Among the prior works, iterative approaches with cascades of regressors tend to be preferred. At each cascade, it can be a single <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref> or even two regressors <ref type="bibr" target="#b40">[41]</ref>. In contrast to aforementioned works that use a fixed 3DMM model, our model and model fitting are learned jointly. This results in a more powerful model: a single-pass encoder, which is learnt jointly with the model, achieves state-of-the-art face alignment performance on AFLW2000 <ref type="bibr" target="#b45">[46]</ref> benchmark dataset. 3D Face Reconstruction. 3DMM also demonstrates its strength in face reconstruction. Since with a single image, present information about the surface is limited; 3D face re- construction must rely on prior knowledge like 3DMM <ref type="bibr" target="#b30">[31]</ref>.</p><p>Besides 3DMM fitting methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref>, recently, Richardson et al. <ref type="bibr" target="#b29">[30]</ref> design a refinement network that adds facial details on top of the 3DMM-based geometry. However, this approach can only learn 2.5D depth map, which loses the correspondence property of 3DMM. The recent work of Tewari et al. reconstruct a 3D face by an elegant encoder-decoder network <ref type="bibr" target="#b34">[35]</ref>. While their ability to decompose lighting with reflectance is satisfactory, our work has a different objective of learning a nonlinear 3DMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conventional Linear 3DMM</head><p>The 3D Morphable Model (3DMM) <ref type="bibr" target="#b3">[4]</ref> and its 2D counterpart, Active Appearance Model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>, provide parametric models for synthesizing faces, where faces are modeled using two components: shape and texture. In <ref type="bibr" target="#b3">[4]</ref>, Blanz et al. propose to describe the 3D face space with PCA:</p><formula xml:id="formula_0">S =S + A?,<label>(1)</label></formula><p>where S ? R 3Q is a 3D face with Q vertices,S ? R 3?Q is the mean shape, ? ? R l S is the shape parameter corresponding to a 3D shape bases A. The shape bases can be further split into A = [A id , A exp ], where A id is trained from 3D scans with neutral expression, and A exp is from the offsets between expression and neutral scans. The texture T (l) ? R 3Q of the face is defined within the mean shapeS, which describes the R, G, B colors of Q corresponding vertices. T (l) is also formulated as a linear combination of texture basis functions:</p><formula xml:id="formula_1">T (l) =T (l) + B?,<label>(2)</label></formula><p>whereT (l) is the mean texture, B is the texture bases, and ? ? R l T is the texture parameter. The 3DMM can be used to synthesize novel views of the face. Firstly, a 3D face is projected onto the image plane with the weak perspective projection model:</p><formula xml:id="formula_2">g(?, m) = V = f * Pr * R * S+t 2d = M (m) * S 1 ,<label>(3)</label></formula><p>where g(?, m) is the model construction and projection function leading to the 2D positions V of 3D vertices, f is the scale factor, Pr = 1 0 0 0 1 0 is the orthographic projection matrix, R is the rotation matrix constructed from three rotation angles pitch, yaw, roll, and t 2d is the translation vector. While the projection matrix M has dimensions 2 ? 4, it has six degrees of freedom, which is parameterized by a 6-dim vector m. Then, the 2D image is rendered using texture and an illumination model as described in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Nonlinear 3DMM</head><p>As mentioned in Sec. 1, the linear 3DMM has the problems such as requiring 3D face scans for supervised learning, unable to leverage massive unconstrained face images for learning, and the limited representation power due to the linear bases. We propose to learn a nonlinear 3DMM model using only large-scale in-the-wild 2D face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Problem Formulation</head><p>In linear 3DMM, the factorization of each components (texture, shape) can be seen as a matrix multiplication between coefficients and bases. From a neural network's perspective, this can be viewed as a shallow network with only one fully connected layer and no activation function. Naturally, to increase the model's representative power, the shallow network can be extended to a deep architecture. In this work, we design a novel learning scheme to learn a deep 3DMM and its inference (or fitting) algorithm.</p><p>Specifically, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we use two deep networks to decode the shape, texture parameters into the 3D facial shape and texture respectively. To make the framework end-to-end trainable, these parameters are estimated by an encoder network, which is essentially the fitting algorithm of our 3DMM. Three deep networks join forces for the ultimate goal of reconstructing the input face image, with the assistance of a geometry-based rendering layer.</p><p>Formally, given a set of 2D face images {I i } N i=1 , we aim to learn an encoder E: I?m, f S , f T that estimates the projection parameter m, and shape and texture parameters </p><formula xml:id="formula_3">f S ? R l S , f T ? R l T , a 3D</formula><p>shape decoder D S : f S ?S that decodes the shape parameter to a 3D shape S, and a texture decoder D T : f T ?T that decodes the texture parameter to a realistic texture T ? R U ?V , with the objective that the rendered image with m, S, and T can approximate the original image well. Mathematically, the objective function is:</p><formula xml:id="formula_4">arg min E,DS ,DT N i=1 R(E m (I i ), D S (E S (I i )), D T (E T (I i ))) ? I i 1 , (4)</formula><p>where R(m, S, T) is the rendering layer (Sec. 3.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Shape &amp; Texture Representation</head><p>Our shape representation is the same as that of the linear 3DMM, i.e., S ? R 3?Q is a set of Q vertices v S = (x, y, z) on the face surface. The shape decoder D S is a MLP whose input is the shape parameter f S from E. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates three possible texture representations. Texture is defined per vertex in the linear 3DMM and recent work such as <ref type="bibr" target="#b34">[35]</ref>  <ref type="figure" target="#fig_2">(Fig. 3(a)</ref>). There is a texture intensity value corresponding to each vertex in the face mesh. Since 3D vertices are not defined on a 2D grid, this representation will be parameterized as a vector, which not only loses the spatial relation of vertices, but also prevents it from leveraging the convenience of deploying CNN on 2D imagery. In contrast, given the rapid progress in image synthesis, it is desirable to choose a 2D image, e.g., a frontal-view face image in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, as a texture representation. However, frontal faces contain little information of two sides, which would lose much texture information for side-view faces.</p><p>In light of these considerations, we use an unwrapped 2D texture as our texture representation ( <ref type="figure" target="#fig_2">Fig. 3(c)</ref>). Specifically, each 3D vertex v S is projected onto the UV space using cylindrical unwarp. Assuming that the face mesh has the top pointing up the y axis, the projection of v S = (x, y, z)</p><formula xml:id="formula_5">onto the UV space v T = (u, v) is computed as: v ? ? 1 .arctan x z + ? 1 , u ? ? 2 .y + ? 2 ,<label>(5)</label></formula><p>where ? 1 , ? 2 , ? 1 , ? 2 are constant scale and translation scalars to place the unwrapped face into the image boundaries. Also, the texture decoder D T is a CNN constructed by fractionally-strided convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">In-Network Face Rendering</head><p>To reconstruct a face image from the texture T, shape S, and projection parameter m, we define a rendering layer R(m, S, T). This is accomplished in three steps. Firstly, the texture value of each vertex in S is determined by its predefined location in the 2D texture T. Usually, it involves sub-pixel sampling via a bilinear sampling kernel:</p><formula xml:id="formula_6">T S (v S ) = u ?{ u , u } v ?{ v , v } T(u , v )(1?|u?u |)(1?|v?v |),<label>(6)</label></formula><p>where v T = (u, v) is the UV space projection of v S via Eqn. 5. Secondly, the 3D shape/mesh S is projected to the image plane via Eqn. 3. Finally, the 3D mesh is then rendered using a Z-buffer renderer, where each pixel is associated with a single triangle of the mesh,</p><formula xml:id="formula_7">I(m, n) = R(m, S, T) m,n = v S ??(g,m,n) ?T S (v S ),<label>(7)</label></formula><p>where ?(g, m, n) = {v</p><formula xml:id="formula_8">(1) S , v<label>(2)</label></formula><formula xml:id="formula_9">S , v<label>(3)</label></formula><p>S } is an operation returning three vertices of the triangle that encloses the pixel (m, n) after projection g. In order to handle occlusions, when a single pixel resides in more than one triangle, the triangle that is closest to the image plane is selected. The value of each pixel is determined by interpolating the intensity of the mesh vertices via barycentric coordinates {? (i) } 3 i=1 . There are alternative designs to our rendering layer. If the texture representation is defined per vertex, as in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, one may warp the input image I i onto the vertex space of the 3D shape S, whose distance to the per-vertex texture representation can form a reconstruction loss. This design is adopted by the recent work of <ref type="bibr" target="#b34">[35]</ref>. In comparison, our rendered image is defined on a 2D grid while the alternative is on top of the 3D mesh. As a result, our rendered image can enjoy the convenience of applying the adversarial loss, which is shown to be critical in improving the quality of synthetic texture. Another design for rendering layer is image warping based on the spline interpolation, as in <ref type="bibr" target="#b9">[10]</ref>. However, this warping is continuous: every pixel in the input will map to the output. Hence this warping operation fails in the occlusion part. As a result, Cole et al. <ref type="bibr" target="#b9">[10]</ref> limit their scope to only synthesizing frontal faces by warping from normalized faces. 2 </p><formula xml:id="formula_10">FC 8?8?320 Conv11 3?3/1 96?96?32 FConv52 3?3/1 8?8?160 Conv12 3?3/1 96?96?64 FConv51 3?3/1 8?8?256 Conv21 3?3/2 48?48?64 FConv43 3?3/2 16?16?256 Conv22 3?3/1 48?48?64 FConv42 3?3/1 16?16?128 Conv23 3?3/1 48?48?128 FConv41 3?3/1 16?16?192 Conv31 3?3/2 24?24?128 FConv33 3?3/2 32?32?192 Conv32 3?3/1 24?24?96 FConv32 3?3/1 32?32?96 Conv33 3?3/1 24?24?192 FConv31 3?3/1 32?32?128 Conv41 3?3/2 12?12?192 FConv23 3?3/2 64?64?128 Conv42 3?3/1 12?12?128 FConv22 3?3/1 64?64?64 Conv43 3?3/1 12?12?256 FConv21 3?3/1 64?64?64 Conv51 3?3/2 6?6?256 FConv13 3?3/2 128?128?64 Conv52 3?3/1 6?6?160 FConv12 3?3/1 128?128?32 Conv53 3?3/1 6?6?(lS+lT +64) FConv11 3?3/1 128?128?3 AvgPool 6?6/1 1?1?(lS+lT +64)</formula><p>FC (for m only) 64?6 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Network Architecture</head><p>We design our E, D T network architecture as in Tab. 1. Also, D S includes two fully connected layers with 1, 000-dim intermediate representation with eLU activation. The entire network is end-to-end trained to reconstruct the input images, with the loss function:</p><formula xml:id="formula_11">L = L rec + ? adv L adv + ? L L L ,<label>(8)</label></formula><p>where the reconstruction loss L rec = N i=1 ||? i ? I i || 1 enforces the rendered image? i to be similar to the input I i , the adversarial loss L adv favors realistic rendering, and the landmark loss L L enforces geometry constraint. Adversarial Loss.</p><p>Based on the principal of Generative Adversarial Network (GAN) <ref type="bibr" target="#b13">[14]</ref>, the adversarial loss is widely used to synthesize photo-realistic images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, where the generator and discriminator are trained alternatively. In our case, networks that generate the rendered image? i is the generator. The discriminator includes a dedicated network D A , which aims to distinguish between the real face image I i and rendered image? i . During the training of the generator, the texture model D T will be updated with the objective that? i is being classified as real faces by D A . Since our face rendering already creates correct global structure of the face image, the global imagebased adversarial loss may not be effective in producing high-quality textures on local facial regions. Therefore, we employ patchGAN <ref type="bibr" target="#b32">[33]</ref> in our discriminator. Here, D A is a CNN consisting of four 3 ? 3 conv layers with stride of 2, and number of filters are 32, 64, 128 and 1, respectively. Finally, one of key reasons we are able to employ adversarial loss is that we are rendering in the 2D image space, rather than the 3D vertices space or unwrapped texture space. This shows the necessity and importance of our rendering layer. Semi-Supervised Pre-Training. Fully unsupervised training using only the mentioned reconstruction and adversarial loss on the rendered image could lead to a degenerate solution, since the initial estimation is far from ideal to render meaningful images. Hence, we introduce pre-training loss functions to guide the training in the early iterations.</p><p>With face profiling technique, Zhu et al. <ref type="bibr" target="#b45">[46]</ref> expands the 300W dataset <ref type="bibr" target="#b31">[32]</ref> into 122, 450 images with the fitted 3DMM shape S and projection parameters m. Given S and m, we create the pseudo groundtruth texture T by referring every pixel in the UV space back to the input image, i.e., backward of our rendering layer. With m, S, T, we define our pre-training loss by:</p><formula xml:id="formula_12">L 0 = L S + ? T L T + ? m L m + ? L L L ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">L S = S ? S 2 ,<label>(10)</label></formula><formula xml:id="formula_14">L T = T ? T 1 ,<label>(11)</label></formula><formula xml:id="formula_15">L m = m ? m 2 .<label>(12)</label></formula><p>Due to the pseudo groundtruth, using L 0 may run into the risk that our solution learns to mimic the linear model. Thus, we switch to the loss of Eqn. 8 after L 0 converges. Sparse Landmark Alignment. To help D T to better learn the facial shape, the landmark loss can be an auxiliary task.</p><formula xml:id="formula_16">L L = M (m) * S(:, d) 1 ? U 2 ,<label>(13)</label></formula><p>where U ? R 2?68 is the manually labeled 2D landmark locations, d is a constant 68-dim vector storing the indexes of 68 3D vertices corresponding to the labeled 2D landmarks. Unlike the three losses above, these landmark annotations are "golden" groundtruth, and hence L L can be used during the entire training process. Different from traditional face alignment work where the shape bases are fixed, our work jointly learns the bases functions (i.e., the shape decoder D S ) as well. Minimizing the landmark loss when updating D S only moves a tiny subset of vertices, since our D S is a MLP consisting of fully connected layers. This could lead to unrealistic shapes. Hence, when optimizing the landmark loss, we fix the decoder D S and only update the encoder. Note that the estimated groundtruth in L 0 and the landmarks are the only supervision used in our training, due to this our learning is considered as weakly supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The experiments study three aspects of the proposed nonlinear 3DMM, in terms of its expressiveness, representation power, and applications to facial analysis. Using facial mesh triangle definition by Basel Face Model (BFM) <ref type="bibr" target="#b25">[26]</ref>, we train our 3DMM using 300W-LP dataset <ref type="bibr" target="#b45">[46]</ref>. The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 ,  and 0.0002 when minimizing L. We set the following parameters: Q = 53, 215, U = V = 128, l S = l T = 160. ? values are set to make losses to have similar magnitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Expressiveness</head><p>Exploring feature space.</p><p>We use the entire CelebA dataset <ref type="bibr" target="#b22">[23]</ref> with ?200k images to feed to our network to obtain the empirical distribution of our shape and texture parameters. By varying the mean parameter along each dimension proportional to their standard deviations, we can get a sense how each element contributes to the final shape and texture. We sort elements in the shape parameter f S based on their differences to the mean 3D shape. <ref type="figure" target="#fig_4">Fig. 5</ref> shows four examples of shape changes, whose differences rank No.1, 40, 80, and 120 among 160 elements. Most of top changes are expression related. Similarly, in <ref type="figure" target="#fig_5">Fig. 6</ref>, we visualize different texture changes by adjusting only one element of f T off the mean parameterf T . The elements with the same 4 ranks as the shape counterpart are selected. Attribute Embedding.</p><p>To better understand different shape and texture instances embedded in our two decoders, we dig into their attribute meaning. For a given attribute, e.g., male, we feed images with that attribute {I i } n i=1 into our encoder to obtain two sets of parameters {f i S } n i=1 and {f i T } n i=1 . These sets represent corresponding empirical distributions of the data in the low dimensional spaces. By computing the mean parametersf S ,f T , and feed into their respective decoders, we can reconstruct the mean shape and texture with that attribute. <ref type="figure">Fig. 7</ref> visualizes the reconstructed shape and texture related to some attributes. Dif-  ferences among attributes present in both shape and texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Representation Power</head><p>Texture. Given a face image, assuming we know the groundtruth shape and projection parameters, we can unwarp the texture into the UV space, as we generate "pseudo groundtruth" texture in the weakly supervised step. With the groundtruth texture, by using gradient descent, we can estimate a texture parameter f T whose decoded texture matches with the groundtruth. Alternatively, we can minimize the reconstruction error in the image space, through the rendering layer with the groundtruth S and m. Empirically, the two methods give similar performances but we choose the first option as it involves only one warping step, instead of rendering in every optimization iteration. For the linear model, we use the fitting results of Basel texture and Phong illumination model <ref type="bibr" target="#b26">[27]</ref> given by <ref type="bibr" target="#b45">[46]</ref>. As in <ref type="figure">Fig. 8</ref>, our nonlinear texture is closer to the groundtruth than the linear model, especially for in-the-wild images (the first two rows). This is expected since the linear model is trained with controlled images. Quantitatively, our nonlinear model has significantly lower L 1 reconstruction error than the lin-   We also compare the power of nonlinear and linear 3DMM in representing real-world 3D scans. We compare with BFM <ref type="bibr" target="#b25">[26]</ref>, the most commonly used 3DMM at present. We use ten 3D face scans provided by <ref type="bibr" target="#b25">[26]</ref>, which are not included in the training set of BFM. As these face meshes are already registered using the same triangle definition with BFM, no registration is necessary. Given the groundtruth shape, by using gradient descent, we can estimate a shape parameter whose decoded shape matches the groundtruth. We define matching criteria on both vertex distances and surface normal direction. This empirically improves fidelity of final results compared to only optimizing vertex distances. Also, to emphasize the compactness of nonlinear models, we train different models with different latent space sizes. <ref type="figure" target="#fig_7">Fig. 9</ref> shows the visual quality of two models' reconstructions. As we can see, our reconstructions closely match the face shapes. Meanwhile the linear model struggles with face shapes outside its PCA span.</p><p>To quantify the difference, we use NME, averaged pervertex errors between the recovered and groundtruth shapes, normalized by inter-ocular distances. Our nonlinear model has a significantly smaller reconstruction error than the linear model, 0.0196 vs. 0.0241 (Tab. 3). Also, the non-linear models are more compact. They can achieve similar performances as linear models whose latent spaces sizes doubled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Applications</head><p>Having shown the capability of our nonlinear 3DMM (i.e., two decoders), now we demonstrate the applications of our entire network, which has the additional encoder. Many  applications of 3DMM are centered on its ability to fit to 2D face images. <ref type="figure" target="#fig_0">Fig. 10</ref> visualizes our 3DMM fitting results on CelebA dataset. Our encoder estimates the shape S, texture T as well as projection parameter m. We can recover personal facial characteristic in both shape and texture. Our texture can have variety skin color or facial hair, which is normally hard to be recovered by linear 3DMM. 2D Face Alignment. Face alignment is a critical step for any facial analysis task such as face recognition. With enhancement in the modeling, we hope to improve this task ( <ref type="figure" target="#fig_0">Fig. 11</ref>). We compare face alignment performance with state-of-the-art methods, SDM <ref type="bibr" target="#b41">[42]</ref> and 3DDFA <ref type="bibr" target="#b45">[46]</ref>, on the AFLW2000 dataset. The alignment accuracy is evaluated by the Normalized Mean Error (NME), the average of visible landmark error normalized by the bounding box size. Here, current state-of-the-art 3DDFA <ref type="bibr" target="#b45">[46]</ref> is a cascade Input Our Richardson16 Tewari17 <ref type="figure" target="#fig_0">Figure 12</ref>: 3D reconstruction results comparison. We achieve comparable visual quality in 3D reconstruction. <ref type="figure" target="#fig_0">Figure 13</ref>: Quantitative evaluation of 3D reconstruction. We obtain a low error that is comparable to optimization-based methods.</p><p>of CNNs that iteratively refines its estimation in multiple steps, meanwhile ours is a single-pass of E and D S . However, by jointly learning model fitting with 3DMM, our network can surpass <ref type="bibr" target="#b45">[46]</ref>'s performance, as in Tab. 4. Another perspective is that in conventional 3DMM fitting <ref type="bibr" target="#b45">[46]</ref>, the texture is used as the input to regress the shape parameter, while ours adopts an analysis-by-synthesis scheme and texture is the output of the synthesis. Further, for a more fair comparison of nonlinear vs. linear models, we train an encoder with the same architecture as our E, whose output parameter will multiple with the linear shape bases A, and train with the landmark loss function (Eqn. 13). Again we observe the higher error from the linear model-based fitting.</p><p>3D Face Reconstruction. We compare our approach to recent works: the CNN-based iterative supervised regressor of Richardson et al. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> and unsupervised regressor method of Tewari et al. <ref type="bibr" target="#b34">[35]</ref>. The work by Tewari et al. <ref type="bibr" target="#b34">[35]</ref> is relevant to us as they also learn to fit 3DMM in an unsupervised fashion. However, they are limited to linear 3DMM bases, which of course are not jointly trained with the model. Also, we only compare with the coarse network in <ref type="bibr" target="#b29">[30]</ref> as their refinement network use SfS, which leads to a 2.5D representation and loses correspondence between different 3D shapes. This is orthogonal to our approach. <ref type="figure" target="#fig_0">Fig. 12</ref> shows visual comparison. Following the same setting in <ref type="bibr" target="#b34">[35]</ref>, we also quantitatively compare our method with prior works on 9 subjects of FaceWarehouse database <ref type="figure" target="#fig_0">(Fig. 13</ref>). We achieve on-par results with Garrido et al. <ref type="bibr" target="#b12">[13]</ref>, an offline optimization method, while surpassing all other regression methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation on Texture Learning</head><p>With great representation power, we would like to learn a realistic texture model from in-the-wild images. The rendering layer opens a possibility to apply adversarial loss in addition to global L 1 loss. Using a global image-based discriminator is redundant as the global structure is guaranteed by the rendering layer. Also, we empirically find that using global image-based discriminator can cause severe artifacts in the resultant texture. <ref type="figure" target="#fig_0">Fig. 14</ref> visualizes outputs of our network with different options of adversarial loss. Clearly, patchGAN offers higher realism and fewer artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Since its debut in 1999, 3DMM has became a cornerstone of facial analysis research with applications to many problems. Despite its impact, it has drawbacks in requiring training data of 3D scans, learning from controlled 2D images, and limited representation power due to linear bases. These drawbacks could be formidable when fitting 3DMM to unconstrained faces, or learning 3DMM for generic objects such as shoes. This paper demonstrates that there exists an alternative approach to 3DMM learning, where a nonlinear 3DMM can be learned from a large set of uncon-strained face images without collecting 3D face scans. Further, the model fitting algorithm can be learnt jointly with 3DMM, in an end-to-end fashion.</p><p>Our experiments cover a diverse aspects of our learnt model, some of which might need the subjective judgment of the readers. We hope that both the judgment and quantitative results could be viewed under the context that, unlike linear 3DMM, no genuine 3D scans are used in our learning. Finally, we believe that unsupervisedly learning 3D models from large-scale in-the-wild 2D images is one promising research direction. This work is one step along this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Conventional 3DMM employs linear bases models for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Jointly learning a nonlinear 3DMM and its fitting algorithm from unconstrained 2D face images, in a weakly supervised fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Three texture representations. (a) Texture value per vertex, (b) Texture as a 2D frontal face, (c) 2D unwarped texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Forward and backward pass of the rendering layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Each column shows shape changes when varying one element of fS. Ordered by the magnitude of shape changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Each column shows texture changes when varying one element of fT .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Nonlinear 3DMM generates shape and texture embedded with different attributes. Texture representation power comparison. Our nonlinear model can better reconstruct the facial texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Shape representation power comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>3DMM fits to faces with diverse skin color, pose, expression, lighting, facial hair, and faithfully recovers these cues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Our 2D face alignment results. Invisible landmarks are marked as red. We can handle extreme pose and/or expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Effects of adversarial losses for texture learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The structures of E and DT networks</figDesc><table><row><cell></cell><cell>E</cell><cell></cell><cell>DT</cell></row><row><cell>Layer</cell><cell>Filter/Stride</cell><cell>Output Size</cell><cell>Layer Filter/Stride Output Size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison of texture representation power.</figDesc><table><row><cell cols="4">Method Linear Nonlinear w. Grad De. Nonlinear w. Network</cell></row><row><cell>L 1</cell><cell>0.103</cell><cell>0.066</cell><cell>0.066</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>3D scan reconstruction comparison (NME).</figDesc><table><row><cell>l S</cell><cell>40</cell><cell>80</cell><cell>160</cell></row><row><cell>Linear</cell><cell cols="2">0.0321 0.0279</cell><cell>0.0241</cell></row><row><cell>Nonlinear</cell><cell cols="3">0.0277 0.0236 0.0196</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Face alignment performance on ALFW2000</figDesc><table><row><cell cols="4">Method Linear SDM [42] 3DDFA [46]</cell><cell>Ours</cell></row><row><cell>NME</cell><cell>5.61</cell><cell>6.12</cell><cell>5.42</cell><cell>4.70</cell></row><row><cell cols="4">ear model (0.066 vs. 0.103, as in Tab. 2).</cell><cell></cell></row><row><cell>3D Shape.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our rendering layer implementation is publicly available at https: //github.com/tranluan/Nonlinear_Face_3DMM.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Inverse rendering of faces with a 3D morphable model. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aldrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Expression invariant 3D face recognition with a morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal step nonrigid ICP algorithms for surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Face recognition based on fitting a 3D morphable model. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A groupwise multilinear correspondence optimization for 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3D face morphable models &quot;In-thewild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A 3D morphable model learnt from 10,000 faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face synthesis from facial identity features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3D face rigs from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A generative shape regularization model for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pose-invariant 3D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via CNN-based dense 3D model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment via CNN-based dense 3D model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment with a single CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yin</surname></persName>
		</author>
		<title level="m">Gaussian mixture 3D morphable face model. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3D face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face model fitting on low resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment with a deformable Hough transform model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond principal components: Deep Boltzmann Machines for face modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Gia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A 3D face model for pose and illumination invariant face recognition. In Advanced video and signal based surveillance, 2009. AVSS&apos;09</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adaptive 3D face reconstruction from unconstrained photo collections. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">300 faces in-the-wild challenge: Database and results. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Describing crouzon and pfeiffer syndrome based on principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angullia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Koudstaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cranio-Maxillofacial Surgery</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disentangled representation learning GAN for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Representation learning by rotating your faces. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regressing a 3D face shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face transfer with multilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A 3D facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards large-pose face frontalization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Face recognition from a single training image under arbitrary unknown lighting using spherical harmonics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
