<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trajectory Optimization for Physics-Based Reconstruction of 3d Human Pose from Monocular Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>G?rtner</surname></persName>
							<email>erik.gartner@math.lth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Lund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<email>mykhayloa@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
							<email>hongyixu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<email>sminchisescu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Trajectory Optimization for Physics-Based Reconstruction of 3d Human Pose from Monocular Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We focus on the task of estimating a physically plausible articulated human motion from monocular video. Existing approaches that do not consider physics often produce temporally inconsistent output with motion artifacts, while state-of-the-art physics-based approaches have either been shown to work only in controlled laboratory conditions or consider simplified body-ground contact limited to feet. This paper explores how these shortcomings can be addressed by directly incorporating a fully-featured physics engine into the pose estimation process. Given an uncontrolled, real-world scene as input, our approach estimates the ground-plane location and the dimensions of the physical body model. It then recovers the physical motion by performing trajectory optimization. The advantage of our formulation is that it readily generalizes to a variety of scenes that might have diverse ground properties and supports any form of self-contact and contact between the articulated body and scene geometry. We show that our approach achieves competitive results with respect to existing physicsbased methods on the Human3.6M benchmark [13], while being directly applicable without re-training to more complex dynamic motions from the AIST benchmark [36] and to uncontrolled internet videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we address the challenge of reconstructing physically plausible articulated 3d human motion from monocular video aiming to complement the recent methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48</ref>] that achieve increasingly more accurate 3d pose estimation results in terms of standard joint accuracy metrics, but still often produce reconstructions that are visually unnatural.</p><p>Our primary mechanism to achieve physical plausibility is to incorporate laws of physics into the pose estima- <ref type="figure">Figure 1</ref>. Example results of our approach on internet videos of dynamic motions. Note that our model can reconstruct physically plausible articulated 3d motion even in the presence of complex contact with the ground: full body contact (top row), feet and hands (middle), and feet and knee contacts (bottom). tion process. This naturally allows us to impose a variety of desirable properties on the estimated articulated motion, such as temporal consistency and balance in the presence of gravity. Perhaps one of the key challenges in using physics for pose estimation is the inherent complexity of adequately modeling the diverse physical phenomena that arise due to interactions of people with the scene. In the recent literature <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b42">43]</ref> it is common to keep the physics model simple to enable efficient inference. For example, most of the recent approaches opt for using simplified contact models (considering foot contact only), ignore potential effects due to interaction with objects other than the ground-plane, and do not model more subtle physical effects such as sliding and rolling friction, or surfaces with varying degrees of softness. Clearly there are many real-world scenarios where leveraging a more feature-complete physical model is necessary. We explore physics-based articulated pose estimation using feature-complete physical simulation as a building block to address this shortcoming. The advantage of such an approach is that it allows our method to be readily applicable to a variety of motions and scenarios that have not previously been tackled in the literature (see <ref type="figure">fig. 1</ref> and 2). Specifically, in contrast to <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b42">43]</ref> our approach can reconstruct motions with any type of contact between the body and the ground plane (see <ref type="figure">fig. 1</ref>). Our approach can also model interaction with obstacles and supporting surfaces such as furniture and allows for varying the stiffness and damping of the ground-plane to represent special cases such as trampoline floor (see <ref type="figure" target="#fig_0">fig. 2</ref>). We rely on the Bullet <ref type="bibr" target="#b6">[7]</ref> engine, which was previously used for simulating human motion in <ref type="bibr" target="#b23">[24]</ref>. However, none of our implementation details are engine-specific, so we envision that the quality of our results might continue to improve with further development in physical simulation.</p><p>The main contribution of this paper is to experimentally evaluate the use of trajectory optimization for physics-based articulated motion estimation on laboratory and real-world data using a generic physics engine as a building block. We demonstrate that combining a feature-complete physics engine and trajectory optimization can reach competitive or better accuracy than state-of-the-art methods while being applicable to a large variety of scenes and motion types. Furthermore, to the best of our knowledge, we are the first to apply physics-based reconstruction to complex real-world motions such as the ones shown in <ref type="figure" target="#fig_0">fig. 1 and 2</ref>. As a second contribution, we generate technical insights such as demonstrating that we can reach excellent alignment of estimated physical motion with 2d input images by automatically adapting the 3d model to the person in the image, and employing appropriate 2d alignment losses. This is in contrast to related work <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b42">43</ref>] that typically does not report 2d alignment error and qualitatively may not achieve good 2d alignment of the physical model with the image. We also contribute to the understanding of the use of the residual root force control <ref type="bibr" target="#b44">[45]</ref>. Such residual root force has been hypothesized as essential to bridge the simulation-toreality gap and compensate for inaccuracies in the physical model. We experimentally demonstrate that the use of physically unrealistic residual force control might not be necessary, even in cases of complex and dynamic motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In the following, we first discuss recent literature on 3d human pose estimation that does not incorporate physical reasoning. We then review the related work on physicsbased human modeling and compare our approach to other physics-based 3d pose estimation approaches. 3d pose estimation without physics. State-of-the-art methods are highly effective in estimating 2d and 3d people poses in images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b48">49]</ref>, and recent work has been able to extend this progress to 3d pose estimation in video <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42]</ref>. The key elements driving the performance of these methods is the ability to estimate data-driven priors on articulated 3d poses <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47]</ref> and learn sophisticated CNNbased representations from large corpora of annotated training images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>. As such, these methods perform very well on common poses but are still challenged by rare poses. Occlusions, difficult imaging conditions, and dynamic motions (e.g. athletics) remain a challenge as these are highly diverse and hard to represent in the training set. As pointed out in <ref type="bibr" target="#b28">[29]</ref>, even for common poses state-ofthe-art methods still often generate reconstructions prone to artifacts such as floating, footskating, and non-physical leaning. We aim to complement the statistical models used in the state-of-the-art approaches by incorporating laws of physics into the inference process and thus adding a component that is universally applicable to any human motion regardless of the statistics of the training or test set.</p><p>In parallel with recent progress in pose estimation, we now have accurate statistical shape and pose models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">44]</ref>. These body models are typically estimated from thousands of scans of people and can generate shape deformations for a given pose. In this paper, we take advantage of these improvements and use a statistical body shape model <ref type="bibr" target="#b43">[44]</ref> to define the dimensions of our physical model and derive the mass from the volume of the body parts. Physics-based human motion modeling. Human motion modeling has been a subject of active research in com- <ref type="figure">Figure 3</ref>. Overview. Given a monocular video of a human motion, we estimate the parameters of a physical human model and motor control trajectories ? (t) such that the physically simulated human motion aligns with the video. We first use an inference network that predicts 2d landmarks li and body semantic segmentation masks from the video frames. From n seed frames we estimate a time-consistent human shape ? and the ground-plane location Tg. These are then kept fixed during a per-frame pose refinement step which provides the 3d kinematic initialization {?i} to the physics optimization. The dynamics stage creates a physical model that mirrors the statistical shape model with appropriate shape and mass. Our dynamics optimization improves 3d motion estimation taking into account 3d kinematics, 2d landmarks and physical constraints. We refer to ?3 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contact model Real-time Physics implementation Residual force Body model Real-world videos</head><p>Li et al. <ref type="bibr" target="#b18">[19]</ref> body joints no custom no fixed yes Rempe et al. <ref type="bibr" target="#b28">[29]</ref> feet no custom no fixed yes PhysCap <ref type="bibr" target="#b30">[31]</ref> feet yes custom yes fixed yes Shimada et al. <ref type="bibr" target="#b29">[30]</ref> feet yes custom yes fixed yes SimPoE <ref type="bibr" target="#b45">[46]</ref> full body yes MuJoCo <ref type="bibr" target="#b34">[35]</ref> yes adapt. no Xie et al. <ref type="bibr" target="#b42">[43]</ref> feet no custom no adapt. no DiffPhy <ref type="bibr" target="#b8">[9]</ref> full body no TDS <ref type="bibr" target="#b11">[12]</ref> no adapt. yes Ours full body no Bullet <ref type="bibr" target="#b6">[7]</ref> no adapt. yes <ref type="table">Table 1</ref>. Comparison of recent physics-based articulated pose estimation approaches. "Contact model" indicates what contact points between body and ground are considered, "Residual force" indicates if the physical model allows application of additional external force to move the person (see <ref type="bibr" target="#b44">[45]</ref>), "Body model" specifies if approach adapts the physical model to person in the video, and "Real-world videos" specifies if approach has also been evaluated on real-world videos or only on videos captured in laboratory conditions. puter graphics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>, robotics <ref type="bibr" target="#b7">[8]</ref> and reinforcement learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref> literature. With a few exceptions, most of the models in these domains have been constructed and evaluated using the motion capture data <ref type="bibr" target="#b1">[2]</ref>. Some work such as <ref type="bibr" target="#b25">[26]</ref> use images as input, aiming to train motion controllers for a simulated character capable of performing the observed motion under various perturbations. That work focuses on training motion controllers for a fixed character, whereas our focus is on estimating the motion of the subject observed in the image. Furthermore, the character's size, shape, and mass are independent of the observed subject. <ref type="bibr" target="#b16">[17]</ref> propose a realistic human model that directly represents muscle activations and a method to learn control policies for it. <ref type="bibr" target="#b40">[41]</ref> generate motions for a variety of character sizes and learn control policies that adapt to each size. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref> and similar results in the graphics literature do not demonstrate this for characters observed in real images and do not deal with challenges of jointly estimating physical motion and coping with ambiguity in image measurements or the 2d to 3d lifting process <ref type="bibr" target="#b32">[33]</ref>.</p><p>Physics-based 3d pose estimation. Physics-based hu-man pose estimation has a long tradition in computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref>. Early works such as <ref type="bibr" target="#b37">[38]</ref> already incorporated physical simulation as prior for 3d pose tracking but only considered simple motions such as walking and mostly evaluated in the multi-view setting in the controlled laboratory conditions. We list some of the properties of the recent works in tab. 1. <ref type="bibr" target="#b18">[19]</ref> demonstrate joint physics-based estimation of human motion and interaction with various tool-like objects. <ref type="bibr" target="#b28">[29]</ref> proposes a formulation that simplifies physics-based reasoning to feet and torso only, and infers positions of other body parts through inverse kinematics, whereas <ref type="bibr" target="#b18">[19]</ref> jointly model all body parts and also include forces due to interaction with an object. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> use a specialized physics-based formulation that solves for groundreaction forces given pre-detected foot contacts and kinematic estimates. In contrast, we do not assume that contacts can be detected a-priori, and in our approach, we estimate these as part of the physical inference. Hence we are not limited to predefined types of contact as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> or their accurate a-priori estimates. We show that we quantitatively improve over <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>, and qualitatively show how we can address more difficult in-the-wild internet videos of activities such as somersaults and sports, which would be difficult to reconstruct using previous methods. Our work is conceptually similar to SimPoE <ref type="bibr" target="#b45">[46]</ref> in that both works use physics simulation. In contrast to SimPoE, we introduce a complete pipeline that is applicable to real-world videos, whereas SimPoE has been tested only in laboratory conditions and requires a calibrated camera. Furthermore, since SimPoE relies on reinforcement learning to train datasetspecific neural network models to control the simulated body, it is not clear how well SimPoE would generalize to variable motions present in real-world videos. One clear advantage of the SimPoE approach is its fast execution at test time, which comes at the cost of lengthy pre-training. Our approach is related to the approach of <ref type="bibr" target="#b42">[43]</ref> which also estimates 3d human motion by minimizing an objective function that incorporates physics constraints. Perhaps the most significant differences to <ref type="bibr" target="#b42">[43]</ref> are that (1) we use the fullfeatured physics model whereas they consider simplified physical model, (2) their model considers physics-based loss, but the output is not required to correspond to actual physical motion, and (3) they do not discuss performance of the approach on real-world data. The advantage of <ref type="bibr" target="#b42">[43]</ref> is that they define a differentiable model that can be readily optimized with gradient descent. Finally, the concurrent work <ref type="bibr" target="#b8">[9]</ref> tackles physics-based human pose reconstruction by minimizing a loss using a differentiable physics simulator given estimated kinematics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>We present an overview of our approach in <ref type="figure">fig. 3</ref>. Given monocular video as input, we first reconstruct the initial kinematic 3d pose trajectory using a kinematic approach of <ref type="bibr" target="#b47">[48]</ref> and use it to estimate body shape and the position of the ground plane relative to the camera. Subsequently, we instantiate a physical person model with body dimensions and weight that match the estimated body shape. Next, we formulate an objective function that measures the similarity between the motion of the physical model and image measurements and includes regularization terms that encourage plausible human poses and penalize jittery motions. Finally, we reconstruct the physical motion by minimizing this objective function with respect to the joint torque trajectories. To realize the physical motion, we rely on the implementation of rigid body dynamics available in Bullet <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Body model and control</head><p>We model the human body as rigid geometric primitives connected by joints. Our model consists of 26 capsules and has 16 3d body joints for a total of 48 degrees of freedom. We rely on a statistical model of human shape <ref type="bibr" target="#b43">[44]</ref> to instantiate our model for a variety of human body types. To that end, given the 3d mesh representing the body shape, we estimate dimensions of the geometric primitives to approximate the mesh following the approach of <ref type="bibr" target="#b1">[2]</ref>. We then compute the mass and inertia of each primitive based on its volume and estimate the mass based on an anatomical weight distribution <ref type="bibr" target="#b27">[28]</ref> from the statistical human shape dataset CAESAR <ref type="bibr" target="#b26">[27]</ref>.</p><p>We do not model body muscle explicitly and instead actuate the model by directly applying the torque at the body joints. We denote the vector of torques applied at time t as ? t , the angular position, and velocity of each joint at time t as q t andq t , and the set of 3d Cartesian coordinates of each joint at time t as x t . Similarly to <ref type="bibr" target="#b24">[25]</ref>, we control the motion of the physical model by introducing a sequence of control targetsq 1:T = {q 1 ,q 2 , . . . ,q t } which are used to derive the torques via a control loop. The body motion in our model is then specified by the initial body state s 0 = (q 0 ,q 0 ), the world geometry G specifying the position and orientation of the ground plane, the control trajectory for each jointq 1:T and the corresponding control rule. We assume the initial acceleration to be 0. To implement the control loop we rely on the articulated islands algorithm 1 (AIA) <ref type="bibr" target="#b33">[34]</ref> that incorporates motor control targets as constraints in the linear complementarity problem (LCP) (cf . (6.3) a, b in <ref type="bibr" target="#b33">[34]</ref>) alongside contact constraints. AIA enables stable simulation already at 100 Hz compared to 1000-2000 Hz for PD control used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Physics-based articulated motion estimation</head><p>Our approach to the task of physical motion estimation is generally similar to other trajectory and spacetime optimization approaches in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39]</ref>. We perform optimization over a sequence of overlapping temporal windows, initializing the start of each subsequent window with the preceding state in the previous window. To reduce the dimensionality of the search space, we use cubic B-spline interpolation to represent the control targetq 1:T and perform optimization over the spline coefficients <ref type="bibr" target="#b5">[6]</ref>. Given the objective function L introduced in ?3.3 we aim to find the optimal motion by minimizing L with respect to the spline coefficients of the control trajectoryq 1:T . We initialize the control trajectory with the kinematic estimates of the body joints (see ?3.4). The initial state is initialized from the corresponding kinematic estimate. We use the finite difference computed on the kinematic motion to estimate the initial velocity. As in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> we minimize the objective function with the evolutionary optimization approach CMA-ES [10] since our simulation environment does not support differentiation with respect to the dynamics variables. We generally observe convergence with CMA-ES after 2000 iterations per window with 100 samples per iteration. The inference takes 20 ? 30 minutes when evaluating 100 samples in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Objective functions</head><p>We use a composite objective function given by a weighted combination of several components. 3d pose. To encourage reconstructed physical motion to be close to the estimated kinematic 3d poses q k 1:T we use the following objective functions</p><formula xml:id="formula_0">L COM (q 1:T ) = t (?c t ? c k t ? 2 2 + ?? t ?? k t ? 2 2 ) (1) L pose = t j?J arccos(|?q tj , q k tj ?|)<label>(2)</label></formula><p>where c t and c k t denote the position of the center of mass at time t in the reconstructed motion and kinematic estimate. L pose measures the angle between observed joint angles and their kinematic estimates and the summation <ref type="formula" target="#formula_0">(2)</ref> is over the set J of all body joints including the base joint which defines the global orientation of the body. 2d re-projection. To encourage alignment of 3d motion with image observations, we use a set of N = 28 landmark points that include the main body joints, eyes, ears, nose, fingers, and endpoints of the feet. Let l t denote the positions of 3d landmarks on the human body at time t, C be the camera projection matrix that maps world points into the image via perspective projection, l d t be the vector of landmark detections by the CNN-detector, and s t the corresponding detection score vector. The 2d landmark re-projection loss is then defined as</p><formula xml:id="formula_1">L 2d = t n s tn ?Cl tn ? l d tn ? 2 .<label>(3)</label></formula><p>See ?3.4 for details on estimating the 2d landmarks.</p><p>Regularization. We include several regularizers into our objective function. Firstly, we use the normalizing flow prior on human poses introduced in <ref type="bibr" target="#b46">[47]</ref> which penalize unnatural poses. The loss is given by</p><formula xml:id="formula_2">L nf = t ?z(q t )? 2 ,<label>(4)</label></formula><p>where z(q t ) is the latent code corresponding to the body pose q t . To discourage jittery motions we a add total variation loss on the acceleration of joints</p><formula xml:id="formula_3">L T V = 1 J t j ?? tj ?? t?1,j ? 1<label>(5)</label></formula><p>Finally, we include a L lim term that adds exponential penalty on deviations from anthropomorphic joint limits. The overall objective L used in physics-based motion estimation is given by the weighted sum of (1-5) and of the term L lim . See the supplemental material for details.</p><p>Model MPJPE-G MPJPE MPJPE-PA HUND <ref type="bibr" target="#b47">[48]</ref> 239 <ref type="table" target="#tab_0">116  72  + S  233  110  71  + SO  178  85  62  + SO + G  148  84  63  + SO + T  186  85  61  + SO + GT  135  80  58   Table 2</ref>. Ablation of kinematics improvements on HUND on a validation subset of Human3.6M. +S indicates time-consistent body shape, +O indicates additional non-linear optimization, +G using ground-plane constraints, and +T temporal smoothness constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Kinematic 3d pose and shape estimation</head><p>In this section, we describe our approach to extracting 2d and 3d evidence from the input video sequence.</p><p>Body shape. Given the input sequence, we proceed first to extract initial per-frame kinematic estimates of the 3d pose and shape using HUND <ref type="bibr" target="#b47">[48]</ref>. As part of its optimization pipeline HUND also recovers the camera intrinsics c and estimates the positions of 2d landmarks, which we use in the 2d re-projection objective in <ref type="bibr" target="#b2">(3)</ref>. HUND is designed to work on single images, so our initial shape and pose estimates are not temporally consistent. Therefore, to improve the quality of kinematic 3d pose initialization, we extend HUND to pose estimation in video. We evaluate the additional steps introduced in this section in the experiments shown in tab. 2 using a validation set of 20 sequences from Human3.6M dataset. In our adaptation, we do not re-train the HUND neural network predictor and instead, directly minimize the HUND loss functions with BFGS. As a first step, we re-estimate the shape jointly over multiple video frames. To keep optimization tractable, we first jointly estimate shape and pose over a subset of n = 5 seed frames and then re-estimate the pose in all video frames keeping the updated shape fixed. The seed frames are selected by the highest average 2d keypoint confidence score. We refer to the HUND approach with re-estimated shape as HUND+S and to our approach where we subsequently also re-estimate the pose as HUND+SO. In tab. 2 we show results for both variants. Note that HUND+SO improves considerably compared to the original HUND results.</p><p>Ground plane. We define the location of the ground plane by the homogeneous transformation T g that maps from the HUND coordinates to the canonical coordinate system in which the ground plane is passing through the origin, and its normal is given by the "y" axis. Let M t be a subset of points on the body mesh at frame t. The signed distance from the mesh points to the ground plane is given by D(M t ) = T g M t e y , where e y = [0, 1, 0, 0] T is the unit vector of the "y" axis in homogeneous coordinates. To estimate the transformation T g we introduce an objective function</p><formula xml:id="formula_4">L gp (T g , M) = t ? min(?, L k (D(M t )))? 2 ,<label>(6)</label></formula><p>where L k (D t ) corresponds to the smallest k = 20 signed distances in D t . This objective favors T g that places body mesh in contact with the ground without making preference for a specific contact points. This objective is also robust to cases when person is in the air by clipping the distance at ?, which we set to 0.2m in the experiments in this paper. We recover T g by minimizing</p><formula xml:id="formula_5">L gp (T g ) =L gp (T g , M l ) + L gp (T g , M r ) + 2L gp (T g , M b ),<label>(7)</label></formula><p>where M l , M r and M b are the meshes of the left foot, right foot and whole body respectively. This biases the ground plane to have contact with the feet, but is still robust to cases when person is jumping or touching the ground with other body parts (e.g. as in the case of a somersault). 3d pose. In the final step, we re-estimate the poses in all frames using the estimated shape and ground plane while adding the temporal consistency objective</p><formula xml:id="formula_6">L temp = t ?M t ? M t?1 ? 2 + ?? t ? ? t?1 ? 2 ,<label>(8)</label></formula><p>where M t is a body mesh and ? t is a HUND body pose vector in frame t. To enforce ground plane constraints we use <ref type="bibr" target="#b5">(6)</ref>, but now keep T g fixed and optimize with respect to body pose. In the experiments in tab. 2 we refer to the variant of our approach that uses temporal constraints in <ref type="bibr" target="#b7">(8)</ref> as HUND+SO+T and to the full kinematic optimization that uses both temporal and ground plane constraints as HUND+SO+GT. Tab. 2 demonstrates that both temporal and ground-truth constraints considerably improve the accuracy of kinematic 3d pose estimation. Even so, the results of our best variant HUND+SO+GT still contain artifacts such as motion jitter and footskating, which are substantially reduced by the dynamical model (see tab. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>Datasets. We evaluate our method on three human motion datasets: Human3.6M <ref type="bibr" target="#b12">[13]</ref>, HumanEva-I <ref type="bibr" target="#b31">[32]</ref> and AIST <ref type="bibr" target="#b35">[36]</ref>. In addition, we qualitatively evaluate on our own "in-the-wild" internet videos. To compare different variants of our approach in tab. 2 and tab. 3 we use a validation set composed of 20 short 100-frame sequences from the Human3.6M dataset. We use the same subset of fulllength sequences as proposed in <ref type="bibr" target="#b42">[43]</ref> for the main evaluation in tab. 4. We use a preprocessed version of the AIST dataset <ref type="bibr" target="#b35">[36]</ref> from <ref type="bibr" target="#b17">[18]</ref> which contains pseudo 3d body pose ground-truth obtained through multi-view reconstruction. For our experiments, we select a subset of fifteen videos featuring diverse dances of single subjects. For the evaluation on HumanEva-I, we follow the protocol defined in <ref type="bibr" target="#b28">[29]</ref> and evaluate on the walking motions from the validation split of the dataset using images from the first camera. We assume known camera extrinsic parameters in the Human3.6M experiments and estimate them for other datasets. In order to speed up the computation of the long sequences of Hu-man3.6M in tab. 4 we compute all temporal windows in parallel and join them together in post-processing. We report results using mean global per-joint position error (mm) overall joints (MPJPE-G), as well as translation aligned (MPJPE) and Procrustes aligned (MPJPE-PA) error metrics. Note that to score on the MPJPE-G metric an approach should be able to both estimate the articulated pose and correctly track the global position of the person in world coordinates. In addition to standard evaluation metrics, we implement the foot skate and floating metrics similar to those introduced in <ref type="bibr" target="#b28">[29]</ref> but detect contacts using a threshold rather than through contact annotation. Finally, we report image alignment (MPJPE-2d) and 3d joint velocity error in m/s. See supplementary for further details. Analysis of model components. In tab. 3 we present ablation results of our approach. Our full dynamical model uses kinematic inputs obtained with HUND+SO+GT introduced in ?3.4 and is denoted as HUND+SO+GT + Dynamics. Our dynamical model performs comparably or slightly better compared to HUND+SO+GT on joint localization metrics (e.g. MPJPE-G improves slightly from 135 to 132 mm) but greatly reduces motion artifacts. The percentage of frames with footskate is reduced from 64 to  <ref type="table">Table 4</ref>. Quantitative results of our models compared to prior work on Human3.6M <ref type="bibr" target="#b12">[13]</ref>, HumanEva-I <ref type="bibr" target="#b31">[32]</ref> and a subset of AIST <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>8 and error in velocity from 0.58 to 0.27 m/s. We also evaluate a dynamic model based on a simpler kinematic variant HUND+SO that does not incorporate ground-plane and temporal constraints when re-estimating poses from video. For HUND+SO, the inference with dynamics similarly improves perceptual metrics considerably. Note that HUND+SO produces output that suffers from both footskating (25% of frames) and floating (40% of frames). Adding ground-plane constraints in (cf . (6)) removes floating artifacts in HUND+SO+GT, but the output still suffers from footskating (64% of the frames). Dynamical inference helps to substantially reduce both types of artifacts both for HUND+SO and HUND+SO+GT. In <ref type="figure" target="#fig_1">fig. 4</ref> we show example output of HUND+SO+GT + Dynamics and compare it to HUND+SO+GT which it uses for initialization. Note that for HUND+SO+GT the person in the output appears to move forward by floating in the air, whereas our dynamics approach infers plausible 3d poses consistent with the subject's global motion. In the bottom part of tab. 3 we report results for our full model HUND+SO+GT + Dynamics while ablating components of the objective function (cf . ?3.3). We observe that all components of the objective function contribute to the overall accuracy. The most important components are the 2d re-projection (cf . (3)) and difference in COM position (cf . <ref type="formula">(1)</ref>). Without these, the MPJPE-G increases from 132 to 154 and 151 mm, respectively. Excluding the 3d joints component leads to only a small loss of accuracy from 132 to 134 mm.</p><p>Comparison to state-of-the-art. In tab. 4 we present the results of our full model on the Human3.6M, HumanEva-I, and AIST datasets. We compare to VIBE <ref type="bibr" target="#b15">[16]</ref> using the publicly available implementation by the authors and use the evaluation results of other approaches as reported in the original publications. Since VIBE generates only rootrelative pose estimates, we use a similar technique as proposed in PhysCap <ref type="bibr" target="#b30">[31]</ref> and estimate the global position and orientation by minimizing the 2d joint reprojection error. On the Human3.6M benchmark, our approach improves over VIBE and our own HUND+SO+GT in terms of joint accuracy and perceptual metrics. Compared to VIBE, the MPJPE-G improves from 208 to 143 mm, MPJPE-2d improves from 16 to 13 px, and the percentage of footskating frames are reduced from 27% to 4%. Interestingly our approach achieves the best MPJPE-PA overall physics-based approaches except the pretrained SimPoE, but reaches somewhat higher MPJPE compared to <ref type="bibr" target="#b29">[30]</ref> and fairly recent work of <ref type="bibr" target="#b42">[43]</ref> (82 mm vs 68 mm for <ref type="bibr" target="#b42">[43]</ref> and 77 mm for <ref type="bibr" target="#b29">[30]</ref>). Note that <ref type="bibr" target="#b42">[43]</ref> start with a stronger kinematic baseline (74 mm MPJPE) and that the performance of other approaches might improve as well given such better kinematic initialization. Furthermore, our dynamics approach improves over the results of <ref type="bibr" target="#b28">[29]</ref>   <ref type="figure">Figure 5</ref>. Example result on AIST <ref type="bibr" target="#b35">[36]</ref>. The kinematic initialization produces poses that are unstable in the presence of gravity (red circle) or poses that are temporally inconsistent (yellow circles). Our physics-based approach corrects both errors.</p><p>improves in terms of MPJPE-G, footskating, and velocity compared to our kinematic initialization. Results on real-world internet video. We show example results of our approach on the AIST dataset <ref type="bibr" target="#b35">[36]</ref> in <ref type="figure">fig. 5</ref> and on the real-world internet videos in <ref type="figure" target="#fig_0">fig. 1, 2 and 6</ref>. To obtain the results with a soft floor shown in <ref type="figure" target="#fig_0">fig. 2</ref> we manually modify the stiffness and damping floor parameters to mimic the trampoline behavior. The sequence with the chair from the Human3.6M dataset shown in <ref type="figure" target="#fig_0">fig. 2 (bottom)</ref> is generated by manually adding a chair to the scene since our approach does not perform reasoning about scene objects.</p><p>In <ref type="figure">fig. 5</ref> we qualitatively compare the output of our full system with physics to our best kinematic approach HUND+SO+GT. We strongly encourage the reader to watch the video in supplemental material 2 to appreciate the differences between the two approaches and to see the qualitative comparison to VIBE <ref type="bibr" target="#b15">[16]</ref>. We observe that our physics approach is often able to correct out-of-balance poses produced by HUND+SO+GT (e.g. second frame in <ref type="figure">fig. 5</ref>) and substantially improves temporal coherence of the reconstruction. Note that typically both HUND+SO+GT and our physics-based approach produce outputs that match 2d observations, but the physics-based approach estimates 3d pose more accurately. For example, in the first sequence in <ref type="figure">fig. 6</ref> the physics-based model infers the pose that en-2 See tiny.cc/traj-opt.</p><p>ables the person to jump in subsequent frames, whereas HUND+SO+GT places the left leg at an angle that would make the jump impossible. Note that the output of the physics-based approach can deviate significantly from the kinematic initialization ( <ref type="figure" target="#fig_2">fig. 7</ref> and second example in <ref type="figure">fig. 6</ref>. This is particularly prominent in the <ref type="figure" target="#fig_2">fig. 7</ref> where we show example result on a difficult sequence where 2d keypoint estimation fails to localize the legs in several frames due to occlusion by the clothing. Note that in this example our full model with dynamics is able to generate reasonable sequence of 3d poses despite multiple failures in the kinematic initialization.</p><p>Failure cases of our approach. We show a few characteristic examples of the failure cases of our approach in <ref type="figure" target="#fig_3">fig. 8</ref>. Note that our physics-based reconstruction depends on the kinematic 3d pose estimation for initialization and also uses it in one of the components of the loss (cf . eq. 2). Therefore our physics-based approach is likely to fail when kinematic reconstruction is grossly incorrect (see <ref type="figure" target="#fig_3">fig. 8 (b)</ref>) or when it fails to estimate position of the limb important to maintain the overall pose (see <ref type="figure" target="#fig_3">fig. 8 (a)</ref>). Our physics-based model might also fail when the estimate of the ground-plane with respect to the camera is inaccurate. Note how in <ref type="figure" target="#fig_3">fig. 8</ref> (c) the kinematic estimate positions the standing person at an angle to the true ground-plane normal vector (red arrow). As a result in this example the physics-based reconstruction tilts  <ref type="figure">Figure 6</ref>. Example results on real-world videos. In the top row sequence, the kinematic initialization incorrectly places the left foot before the jump. We highlight the mistake by showing the scene from another viewpoint (red circle). The kinematic initialization also fails to produce temporally consistent poses in the example in the bottom row (yellow circles). Our physics-based inference corrects both errors and generates a more plausible motion. See tiny.cc/traj-opt for more results. the person at the torso to maintain stable pose given the incorrect gravity vector (see the two bottom rows in <ref type="figure" target="#fig_3">fig. 8 (c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a physics-based approach to 3d articulated video reconstruction of humans. By closely combining kinematic and dynamic constraints within an optimization process that is contact, mass, and inertia aware, with values informed by body shape estimates, we are able to improve the physical plausibility and reduce reconstruction artifacts compared to purely kinematic ap-proaches. One of the primary goals of our work has been to demonstrate the advantages of incorporating an expressive physics model into the 3d pose estimation pipeline. Clearly, such a model makes inference more involved compared to specialized physics-based approaches such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43]</ref>, but with the added benefit of being more capable and general.</p><p>Ethical considerations. This work aims to improve the quality of human pose reconstruction through the inclusion of physical constraints. We believe that the level of detail in our physical model limits its applications in tasks such as person identification or surveillance. The same limitation While this is typically seen as an advantage, it also means that failure to estimate one limb correctly can propagate to other body limbs. For example in (a) our approach failed to correctly estimate position of the left arm which is used to support the body. As a result the overall 3d pose is worse for the dynamics (forth row) compared to the kinematic initialization (third row). Our physics-based reconstruction might also fail due to poor kinematics initialization (b) or due to failure to correctly estimate the orientation of the ground plane relative to the camera (c).</p><p>also prevents its use in the generation of e.g. deepfakes, particularly as the model lacks a photorealistic appearance. We believe our model is inclusive towards and supports a variety of different body shapes and sizes. While we do not study this in the paper, we consider it important future work. <ref type="figure">Figure 9</ref>. The physical body model's shape and mass parameters are based on an associated GHUM <ref type="bibr" target="#b43">[44]</ref> mesh.</p><p>Furthermore, we learn a nonlinear regressor ?(?) with an MLP that performs fast shape approximation at run time. The regressor consists of two 256-dimensional fully connected layers, and is trained with 50K shapes generated with Gaussian sampling of the latent shape space ? together with the paired optimal primitive parameters using <ref type="bibr" target="#b8">(9)</ref>.</p><p>Our physical model share an identical skeleton topology with GHUM but does not model the face and finger joints, due to the focused interest on the body dynamics in this work. Extending with finger joints, however, would enable simulation of hand-object interactions which would be interesting, but we leave this for future work. We note that there is a bijective mapping for the shared 16 body joints between our model and GHUM, which allows for fast conversion between the physical and stastical representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simulation Details</head><p>We run the Bullet simulation at 200 Hz, with friction coefficient ? = 0.9 and gravitational acceleration constant 9.8 m/s 2 . The PD-controllers controlling each torque motor is tuned with position gain k p = 4.0, velocity gain k d = 0.3, and torque limits similar to those presented in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Metrics</head><p>In addition to the standard 2d and 3d joint position error metrics, we evaluate our reconstructions using physical plausibility metrics similar to those proposed in <ref type="bibr" target="#b28">[29]</ref>. Since the authors were unable to share their code we implement our own versions the metrics which doesn't require foot-ground contact annotations. A foot contact is defined as at least N = 10 vertices of a foot mesh being in contact with the ground plane. We set the contact threshold to d = 0.005 m for kinematics. To account for the modeling error when approximating the foot with a box primitive we set the contact threshold for dynamics to d = ?0.015 m. Footskate. The percentage of frames in a sequence where either foot joint moves more than 2 cm between two adjacent frames while the corresponding foot was in contact with the ground-plane. Float. The percentage of frames in a sequence where at least one of the feet was not in contact but was within 2 cm of the ground-plane. This metric captures the common issue of reconstructions floating above the ground while not penalizing correctly reconstructed motion of e.g. jumps.</p><p>Velocity. The mean error between the 3d joint velocities in the ground-truth data and the joint velocity in the reconstruction. High error velocity indicates that the estimated motion doesn't smoothly follow the trajectory of the true motion. We define the velocity error as</p><formula xml:id="formula_7">e v = 1 N N i=1 k?K |? i k ?? i k |,<label>(10)</label></formula><p>where? i k is the magnitude of the ground-truth 3d joint velocity vector (in m/s) for joint k at frame i and where? i w nf 1.0 <ref type="table">Table 5</ref>. Weights of the objective function described in ?3.3 (mp)and (11) for our three main datasets: Human3.6M <ref type="bibr" target="#b12">[13]</ref>, AIST <ref type="bibr" target="#b35">[36]</ref>, and HumanEva-I <ref type="bibr" target="#b31">[32]</ref>. "Grid" specifies the values evaluated while selecting hyperparameter values. Note that we did not exhaustively explore all combination.  <ref type="table">Table 6</ref>. The subset of Human3.6M used in the ablation experiments. Note that the data was downsampled from 50 to 25 FPS. using finite differences from 3d joint positions and use first frame translation aligned joint estimates (as in MPJPE-G).</p><formula xml:id="formula_8">1.0 1.0 {0.001, 0.1, 1, 10} w T V 1.0 1.0 1.0 {0.1, 1, 10} w lim 1.0 1.0 1.0 {0.1, 1, 10}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Subject Camera Id Frames</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Datasets</head><p>Human3.6M. We use two subsets for our experiments on Human3.6M <ref type="bibr" target="#b12">[13]</ref>. When we compare our method to stateof-the-art methods we use a dataset split similar to the one used in <ref type="bibr" target="#b42">[43]</ref>. See tab. 8 for the complete lists of sequences we use. Similarly to <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43]</ref>, we down sample the sequences from 50 FPS to 25 FPS. When perform ablations of our model we a smaller subset where we select 20 4-sec sequences from the test split of Human3.6M dataset (subjects 9 and 11). We selected sequences that show various dynamic motions such as walk-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence</head><p>Frames gBR_sBM_c06_d06_mBR4_ch06</p><p>1-120 gBR_sBM_c07_d06_mBR4_ch02</p><p>1-120 gBR_sBM_c08_d05_mBR1_ch01</p><p>1-120 gBR_sFM_c03_d04_mBR0_ch01</p><p>1-120 gJB_sBM_c02_d09_mJB3_ch10</p><p>1-120 gKR_sBM_c09_d30_mKR5_ch05</p><p>1-120 gLH_sBM_c04_d18_mLH5_ch07 1-120 gLH_sBM_c07_d18_mLH4_ch03 1-120 gLH_sBM_c09_d17_mLH1_ch02 1-120 gLH_sFM_c03_d18_mLH0_ch15 1-120 gLO_sBM_c05_d14_mLO4_ch07 1-120 gLO_sBM_c07_d15_mLO4_ch09 1-120 gLO_sFM_c02_d15_mLO4_ch21 1-120 gMH_sBM_c01_d24_mMH3_ch02 1-120 gMH_sBM_c05_d24_mMH4_ch07 1-120 <ref type="table">Table 7</ref>. Sequences used for evaluation on AIST.</p><p>ing dog, running and phoning (with large motion range), to sitting and purchasing (with occluded body parts). For each sequence, we randomly selected one of the four cameras. We list the sequences in tab. 6.</p><p>HumanEva-I. We evaluate our method on the subset of HumanEva-I walking sequences <ref type="bibr" target="#b31">[32]</ref> as selected by <ref type="bibr" target="#b28">[29]</ref>, see tab. 9.</p><p>AIST. We select four second video sequences from the public dataset <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>, showing fast and complex dancing motions, picked randomly from one of the 10 cameras. We list our selected sequences in tab. 7.</p><p>"In-the-wild" internet videos. We perform qualitative evaluation of our model on videos of dynamic motions rarely found in laboratory captured datasets. These videos were made available on the internet under a CC-BY license which grants the express permission to be used for any purpose. Note that we only used the videos to perform qualitative analysis of our approach -the videos will not be redistributed as a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence</head><p>Subject Camera Id S11 Directions_1 60457274 S11 Discussion_1 60457274 S11 Greeting_1 60457274 S11</p><p>Posing_1 60457274 S11</p><p>Purchases_1 60457274 S11</p><p>TakingPhoto_1 60457274 S11</p><p>Waiting_1 60457274 S11</p><p>WalkDog_1 60457274 S11</p><p>WalkTogether_1 60457274 S11</p><p>Walking_1 Walking_1 60457274 <ref type="table">Table 8</ref>. The evaluation subset of Human3.6M used in the main evaluation. The subset is similar to the one used in <ref type="bibr" target="#b30">[31]</ref>. We downsampled the data from 50 FPS to 25 FPS. <ref type="table" target="#tab_0">Walking  S1  C1  1-561  Walking  S2  C1  1-438  Walking  S3  C1  1-490   Table 9</ref>. Sequences used for evaluation on HumanEva-I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Subject Camera Id Frames</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Human Data Usage</head><p>This work relies on recorded videos of humans. Our main evaluation is performed on two standard human pose benchmarks: Human3.6M 3 <ref type="bibr" target="#b12">[13]</ref> and AIST 4 <ref type="bibr" target="#b35">[36]</ref>. These datasets have been approved for research purposes according to their respective websites. Both datasets contain recordings of actors in laboratory settings. To complement this, we perform qualitative evaluation on videos released on the internet under creative commons licenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hyperparameters</head><p>The most important hyperparameters are the weights of the weighted objected function described in ?3.3 (mp). Where combined loss function is given by </p><p>We tuned the weights on sequences from the training splits. The goal was to scale the different components such that they have roughly equal magnitudes while minimizing the MPJPE-G error. See tab. 5 for details regarding the search grid and the chosen parameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Resources</head><p>For running small experiments we used a desktop workstation equipped with an "Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz" CPU, 128 GB system memory and two NVIDIA Titan Xp GPUs. We ran kinematics in the cloud using instances with a V100 GPU, 48 GB of memory and 8 vCPUs. In the dynamics experiments, we used instances with 100 vCPUs and 256 GB of memory for the CMA-ES <ref type="bibr" target="#b9">[10]</ref> optimization. Optimizing a window of 1 second of video takes roughly 20 min using a 100 vCPUs instance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Examples results of our approach for scene with soft ground (top) and interaction with a chair (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on the Human3.6M dataset. Note how the dynamical model (right) recovers plausible locomotion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Example results on a difficult real-world video in which the legs of the person are occluded by the clothing. Note that 2d keypoints on the legs are incorrectly localized in multiple consecutive frames due to severe occlusion (second row) which results in poor 3d pose estimation by the kinematic model (third row). Interestingly our full model with dynamic is able to recover from errors in the kinematic initialization and generates reasonable sequence of 3d body poses (fourth row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Examples of the characteristic failure cases of our approach on the real-world videos. Note that physics-based modeling introduces additional coupling between positions of the body limbs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3</head><label></label><figDesc>http://vision.imar.ro/human3.6m/ 4 https://aistdancedb.ongaaccel.jp/ L = w COM L COM + w pose L pose+ w 2d L 2d + w nf L nf + w T V L T V + w lim L lim .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiments of the dynamics model on a validation set of 20 sequences from the Human3.6M dataset.</figDesc><table><row><cell>Model</cell><cell cols="8">MPJPE-G MPJPE MPJPE-PA MPJPE-2d Velocity Footskate (%) Float (%)</cell></row><row><cell>HUND+SO</cell><cell></cell><cell>178</cell><cell>85</cell><cell>62</cell><cell>12</cell><cell>1.3</cell><cell>25</cell><cell>40</cell></row><row><cell cols="2">HUND+SO + Dynamics</cell><cell>167</cell><cell>87</cell><cell>62</cell><cell>12</cell><cell>0.45</cell><cell>7</cell><cell>1</cell></row><row><cell cols="2">HUND+SO+GT</cell><cell>135</cell><cell>80</cell><cell>58</cell><cell>12</cell><cell>0.58</cell><cell>64</cell><cell>0</cell></row><row><cell cols="2">HUND+SO+GT + Dynamics</cell><cell>132</cell><cell>80</cell><cell>57</cell><cell>11</cell><cell>0.27</cell><cell>8</cell><cell>0</cell></row><row><cell cols="2">HUND+SO+GT + Dynamics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o 2d re-projection, (3)</cell><cell>154</cell><cell>104</cell><cell>68</cell><cell>17</cell><cell>0.32</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">w/o 3d joints, (2)</cell><cell>134</cell><cell>84</cell><cell>60</cell><cell>11</cell><cell>0.27</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">w/o COM, (1)</cell><cell>149</cell><cell>81</cell><cell>57</cell><cell>11</cell><cell>0.31</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">w/o COM and 3d joints, (1, 2)</cell><cell>151</cell><cell>85</cell><cell>59</cell><cell>11</cell><cell>0.33</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">w/o pose prior, (4)</cell><cell>138</cell><cell>80</cell><cell>57</cell><cell>11</cell><cell>0.24</cell><cell>-</cell><cell>-</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell></cell><cell cols="6">MPJPE-G MPJPE MPJPE-PA MPJPE-2d Velocity Footskate (%)</cell></row><row><cell></cell><cell>VIBE [16]</cell><cell></cell><cell>208</cell><cell>69</cell><cell>44</cell><cell>16</cell><cell>0.32</cell><cell>27</cell></row><row><cell></cell><cell>PhysCap [31]</cell><cell></cell><cell>-</cell><cell>97</cell><cell>65</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SimPoE [46]</cell><cell></cell><cell>-</cell><cell>57</cell><cell>42</cell><cell>-</cell><cell>-</cell></row><row><cell>Human3.6M</cell><cell cols="2">Shimada et al. [30] Xie et al. [43] (Kinematics)</cell><cell>--</cell><cell>77 74</cell><cell>58 -</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell cols="2">Xie et al. [43] (Dynamics)</cell><cell>-</cell><cell>68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Ours: HUND+SO+GT</cell><cell></cell><cell>145</cell><cell>83</cell><cell>56</cell><cell>14</cell><cell>0.46</cell><cell>48</cell></row><row><cell></cell><cell cols="2">Ours: HUND+SO+GT + Dynamics</cell><cell>143</cell><cell>84</cell><cell>56</cell><cell>13</cell><cell>0.24</cell><cell>4</cell></row><row><cell></cell><cell cols="2">Rempe et al. [29] (Kinematics)</cell><cell>408</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HumanEva-I</cell><cell cols="2">Rempe et al. [29] (Dynamics) Ours: HUND+SO+GT</cell><cell>422 208</cell><cell>-90</cell><cell>-76</cell><cell>-14</cell><cell>-0.51</cell><cell>-40</cell></row><row><cell></cell><cell cols="2">Ours: HUND+SO+GT + Dynamics</cell><cell>196</cell><cell>91</cell><cell>74</cell><cell>14</cell><cell>0.27</cell><cell>4</cell></row><row><cell>AIST</cell><cell cols="2">Ours: HUND+SO+GT Ours: HUND+SO+GT + Dynamics</cell><cell>156 154</cell><cell>107 113</cell><cell>67 69</cell><cell>10 13</cell><cell>0.59 0.41</cell><cell>51 4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">"POSITION_CONTROL" mode in Bullet.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Erwin Coumans for his help with the project, as well as the supportive anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This supplementary material provides further details on our methodology and the data we used. ?A presents details on our physical human body model, ?B provides details regarding our simulation parameters, ?C presents our physics metrics, in ?D we present the datasets used in our experiments, ?E provides details about our method's hyperparameters, and lastly ?F summarizes our computational setup. When referring to equations or material in the main paper we will denote this by (mp). Finally, please see our supplemental video for qualitative results of our method at tiny.cc/traj-opt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Physical Body Model</head><p>Given a GHUM <ref type="bibr" target="#b43">[44]</ref> body mesh M(?, ? 0 ) associated with the shape parameters ? and the rest pose ? 0 , we build a simulation-ready rigid multibody human model that best approximates the mesh with a set of parameterized geometric primitives (cf . <ref type="figure">fig. 9</ref>). The hands and feet are approximated with boxes whereas the rest of the body links are approximated with capsules. The primitives are connected and articulated with the GHUM body joints.</p><p>Inspired by <ref type="bibr" target="#b1">[2]</ref>, we optimize the primitive parameters by minimizing</p><p>where ? are the size parameters for the primitives, i.e. length and radius for the capsules, and depth, height and width for the boxes. The loss penalizes the bi-directional distances between pairs of nearest points on the GHUM mesh M b and surface of the primitive geometryM b associated with the body link b.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trajectory optimization for full-body movements with complex contacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Mazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Borno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>De Lasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1405" to="1419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust Physics-based Motion Retargeting with Realistic Body Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Mazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Borno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Righetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">L</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating contact dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2389" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive spacetime control for animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pybullet, a python module for physics simulation for games, robotics and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://pybullet.org" />
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simulation of human motion data using short-horizon model-predictive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Da</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="371" to="380" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differentiable dynamics for articulated 3d human motion reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The CMA Evolution Strategy: A Comparing Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Emergence of locomotion behaviours in rich environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Dhruva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Lemmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1707.02286</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">NeuralSim: Augmenting differentiable simulators with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Heiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Millard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav S</forename><surname>Sukhatme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2014-07-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable muscle-actuated human simulation and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learn to dance with aist++: Music conditioned 3d dance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating 3d motion and forces of person-object interactions from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongmian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Sedlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mansard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AMASS: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5442" to="5451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Physics-Based Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Kluwer Academic Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
		<idno>143:1-143:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sfv: Reinforcement learning of physical skills from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building statistical shape spaces for 3d human modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Helten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Christian Theobalt, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Anatomical data for analyzing human motion. Research quarterly for exercise and sport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Plagenhoef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaynor</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Abdelnour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contact and human dynamics from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural monocular 3d human motion capture with physical awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Physcap: Physically plausible monocular 3d motion capture in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2020-12-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kinematic jump processes for monocular 3d human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Physics-Based Animation of Articulated Rigid Body Systems for Virtual Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Stepien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Fukayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Hamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference<address><addrLine>Delft, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Physical simulation for probabilistic motion tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spacetime constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A scalable approach to control diverse behaviors for physically simulated characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungdam</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning body shape variation in physics-based characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungdam</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Physics-based human motion estimation and synthesis from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunrong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Shkurti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">GHUM &amp; GHUML: Generative 3d human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Residual force control for agile human behavior imitation and extended motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simpoe: Simulated character control for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10350</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Neural descent for visual 3d human pose and shape. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8410" to="8419" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
