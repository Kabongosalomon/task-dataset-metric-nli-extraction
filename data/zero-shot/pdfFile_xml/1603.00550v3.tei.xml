<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthesized Classifiers for Zero-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<email>weilunc@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>bgong@crcv.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><surname>Fei</surname></persName>
							<email>feisha@cs.ucla.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">U. of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">U. of Central Florida Orlando</orgName>
								<address>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">U. of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Synthesized Classifiers for Zero-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given semantic descriptions of object classes, zeroshot learning aims to accurately recognize objects of the unseen classes, from which no examples are available at the training stage, by associating them to the seen classes, from which labeled examples are provided. We propose to tackle this problem from the perspective of manifold learning. Our main idea is to align the semantic space that is derived from external information to the model space that concerns itself with recognizing visual features. To this end, we introduce a set of "phantom" object classes whose coordinates live in both the semantic space and the model space. Serving as bases in a dictionary, they can be optimized from labeled data such that the synthesized real object classifiers achieve optimal discriminative performance. We demonstrate superior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 dataset with more than 20,000 unseen classes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual recognition has made significant progress due to the widespread use of deep learning architectures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref> that are optimized on large-scale datasets of humanlabeled images <ref type="bibr" target="#b39">[40]</ref>. Despite the exciting advances, to recognize objects "in the wild" remains a daunting challenge. Many objects follow a long-tailed distribution: in contrast to common objects such as household items, they do not occur frequently enough for us to collect and label a large set of representative exemplar images.</p><p>For example, this challenge is especially crippling for fine-grained object recognition (classifying species of birds, designer products, etc.). Suppose we want to carry a visual search of "Chanel Tweed Fantasy Flap * Equal contributions Handbag". While handbag, flap, tweed, and Chanel are popular accessory, style, fabric, and brand, respectively, the combination of them is rare -the query generates about 55,000 results on Google search with a small number of images. The amount of labeled images is thus far from enough for directly building a high-quality classifier, unless we treat this category as a composition of attributes, for each of which more training data can be easily acquired <ref type="bibr" target="#b24">[25]</ref>.</p><p>It is thus imperative to develop methods for zero-shot learning, namely, to expand classifiers and the space of possible labels beyond seen objects, of which we have access to the labeled images for training, to unseen ones, of which no labeled images are available <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. To this end, we need to address two key interwoven challenges <ref type="bibr" target="#b33">[34]</ref>: <ref type="bibr" target="#b0">(1)</ref> how to relate unseen classes to seen ones and (2) how to attain optimal discriminative performance on the unseen classes even though we do not have their labeled data.</p><p>To address the first challenge, researchers have been using visual attributes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> and word vectors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> to associate seen and unseen classes. We call them the semantic embeddings of objects. Much work takes advantage of such embeddings directly as middle layers between input images and output class labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>, whereas others derive new representations from the embeddings using, for example, Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> or sparse coding <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref>. For the second challenge, the hand-designed probabilistic models in <ref type="bibr" target="#b24">[25]</ref> have been competitive baselines. More recent studies show that nearest neighbor classifiers in the semantic space are very effective <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>. Finally, classifiers for the unseen classes can directly be constructed in the input feature space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>In this paper, we tackle these two challenges with ideas from manifold learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, converging to a two-pronged approach. We view object classes in a se-  <ref type="figure">Figure 1</ref>: Illustration of our method for zero-shot learning. Object classes live in two spaces. They are characterized in the semantic space with semantic embeddings (as) such as attributes and word vectors of their names. They are also represented as models for visual recognition (ws) in the model space. In both spaces, those classes form weighted graphs. The main idea behind our approach is that these two spaces should be aligned. In particular, the coordinates in the model space should be the projection of the graph vertices from the semantic space to the model space -preserving class relatedness encoded in the graph. We introduce adaptable phantom classes (b and v) to connect seen and unseen classes -classifiers for the phantom classes are bases for synthesizing classifiers for real classes. In particular, the synthesis takes the form of convex combination. mantic space as a weighted graph where the nodes correspond to object class names and the weights of the edges represent how they are related. Various information sources can be used to infer the weights -humandefined attributes or word vectors learnt from language corpora. On the other end, we view models for recognizing visual images of those classes as if they live in a space of models. In particular, the parameters for each object model are nothing but coordinates in this model space whose geometric configuration also reflects the relatedness among objects. <ref type="figure">Fig. 1</ref> illustrates this idea conceptually.</p><p>But how do we align the semantic space and the model space? The semantic space coordinates of objects are designated or derived based on external information (such as textual data) that do not directly examine visual appearances at the lowest level, while the model space concerns itself largely for recognizing lowlevel visual features. To align them, we view the coordinates in the model space as the projection of the vertices on the graph from the semantic space -there is a wealth of literature on manifold learning for computing (low-dimensional) Euclidean space embeddings from the weighted graph, for example, the well-known algorithm of Laplacian eigenmaps <ref type="bibr" target="#b4">[5]</ref>.</p><p>To adapt the embeddings (or the coordinates in the model space) to data, we introduce a set of phantom object classes -the coordinates of these classes in both the semantic space and the model space are adjustable and optimized such that the resulting model for the real object classes achieve the best performance in discriminative tasks. However, as their names imply, those phantom classes do not correspond to and are not optimized to recognize any real classes directly. For mathematical convenience, we parameterize the weighted graph in the semantic space with the phantom classes in such a way that the model for any real class is a convex combinations of the coordinates of those phantom classes. In other words, the "models" for the phantom classes can also be interpreted as bases (classifiers) in a dictionary from which a large number of classifiers for real classes can be synthesized via convex combinations. In particular, when we need to construct a classifier for an unseen class, we will compute the convex combination coefficients from this class's semantic space coordinates and use them to form the corresponding classifier.</p><p>To summarize, our main contribution is a novel idea to cast the challenging problem of recognizing unseen classes as learning manifold embeddings from graphs composed of object classes. As a concrete realization of this idea, we show how to parameterize the graph with the locations of the phantom classes, and how to derive embeddings (i.e., recognition models) as convex combinations of base classifiers. Our empirical studies extensively test our synthesized classifiers on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 release <ref type="bibr" target="#b7">[8]</ref> with 20,345 unseen classes. The experimental results are very encouraging; the synthesized classifiers outperform several state-ofthe-art methods, including attaining better or matching performance of Google's ConSE algorithm <ref type="bibr" target="#b32">[33]</ref> in the large-scale setting.</p><p>The rest of the paper is organized as follows. We give an overview of relevant literature in Section 2, describe our approach in detail in Section 3, demonstrate its effectiveness in Section 4, and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In order to transfer knowledge between classes, zeroshot learning relies on semantic embeddings of class labels, including attributes (both manually defined <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47]</ref> and discriminatively learned <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b48">49]</ref>), word vectors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref>, knowledge mined from the Web <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, or a combination of several embeddings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Given semantic embeddings, existing approaches to zero-shot learning mostly fall into embedding-based and similarity-based methods. In the embedding-based approaches, one first maps the input image representations to the semantic space, and then determines the class labels in this space by various relatedness measures implied by the class embeddings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>. Our work as well as some recent work combine these two stages <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref>, leading to a unified framework empirically shown to have, in general, more accurate predictions. In addition to directly using fixed semantic embeddings, some work maps them into a different space through CCA <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> and sparse coding <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>In the similarity-based approaches, in contrast, one builds the classifiers for unseen classes by relating them to seen ones via class-wise similarities <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. Our approach shares a similar spirit to these models but offers richer modeling flexibilities thanks to the introduction of phantom classes.</p><p>Finally, our convex combination of base classifiers for synthesizing real classifiers can also be motivated from multi-task learning with shared representations <ref type="bibr" target="#b3">[4]</ref>.</p><p>While labeled examples of each task are required in <ref type="bibr" target="#b3">[4]</ref>, our method has no access to data of the unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We describe our methods for addressing zero-shot learning where the task is to classify images from unseen classes into the label space of unseen classes.</p><formula xml:id="formula_0">Notations Suppose we have training data D = {(x n ? R D , y n )} N n=1</formula><p>with the labels coming from the label space of seen classes S = {1, 2, ? ? ? , S}. Denote by U = {S + 1, ? ? ? , S + U} the label space of unseen classes.</p><p>We focus on linear classifiers in the visual feature space R D that assign a label? to a data point x b?</p><formula xml:id="formula_1">y = arg max c w T c x,<label>(1)</label></formula><p>where w c ? R D , although our approach can be readily extended to nonlinear settings by the kernel trick <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Main idea</head><p>Manifold learning The main idea behind our approach is shown by the conceptual diagram in <ref type="figure">Fig. 1</ref>.</p><p>Each class c has a coordinate a c and they live on a manifold in the semantic embedding space. In this paper, we explore two types of such spaces: attributes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46]</ref> and class name embeddings via word vectors <ref type="bibr" target="#b31">[32]</ref>. We use attributes in this text to illustrate the idea and in the experiments we test our approach on both types. Additionally, we introduce a set of phantom classes associated with semantic embeddings b r , r = 1, 2, . . . , R. We stress that they are phantom as they themselves do not correspond to any real objects -they are introduced to increase the modeling flexibility, as shown below.</p><p>The real and phantom classes form a weighted bipartite graph, with the weights defined as</p><formula xml:id="formula_2">s cr = exp{?d(a c , b r )} R r=1 exp{?d(a c , b r )}<label>(2)</label></formula><p>to correlate a real class c and a phantom class r, where</p><formula xml:id="formula_3">d(a c , b r ) = (a c ? b r ) T ? ?1 (a c ? b r ),<label>(3)</label></formula><p>and ? ?1 is a parameter that can be learned from data, modeling the correlation among attributes. For simplicity, we set ? = ? 2 I and tune the scalar free hyperparameter ? by cross-validation. The more general Mahalanobis metric can be used and we propose one way of learning such metric as well as demonstrate its effectiveness in the Suppl. The specific form of defining the weights is motivated by several manifold learning methods such as SNE <ref type="bibr" target="#b17">[18]</ref>. In particular, s cr can be interpreted as the conditional probability of observing class r in the neighborhood of class c. However, other forms can be explored and are left for future work.</p><p>In the model space, each real class is associated with a classifier w c and the phantom class r is associated with a virtual classifier v r . We align the semantic and the model spaces by viewing w c (or v r ) as the embedding of the weighted graph. In particular, we appeal to the idea behind Laplacian eigenmaps <ref type="bibr" target="#b4">[5]</ref>, which seeks the embedding that maintains the graph structure as much as possible; equally, the distortion error is minimized. This objective has an analytical solution</p><formula xml:id="formula_4">w c = R r=1 s cr v r , ? c ? T = {1, 2, ? ? ? , S + U} (4)</formula><p>In other words, the solution gives rise to the idea of synthesizing classifiers from those virtual classifiers v r . For conceptual clarity, from now on we refer to v r as base classifiers in a dictionary from which new classifiers can be synthesized. We identify several advantages. First, we could construct an infinite number of classifiers as long as we know how to compute s cr . Second, by making R S, the formulation can significantly reduce the learning cost as we only need to learn R base classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning phantom classes</head><p>Learning base classifiers We learn the base classifiers {v r } R r=1 from the training data (of the seen classes only). We experiment with two settings. To learn oneversus-other classifiers, we optimize,</p><formula xml:id="formula_5">min v1,??? ,v R S c=1 N n=1 (x n , I yn,c ; w c ) + ? 2 S c=1 w c 2 2 , (5) s.t. w c = R r=1 s cr v r , ? c ? T = {1, ? ? ? , S}</formula><p>where (x, y; w) = max(0, 1 ? yw T x) 2 is the squared hinge loss. The indicator I yn,c ? {?1, 1} denotes whether or not y n = c. Alternatively, we apply the Crammer-Singer multi-class SVM loss <ref type="bibr" target="#b6">[7]</ref>, given by</p><formula xml:id="formula_6">cs (x n , y n ; {w c } S c=1 ) = max(0, max c?S?{yn} ?(c, y n ) + w c T x n ? w yn T x n ),</formula><p>We have the standard Crammer-Singer loss when the structured loss ?(c, y n ) = 1 if c = y n , which, however, ignores the semantic relatedness between classes. We additionally use the 2 distance for the structured loss ?(c, y n ) = a c ? a yn 2 2 to exploit the class relatedness in our experiments. These two learning settings have separate strengths and weaknesses in empirical studies.</p><p>Learning semantic embeddings The weighted graph eq. (2) is also parameterized by adaptable embeddings of the phantom classes b r . For this work, however, for simplicity, we assume that each of them is a sparse linear combination of the seen classes' attribute vectors:</p><formula xml:id="formula_7">b r = S c=1</formula><p>? rc a c , ?r ? {1, ? ? ? , R}, Thus, to optimize those embeddings, we solve the following optimization problem</p><formula xml:id="formula_8">min {vr} R r=1 ,{?rc} R,S r,c=1 S c=1 N n=1 (x n , I yn,c ; w c ) + ? 2 S c=1 w c 2 2 + ? R,S r,c=1 |? rc | + ? 2 R r=1 ( b r 2 2 ? h 2 ) 2 , s.t. w c = R r=1 s cr v r , ? c ? T = {1, ? ? ? , S},</formula><p>where h is a predefined scalar equal to the norm of real attribute vectors (i.e., 1 in our experiments since we perform 2 normalization). Note that in addition to learning {v r } R r=1 , we learn combination weights {? rc } R,S r,c=1 . Clearly, the constraint together with the third term in the objective encourages the sparse linear combination of the seen classes' attribute vectors. The last term in the objective demands that the norm of b r is not too far from the norm of a c .</p><p>We perform alternating optimization for minimizing the objective function with respect to {v r } R r=1 and {? rc } R,S r,c=1 . While this process is nonconvex, there are useful heuristics to initialize the optimization routine. For example, if R = S, then the simplest setting is to let b r = a r for r = 1, . . . , R. If R ? S, we can let them be (randomly) selected from the seen classes' attribute vectors {b 1 , b 2 , ? ? ? , b R } ? {a 1 , a 2 , ? ? ? , a S }, or first perform clustering on {a 1 , a 2 , ? ? ? , a S } and then let each b r be a combination of the seen classes' attribute vectors in cluster r. If R &gt; S, we could use a combination of the above two strategies. We describe in more detail how to optimize and cross-validate hyperparameters in the Suppl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison to several existing methods</head><p>We contrast our approach to some existing methods. <ref type="bibr" target="#b29">[30]</ref> combines pre-trained classifiers of seen classes to construct new classifiers. To estimate the semantic embedding (e.g., word vector) of a test image, <ref type="bibr" target="#b32">[33]</ref> uses the decision values of pre-trained classifiers of seen objects to weighted average the corresponding semantic embeddings. Neither of them has the notion of base classifiers, which we introduce for constructing the classifiers and nothing else. We thus expect them to be more effective in transferring knowledge between seen and unseen classes than overloading the pretrained and fixed classifiers of the seen classes for dual duties. We note that <ref type="bibr" target="#b0">[1]</ref> can be considered as a special case of our method. In <ref type="bibr" target="#b0">[1]</ref>, each attribute corresponds to a base and each "real" classifier corresponding to an actual object is represented as a linear combination of those bases, where the weights are the real objects' "descriptions" in the form of attributes. This modeling is limiting as the number of bases is fundamentally limited by the number of attributes. Moreover, the model is strictly a subset of our model. <ref type="bibr" target="#b0">1</ref> Recently, <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b51">51]</ref> propose similar ideas of aligning the visual and semantic spaces but take different approaches from ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our methods and compare to existing state-of-the-art models on several benchmark datasets. While there is a large degree of variations in the current empirical studies in terms of datasets, evaluation protocols, experimental settings, and implementation details, we strive to provide a comprehensive comparison to as many methods as possible, not only citing the published results but also reimplementing some of those methods to exploit several crucial insights we have discovered in studying our methods.</p><p>We summarize our main results in this section. More extensive details are reported in the Suppl. We provide not only comparison in recognition accuracy but also analysis in an effort to understand the sources of better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets We use four benchmark datasets in our experiments: the Animals with Attributes (AwA) <ref type="bibr" target="#b24">[25]</ref>, CUB-200-2011 Birds (CUB) <ref type="bibr" target="#b45">[46]</ref>, SUN Attribute (SUN) <ref type="bibr" target="#b35">[36]</ref>, and the ImageNet (with full 21,841 classes) <ref type="bibr" target="#b39">[40]</ref>. <ref type="table" target="#tab_0">Table 1</ref> summarizes their key characteristics. The Suppl. provides more details.</p><p>Semantic spaces For the classes in AwA, we use 85dimensional binary or continuous attributes <ref type="bibr" target="#b24">[25]</ref>, as well as the 100 and 1,000 dimensional word vectors <ref type="bibr" target="#b30">[31]</ref>, derived from their class names and extracted by Fu et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. For CUB and SUN, we use 312 and 102 dimensional continuous-valued attributes, respectively. We also thresh them at the global means to obtain binary-valued attributes, as suggested in <ref type="bibr" target="#b24">[25]</ref>. Neither datasets have word vectors for their class names. For ImageNet, we train a skip-gram language model [31,  <ref type="bibr" target="#b24">[25]</ref>. ? : 4 (or 10, respectively) random splits, reporting average. ? : Seen and unseen classes from ImageNet ILSVRC 2012 1K <ref type="bibr" target="#b39">[40]</ref> and Fall 2011 release <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>. <ref type="bibr" target="#b31">32</ref>] on the latest Wikipedia dump corpus 2 (with more than 3 billion words) to extract a 500-dimensional word vector for each class. Details of this training are in the Suppl. We ignore classes without word vectors in the experiments, resulting in 20,345 (out of 20,842) unseen classes. For both the continuous attribute vectors and the word vector embeddings of the class names, we normalize them to have unit 2 norms unless stated otherwise.</p><p>Visual features Due to variations in features being used in literature, it is impractical to try all possible combinations of features and methods. Thus, we make a major distinction in using shallow features (such as color histograms, SIFT, PHOG, Fisher vectors) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref> and deep learning features in several recent studies of zero-shot learning. Whenever possible, we use (shallow) features provided by those datasets or prior studies. For comparative studies, we also extract the following deep features: AlexNet <ref type="bibr" target="#b22">[23]</ref> for AwA and CUB and GoogLeNet <ref type="bibr" target="#b44">[45]</ref> for all datasets (all extracted with the Caffe package <ref type="bibr" target="#b20">[21]</ref>). For AlexNet, we use the 4,096-dimensional activations of the penultimate layer (fc7) as features. For GoogLeNet, we take the 1,024dimensional activations of the pooling units, as in <ref type="bibr" target="#b1">[2]</ref>. Details on how to extract those features are in the Suppl.</p><p>Evaluation protocols For AwA, CUB, and SUN, we use the (normalized, by class-size) multi-way classification accuracy, as in previous work. Note that the accuracy is always computed on images from unseen classes.</p><p>Evaluating zero-shot learning on the large-scale Ima-geNet requires substantially different components from evaluating on the other three datasets. First, two evaluation metrics are used, as in <ref type="bibr" target="#b12">[13]</ref>: Flat hit@K (F@K) and Hierarchical precision@K (HP@K).</p><p>F@K is defined as the percentage of test images for which the model returns the true label in its top K predictions. Note that, F@1 is the multi-way classification accuracy. HP@K takes into account the hierarchical organization of object categories. For each true label, we generate a ground-truth list of K closest categories in the hierarchy and compute the degree of overlapping (i.e., precision) between the ground-truth and the model's top K predictions. For the detailed description of this metric, please see the Appendix of <ref type="bibr" target="#b12">[13]</ref> and the Suppl.</p><p>Secondly, following the procedure in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>, we evaluate on three scenarios of increasing difficulty:</p><p>? 2-hop contains 1,509 unseen classes that are within two tree hops of the seen 1K classes according to the ImageNet label hierarchy 3 .</p><p>? 3-hop contains 7,678 unseen classes that are within three tree hops of seen classes.</p><p>? All contains all 20,345 unseen classes in the Ima-geNet 2011 21K dataset that are not in the ILSVRC 2012 1K dataset.</p><p>The numbers of unseen classes are slightly different from what are used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref> due to the missing semantic embeddings (i.e., word vectors) for certain class names.</p><p>In addition to reporting published results, we have also reimplemented the state-of-the-art method ConSE <ref type="bibr" target="#b32">[33]</ref> on this dataset, introducing a few improvements. Details are in the Suppl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We cross-validate all hyperparameters -details are in the Suppl. For convenience, we set the number of phantom classes R to be the same as the number of seen classes S, and set b r = a c for r = c. We also experiment setting different R and learning b r . Our study (cf. <ref type="figure" target="#fig_2">Fig. 2</ref>) shows that when R is about 60% of S, the performance saturates. We denote the three variants of our methods in constructing classifiers (Section 3.2) by Ours o-vs-o (one-versus-other), Ours cs (Crammer-Singer) and Ours struct (Crammer-Singer with structured loss). <ref type="table">Table 2</ref> compares the proposed methods to the state-ofthe-art from the previously published results on benchmark datasets. While there is a large degree of variations <ref type="table">Table 2</ref>: Comparison between our results and the previously published results in multi-way classification accuracies (in %) on the task of zero-shot learning. For each dataset, the best is in red and the 2nd best is in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Main results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>AwA CUB SUN ImageNet DAP <ref type="bibr" target="#b24">[25]</ref> 41.4</p><formula xml:id="formula_9">- 22.2 - IAP [25] 42.2 - 18.0 - BN [47] 43.4 - - - ALE [1] 37.4 18.0 ? - - SJE [2] 66.7 50.1 ? - - ESZSL [39] 49.3 - - - ConSE[33] - - - 1.4 SSE-ReLU [51]</formula><p>76. <ref type="bibr" target="#b2">3</ref>  in implementation details, the main observation is that our methods attain the best performance in most scenarios. In what follows, we analyze those results in detail.</p><p>We also point out that the settings in some existing work are highly different from ours; we do not include their results in the main text for fair comparison <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49</ref>] -but we include them in the Suppl. In some cases, even with additional data and attributes, those methods underperform ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Large-scale zero-shot learning</head><p>One major limitation of most existing work on zero-shot learning is that the number of unseen classes is often small, dwarfed by the number of seen classes. However, real-world computer vision systems need to face a very large number of unseen objects. To this end, we evaluate our methods on the large-scale ImageNet dataset. <ref type="table" target="#tab_2">Table 3</ref> summarizes our results and compares to the ConSE method <ref type="bibr" target="#b32">[33]</ref>, which is, to the best of our knowledge, the state-of-the-art method on this dataset. <ref type="bibr" target="#b3">4</ref> Note that in some cases, our own implementation ("ConSE by us" in the table) performs slightly worse than the reported results, possibly attributed to differences in visual features, word vector embeddings, and other implementation details. Nonetheless, the proposed methods (using the same setting as "ConSE by us") always outperform both, especially in the very challenging scenario of All where the number of unseen classes is 20,345, significantly larger than the number of seen classes. Note also that, for both types of metrics, when K is larger, the improvement over the existing approaches is more pronounced. It is also not surprising to notice that as the number of unseen classes increases from the setting 2hop to All, the performance of both our methods and ConSE degrade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Detailed analysis</head><p>We experiment extensively to understand the benefits of many factors in our and other algorithms. While trying all possible combinations is prohibitively expensive, we have provided a comprehensive set of results for comparison and drawing conclusions.</p><p>Advantage of continuous attributes It is clear from <ref type="table">Table 4</ref> that, in general, continuous attributes as semantic embeddings for classes attain better performance than binary attributes. This is especially true when deep learning features are used to construct classifiers. It is somewhat expected that continuous attributes provide a more accurate real-valued similarity measure among classes. This presumably is exploited further by more powerful classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantage of deep features</head><p>It is also clear from Table 4 that, across all methods, deep features significantly boost the performance based on shallow features. We use GoogLeNet and AlexNet (numbers in parentheses) and GoogLeNet generally outperforms AlexNet. It is worthwhile to point out that the reported results under deep features columns are obtained using linear classifiers, which outperform several nonlinear classifiers that use shallow features. This seems to suggest that deep features, often thought to be specifically adapted to seen training images, still work well when transferred to unseen images <ref type="bibr" target="#b12">[13]</ref>.</p><p>Which types of semantic space? In <ref type="table" target="#tab_3">Table 5</ref>, we show how effective our proposed method (Ours o-vs-o ) exploits the two types of semantic spaces: (continuous) attributes and word-vector embeddings on AwA (the only dataset with both embedding types). We find that attributes yield better performance than word-vector embeddings. However, combining the two gives the best result, suggesting that these two semantic spaces could be complementary and further investigation is ensured. <ref type="table" target="#tab_5">Table 6</ref> takes a different view on identifying the best semantic space. We study whether we can learn optimally the semantic embeddings for the phantom classes that correspond to base classifiers. These preliminary studies seem to suggest that learning attributes could have a positive effect, though it is difficult to improve over word-vector embeddings. We plan to study this issue more thoroughly in the future.</p><p>How many base classifiers are necessary? In <ref type="figure" target="#fig_2">Fig. 2</ref>, we investigate how many base classifiers are needed - <ref type="table">Table 4</ref>: Detailed analysis of various methods: the effect of feature and attribute types on multi-way classification accuracies (in %). Within each column, the best is in red and the 2nd best is in blue. We cite both previously published results (numbers in bold italics) and results from our implementations of those competing methods (numbers in normal font) to enhance comparability and to ease analysis (see texts for details). We use the shallow features provided by <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref> for AwA, CUB, SUN, respectively.  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. ? : On the attribute vectors without 2 normalization, while our own implementation shows that normalization helps in some cases. : As co-occurrence statistics are not available, we combine pre-trained classifiers with the weights defined in eq. (2). so far, we have set that number to be the number of seen classes out of convenience. The plot shows that in fact, a smaller number (about 60% -70%) is enough for our algorithm to reach the plateau of the performance curve. Moreover, increasing the number of base classifiers does not seem to have an overwhelming effect. Further details and analysis are in the Suppl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have developed a novel classifier synthesis mechanism for zero-shot learning by introducing the notion of "phantom" classes. The phantom classes connect the dots between the seen and unseen classes -the classifiers of the seen and unseen classes are constructed from the same base classifiers for the phantom classes and with the same coefficient functions. As a result, we can conveniently learn the classifier synthesis mechanism leveraging labeled data of the seen classes and then readily apply it to the unseen classes. Our approach outperforms the state-of-the-art methods on four benchmark datasets in most scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Synthesized Classifiers for Zero-Shot Learning</head><p>In this Supplementary Material, we provide details omitted in the main text.</p><p>? Section A: cross-validation strategies (Section 3.2 of the main paper).</p><p>? Section B: learning metrics for semantic similarity (Section 3.1 of the main paper).</p><p>? Section C: details on experimental setup (Section 4.1 of the main paper).</p><p>? Section D: implementation details (Section 4.1 and 4.2.3 of the main paper).</p><p>? Section E: additional experimental results and analyses (Section 4.2 of the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross-validation (CV) strategies</head><p>There are a few free hyper-parameters in our approach (cf. Section 3.2 of the main text). To choose the hyper-parameters in the conventional cross-validation (CV) for multi-way classification, one splits the training data into several folds such that they share the same set of class labels with one another. Clearly, this strategy is not sensible for zero-shot learning as it does not imitate what actually happens at the test stage. We thus introduce a new strategy for performing CV, inspired by the hyper-parameter tuning in <ref type="bibr" target="#b38">[39]</ref>. The key difference of the new scheme to the conventional CV is that we split the data into several folds such that the class labels of these folds are disjoint. For clarity, we denote the conventional CV as sample-wise CV and our scheme as class-wise CV. <ref type="figure" target="#fig_5">Figure 3</ref>(b) and 3(c) illustrate the two scenarios, respectively. We empirically compare them in Section E.1. Note that several existing models [2, 10, 39, 51] also follow similar hyper-prameter tuning procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Learning semantic embeddings</head><p>We propose an optimization problem for learning semantic embeddings in Section 3.2 of the main text. There are four hyper-parameters ?, ?, ?, and ? to be tuned. To reduce the search space during crossvalidation, we first fix b r = a r for r = 1, . . . , R and tune ?, ?. Then we fix ? and ? and tune ? and ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning metrics for computing similarities between semantic embeddings</head><p>Recall that, in Section 3.1 of the main text, the weights in the bipartite graph are defined based on the distance d(a c , b r ) = (a c ? b r ) T ? ?1 (a c ? b r ). In this section, we describe an objective for learning a more general Mahalnobis metric than ? ?1 = ? 2 I. We focus on the case when R = S and on learning a diagonal metric ? ?1 = M T M , where M is also diagonal. We solve the following optimization problem. </p><formula xml:id="formula_10">+ ? 2 R r=1 v r 2 2 + ? 2 M ? ?I 2 F ,<label>(6)</label></formula><formula xml:id="formula_11">s.t. w c = R r=1 s cr v r , ? c ? T = {1, ? ? ? , S}<label>(7)</label></formula><p>where (x, y; w) = max(0, 1 ? yw T x) 2 is the squared hinge loss. The indicator I yn,c ? {?1, 1} denotes whether or not y n = c. Again, we perform alternating optimization for minimizing the above objective function. At first, we fix M = ?I and optimize {v 1 , ? ? ? , v R } to obtain a reasonable initialization. Then we perform alternating optimization. To further prevent over-fitting, we alternately optimize M and {v 1 , ? ? ? , v R } on different but overlapping subsets of training data. In particular, we split data into 5 folds and optimize {v 1 , ? ? ? , v R } on the first 4 folds and M on the last 4 folds. We report results in Section E.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details on the experimental setup</head><p>We present details on the experimental setup in this section and additional results in Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Datasets</head><p>We use four benchmark datasets in our experiments. The Animals with Attributes (AwA) dataset <ref type="bibr" target="#b24">[25]</ref> consists of 30,475 images of 50 animal classes. Along with the dataset, a standard data split is released for zero-shot learning: 40 seen classes (for training) and 10 unseen classes. The second dataset is the CUB-200-2011 Birds (CUB) <ref type="bibr" target="#b45">[46]</ref>. It has 200 bird classes and 11,788 images. We randomly split the 200 classes into 4 disjoint sets (each with 50 classes) and treat each of them as the unseen classes in turn. We report the average results from the four splits. The SUN Attribute (SUN) dataset <ref type="bibr" target="#b35">[36]</ref>   contains 14,340 images of 717 scene categories (20 images from each category). Following <ref type="bibr" target="#b24">[25]</ref>, we randomly split the 717 classes into 10 disjoint sets (each with 71 or 72 classes) in a similar manner to the class splitting on CUB. We note that some previous published results <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref> are based on a simpler setting with 707 seen and 10 unseen classes. For comprehensive experimental comparison, we also report our results on this setting in <ref type="table" target="#tab_8">Table 9</ref>.</p><p>For the large-scale zero-shot experiment on the Ima-geNet dataset <ref type="bibr" target="#b7">[8]</ref>, we follow the setting in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. The ILSVRC 2012 1K dataset <ref type="bibr" target="#b39">[40]</ref> contains 1,281,167 training and 50,000 validation images from 1,000 categories and is treated as the seen-class data. Images of unseen classes come from the rest of the ImageNet Fall 2011 release dataset <ref type="bibr" target="#b7">[8]</ref> that do not overlap with any of the 1,000 categories. We will call this release the ImageNet 2011 21K dataset (as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>). Overall, this dataset contains 14,197,122 images from 21,841 classes, and we conduct our experiment on 20,842 unseen classes 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Semantic spaces</head><p>SUN Each image is annotated with 102 continuousvalued attributes. For each class, we average attribute vectors over all images belonging to that class to obtain a class-level attribute vector.</p><p>ImageNet We train a skip-gram language model <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> on the latest Wikipedia dump corpus 6 (with more than 3 billion words) to extract a 500-dimensional word vector for each class. In particular, we train the model using the word2vec package 7 -we preprocess the corpus with the word2phrase function so that we can directly obtain word vectors for both single-word and multiple-word terms, including those terms in the Im-ageNet synsets 8 . We impose no restriction on the vocabulary size. Following <ref type="bibr" target="#b12">[13]</ref>, we use a window size of 20, apply the hierarchical softmax for predicting adjacent terms, and train the model with a single pass through the corpus. As one class may correspond to multiple word vectors by the nature of synsets, we simply average them to form a single word vector for each class. We obtain word vectors for all the 1,000 seen classes and for 20,345 (out of 20,842) unseen classes. We ignore classes without word vectors in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Visual features</head><p>We denote features that are not extracted by deep learning as shallow features.</p><p>Shallow features On AwA, many existing approaches take traditional features such as color histograms, SIFT, and PHOG that come with the dataset <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>, while others use the Fisher vectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The SUN dataset also comes with several traditional shallow features, which are used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>In our experiments, we use the shallow features provided by <ref type="bibr" target="#b24">[25]</ref> , <ref type="bibr" target="#b19">[20]</ref>, and <ref type="bibr" target="#b35">[36]</ref> for AwA, CUB, and SUN, respectively, unless stated otherwise.</p><p>Deep features Given the recent impressive success of deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b22">[23]</ref> on image classification, we conduct experiments with deep features on all datasets. We use the Caffe package <ref type="bibr" target="#b20">[21]</ref> to extract AlexNet <ref type="bibr" target="#b22">[23]</ref> and GoogLeNet <ref type="bibr" target="#b44">[45]</ref> features for images from AwA and CUB. Observing that GoogLeNet give superior results over AlexNet on AwA and CUB, we focus on GoogLeNet features on large datasets: SUN and ImageNet. These networks are pre-trained on the ILSVRC 2012 1K dataset <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref> for AwA, CUB, and ImageNet. For SUN, the networks are pre-trained on the Places database <ref type="bibr" target="#b52">[52]</ref>, which was shown to outperform the networks pre-trained on ImageNet on scene classification tasks. For AlexNet, we use the 4,096-dimensional activations of the penultimate layer (fc7) as features, and for GoogLeNet we extract features by the 1,024-dimensional activations of the pooling units following the suggestion by <ref type="bibr" target="#b1">[2]</ref>.</p><p>For CUB, we crop all images with the provided bounding boxes. For ImageNet, we center-crop all images and do not perform any data augmentation or other preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Evaluation protocols on ImageNet</head><p>When computing Hierarchical precision@K (HP@K), we use the algorithm in the Appendix of <ref type="bibr" target="#b12">[13]</ref> to compute a set of at least K classes that are considered to be correct. This set is called hCorrectSet and it is computed for each K and class c. See Algorithm 1 for more details. The main idea is to expand the radius around the true class c until the set has at least K classes.</p><p>Note that validRadiusSet depends on which classes are in the label space to be predicted (i.e., depending on whether we consider 2-hop, 3-hop, or All. We obtain the label sets for 2-hop and 3-hop from the authors of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. We implement Algorithm 1 to derive hCorrectSet ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation details D.1. How to avoid over-fitting?</head><p>Since during training we have access only to data from the seen classes, it is important to avoid over-fitting Algorithm 1 Algorithm for computing hCorrectSet for H@K <ref type="bibr" target="#b12">[13]</ref> 1: Input: K, class c, ImageNet hierarchy 2: hCorrectSet ? ? 3: R ? 0 4: while NumberElements(hCorrectSet) &lt; K do <ref type="bibr">5:</ref> radiusSet ? all nodes in the hierarchy which are R hops from c <ref type="bibr">6:</ref> validRadiusSet ? ValidLabelNodes(radiusSet) <ref type="bibr">7:</ref> hCorrectSet ? hCorrectSet ? validRadiusSet 8:</p><p>R ? R + 1 9: end while 10: return hCorrectSet to those seen classes. We apply the class-wise crossvalidation strategy (Section A), and restrict the semantic embeddings of phantom classes to be close to the semantic embeddings of seen classes (Section 3.2 of the main text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Combination of attributes and word vectors</head><p>In <ref type="table" target="#tab_3">Table 5</ref> of the main text and Section E.2 of this material, we combine attributes and word vectors to improve the semantic embeddings. We do so by first computing s rc in eq. (2) of the main text for different semantic sources, and then perform convex combination on s rc of different sources to obtain the final s rc . The combining weights are determined via cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Initialization</head><p>All variables are randomly initialized, unless stated otherwise. Other details on initialization can be found in Section 3.2 of the main text and Section B and D.5 of this material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. ConSE [33]</head><p>Instead of using the CNN 1K-class classifiers directly, we train (regularized) logistic regression classifiers using recently released multi-core version of LIB-LINEAR <ref type="bibr" target="#b10">[11]</ref>. Furthermore, in <ref type="bibr" target="#b32">[33]</ref>, the authors use the averaged word vectors for seen classes, but keep for each unseen class the word vectors of all synonyms. In other words, each unseen class can be represented by multiple word vectors. In our implementation, we use averaged word vectors for both seen and unseen classes for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Varying the number of base classifiers</head><p>In Section 4.2.3 and <ref type="figure" target="#fig_2">Figure 2</ref> of the main text, we examine the use of different numbers of base classifiers (i.e., R). The semantic embedding b r of the phantom classes are set equal to a r , ?r ? {1, ? ? ? , R} at 100% (i.e., R = S). For percentages smaller than 100%, we perform K-means and set b r to be the cluster centroids after 2 normalization (in this case, R = K). For percentages larger than 100%, we set the first S b r to be a r , and the remaining b r as the random combinations of a r (also with 2 normalization on b r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional experimental results and analyses</head><p>We present in this section some additional experimental results on zero-shot learning. Unless stated otherwise, we focus on learning with the one-versus-other loss (cf. eq. (5) of the main text). <ref type="table" target="#tab_6">Table 7</ref> shows the results on CUB (averaged over four splits) using the hyper-parameters tuned by classwise CV and sample-wise CV, respectively. The results based on class-wise CV are about 2% better than those of sample-wise CV, verifying the necessity of simulating the zero-shot learning scenario while we tune the hyperparameters at the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Cross-validation (CV) strategies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Additional comparison of different semantic spaces for embedding classes</head><p>Our method for synthesizing classifiers accepts different semantic embedding spaces. We expand our results on <ref type="table" target="#tab_3">Table 5</ref> of the main text to include AlexNet features as well. The results are in <ref type="table" target="#tab_7">Table 8</ref>. We use word vectors provided by Fu et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which are of 100 and 1000 dimensions per class, respectively. We see that the two types of features, AlexNet and GoogLeNet, demonstrate very similar trends. First, higher-dimensional word vectors often give rise to better performance. Second, human-annotated attributes outperform automatically-learned word vectors. Third, combining the word vectors and the attributes leads to better results than separately using either one of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Comparison to other state-of-the-art methods</head><p>In <ref type="table" target="#tab_8">Table 9</ref>, we contrast our methods to several other state-of-the-art methods, in addition to <ref type="table">Table 2</ref> of the main text. We note subtle differences in the experiment setup of some of these methods from ours:</p><p>? TMV-BLP and TMV-HLP <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. These methods focus on the transductive setting, where they have access to unlabeled test data from unseen classes during the training stage. Additionally, they use OverFeat <ref type="bibr" target="#b41">[42]</ref> features for CUB, Over-Feat+DeCAF <ref type="bibr" target="#b8">[9]</ref> for AwA, and both attributes and word vectors for class embeddings.</p><p>? <ref type="bibr" target="#b21">[22]</ref>. This method works on the transductive setting. It uses OverFeat features for both AwA and CUB, and combines attributes and word vectors for class embeddings.</p><p>? <ref type="bibr" target="#b26">[27]</ref> This method works on the semi-supervised setting, where a portion of unlabeled data (not used for testing) from unseen classes are available at training.</p><p>? HAT-n <ref type="bibr" target="#b2">[3]</ref>. This method uses extra semantic information (WordNet class hiearchy). It uses CNN-M2K features <ref type="bibr" target="#b5">[6]</ref> and extra cropped images.</p><p>? AMP (SR+SE) <ref type="bibr" target="#b15">[16]</ref>. This method uses attributes and (100-dimensional) word vectors and OverFeat features in their experiments.</p><p>? <ref type="bibr" target="#b48">[49]</ref>. This method focuses on mining/discovering new (category-level) attributes. It requires extra human efforts to annotate the new attributes for unseen classes.</p><p>? <ref type="bibr" target="#b18">[19]</ref>. The best result on AwA presented in this paper uses the discovered attributes in <ref type="bibr" target="#b48">[49]</ref>.</p><p>As shown in <ref type="table" target="#tab_8">Table 9</ref>, our method outperforms all of them on the dataset CUB despite the fact that they employ extra images or semantic embedding information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.1 SUN-10</head><p>Some existing work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref> considers another setting for SUN dataset -with 707 seen classes and 10 unseen classes. Moreover, <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref> use the VGGverydeep-19 <ref type="bibr" target="#b42">[43]</ref> CNN features. In <ref type="table" target="#tab_8">Table 9</ref>, we provide results of our approach based on this splitting. Compared to previously published results, our method again clearly shows superior performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Discussion on the numbers of seen and unseen classes</head><p>In this subsection, we analyze the results under different numbers of seen/unseen classes in performing zeroshot learning using the CUB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4.1 Varying the number of seen classes</head><p>We first examine the performance of zero-shot learning under different numbers of seen classes (e.g., 50, 100, and 150) while fixing the number of unseen classes to be 50. We perform 20 random selections of seen/unseen classes. Unsurprisingly, <ref type="table" target="#tab_0">Table 10</ref> shows that increasing the number of seen classes in training leads to improved performance on zero-shot learning.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4.2 Varying the number of unseen classes</head><p>We then examine the performance of our approach to zero-shot learning under different numbers of unseen classes (e.g., within [0, 150]), with the number of seen classes fixed to be 50 during training. We again conduct experiments on CUB, and perform 20 random selections of seen/unseen classes. The results are presented in <ref type="figure" target="#fig_6">Figure 4</ref>. We see that the accuracy drops as the number of unseen classes increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Results on learning metrics for computing semantic similarities</head><p>We improve our method by also learning metrics for computing semantic similarity. Please see Section B for more details. Preliminary results on AwA in <ref type="table" target="#tab_0">Table 11</ref> suggest that learning metrics can further improve upon our current one-vs-other formulation. <ref type="table" target="#tab_0">Table 12</ref> provides expanded zero-shot learning results on ImageNet (cf. <ref type="table" target="#tab_2">Table 3</ref> of the main text). Note that ConSE <ref type="bibr" target="#b32">[33]</ref> has a free parameter T , corresponding to how many nearest seen classes to use for convex combination. In our implementation, we follow the paper to test on T = 1, 10, and 1,000. We further apply the classwise cross-validation (cf. Section A of this material) to automatically set T . We also report the published best result in <ref type="bibr" target="#b32">[33]</ref>. Our methods (Ours o-vs-o and Ours struct ) achieve the highest accuracy in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6. Detailed results and analysis of experiments on ImageNet</head><p>As mentioned in the main text, the three sets of unseen classes, 2-hop, 3-hop, and All are built according to the ImageNet label hierarchy. Note that they are not mutually exclusive. Indeed, 3-hop contains all the classes in 2-hop, and All contains all in 3-hop. To examine if the semantic similarity/dissimilarity to the 1K seen classes (according to the label hierarchy) would affect the classification accuracy, we split All into three disjoint sets, 2hop, pure 3-hop, and others, which contain 1,509, 6,169, and 12,667 classes, respectively (totally 20,345). We then test on All, but report accuracies of images belonging to different disjoint sets separately. <ref type="figure" target="#fig_8">Figure 5</ref> summarizes the results. Our method outperforms ConSE in almost all cases. The decreasing accuracies from 2-hop, pure 3-hop, to others (by both methods) verify the high correlation of the semantic relationship to the classification accuracy in zero-shot learning. This observation suggests an obvious potential limitation: it is unrealistic to expect good performance on unseen classes that are semantically too dissimilar to seen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7. Qualitative results</head><p>In this subsection, we present qualitative results of our method. We first illustrate what visual information the models (classifiers) for unseen classes capture, when provided with only semantic embeddings (no example images). In <ref type="figure">Figure 6</ref>, we list (on top) the 10 unseen class labels of AwA, and show (in the middle) the top-5 images classified into each class c, according to the decision values w T c x (cf. eq. (1) and (4) of the main text). Misclassified images are marked with red boundaries. At the bottom, we show the first (highest score) misclassified image (according to the decision value) into each class and its ground-truth class label. According to the top images, our method reasonably captures discriminative visual properties of each unseen class based solely on its semantic embedding. We can also see that the misclassified images are with appearance so similar to that of predicted class that even humans cannot easily distin- <ref type="table" target="#tab_0">Table 12</ref>: Flat Hit@K and Hierarchial precision@K performance (in %) on the task of zero-shot learning on ImageNet. We mainly compare it with ConSE(T ) <ref type="bibr" target="#b32">[33]</ref>, where T is the number of classifiers to be combined in their paper. For ConSE(CV), T is obtained by class-wise CV. Lastly, the best published results in <ref type="bibr" target="#b32">[33]</ref> are also reported, corresponding to ConSE(10) <ref type="bibr" target="#b32">[33]</ref>. For both types of metrics, the higher the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenarios</head><p>Methods   guish between the two. For example, the pig image at the bottom of the second column looks very similar to the image of hippos. <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref> present the results in the same format on CUB and SUN, respectively (both on a subset of unseen class labels). We further analyze the success and failure cases; i.e., why an image from unseen classes is misclassified. The illustrations are in <ref type="figure">Figure 9</ref>, 10, and 11 for AwA, CUB, and SUN, respectively. In each figure, we consider (Left) one unseen class and show its convex combination weights s c = {s c1 , ? ? ? , s cR } as a histogram. We then present (Middle-Left) the top-3 semantically simi-  lar (in terms of s c ) seen classes and their most representative images. As our model exploits phantom classes to connect seen and unseen classes in both semantic and model spaces, we expect that the model (classifier) for such unseen class captures similar visual information as those for semantically similar seen classes do. (Middle-Right) We examine two images of such unseen class, where the top one is correctly classified; the bottom one, misclassified. We also list (Right) the top-3 predicted labels (within the pool of unseen classes) and their most representative images. Green corresponds to correct labels. We see that, in the misclassified cases, the test images are visually dissimilar to those of the semantically similar seen classes. The synthesized unseen classifiers, therefore, cannot correctly recognize them, leading to incorrect predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8. Comparison between shallow and deep features of our approach</head><p>In <ref type="table">Table 4</ref> of the main text, our approach performs better with deep features than with shallow features compared to other methods. We propose explanations for this phenomenal. Deep features are learned hierarchically and expected to be more abstract and semantically meaningful. Arguably, similarities between them (measured in inner products between classifiers) might be more congruent with similarities computed in the semantic embedding space for combining classifiers. Additionally, shallow features have higher dimensions (around 10,000) than deep features (e.g., 1024 for GoogLeNet) so they might require more phantom classes to synthesize classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.9. Analysis on the number of base classifiers</head><p>In <ref type="figure" target="#fig_2">Fig. 2</ref> of the main text, we show that even by using fewer base (phantom) classifiers than the number of seen classes (e.g., around 60 %), we get comparable or even better results, especially for CUB. We surmise that this is because CUB is a fine-grained recognition benchmark and has higher correlations among classes, and provide analysis in <ref type="figure" target="#fig_2">Fig. 12</ref> to justify this.</p><p>We train one-versus-other classifiers for each value of the regularization parameter (i.e., ? in eq. (5) of the main text) on both AwA and CUB, and then perform PCA on the resulting classifier matrices. We then plot the required number (in percentage) of PCA components to capture 95% of variance in the classifiers. Clearly, AwA requires more. This explains why we see the drop in accuracy for AwA but not CUB in <ref type="figure" target="#fig_2">Fig. 2</ref> of the main text when using even fewer base classifiers. Particularly, the low percentage for CUB in <ref type="figure" target="#fig_2">Fig. 12</ref> implies that fewer </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>We vary the number of phantom classes R as a percentage of the number of seen classes S and investigate how much that will affect classification accuracy (the vertical axis corresponds to the ratio with respect to the accuracy when R = S). The base classifiers are learned with Ours o-vs-o .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, I yn,c ; w c )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Data splitting for different cross-validation (CV) strategies: (a) the seen-unseen class splitting for zero-shot learning, (b) the sample-wise CV, (c) the class-wise CV (cf. Section 3.2 of the main paper).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>UFigure 4 :</head><label>4</label><figDesc>and S S = 50 S = 100 S = 150 U Performance of our method under different numbers of unseen classes on CUB. The number of seen classes is fixed to be 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>On the ImageNet dataset, we outperform ConSE (i.e., ConSE(10) of our implementation) on different disjoint sets of categories in the scenario All in almost all cases. See Section E.6 of this material for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Qualitative results of our method (Ours o-vs-o ) on SUN. (Top) We list a subset of unseen class labels. (Middle) We show the top-5 images classified into each class, according to the decision values. Misclassified images are marked with red boundaries. (Bottom) We show the first misclassified image (according to the decision value) into each class and its ground-truth class label. Success/failure case analysis of our method (Ours struct ) on AwA: (Left) an unseen class label, (Middle-Left) the top-3 semantically similar seen classes to that unseen class, (Middle-Right) two test images of such unseen class, and (Right) the top-3 predicted unseen classes. The green text corresponds to the correct label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Success/failure case analysis of our method (Ours struct ) on CUB. (Left) an unseen class label, (Middle-Left) the top-3 semantically similar seen classes to that unseen class, (Middle-Right) two test images of such unseen classes, and (Right) the top-3 predicted unseen class. The green text corresponds to the correct label. Success/failure case analysis of our method (Ours o-vs-o ) on SUN. (Left) an unseen class label, (Middle-Left) the top-3 semantically similar seen classes to that unseen class, (Middle-Right) two test images of such unseen classes, and (Right) the top-3 predicted unseen class. The green text corresponds to the correct label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Percentages of basis components required to capture 95% of variance in classifier matrices for AwA and CUB. base classifiers are possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Key characteristics of studied datasets</figDesc><table><row><cell>Dataset</cell><cell cols="2"># of seen # of unseen</cell><cell>Total #</cell></row><row><cell>name</cell><cell>classes</cell><cell>classes</cell><cell>of images</cell></row><row><cell>AwA  ?</cell><cell>40</cell><cell>10</cell><cell>30,475</cell></row><row><cell>CUB  ?</cell><cell>150</cell><cell>50</cell><cell>11,788</cell></row><row><cell>SUN  ?</cell><cell>645/646</cell><cell>72/71</cell><cell>14,340</cell></row><row><cell>ImageNet  ?</cell><cell>1,000</cell><cell>20,842</cell><cell>14,197,122</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? : Following the prescribed split in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Results were just brought to our attention. Note that VGG<ref type="bibr" target="#b42">[43]</ref> instead of GoogLeNet features were used, improving on AwA but worsening on CUB. Our results using VGG will appear in a longer version of this paper.</figDesc><table><row><cell></cell><cell></cell><cell>30.4  ?</cell><cell>-</cell><cell>-</cell></row><row><cell>[50]</cell><cell cols="2">80.5 42.1  ?</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours o-vs-o</cell><cell>69.7</cell><cell>53.4</cell><cell>62.8</cell><cell>1.4</cell></row><row><cell>Ours cs</cell><cell>68.4</cell><cell>51.6</cell><cell>52.9</cell><cell>-</cell></row><row><cell>Ours struct</cell><cell>72.9</cell><cell>54.7</cell><cell>62.7</cell><cell>1.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? : Results reported on a particular seen-unseen split.:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between results by ConSE and our method on ImageNet. For both types of metrics, the higher the better. 24.7 32.7 41.8 21.4 24.7 26.9 28.4 ConSE by us 8.3 12.9 21.8 30.9 41.7 21.5 23.8 27.5 31.3 Ours o-vs-o 10.5 16.7 28.6 40.1 52.0 25.1 27.7 30.3 32.1</figDesc><table><row><cell>Scenarios</cell><cell>Methods</cell><cell></cell><cell></cell><cell cols="2">Flat Hit@K</cell><cell></cell><cell cols="4">Hierarchical precision@K</cell></row><row><cell></cell><cell>K=</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>2-hop</cell><cell cols="10">ConSE [33] 15.1 Ours struct 9.4 9.8 15.3 25.8 35.8 46.5 23.8 25.8 28.2 29.6</cell></row><row><cell>3-hop</cell><cell>ConSE [33]</cell><cell>2.7</cell><cell>4.4</cell><cell>7.8</cell><cell cols="2">11.5 16.1</cell><cell>5.3</cell><cell cols="3">20.2 22.4 24.7</cell></row><row><cell></cell><cell>ConSE by us</cell><cell>2.6</cell><cell>4.1</cell><cell>7.3</cell><cell cols="2">11.1 16.4</cell><cell>6.7</cell><cell cols="3">21.4 23.8 26.3</cell></row><row><cell></cell><cell>Ours o-vs-o</cell><cell>2.9</cell><cell>4.9</cell><cell>9.2</cell><cell cols="2">14.2 20.9</cell><cell>7.4</cell><cell cols="3">23.7 26.4 28.6</cell></row><row><cell></cell><cell>Ours struct</cell><cell>2.9</cell><cell>4.7</cell><cell>8.7</cell><cell cols="2">13.0 18.6</cell><cell>8.0</cell><cell cols="3">22.8 25.0 26.7</cell></row><row><cell>All</cell><cell>ConSE [33]</cell><cell>1.4</cell><cell>2.2</cell><cell>3.9</cell><cell>5.8</cell><cell>8.3</cell><cell>2.5</cell><cell>7.8</cell><cell>9.2</cell><cell>10.4</cell></row><row><cell></cell><cell>ConSE by us</cell><cell>1.3</cell><cell>2.1</cell><cell>3.8</cell><cell>5.8</cell><cell>8.7</cell><cell>3.2</cell><cell>9.2</cell><cell cols="2">10.7 12.0</cell></row><row><cell></cell><cell>Ours o-vs-o</cell><cell>1.4</cell><cell>2.4</cell><cell>4.5</cell><cell>7.1</cell><cell>10.9</cell><cell>3.1</cell><cell>9.0</cell><cell cols="2">10.9 12.5</cell></row><row><cell></cell><cell>Ours struct</cell><cell>1.5</cell><cell>2.4</cell><cell>4.4</cell><cell>6.7</cell><cell>10.0</cell><cell>3.6</cell><cell>9.6</cell><cell cols="2">11.0 12.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Effect of types of semantic embeddings on AwA.</figDesc><table><row><cell>Semantic embeddings</cell><cell cols="2">Dimensions Accuracy (%)</cell></row><row><cell>word vectors</cell><cell>100</cell><cell>42.2</cell></row><row><cell>word vectors</cell><cell>1000</cell><cell>57.5</cell></row><row><cell>attributes</cell><cell>85</cell><cell>69.7</cell></row><row><cell>attributes + word vectors</cell><cell>185</cell><cell>73.2</cell></row><row><cell>attributes + word vectors</cell><cell>1085</cell><cell>76.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Effect of learning semantic embeddings</figDesc><table><row><cell cols="3">Datasets Types of embeddings w/o learning</cell><cell>w/ learning</cell></row><row><cell>AwA</cell><cell>attributes</cell><cell>69.7%</cell><cell>71.1%</cell></row><row><cell></cell><cell>100-d word vectors</cell><cell>42.2%</cell><cell>42.5%</cell></row><row><cell></cell><cell>1000-d word vectors</cell><cell>57.6%</cell><cell>56.6%</cell></row><row><cell>CUB</cell><cell>attributes</cell><cell>53.4%</cell><cell>54.2%</cell></row><row><cell>SUN</cell><cell>attributes</cell><cell>62.8%</cell><cell>63.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison between sample-and class-wise crossvalidation for hyper-parameter tuning on CUB (learning with the one-versus-other loss).</figDesc><table><row><cell>CV</cell><cell>CUB</cell><cell>CUB</cell></row><row><cell>Scenarios</cell><cell cols="2">(AlexNet) (GoogLeNet)</cell></row><row><cell>Sample-wise</cell><cell>44.7</cell><cell>52.0</cell></row><row><cell>Class-wise</cell><cell>46.6</cell><cell>53.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison between different semantic embedding spaces for our approach.</figDesc><table><row><cell>Methods</cell><cell>Semantic embeddings</cell><cell>Dimensions</cell><cell>Features</cell><cell>AwA</cell></row><row><cell>Ours o-vs-o</cell><cell>word vectors</cell><cell>100</cell><cell>AlexNet</cell><cell>37.6</cell></row><row><cell>Ours o-vs-o</cell><cell>word vectors</cell><cell>1000</cell><cell>AlexNet</cell><cell>52.4</cell></row><row><cell>Ours o-vs-o</cell><cell>attributes</cell><cell>85</cell><cell>AlexNet</cell><cell>64.0</cell></row><row><cell cols="2">Ours o-vs-o attributes + word vectors</cell><cell>85 + 100</cell><cell>AlexNet</cell><cell>65.6</cell></row><row><cell cols="2">Ours o-vs-o attributes + word vectors</cell><cell>85 + 1000</cell><cell>AlexNet</cell><cell>68.0</cell></row><row><cell>SJE[2]</cell><cell>word vectors</cell><cell>400</cell><cell cols="2">GoogLeNet 51.2</cell></row><row><cell>Ours o-vs-o</cell><cell>word vectors</cell><cell>100</cell><cell cols="2">GoogLeNet 42.2</cell></row><row><cell>Ours o-vs-o</cell><cell>word vectors</cell><cell>1000</cell><cell cols="2">GoogLeNet 57.5</cell></row><row><cell>Ours o-vs-o</cell><cell>attributes</cell><cell>85</cell><cell cols="2">GoogLeNet 69.7</cell></row><row><cell cols="2">Ours o-vs-o attributes + word vectors</cell><cell>85 + 100</cell><cell cols="2">GoogLeNet 73.2</cell></row><row><cell cols="2">Ours o-vs-o attributes + word vectors</cell><cell>85 + 1000</cell><cell cols="2">GoogLeNet 76.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparison between our method and other state-of-the-art methods for zero-shot learning. Our results are based on the GoogLeNet features and attributes or attributes + word vectors as semantic embeddings. See Section E.3 for the details of other methods and SUN-10. Based on Fisher vectors as shallow features. Ours with attributes + (1000-dimensional) word vectors.</figDesc><table><row><cell>Methods</cell><cell cols="3">Shallow features</cell><cell></cell><cell cols="2">Deep features</cell><cell></cell></row><row><cell></cell><cell>AwA</cell><cell>CUB</cell><cell cols="5">SUN-10 AwA CUB SUN SUN-10</cell></row><row><cell>TMV-BLP [14]</cell><cell>47.7</cell><cell>16.3  ? ?</cell><cell>-</cell><cell>77.8</cell><cell>45.2  ?</cell><cell>-</cell><cell>-</cell></row><row><cell>TMV-HLP [15]</cell><cell cols="2">49.0 19.5  ? ?</cell><cell>-</cell><cell>80.5</cell><cell>47.9  ?</cell><cell>-</cell><cell>-</cell></row><row><cell>[22]</cell><cell>49.7</cell><cell>28.1  ? ?</cell><cell>-</cell><cell>75.6</cell><cell>40.6  ?</cell><cell>-</cell><cell>-</cell></row><row><cell>[27]</cell><cell>40.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HAT-n [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">68.8 48.6  ?</cell><cell>-</cell><cell>-</cell></row><row><cell>AMP (SR+SE) [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>[49]</cell><cell>48.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>[19]</cell><cell>48.7</cell><cell>-</cell><cell>56.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ESZSL [39]</cell><cell>-</cell><cell>-</cell><cell>65.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.1</cell></row><row><cell>SSE-ReLU [51]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.5</cell></row><row><cell>[50]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.8</cell></row><row><cell>SJE[2]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.0</cell></row><row><cell>Ours o-vs-o</cell><cell>42.6</cell><cell>35.0</cell><cell>-</cell><cell>69.7</cell><cell>53.4</cell><cell>62.8</cell><cell>90.0</cell></row><row><cell>Ours cs</cell><cell>42.1</cell><cell>34.7</cell><cell>-</cell><cell>68.4</cell><cell>51.6</cell><cell>52.9</cell><cell>87.0</cell></row><row><cell>Ours struct</cell><cell>41.5</cell><cell>36.4</cell><cell>-</cell><cell>72.9</cell><cell>54.5</cell><cell>62.7</cell><cell>85.0</cell></row><row><cell>? Ours o-vs-o</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? : Results reported by the authors on a particular seen-unseen split.? :? :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Performance of our method under different numberS of seen classes on CUB. The number of unseen classes U is fixed to be 50.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Effect of learning metrics for computing semantic similarity on AwA.</figDesc><table><row><cell cols="4">Dataset Type of embeddings w/o learning w/ learning</cell></row><row><cell>AwA</cell><cell>attributes</cell><cell>69.7%</cell><cell>73.4%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For interested readers, if we set the number of attributes as the number of phantom classes (each br is the one-hot representation of an attribute), and use Gaussian kernel with anisotropically diagonal covariance matrix in eq. (3) with properly set bandwidths (either very small or very large) for each attribute, we will recover the formulation in<ref type="bibr" target="#b0">[1]</ref> when the bandwidths tend to zero or infinity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://dumps.wikimedia.org/enwiki/latest/ enwiki-latest-pages-articles.xml.bz2 on September 1, 2015</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.image-net.org/api/xml/structure_ released.xml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We are aware of recent work by Lu<ref type="bibr" target="#b28">[29]</ref> that introduces a novel form of semantic embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">There is one class in the ILSVRC 2012 1K dataset that does not appear in the ImageNet 2011 21K dataset. Thus, we have a total of 20,842 unseen classes to evaluate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://dumps.wikimedia.org/enwiki/latest/ enwiki-latest-pages-articles.xml.bz2 on September 1st, 2015 7 https://code.google.com/p/word2vec/ 8 Each class of ImageNet is a synset: a set of synonymous terms, where each term is a word or phrase.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How to transfer? zeroshot object recognition via hierarchical transfer of semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convex multitask feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="243" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Write a classifier: Zero-shot learning using purely textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active transfer learning with zero-shot priors: Reusing past datasets for future tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decorrelating semantic visual attributes by resisting the urge to share</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised zero-shot classification with label representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attributes make sense on segmented objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised learning of neural network outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00990</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Costa: Co-occurrence statistics for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The sun attribute database: Beyond categories for deeper scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a largescale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What helps where-and why? semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Zeroshot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A unified probabilistic approach modeling relationships between attributes and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A unified perspective on multi-domain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Classifying unseen instances by learning class-independent similarity functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04512</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
