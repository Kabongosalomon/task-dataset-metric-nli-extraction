<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics and Computing</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
							<email>fan6@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics and Computing</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics and Computing</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we newly introduce the concept of temporal attention filters, and describe how they can be used for human activity recognition from videos. Many high-level activities are often composed of multiple temporal parts (e.g., sub-events) with different duration/speed, and our objective is to make the model explicitly learn such temporal structure using multiple attention filters and benefit from them. Our temporal filters are designed to be fully differentiable, allowing end-of-end training of the temporal filters together with the underlying frame-based or segment-based convolutional neural network architectures. This paper presents an approach of learning a set of optimal static temporal attention filters to be shared across different videos, and extends this approach to dynamically adjust attention filters per testing video using recurrent long short-term memory networks (LSTMs). This allows our temporal attention filters to learn latent sub-events specific to each activity. We experimentally confirm that the proposed concept of temporal attention filters benefits the activity recognition, and we visualize the learned latent sub-events.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Human activity recognition is the problem of identifying events performed by humans given a video input. It is formulated as a binary (or multiclass) classification problem of outputting activity class labels, and researchers have been studying better features, representations, and learning algorithms to improve the classification <ref type="bibr" target="#b0">(Aggarwal and Ryoo 2011)</ref>. Such classification not only allows categorization of videos pre-segmented to contain one single activity, but also enables the 'detection' of activities from streaming videos together with temporal window proposal methods like the sliding window or selective search . Activity recognition is an important problem with many societal applications including smart surveillance, video search/retrieval, intelligent robots, and other monitoring systems.</p><p>Particularly, in the past 2-3 years, activity recognition approaches taking advantage of convolutional neural networks (CNNs) have received a great amount of attention. Motivated by the success of image-based object recognition using CNNs, researchers attempted developing CNNs for videos. Some approaches directly took advantage of image-based CNN architectures by applying them to every * These authors contributed equally to the paper. video frame <ref type="bibr" target="#b3">(Jain, van Gemert, and Snoek 2014;</ref><ref type="bibr" target="#b4">Ng et al. 2015)</ref>, while some tried to learn 3-D XYT spatio-temporal convolutional filters from short video segments <ref type="bibr" target="#b6">(Tran et al. 2015)</ref>. In order to represent each video, temporal pooling (e.g., max/average pooling) were often applied on top of multiple (sampled) per-frame or per-video-segment CNNs <ref type="bibr" target="#b3">(Jain, van Gemert, and Snoek 2014;</ref><ref type="bibr" target="#b3">Karpathy et al. 2014;</ref><ref type="bibr" target="#b6">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b4">Ng et al. 2015;</ref><ref type="bibr" target="#b6">Tran et al. 2015)</ref>. Similar to the object recognition, these approaches obtained superior results compared to traditional approaches of using hand-crafted features.</p><p>However, in terms of learning and considering activities' temporal structure in videos, previous CNN approaches were limited. Many high-level activities are often composed of multiple temporal parts (i.e., sub-events) with different duration/speed, but approaches to learn explicit activity temporal structure together with CNN parameters have not been studied in depth. For instance, the typical strategy of taking max (or average) pooling over sampled per-frame or per-segment CNN responses <ref type="bibr" target="#b3">(Jain, van Gemert, and Snoek 2014;</ref><ref type="bibr" target="#b6">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b4">Ng et al. 2015;</ref><ref type="bibr" target="#b6">Tran et al. 2015)</ref> completely ignores such temporal structure in longer activity videos. <ref type="bibr" target="#b6">(Ryoo, Rothrock, and Matthies 2015)</ref> showed a potential that making the system consider multiple video sub-intervals using a temporal pyramid benefits the recognition, but it was done with predetermined intervals without any learning. LSTMbased recurrent neural network approaches <ref type="bibr" target="#b4">(Ng et al. 2015;</ref><ref type="bibr" target="#b7">Yeung et al. 2016)</ref> were able to process per-frame CNN responses sequentially, but no explicit sub-event or interval learning was attempted.</p><p>What we need instead is an approach that explicitly 'learns' to focus on important sub-intervals of the activity videos while also optimizing their temporal resolution for the recognition. This is a challenging problem since we want to make this sub-event learning done in an end-to-end fashion together with the training of underlying CNN parameters. Furthermore, it is often the case that ground truth labels of sub-events to be learned are not provided, making them latent variables. This paper presents a new video classification approach that overcomes such limitations using temporal attention filters. We newly introduce the concept of fully differentiable temporal attention filters and describe how they can be learned and used to enable better recognition of human activ-</p><formula xml:id="formula_0">... ... ... ... ... ... ... v 1 v 2 v 3 ? v d v 1 v 2 v 3 ? v d v 1 v 2 v 3 ? v d v 1 v 2 v 3 ? v d v 1 v 2 v 3 ? v d v 1 v 2 v 3 ? v d</formula><p>Temporal attention filters FC Layer Per-frame CNNs Raw video input FC Layer <ref type="figure">Figure 1</ref>: Illustration of our overall recognition architecture with temporal attention filters. M number of temporal filters are learned to focus on different temporal part of video frame features (i.e., latent sub-events). Each filter is composed of a set of Gaussian filters which take a weighted sum of local information. Outputs of the temporal filters are concatenated, and attached with a fully connected layers to perform video classification.</p><p>ities from videos. The main idea is to make the network learn and take advantage of multiple temporal attention filters to be applied on top of per-frame CNNs <ref type="figure">(Figure 1</ref>). Each learned attention filter corresponds to a particular sub-interval of the activity the system should focus on (i.e., latent sub-events), and is represented with its center location, duration, and resolution in the relative temporal coordinate. Our approach abstracts per-frame (or per-segment) CNN responses within the sub-interval corresponding to the attention filter, allowing the system to use their results for the classification. Notably, our temporal attention filters are designed to be fully differentiable, motivated by the spatial attention filters for images <ref type="bibr" target="#b2">(Gregor et al. 2015)</ref>. This allows the end-to-end training of the parameters deciding attention filters; the system learns temporal attention filters jointly with the parameters of the underlying per-frame (or per-segment) CNNs. As a result, our temporal filters are trained to be optimized for the recognition tasks, automatically learning its location/scale from the training data without sub-event labels.</p><p>The paper not only presents an approach of learning optimal static temporal attention filters to be shared across different videos, but also present an approach of dynamically adjusting attention filters per testing video using recurrent long short-term memory cells (LSTMs). Instead of learning static temporal filters who location/duration/resolution is shared by all videos, our LSTM based approach dynamically and adaptively adjusts its filter parameters depending on the video, by going through multiple iterations. Our proposed approach is able to function in conjunction with any per-frame or per-video-segment CNNs as well as with other types of feature representations (e.g., Fisher vectors), making it very generally applicable for many video understanding scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous works</head><p>As described in the introduction, the direction of using convolutional neural networks for video classification is becoming increasingly popular, since it allows end-to-end training of convolutional filters optimized for the training data. However, these prior works focused only on capturing dynamics in very short video intervals without much consideration on long-term temporal structure of activity videos. Optical flow only captures differences between two consecutive frames (Simonyan and Zisserman 2014). Even with the video-based 3-D XYT CNNs <ref type="bibr" target="#b6">(Tran et al. 2015)</ref> or trajectory CNNs <ref type="bibr" target="#b7">(Wang, Qiao, and Tang 2015)</ref>, only the temporal dynamics within short intervals with a fixed duration (e.g., 15 frames) were captured without considering longerterm structure or attempting to learn latent sub-events. <ref type="bibr" target="#b6">(Ryoo, Rothrock, and Matthies 2015)</ref> showed a potential that considering temporal structure in terms of sub-intervals (e.g., temporal pyramid) may benefit the recognition, but they did not attempt any learning. Similarly, <ref type="bibr" target="#b4">(Li et al. 2016)</ref> considered multiple different temporal scales, but learning of how the system should choose such scales were not attempted. <ref type="bibr" target="#b7">(Varol, Laptev, and Schmid 2016)</ref> also used fixed intervals. Recurrent neural networks such as LSTMs were also used to model sequences <ref type="bibr" target="#b4">(Ng et al. 2015)</ref>, but they were unable to explicitly consider different temporal sub-intervals and their structure. That is, learning to consider different intervals with different temporal resolution was not possible, and no subevent learning as involved. <ref type="bibr" target="#b7">(Yeung et al. 2016)</ref> proposed the use of LSTM to make the system focus on different frames of videos, but it was unable to represent intervals.</p><p>The main contribution of this paper is the introduction of the temporal attention filters that allow their end-to-end training together with underlying CNN architectures. We illustrate that our temporal attention filters can be learned to focus on different temporal aspects of videos (i.e., intervals with different temporal resolutions), and experimentally confirm that such learning benefits the activity recognition. The main difference between our approach and previous temporal structure learning methods for activity recognition (e.g., <ref type="bibr" target="#b5">(Niebles, Chen, and Fei-Fei 2010;</ref><ref type="bibr" target="#b5">Ryoo and Matthies 2013)</ref>) is that our temporal filters are designed to be fully differentiable, which allows their joint learning and testing with modern CNN architectures.</p><p>To our knowledge, this paper is the first paper to enable learning of latent temporal sub-events in an end-to-end fashion using CNN architectures for activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition approach</head><p>We design our model as a set of temporal filters, each corresponding to a particular sub-event, placed on top of per-frame (or per-segment) CNN architectures ( <ref type="figure">Figure 1</ref>). The idea is to train this fully differential model in an end-to-end fashion, jointly learning latent sub-events composing each activity, underlying CNN parameters, and the activity classifier.</p><p>In this section, we introduce the concept of temporal attention filters, which extends the spatial attention filter (Gregor et al. 2015) originally designed for digit detection and digit/object synthesis. Next, we present how our proposed model takes advantage of temporal attention filters to learn latent sub-events. The approach learns/mines temporal subevents optimized for the classification without their ground truth annotations, such as 'stretching an arm' in the activity 'punching'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal attention filters</head><p>Each temporal attention filter learns three parameters: a center g, a stride ? and a width ?. These parameters determine where the filter is placed and the size of the interval focused on. A filter consists of N Gaussian filters separated by a stride of ? frames. The goal of this model is to learn where in the video the most useful features appear. Because the videos are of variable length, ? and g are relative to the length of the video. Based on the attention model presented in <ref type="bibr" target="#b2">(Gregor et al. 2015)</ref>, we use the following equations to obtain the mean of the Gaussian filters: g n = 0.5 ? T ? ( g n + 1)</p><formula xml:id="formula_1">? n = T N ? 1 ? n ? i n = g n + (i ? 0.5N + 0.5)? n<label>(1)</label></formula><p>Using ? and ?, the N Gaussian filters are defined by:</p><formula xml:id="formula_2">F m [i, t] = 1 Z m exp(? (t ? ? i m ) 2 2? 2 m ) i ? {0, 1, . . . , N ? 1}, t ? {0, 1, . . . , T ? 1} (2)</formula><p>where Z m is a normalization constant. <ref type="figure">Figure 2</ref> shows an illustration of our temporal attention filter.</p><p>If each frame has D-dimensional features, the filters, F , are applied to each dimension, taking the input of size T ? D to N ? D where T is the number of frames in the video. That is, each temporal filter F generates a N ? D-dimensional vector as an output for any video, which can be passed to a neural network for the classification. Since this model is fully differentiable, all parameters can be learned using gradient descent.</p><p>Let f m [i, d] be the output of our mth temporal attention filter, given the</p><formula xml:id="formula_3">T ? D dimensional input x. Each f m [i, d]</formula><p>describes the response from the ith Gaussian filter on the dth elements of the input vectors. Then,</p><formula xml:id="formula_4">f m [i, d] = F m [i, :] ? x[:, d] = T ?1 t=0 F m [i, t] ? x[t, d] i ? {0, 1, . . . , N ? 1}, d ? {0, 1, . . . , D ? 1} (3) where x[:, d]</formula><p>is the T -dimensional vector corresponding to the sequence of the dth element values in the underlying CNN feature vectors. <ref type="figure">Figure 3</ref> shows how each temporal attention filter is able to capture features from the corresponding sub-interval of the provided video with different temporal resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition with temporal attention filters</head><p>As described in <ref type="figure">Figure 1</ref>, we take advantage of multiple different temporal attention filters by placing them on top of a sequence of per-frame (or per-segment) CNN models. As a result, our model is able to focus on different sub-intervals of video inputs with different temporal resolutions. Outputs of each temporal filters are concatenated and are connected to fully connected layers performing activity classification. Each of our filters learns a latent sub-event, and concatenating the results allows the later FC-layers to look at the features for each sub-event and classify the activity based on them. If we denote the per-frame feature size as D and we have M number of temporal attention filters, each temporal filter generates the output of size N ? D, resulting the total dimensionality to be M ? N ? D. We used 2 fully connected layers (i.e., one hidden layer and one soft-max layer) for the classification.</p><p>Because of the property that our temporal filters are designed to be differentiable, we are able to backpropagate the errors through temporal attention filters reaching the underlying per-frame convolutional layers. This makes end-to-end training of the proposed model possible with video training data. Per-frame CNNs were assumed to share all parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent neural networks with temporal filters</head><p>Although the model presented in the previous subsection allows us to learn temporal attention filters from the training data, it was assumed that the temporal filters are static once learned and are shared across all videos. However, such assumption that relative locations of sub-events are exactly identical across all activity videos can be dangerous. A particular sub-event of the activity (e.g., a person stretching an arm in the case of 'shake hands') may occur earlier or faster in one video than those in the other videos, due to human action style variations. In such cases, using static temporal filters will fail to capture exact sub-events. Rather, the recognition system must learn how to dynamically and adaptively adjust locations of temporal filters depending on the video content.</p><p>Thus, in this subsection, we propose an alternative approach of using a recurrent neural network, the long shortterm memory (LSTM). <ref type="figure">Figure 4</ref> describes our overall LSTM architecture. At each iteration, our LSTM takes the entire video frames as an input and applies per-frame CNNs identical to the previous subsection. Next, instead of using the learned static temporal filters, previous LSTM outputs are used to decided the temporal attention filter parameters in an  <ref type="figure">Figure 4</ref>: An illustration of our temporal attention filter using an LSTM. The LSTM provides the parameters for the filters at each time step. The filters provide input to the next iteration allowing the LSTM to adjust the location of the filters. After S iterations, a fully-connected layer and softmax layer classify the video.</p><p>adaptive fashion. Our approach learns weights that models how previous LSTM iteration outputs (i.e., the abstraction of video information in the previous round) can lead to the better temporal filters in the next iteration More specifically, our attention filter parameters become the function of previous iteration LSTM outputs:</p><formula xml:id="formula_5">(g t , ? t , ? t ) = W n (h h?1 ) = i w i ? h t?1 (i)<label>(4)</label></formula><p>where W n is the function we need to learn modeled as a weighted sum, h t?1 is the LSTM hidden state vector at iteration t ? 1 and h t?1 (i) are its elements. These weights are initialized such that the initial iteration places g at the center of the video, and ? spans the entire duration, allowing the LSTM to get input from the entire sequence of frames.</p><p>Because of the nature that our temporal attention filters are differentiable, we learn the function W n through the backpropagation.</p><p>Notice that our usage of LSTMs is different from previous approaches <ref type="bibr" target="#b4">(Ng et al. 2015)</ref> using LSTMs to capture sequential per-frame feature changes. In our case, the goal of each LSTM iteration is to adjust the temporal filter locations to match the video input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In order to evaluate the effectiveness of our proposed latent sub-event learning approach with fully differentiable temporal attention filters, we conducted a set of experiments comparing our approach using temporal filters against the previous conventional approaches without them while making the approaches use the same features and classifiers. These approaches were evaluated with multiple different features and multiple different datasets.  <ref type="bibr" target="#b7">(Wang, Qiao, and Tang 2015)</ref> which were used as inputs to our temporal filter model. VGG is an imagebased convolutional neural network originally designed for object classification tasks. ITF and TDD are the state-of-theart trajectory-based local video features, each taking advantage of HOG/HOF or CNN feature maps observed around of trajectories. We used the source codes of all these features provided by the authors of the corresponding papers. For TDD, we used the single-scale version of the TDD feature, since it showed the better performance. We used 3-frame short video segments as our unit observations. In the case of VGG features, we applied its CNN architecture to one image frame in every 3 frames, obtaining 4K-dimensional vectors from the final fully connected layer. In the case of trajectory features, we considered the trajectories ending within the 3-frame segment as inputs corresponding to the segment and took advantage of their Fisher vector representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We conducted experiments with two different public video dataset: DogCentric activity dataset <ref type="bibr" target="#b2">(Iwashita et al. 2014) and</ref><ref type="bibr">HMDB dataset (Kuehne et al. 2011)</ref>. The Dog-Centric dataset is a first-person video dataset, and it was chosen because that the previous pooled time series <ref type="bibr">(PoT)</ref> representation <ref type="bibr" target="#b6">(Ryoo, Rothrock, and Matthies 2015)</ref>, which illustrated potential that considering multiple sub-intervals of videos benefit the CNN-based recognition, achieved the state-of-the-art performance on it. The first-person videos in this dataset display a significant amount of ego-motion (i.e., camera motion) of the camera wearer, and it is an extremely challenging dataset. HMDB was chosen due to its popularity. Our experiments were conducted by following each dataset's standard evaluation setting. Both the datasets are designed for multiclass activity video classification. Implementation and baseline classifiers: As mentioned above, we confirm the advantage of our approach with four different types of underlying feature models: VGG, ITF, C3D, and TDD. Our temporal filters were applied on top of them. In all our experiments, we used 2 hidden layers on top of our (multiple) temporal filters: one of size 2048 and one of size 10 and softmax classification.</p><p>As the basic baselines, we tested (1) max-pooling, (2) sumpooling, and (3) mean-pooling across the time dimension, resulting in features of size 1 ? D. These 1 ? D features were fed to the hidden layers for the classification, which was a standard practice as described in <ref type="bibr" target="#b3">(Jain, van Gemert, and Snoek 2014;</ref><ref type="bibr" target="#b3">Karpathy et al. 2014;</ref><ref type="bibr" target="#b6">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b4">Ng et al. 2015;</ref><ref type="bibr" target="#b6">Tran et al. 2015)</ref>. In addition, in order to explicitly confirm the power of our approach of 'learning' temporal attention filters with different location/duration/resolution, we also implemented the baselines of using fixed-predetermined temporal filters (i.e., without learning). The main idea was to make the systems take advantage of temporal filters identical to ours while disabling their learning capability. This makes the baselines behave very similar to previous pooled time series method <ref type="bibr" target="#b6">(Ryoo, Rothrock, and Matthies 2015)</ref>. We tested (4) an approach of using a single fixed filter (i.e., the level 1 temporal pyramid), which essentially computes a weighted sum of the whole video. We then used (5) a temporal pyramid of level 4, having 15 filters: 1 viewing the whole video, 2 viewing half the video, 4 viewing a forth, and 8 viewing an eighth, giving a vector of size 15 ? N ? D.</p><p>We implemented our CNN-based recognition architecture with learnable temporal attention filters as described in this paper. First, we implemented our approach of learning static temporal filters. We tested our model's ability to learn filters by using 15 filters (i.e., M = 15) with N = 1 or N = 3. Finally, we modified the model to use a LSTM to dynamically choose where to look. At each step, the temporal model took the hidden state, h from the LSTM as input, and did a linear transformation with learned weights and bias W ? h + b to obtain g, ? and ?. We then used the same equations as above to create the filters. The LSTM ran for 4 steps and either had 1 or 3 filters (i.e., M = 1 or 3) with N = 5.</p><p>Training the network: To increase training data, we apply random cropping on each video frame, and randomly skip several video frames at beginning. We use log-scale of stride and variance to ensure positivity as <ref type="bibr" target="#b2">(Gregor et al. 2015)</ref>. We initialize each filter bank parameters ( g m , log ? m , log ? 2 m ) with normal distribution for g m and 0 for log ? m and log ? 2 m . For all experiments, the first fully-connected layer had 4096 nodes and used a ReLU activation function. The second had either 10 nodes (for DogCentric) or 51 nodes (for HMDB) and used soft-max. The network was trained for 10000 iterations with a batch size of 100 and stochastic gradient descent with momentum set to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DogCentric dataset</head><p>The DogCentric dataset consists of 209 videos (102 training and 107 testing videos) and 10 classes. As mentioned above, it is a very challenging dataset with severe camera motion. All of the baselines described above as well as 4 different versions of our approach (2 with static filter learning and 2 with LSTMs) were compared while using 3 different types of underlying features. <ref type="figure" target="#fig_3">Figure 5</ref> shows the results. We are able to clearly observe that the consideration of multiple subintervals improves the recognition performances. The performance increased by using predetermined temporal pyramid, and our proposed approach of learning temporal filters were able to further improve the performance. Additionally, using the LSTM to dynamically choose sub-event locations gives the best performance.</p><p>The overall difference between the conventional approach of max/sum/mean pooling and our learned temporal attention filters are around 5% in VGG, 1% in ITF, and 4?5% in TDD. We believe our approach was more effective with VGG since it's a pure CNN-based feature allowing our differentiable filters to better cope with them. ITF is a completely handcrafted feature and TDD is a partially hand-crafted feature. <ref type="table" target="#tab_1">Table 1</ref> compares the performances of our approach with the previously reported state-of-the-arts. The base features we used for this table was TDD. By taking advantage of temporal filter learning and also LSTMs to dynamically adjust the filters matching sub-events, we were able to outperform the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMDB</head><p>HMDB is a relatively large-scale video dataset with 51 activity classes and more than 5000 videos.</p><p>In our first experiment, we made our model to learn general temporal filters to be shared by all 51 activity classes, testing whether our approach can learn globally optimal filters.   <ref type="table" target="#tab_2">Table 2</ref> shows the results of our approach with the temporal attention filters compared against the approaches without them. We tested this with CNN-based features: VGG, C3D, and TDD. We are able to observe that our approach of learning latent sub-events using temporal attention filters clearly benefits the classification in all cases. Finally, instead of making our model to learn general filters, we made our models to learn 1-vs-all binary classifier for each activity. This enables each activity classification model to learn class-specific (latent) sub-events tailored for the activity, allowing the model to fully take advantage of our filter learning method. These 1-vs-all results were combined for the final 51-class classification. We were able to get an accuracy of 68.4% <ref type="table" target="#tab_3">(Table 3)</ref>. This is significant considering that the base feature performance (i.e., TDD) is 57% with max/mean pooling. The setting was N = 2 and M = 3. The performance increase gained by learning such latent sub-events per activity was significant and consistent: it was 9% increase over fixed temporal pyramid filters with C3D and 10% increase over temporal pyramid with TDD. There are few existing approaches performing comparable to our approach by fusing multiple features (e.g., 69.2 of (Feichtenhofer, Pinz, and Zisserman 2016) + ITF and 67.2 of (Varol, Laptev, and Schmid 2016) + ITF), but our approach using one feature performed superior to them in their original form. <ref type="figure">Figure 6</ref> shows examples of the learned sub-events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We present a new activity recognition approach using temporal attention filters. Our approach enables end-to-end learning of the classifier jointly with latent sub-events and underlying CNN architectures. An approach to adaptively update the   <ref type="bibr" target="#b7">(Wang, Qiao, and Tang 2015)</ref> 63.2 % LTC <ref type="bibr" target="#b7">(Varol, Laptev, and Schmid 2016)</ref> 64.8 % S+T <ref type="bibr" target="#b1">(Feichtenhofer, Pinz, and Zisserman 2016)</ref> 65.4 % Max pooling -C3D 48.5 % Temporal pyramid -C3D 49.7 % Ours (temporal filters) -C3D 57.7 % Max pooling -TDD 57.1 % Temporal pyramid -TDD 58.9 % Ours (temporal filters) -TDD 68.4 % temporal filter location/duration/resolution using multiple recurrent LSTM iterations was also proposed, and its potential was experimentally confirmed. The concept of learning latent sub-events using temporal attention clearly benefited the recognition, particularly compared to the baselines using the same features without such learning.</p><p>Discussions: Unfortunately, we were unable to replicate the TDD's reported recognition performance of 63.2 % with the code provided by the authors. We only obtained 57%. This probably is due to the difference in detailed parameter settings and engineering tricks. If we can replicate the performance of TDD reported in its paper, we would be able to further increase our method's performance using it as a base.  <ref type="figure">Figure 6</ref>: Example frames of the learned latent sub-events. The top row of each sub-figure (a-c) shows an example frame sequence, and the green boxes below shows the locations of the temporal filters learned to capture (latent) sub-events. Actual frames corresponding to each sub-event are also illustrated. We are able to observe that semantic sub-events such as (a) 'doing down' in the pushup activity, (b) 'stretch' in the punch activity and (c) 'roll' and 'stand up' in the somersault activity are captured.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(Simonyan and Zisserman 2014) used optical flows in addition to image feature. (Ng et al. 2015) tested multiple different types of pooling strategies on top of per-frame CNNs, and found that the simple global max pooling of per-frame features over the entire interval performs the best for sports videos. (Karpathy et al. 2014) also tried multiple different (temporal) pooling strategies, gradually combining per-frame CNN responses over a short interval using their 'slow fusion'. (Tran et al. 2015) proposed to do XYT convolution, learning space-time convolutional filters. (Wang, Qiao, and Tang 2015) used local CNN feature maps around tracklets in videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>An illustration of our temporal attention filter. The filter is differentiable and represented with three parameters. Examples illustrating how our temporal attention filters work once learned. They are shown with two different Gaussian filter variances. For this visualization, raw video frames are used as inputs directly. In our actual implementation, the input to temporal filters are not raw video frames but CNN feature responses obtained from frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Features:</head><label></label><figDesc>We extracted VGG features (Simonyan and Zisserman 2015), INRIA's improved trajectory features (ITF) (Wang and Schmid 2013), C3D features (Tran et al. 2015), and TDD features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Classification accuracy of baselines and our proposed approach using VGG, ITF and VGG features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Recognition performances of our approach on the DogCentric dataset, compared against previously reported results of state-of-the-art approaches.</figDesc><table><row><cell>Approach</cell><cell>Accuracy</cell></row><row><cell>VGG (Simonyan and Zisserman 2015)</cell><cell>59.9 %</cell></row><row><cell>Iwashita et al. 2014</cell><cell>60.5 %</cell></row><row><cell>ITF (Wang and Schmid 2013)</cell><cell>67.7 %</cell></row><row><cell>ITF + CNN (Jain, van Gemert, and Snoek 2014)</cell><cell>69.2 %</cell></row><row><cell>PoT (Ryoo, Rothrock, and Matthies 2015)</cell><cell>73.0 %</cell></row><row><cell>TDD (Wang, Qiao, and Tang 2015)</cell><cell>76.6 %</cell></row><row><cell>Ours (temporal filters)</cell><cell>79.6 %</cell></row><row><cell>Ours (temporal filters + LSTM)</cell><cell>81.4 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>A table comparing the performance of the approaches using the HMDB Dataset. In this experiment, we made the model to learn general temporal filters to be shared across all 51 activities.</figDesc><table><row><cell>Method</cell><cell>VGG</cell><cell>C3D</cell><cell>TDD</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Max Pooling</cell><cell cols="3">37.77 % 48.45 % 57.07 %</cell></row><row><cell>Sum Pooling</cell><cell cols="3">37.00 % 48.58 % 55.77 %</cell></row><row><cell>Mean Pooling</cell><cell cols="3">37.73 % 49.30 % 57.17 %</cell></row><row><cell>Fixed Temporal Filters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pyramid 4</cell><cell cols="3">41.56 % 49.69 % 58.87 %</cell></row><row><cell>Learned Temporal Filters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>N = 1</cell><cell cols="3">41.23 % 50.35 % 58.87 %</cell></row><row><cell>N = 3</cell><cell cols="3">42.50 % 50.00 % 59.03 %</cell></row><row><cell>LSTM 1 filter</cell><cell cols="3">42.72 % 51.20 % 58.04 %</cell></row><row><cell>LSTM 3 filters</cell><cell cols="3">43.03 % 49.81 % 58.93 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Final recognition performances of our approach with per-activity temporal filters, tested with HMDB. Results are compared against the state-of-the-arts.</figDesc><table><row><cell>Approach</cell><cell>Accuracy</cell></row><row><cell>ITF (Wang and Schmid 2013)</cell><cell>57.2 %</cell></row><row><cell>2-stream CNN (Simonyan and Zisserman 2014)</cell><cell>59.4 %</cell></row><row><cell>TDD</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinz</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">First-person animal activity recognition from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
		<idno>abs/1502.04623</idno>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>DRAW: A recurrent neural network for image generation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<idno>Kuehne et al. 2011</idno>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition by learning deep multigranular spatio-temporal video representation</title>
		<idno>Ng et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei ;</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>CVPR. Ryoo et al. 2015. Robot-centric activity prediction from first-person videos: What will they do to me? In HRI</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rothrock</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>ICLR. [Tran et al. 2015</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deepconvolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laptev</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04494</idno>
		<idno>Yeung et al. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">Long-term Temporal Convolutions for Action Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
