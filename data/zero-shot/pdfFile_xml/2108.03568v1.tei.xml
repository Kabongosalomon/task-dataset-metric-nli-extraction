<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LeafMask: Towards Greater Accuracy on Leaf Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohao</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Electrical Engineering</orgName>
								<address>
									<country>China Agricultural University</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Agriculture 5 National Innovation Center for Digital Fishery</orgName>
								<orgName type="laboratory">Key Laboratory of Agricultural Information Acquisition Technology</orgName>
								<address>
									<country>China Agricultural University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liao</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Electrical Engineering</orgName>
								<address>
									<country>China Agricultural University</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Agriculture 5 National Innovation Center for Digital Fishery</orgName>
								<orgName type="laboratory">Key Laboratory of Agricultural Information Acquisition Technology</orgName>
								<address>
									<country>China Agricultural University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dantong</forename><surname>Niu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">EECS department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Electrical Engineering</orgName>
								<address>
									<country>China Agricultural University</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Agriculture 5 National Innovation Center for Digital Fishery</orgName>
								<orgName type="laboratory">Key Laboratory of Agricultural Information Acquisition Technology</orgName>
								<address>
									<country>China Agricultural University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yue</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information and Electrical Engineering</orgName>
								<orgName type="institution">LuDong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LeafMask: Towards Greater Accuracy on Leaf Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Leaf segmentation is the most direct and effective way for high-throughput plant phenotype data analysis and quantitative researches of complex traits. Currently, the primary goal of plant phenotyping is to raise the accuracy of the autonomous phenotypic measurement. In this work, we present the LeafMask neural network, a new end-to-end model to delineate each leaf region and count the number of leaves, with two main components: 1) the mask assembly module merging position-sensitive bases of each predicted box after non-maximum suppression (NMS) and corresponding coefficients to generate original masks; 2) the mask refining module elaborating leaf boundaries from the mask assembly module by the point selection strategy and predictor. In addition, we also design a novel and flexible multi-scale attention module for the dual attention-guided mask (DAG-Mask) branch to effectively enhance information expression and produce more accurate bases. Our main contribution is to generate the final improved masks by combining the mask assembly module with the mask refining module under the anchor-free instance segmentation paradigm. We validate our LeafMask through extensive experiments on Leaf Segmentation Challenge (LSC) dataset. Our proposed model achieves the 90.09% BestDice score outperforming other state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Plant phenotyping is a series of quantitative descriptions and methodologies for the morphological, physiological, genetical, and biochemical characteristics of the plant. Phenotyping is often used to improve agricultural management and select excellent crop breeds based on the desired traits * Corresponding author. E-mail: lizb@cau.edu.cn <ref type="figure">Figure 1</ref>. Example images and corresponding predicted masks of Arabidopsis and Tobacco from the LSC test sets (A1-A4). and environments. Traditional phenotypic measurement and analysis are laborious, expensive, destructive, and timeconsuming. With the rapid development of non-invasive and digital technologies, image-based phenotyping has become a crucial tool for measuring and accessing the plant's structural and functional properties in high-throughput phenotyping <ref type="bibr" target="#b14">[14]</ref>.</p><p>Among all the organs in most plants, leaves account for the largest proportion and play a vital role in the growth and development of vegetation. Leaf properties include but are not limited to leaf count, leaf area, leaf shape characteristics (e.g., leaf length, leaf width, leaf angle, etc.), and leaf nutrient content (e.g., water content, trace element content, etc.). Leaf properties are closely related to many biological and physical processes of plants, such as photosynthesis, respiration, transpiration, and carbon and nutrient cycles <ref type="bibr" target="#b23">[23]</ref>. Therefore, the estimation of leaf anatomical structure and ontogenetical parameters is of great significance to plant growth monitoring <ref type="bibr" target="#b4">[4]</ref>. In addition, observation of leaves can also reveal their growth status and ultimately help us to identify genetic contributions, improve plant genetic characteristics, and increase crop yield <ref type="bibr" target="#b16">[16]</ref>.</p><p>In high-throughput phenotyping, the automatic segmentation of plant leaves is a pre-requisite for measuring and identifying more complex phenotypic traits. However, in the perspective of computer vision, obtaining such detailed information at the individual leaf level is particularly difficult <ref type="bibr" target="#b22">[22]</ref>. Despite the clear appearance and shape characteristics, individual leaf segmentation faces the following challenges: the occlusion and overlap of leaves, half-covered young leaf and petiole, over-segmentation of the leaf along a visually prominent vein, hard distinct shadowing on the large leaf, reflective leaf surface, as well as the variability in leaf shapes and sizes over the life-cycle of the constantly changing plant <ref type="bibr" target="#b26">[26]</ref>.</p><p>Deep learning is proved to be a powerful tool to segment and count objects, which can avoid the manual design of feature extractors and the laborious selection of parameters. In general, most instance segmentation methods based on convolutional neural networks (CNNs) are split into two branches: object detection and segmentation. The former detects and distinguishes instances (or leaves) to generate bounding boxes, while the latter is employed to separate the foreground and background in the above bounding boxes. According to the object detectors, instance segmentation can be divided into anchor-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">7]</ref> and anchor-free methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">12]</ref>. However, the above methods are used for general computer vision tasks and common object datasets, and their effectiveness in the leaf segmentation task or other agricultural applications needs further exploration. Our main purpose of this paper is to propose an anchor-free method for leaf segmentation.</p><p>In this paper, we present a new end-to-end leaf segmentation network called LeafMask (leaf segmentation results are displayed in <ref type="figure">Figure 1</ref>). The innovations of Leaf-Mask are summarized as follows: 1) Mask assembly module. The proposed LeafMask can learn to localize and distinguish different leaves via the mask assembly module which merges position-sensitive bases and corresponding predicted coefficients. This framework can effectively avoid some drawbacks of anchor-based methods: (i) The hyper-parameters of the anchor boxes (e.g., the sizes, aspect ratios, quantities, etc.) are sensitive and needed to be carefully tuned. (ii) Due to the fixed sizes and aspect ratios of anchor boxes, anchor-based methods are difficult to deal with object candidates with large shape variations, especially for small objects. (iii) Most of the dense anchor boxes are labelled as negative samples during training, which aggravates the imbalance between positive and negative samples. 2) DAG-Mask branch. Since multi-scale attention module aggregates global and local features, it can capture more detailed information and suppress noise of irrelevant clutters. Therefore, it is able to trade off the quality of the model on large blades with that on small leaves. 3) Mask refining module. Mask refining module is a simple addi-tional component that adaptively selects points in boundaries of leaves (the most uncertain point set) and efficiently computes sharp boundaries between leaves to avoid aliasing effects. After mask refining, leaf masks are more accurate, particularly in the leaf boundaries.</p><p>To provide evidence for the above innovations, we evaluate LeafMask on leaf segmentation task using the challenging LSC dataset. We carry out extensive ablation studies to discover the optimal hyper-parameters. The performance of our model achieves the 90.09% BestDice score on the test set and outperforms other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Leaf segmentation</head><p>Several authors applied context information in timelapse images to segment and track leaves. For instance, Dellen et al. segmented and tracked the leaves in a set of tobacco-plant growth sequences by graph-based tracking algorithm <ref type="bibr" target="#b6">[6]</ref>. Yin et al. used a Chamfer matching formulation to separate and track the leaves of Arabidopsis in the subsequent fluorescence video frames <ref type="bibr" target="#b28">[28]</ref>. Evidently, the above approaches rely on extra context or temporal information, and therefore are unsuitable for a common reference dataset of individual images. In order to solve this problem, Pape and Klukas utilized 3D histograms of LAB color space to discriminate foreground from background, and then adopted distance maps and region growing algorithm to separate the individual leaves <ref type="bibr" target="#b0">[1]</ref>. Another study employed a superpixel-based unsupervised method to extract foreground, and then individual leaves were divided by computing distance maps and using a watershed transform <ref type="bibr" target="#b22">[22]</ref>. In conclusion, the two methods depend on accurate post-processing of distance maps to segment leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Instance segmentation</head><p>Anchor-based instance segmentation. Mask R-CNN <ref type="bibr" target="#b7">[7]</ref> is a representative anchor-based and two-stage instance segmentation approach that first generates a set of candidate Regions of Interests (RoIs) by Region Proposal Network (RPN) on CNN feature maps and then classifies and segments those RoIs in the second stage. Ward et al. trained a Mask R-CNN with a combination of real and synthetic images of rosette plants with different shapes, and used the trained model to segment real leaves <ref type="bibr" target="#b26">[26]</ref>. In order to predict masks with substantially finer detail around object boundaries, Kirillov et al. proposed a new module called PointRend <ref type="bibr" target="#b10">[10]</ref> that viewed image segmentation as a rendering problem and applied a subdivision strategy to adaptively select a non-uniform set of points and efficiently produce more accurate segmentation maps. Compared with Mask R-CNN and PointRend, YOLACT <ref type="bibr" target="#b1">[2]</ref> is an anchor-based but one-stage instance segmentation method, which closely  <ref type="figure" target="#fig_4">Figure 6</ref>). By selecting a series of uncertain points based on point selection strategy, mask refining module recalculates these points from coarse and fine-grained features to generate the final segmentation image. follows a one-stage detector named RetinaNet with an advantage on speed. Instead of using position-controlled tiles or localization steps (e.g., RoI Align), a set of mask coefficients and prototype masks are produced by f c layers and conv layers, respectively. Then this set of coefficients and bottom mask bases can be implemented as a single matrix multiplication to generate the final mask.</p><p>Anchor-free instance segmentation. Recent advances in anchor-free segmentation proved that anchor-free methods can outperform their anchor-based counterparts in accuracy <ref type="bibr" target="#b2">[3]</ref>. Instances are freely matched to prediction features without the restrictions of predefined anchor boxes and largely improve the efficiency and precision of segmentation. BlendMask <ref type="bibr" target="#b2">[3]</ref> consists of an anchor-free detector network and a mask branch. The framework builds upon the FCOS object detector <ref type="bibr" target="#b25">[25]</ref> and appends a single convolution layer to predict top-level attentions. Unlike the mask coefficients in YOLACT, which merely performs a weighted sum of the channels of the prototypes, the attention map is a tensor corresponding to the weights at each location of the prototypes. Therefore, BlendMask can provide and encode more instance-level information such as the coarse shape and pose of the object. CenterMask <ref type="bibr" target="#b12">[12]</ref> is also a simple but efficient anchor-free instance segmentation, which adds a novel spatial attention-guided branch to predict a segmentation mask on each box. Our idea is inspired by the above anchor-free instance segmentation methods and LeafMask relies on this segmentation paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Attention mechanism</head><p>Attention mechanism plays a vital role in various tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24]</ref>. Attention modules can selectively establish long-range context dependencies and focus on the significant information from a large amount of information to improve the efficiency and accuracy of processing models. SENet <ref type="bibr" target="#b9">[9]</ref> squeezes the channel-wise global spatial information of the input feature map into a channel descriptor to model inter-channel dependencies by using global average pooling. Compared with the SENet, Woo et al. experimentally confirmed that global max pooling can obtain other crucial clues about distinctive object features <ref type="bibr" target="#b27">[27]</ref>. Accordingly, using global average-pooled and max-pooled features simultaneously to infer finer channel-wise attention greatly improves the ability of representations produced by a network rather than using each independently.</p><p>Traditional attention module discriminates the feature representations for scene-and-object understanding by capturing the global information. However, the global attention only learns one single scalar value for a spatial position or feature map. We consider that using global and coarse descriptors to encode leaves is suboptimal, which can potentially ignore or suppress most of the image signal present in small leaves. As a result, we propose the multi-scale attention module with both spatial and channel descriptions to combine the global and local contexts inside the dual attention-guided mask branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We separate leaf segmentation into two parallel parts: detection and mask branch, first detecting and generating the leaf bounding boxes, and then predicting the foreground masks on each box. The former adopts the state-of-theart FCOS object detector with minimal parameter modification. The latter mainly includes three parts: dual attentionguided mask (DAG-Mask) branch, mask assembly module, and mask refining module ( <ref type="figure" target="#fig_0">Figure 2</ref>). Similar to other convolutional instance-aware segmentation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, we add a bottom module and use the decoder of DeepLab V3+ to generate a set of non-local score maps (i.e., bases) over the entire image. To better boost the representation power of DeepLab V3+ and focus on target objects properly, we propose a new dual attention module with spatial and channel descriptions, which aggregates global and local features in DAG-Mask branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DAG-Mask branch</head><p>Given the feature pyramid map as input, we first feed it into convolutional layers to expand channels and produce a new feature map. Then, we apply the multi-scale attention module to sequentially infer spatial attention map and channel attention map as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Spatial attention module. Spatial attention module mainly focuses on the inter-spatial dependencies of the convolutional features and generates spatial attention matrices which highlight informative regions. To calculate the spatial attention maps, we operate global average pooling and max pooling along the channel axis to generate two feature descriptors that indicate max-pooled and average-pooled features. Next, we concatenate the above descriptors and apply convolutional layers to produce the global spatial attention map (see <ref type="figure" target="#fig_2">Figure 4</ref>). In order to decrease parameter and improve the robustness of training, the first convolution kernel size is set to R 4C r ?H?W , where r is the channel dimension reduction ratio.</p><p>We add a parallel local branch inside the spatial attention module to enrich feature contexts and improve multi-scale information expression. In this branch, we use the pointwise convolution as the local channel context extractor for each spatial position, among which the convolution kernel size is 1. Finally, we merge the output feature matrices using broadcasting addition. To emphasize multiple spatialwise features instead of one-hot activation, we exploit sigmoid function to activate summation. Then we obtain the final spatial weights, which is used to rescale inputs. Channel attention module. Different from the spatial attention module, channel attention module is able to capture the inter-dependencies between the channels and learn the inter-channel relationship of features, with the goal of assigning higher weights to the channels with more information. To compute channel attention map effectively, we take the feature map into global spatial module and generate two sets of channel-wise descriptors. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, both descriptors are forwarded to a shared multilayer convolution subnetwork to produce our global channel attention map. The shared subnetwork is composed of two point-wise convolutional layers instead of f c layers.</p><p>Similar to the spatial attention, we also inset a parallel local branch into the channel attention module and maintain the same architecture as local spatial attention. Then, we aggregate the output feature maps using broadcasting addition with sigmoid activation function. Spatial weights are used to rescale the input feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mask assembly module</head><p>For maintaining mask spatial coherence (i.e., pixels close to each other are likely to be part of the same instance) in the feature space, we split the leaf segmentation into two parallel branches: bottom and top branch. The former predicts a set of position-sensitive masks shared by all leaves, since multiple convolutional layers with padding (e.g., with value 0) are inherently translation variant and give the network the ability to distinguish and localize leaves by using dif-ferent activations. The latter produces corresponding toplevel 2D coefficient matrices alongside the box predictions to encode the leaf information. In order to produce original leaf masks, we linearly combine bases according to the mask coefficients. These operations are integrated in mask assembly module. The bottom branch is similar to the end-to-end semantic segmentation network, but its weight supervision comes from the mask after assembly. We take ResNet as an encoder to extract more low-level visual characteristics and high-level semantic information. The decoder is comprised of FPN, DAG-Mask, and an upsampling layer, which can output spatial-coherent score maps called bases with the size of N ? K ? H 4 ? W 4 , where N is the batch size and K is the number of score maps.</p><p>The top branch follows the object detector FCOS to generate bounding boxes and utilizes NMS to select the best bounding box. In order to learn bases' coefficients, a single convolution layer is appended on FCOS heads to encode instance-level information including the position and shape of each leaf. Thus, the output of top branch is a set of 2D tensors with the number of N ? K ? P , where P is the number of bounding boxes.</p><formula xml:id="formula_0">Given bases ? R N ?K? H 4 ? W 4</formula><p>, coef f icients ? R N ?K?P ?R C ?R C , and boxes ? R P ?4 , we first resize them to suitable shapes and then combine them linearly. More specifically, we apply RoIAlign to crop bases with all bounding boxes and resize the region to R B ? R B . Since the size R C is smaller than R B , we interpolate coefficients from R C to R B . The coefficients are illustrated in <ref type="figure" target="#fig_4">Figure 6</ref> (b). In the end, we utilize element-wise multiplication between each adjusted coefficient and base, and operate summation along K (K = 4 in <ref type="figure" target="#fig_4">Figure 6</ref>) dimension to get the original mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mask refining module</head><p>After extensive experiments, we notice that leaf boundaries tend to be serrated in masks. Pixels located on the boundaries are hard to be accurately classified because of large ambiguity <ref type="bibr" target="#b13">[13]</ref>. Image segmentation is generally operated on regular grids in CNNs, and it will follow the regular sampling pattern between the smooth areas and object boundaries. However, it unnecessarily over-samples the low-frequency regions while concurrently under-samples high-frequency regions. To adaptively select pixels and efficiently computes sharp boundaries, we implement a mask refining module to generate a final anti-aliased and highresolution mask. The input of the mask refining module is the P 3 feature vector from FPN and the results from the mask assembly module. Next, we divide the procedure into two parts including point sampling and point prediction.</p><p>Point Sampling. During training, we define the set of points to be optimized of each instance as Set i . For each instance, we select the most uncertain points into Set i according to the output prediction logits from the mask assembly module. To enhance the robustness of the model, we also add some randomly selected points to Set i . Specifically, we define ? (? &gt; 1) as oversampling rate and ? (0 &lt; ? ? 1) as importance rate. For each instance, we uniformly sample ?N points from the mask as a set of selectable points U i . We first select the top ?N points with logits closest to 0.5 from U i and put them into Set i , then randomly select another (1 ? ?) ? N points from the remaining points into Set i . Algorithm 1 provides the pseudo-code of this process. <ref type="figure">Figure 7</ref>. Point sampling strategy. In each step, we select the most uncertain points in the current mask to update (blue dots shown in the figure), further the N ? N grid is upsampled 2? using bilinear interpolation. This process is repeated until it reaches the targeted resolution.</p><p>During inference, we use the same point selection strategy for each instance, and select the most uncertain top N points from U i . After reclassifying these most uncertain points, we use bilinear interpolation to amplify the mask and then select the points again. By repeating this process for x times, we can obtain a mask that has been refined step by step <ref type="figure">(Figure 7)</ref>.</p><p>Point prediction. In order to optimize the logits of the points selected from the above part, we introduce a n-layer point-wise convolutional network. The weight of the network in the training phase is shared by all points, that is, it is not affected by the above defined point sampling method. The input of network is the high-precision feature in the P 3 layer and the output coarse logits from the mask assembly module corresponding to each point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Point Selection Strategy</head><p>Input: mask logits M i , S S is either train or inf erence</p><formula xml:id="formula_1">Set i ? {} if S = train then U ? uncertainty(M i ) P ? topK(U ) Set i ? Set i ? P else if S = inf erence then for i = 1 to step do M i ? interpolation(M i ) U ? uncertainty(M i ) P ? topK(U ) Set i ? Set i ? P end for end if</formula><p>During training time, we define a multi-task loss as:</p><formula xml:id="formula_2">L = L cls + L ctr + L loc + L mask + L sem + L points (1)</formula><p>where the classification loss L cls , centerness loss L ctr , location loss L loc are as same as those in <ref type="bibr" target="#b25">[25]</ref>. Mask loss L mask is identical as in <ref type="bibr" target="#b7">[7]</ref>. Semantic segmentation supervision as auxiliary loss with weight 0.3 is added to GL-DAM mask module and its loss is L sem . L points is the average binary cross-entropy loss of selected points as defined in point sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details and dataset</head><p>For all experiments, we use a Dell workstation, which is equipped with Intel Xeon (R) CPU E5 ? 2683V 3 processor and GTX 1080 Ti GPU. All models are trained on a single 1080 Ti GPU (take about 1 day). We adopt PyTorch and Detectron2 as the deep learning frameworks.</p><p>Considering the computational resource and model performance, we set the batch size to 4. ResNet-101 pretrained on ImageNet is used as the backbone. We adopt the Kaiming initialization method <ref type="bibr" target="#b8">[8]</ref> with the rectifier's negative slope of 1 to initialize other convolution layers. Input images are uniformly normalized to have the shorter side 640 and longer side at maximum 1440. Other hyperparameters are set to be the same as FCOS <ref type="bibr" target="#b25">[25]</ref>.</p><p>We use a benchmark dataset of raw and annotated images of plants from LSC of the Computer Vision in Plant Phenotyping and Agriculture (CVPPA) workshop. All the RGB images in the CVPPA LSC belong to rosette plants (Arabidopsis and Tobacco), which are top-view 2D visiblelight images from an indoor plant phenotyping platform.</p><p>To better evaluate leaf segmentation accuracy, we utilize BestDice metric to estimate the degree of overlap among ground truth and prediction masks. The metric mainly is based on the Dice score of binary segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation experiments</head><p>We carry out a number of ablation experiments to verify the effectiveness of the architectures and hyper-parameters. Mask assembly module. We experimentally measure the performances of the mask assembly module with different R C , R B , and K. The combination methods and results are shown in <ref type="table" target="#tab_0">Table 1</ref>, which shows that: 1) increasing the resolution of coefficient and base can learn more detailed information about leaf instances without introducing much computation; 2)through our experiments, we finally set R B = 56, R C = 14 and K = 4 for our baseline model under which the BestDice is optimal and inference time does not increase much; 3)further increasing R B , R C and k are ineffective mainly because they will clearly increase the training complexity and make it very difficult to predict. Since the bases and corresponding coefficients are linearly combined, it will directly lose the balance and cause poor performance of the network when one base or coefficient matrix is wrong.</p><p>Attention module. To investigate the effect of multiscale attention module, we compare the module with SE and CBAM. By contrast, the proposed multi-scale attention module surpasses the other methods (see <ref type="table">Table 2</ref>). One crucial reason is that global attention modules lack the local context information, which aggravates the problems brought by the scale variation of leaf. We construct seven attention modules to select the optimal architecture. <ref type="table">Table  2</ref> presents the experimental results on the LSC dataset. It <ref type="table">Table 2</ref>. Comparison of different attention modules. Using multi-scale contextual aggregation and dual attention is crucial. The best combination strategy (sequential spatial-channel) further improves the accuracy of leaf segmentation. For all models, we use RB = 56, RC = 14 and K = 4 and append mask refining module. can be seen that utilizing multi-scale contextual aggregation (global and local information) to generate attention maps outperforms single attention, and the spatial-first order is the best arrangement mode. Mask refining module. We compare the performance of the model under different ? and ?, and the model performs best with ? = 3 and ? = 0.75. When the value of ? is small, the selected points tend to be uniformly distributed, so their logits are also uniformly distributed between 0 ? 1. The feature space that n-layer network needs to learn is relatively large, which possibly exceeds its learning ability, so the model does not perform well. On the contrary, when the value of ? is high, the logits of the selected points is concentrated around 0.5, and the n-layer network lacks other points' information to accurately classify these points, so it will also get poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art methods</head><p>We compare LeafMask with other state-of-the-art algorithms on LSC dataset. As shown in <ref type="table" target="#tab_3">Table 4</ref>, our proposed model achieves 90.09% mean BestDice score for all test sets, which is better than other approaches. Our LeafMask also outperforms the Mask R-CNN and BlendMask with the same backbone-neck ResNet-101-FPN by 3.2% and 2.2% in BestDice score, respectively. Note that most works report results on A1 subset only, and the A4,A5 subsets are added later. The qualitative results of our LeafMask are also shown in <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref>. Among the main-stream leaf segmentation methods (see <ref type="figure">Figure 8</ref>), it is difficult for Pape and klukas to accurately segment two or more overlapping blades, and the boundary of the two leaves is often mixed. For Ward and Kuznichov, it's hard to detect and segment small blades that the small blades are often missed. The RIS segmentation of the leaf boundary tends to have a more obvious jagged mask and other methods also cause different degrees of segmentation loss on the boundary of the instance, especially for the petiole. Our algorithm has solved the above problems well. As shown in <ref type="figure">Figure 9</ref>, compared with Mask R-CNN and BlendMask, LeafMask can segment the arc-shaped boundary of the blade well. What's more, small leaves will not be missed in dense regions of petioles or small leaves. <ref type="figure">Figure 8</ref>. Qualitative results with main-stream leaf segmentation methods. In each set of comparisons, the middle is the predicted mask provided by the original paper (Pape <ref type="bibr" target="#b18">[18]</ref>, RIS <ref type="bibr" target="#b20">[20]</ref>, Klukas <ref type="bibr" target="#b17">[17]</ref>, Ward <ref type="bibr" target="#b26">[26]</ref>, Kuznichov <ref type="bibr" target="#b11">[11]</ref>), and the right is the corresponding results of Leafmask. <ref type="figure">Figure 9</ref>. Qualitative results with other instance segmentation approaches. To make a fair comparison with Mask R-CNN, Blend-Mask, and LeafMask, the code base we use for qualitative results is Detectron2. Recently released Detectron2 originates from maskrcnn_benchmark with significant enhancements for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a new end-to-end anchor-free onestage instance segmentation model towards greater accuracy on leaf segmentation. By inserting multi-scale attention module into DAG-Mask branch, combining assembly module with coefficient predictor, and adding mask refining module, LeafMask have achieved 90.1% best dice and outperformed all state-of-the-art approaches on CVPPA LSC test sets. We hope that our LeafMask will serve as a base-line to motivate further investigation of leaf segmentation for various plant phenotyping tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>LeafMask Architecture. The backbone and neck of our model adopt ResNet101 and FPN, respectively. Using FPN features, the state-of-the-art FCOS detector predicts bounding boxes including classification, box regression, and centerness. The DAG-Mask branch consists of a new proposed multi-scale attention module and DeepLab V3+ decoder to produce original segmentation masks inside each detected leaf from the shared FCOS head. Mask assembly module linearly combines the outputs of coefficient predictor and DAG-Mask branch (specific assemble process is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Diagram of the proposed DAG-Mask branch, where denotes the element-wise addition and denotes the elementwise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Diagram of the proposed spatial attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Diagram of the proposed channel attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Diagram of assembly process. We illustrate an example of the bases and coefficients. The number of them is 4 and 76(= 4 ? 19), respectively. 'a' is bases of all leaves and 'b' is the corresponding 2D coefficient matrices. Each base multiplies its coefficient and then is summed to output the final mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different resolutions and numbers. Performance with base resolution 28 ? 28 and 56 ? 56, coefficient resolution from 7 to 28, and number of bases varying from 1 to 8. LeafMask uses multi-scale attention module and appends mask refining module. The performance increases as the RB, RC , and K grow, saturating in 56, 14, and 4, respectively.</figDesc><table><row><cell>RB</cell><cell>RC</cell><cell cols="3">K Time(ms) Mean</cell></row><row><cell></cell><cell>7 ? 7</cell><cell>1</cell><cell>123.70</cell><cell>78.60</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>126.28</cell><cell>88.73</cell></row><row><cell>28 ? 28</cell><cell></cell><cell>4</cell><cell>135.09</cell><cell>89.68</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell>137.86</cell><cell>89.81</cell></row><row><cell></cell><cell cols="2">14 ? 14 4</cell><cell>140.77</cell><cell>89.29</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell>140.81</cell><cell>89.66</cell></row><row><cell></cell><cell>14?14</cell><cell>4</cell><cell>144.69</cell><cell>90.09</cell></row><row><cell>56?56</cell><cell></cell><cell>8</cell><cell>145.79</cell><cell>89.62</cell></row><row><cell></cell><cell cols="2">28 ? 28 4</cell><cell>162.41</cell><cell>89.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different ? and ?. Performance of LeafMask with different oversampling rate ? and importance rate ?. The last column indicates the model performance without the mask refining module.</figDesc><table><row><cell cols="2">Point refining ?</cell><cell>?</cell><cell>Mean</cell></row><row><cell></cell><cell>1</cell><cell>0.5</cell><cell>89.47</cell></row><row><cell></cell><cell cols="3">3 0.75 90.09</cell></row><row><cell></cell><cell>5</cell><cell>1</cell><cell>89.21</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>CVPPA LSC results. Segmentation performance comparison (BestDice). Statistical evaluation results provided by the Leaf Segmentation Challenge board, based on the submitted image analysis results for the testing-dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Mean A1</cell><cell>A2</cell><cell>A3</cell><cell>A4</cell><cell>A5</cell></row><row><cell>RIS+CRF[20]</cell><cell>-</cell><cell>66.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MSU[22]</cell><cell>-</cell><cell cols="3">66.7 66.6 59.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Nottingham[22]</cell><cell>-</cell><cell cols="3">68.3 71.3 51.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Wageningen[28]</cell><cell>-</cell><cell cols="3">71.1 75.7 57.6</cell><cell>-</cell><cell>-</cell></row><row><cell>IPK[17]</cell><cell cols="4">62.6 74.4 76.9 53.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Pape[18]</cell><cell cols="4">71.3 80.9 78.6 64.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Salvador[21]</cell><cell>-</cell><cell>74.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Brabandere[5]</cell><cell>-</cell><cell>84.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ren[19]</cell><cell>-</cell><cell>84.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhu[29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.9</cell><cell>-</cell></row><row><cell>Ward[26]</cell><cell cols="6">81.0 90.0 81.0 51.0 88.0 82.0</cell></row><row><cell>Kuznichov[11]</cell><cell cols="6">86.7 88.7 84.8 83.3 88.6 85.9</cell></row><row><cell>Mask R-CNN</cell><cell cols="6">86.9 89.4 85.0 87.6 86.4 86.8</cell></row><row><cell>BlendMask</cell><cell cols="6">87.9 90.1 87.4 89.8 87.1 87.9</cell></row><row><cell>LeafMask</cell><cell cols="6">90.1 92.5 89.7 91.8 89.3 90.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computer Vision-ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part III</title>
		<meeting>Part III</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09-06" />
			<biblScope unit="volume">8927</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural selection acting on integrated phenotypes: covariance among functional leaf traits increases plant fitness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X?chitl</forename><surname>Dami?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sof?a</forename><surname>Ochoa-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurora</forename><surname>Gaxiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Fornoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>C?sar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karina</forename><surname>Dom?nguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytologist</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="546" to="557" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Growth signatures of rosette plants from time-lapse video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babette</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carme</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1470" to="1478" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9799" to="9808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data augmentation for leaf segmentation and counting tasks in rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kuznichov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Zvirin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Honen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CenterMask: Real-Time Anchor-Free Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3193" to="3202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review of computer vision technologies for plant phenotyping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaru</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page">105672</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing crop yield by optimizing plant developmental features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyotirmaya</forename><surname>Mathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhi</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aashish</forename><surname>Ranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Development</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3283" to="3294" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3-d histogrambased segmentation and leaf detection for rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Klukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Utilizing machine learning approaches to improve the prediction of leaf counts and individual leaf segmentation of rosette plant images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Klukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP)</title>
		<meeting>the Computer Vision Problems in Plant Phenotyping (CVPPP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6656" to="6664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hilaire Sean</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="312" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Baradad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00617</idno>
		<title level="m">Recurrent neural networks for semantic instance segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leaf segmentation in plant phenotyping: a collation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imanol</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerrit</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijela</forename><surname>Polder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vukadinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine vision and applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="585" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Soil-plant-atmosphere interactions: structure, function, and predictive scaling for climate change mitigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant and Soil</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image annotation by k nnsparse graph-based label propagation over noisily tagged web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep leaf segmentation using synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Moghadam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC) workshop on Computer Vision Problems in Plant Pheonotyping (CVPPP2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-leaf tracking from fluorescence plant videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="408" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joaquin vanschoren, and high tech campus. data augmentation using conditional generative adversarial networks for leaf counting in arabidopsis plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Aoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Krijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
