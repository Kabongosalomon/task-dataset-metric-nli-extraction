<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Panagiotis</roleName><forename type="first">Panagiotis</forename><surname>Antoniadis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of E.C.E</orgName>
								<orgName type="institution">National Technical University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paraskevas</forename><surname>Filntisis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of E.C.E</orgName>
								<orgName type="institution">National Technical University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maragos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of E.C.E</orgName>
								<orgName type="institution">National Technical University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past few years, deep learning methods have shown remarkable results in many face-related tasks including automatic facial expression recognition (FER) in-thewild. Meanwhile, numerous models describing the human emotional states have been proposed by the psychology community. However, we have no clear evidence as to which representation is more appropriate and the majority of FER systems use either the categorical or the dimensional model of affect. Inspired by recent work in multi-label classification, this paper proposes a novel multi-task learning (MTL) framework that exploits the dependencies between these two models using a Graph Convolutional Network (GCN) to recognize facial expressions in-the-wild. Specifically, a shared feature representation is learned for both discrete and continuous recognition in a MTL setting. Moreover, the facial expression classifiers and the valence-arousal regressors are learned through a GCN that explicitly captures the dependencies between them. To evaluate the performance of our method under real-world conditions we perform extensive experiments on the AffectNet and Aff-Wild2 datasets. The results of our experiments show that our method is capable of improving the performance across different datasets and backbone architectures. Finally, we also surpass the previous state-of-the-art methods on the categorical model of AffectNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Facial expressions are one of the most powerful nonverbal ways for human beings to convey their emotional state <ref type="bibr" target="#b10">[11]</ref>. Facial expression recognition (FER) has been a topic of study for decades due to its potential applications in various fields including human-computer interaction, digital entertainment, advertisement, health care and intelligent robot systems <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b19">[20]</ref>. However, recognizing facial expressions in the wild is still very challenging due to variations, occlusions and the ambiguity of human emotion <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b55">[55]</ref>.</p><p>While the cultural and ethnic background of a person can affect his expressive style, Ekman indicated that humans perceive certain basic emotions in the same way regardless of their culture <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>. These six universal facial expressions (happiness, sadness, surprise, fear, disgust and anger) constitute the categorical model. Contempt was subsequently added as one of the basic emotions <ref type="bibr" target="#b42">[42]</ref>. Due to its direct and intuitive definition of facial expressions, the categorical model is used in the majority of FER algorithms ( <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr">[1]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b25">[26]</ref>, etc) and large-scale databases (AffectNet <ref type="bibr" target="#b44">[44]</ref>, RAF-DB <ref type="bibr" target="#b35">[36]</ref>, SFEW <ref type="bibr" target="#b12">[13]</ref>, FER-2013 <ref type="bibr" target="#b21">[22]</ref>, EmotionNet <ref type="bibr" target="#b18">[19]</ref>, etc). However, the subjectivity and ambiguity of restricting human emotion to discrete categories result in large intraclass variations and small inter-class differences.</p><p>This work was not supported by any organization. <ref type="figure" target="#fig_4">Fig. 1</ref>: Distribution of the basic expressions in the VA space using the validation set of AffectNet that illustrates the emotional dependencies between the categorical and the dimensional model. The basic emotions are located around the neutral emotion that appears when valence and arousal are close to zero.</p><p>Recently, the dimensional model proposed by <ref type="bibr">Russell</ref>  <ref type="bibr" target="#b51">[51]</ref> has gained a lot of attention where emotion is described using a set of 2 latent dimensions that are valence (how pleasant or unpleasant a feeling is) and arousal (how likely is the person to take action under the emotional state). Another dimension called dominance is used sometimes to know whether the person is controlling the situation or not. Since a continuous representation can distinguish between subtly different displays of affect and encode small changes in the intensity, some recent algorithms <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b30">[31]</ref> and databases (Aff-Wild <ref type="bibr" target="#b60">[60]</ref> and AFEW-VA <ref type="bibr" target="#b31">[32]</ref>) have utilized the dimensional model for uncontrolled FER. Even so, predicting a 2-dimensional continuous value instead of a category increases a lot the task complexity and lacks intuitiveness.</p><p>Altogether, the categorical and the dimensional model have their respective benefits and drawbacks <ref type="bibr" target="#b27">[28]</ref>. Therefore, recent studies try to leverage both representations, along with Action Units (AU) detection, through multi-task learning (MTL) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b58">[58]</ref>. However, the strong dependence between the categorical and the dimensional model is not fully exploited when they only share a common feature representation. <ref type="figure" target="#fig_4">Fig. 1</ref> illustrates this relation using the validation set of AffectNet.</p><p>In multi-label classification, there has been a lot of re-search on how to properly capture and explore the correlation between labels <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b34">[35]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, Chen proposed ML-GCN model for multi-label image recognition that explicitly learns the labels correlation by generating the object classifiers via a Graph Convolutional Network (GCN) <ref type="bibr" target="#b29">[30]</ref>. Inspired by this architecture, we propose Emotion-GCN a novel GCN based MTL framework for FER in the wild. The main idea in this paper is to to generate dependent expression classifiers and valence-arousal (VA) regressors though a GCN based mapping function instead of learning them as separate parameter vectors. The generated vectors are then applied to an extracted image representation to enable endto-end training. Hence, the dependence between the categorical and the dimensional emotion models is explicitly captured through both a shared feature representation and the dependent classifiers and regressors. Experiments on the AffectNet and Aff-Wild2 datasets indicate that Emotion-GCN increases the performance across different datasets and backbone networks managing to achieve state-of-the-art results on the categorical model of AffectNet. The rest of this paper is organized as follows. Section II presents the recent work on FER systems and MTL. Section III describes the deep learning method that this paper proposes. Section IV discusses the experimental results, providing a clear view of the accuracy improvements introduced by our method. This section also includes a description of the data used during the experiments. Finally, Section V summarizes the key aspects of our work and concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Image-based FER has been extensively studied for many years. Typically, a FER system consists of three stages: face detection, feature extraction and classification. Traditional approaches tend to conduct FER by using handcrafted features, such as Local Binary Patterns <ref type="bibr" target="#b53">[53]</ref>, Gabor wavelets <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b45">[45]</ref> and Histogram of Oriented Gradients <ref type="bibr" target="#b4">[5]</ref>. While there is a lot of intuition behind these features and their performance on several lab-controlled databases is impressive, they lack generalizability and sufficient learning capacity <ref type="bibr" target="#b37">[37]</ref>.</p><p>Later, many in-the-wild facial expression databases were developed that enabled the research of FER in more challenging environments. Deep Convolutional Neural Networks (CNNs) have achieved promising recognition performance by learning powerful high-level features <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b59">[59]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In <ref type="bibr" target="#b39">[39]</ref> and <ref type="bibr" target="#b57">[57]</ref> region-based attention networks were designed for pose and occlusion aware FER, where the regions are either cropped from landmark points or fixed positions. Facial Motion Prior Networks were proposed in <ref type="bibr" target="#b7">[8]</ref> that generate a facial mask to focus on facial muscle moving regions. In <ref type="bibr" target="#b24">[25]</ref> deep Siamese neural networks equipped with a supervised loss function were used to reduce the high intraclass variation of the task. In <ref type="bibr" target="#b20">[21]</ref> deep and handcrafted features were combined and a local learning framework was used at test time based on SVMs. Recently, Self-Cure network <ref type="bibr" target="#b56">[56]</ref> was proposed that suppresses the uncertainties caused by ambiguous expressions and the suggestiveness of the annotators.</p><p>MTL was first explored in <ref type="bibr" target="#b5">[6]</ref> based on the idea that learning complementary tasks in parallel while using a shared representation improves the generalization performance. Since then, MTL has been used in many areas of computer vision, such as classification and detection <ref type="bibr" target="#b52">[52]</ref> or geometry and regression tasks <ref type="bibr" target="#b15">[16]</ref>. In the facial analysis domain, a multi-purpose algorithm for seven face-related tasks was proposed in <ref type="bibr" target="#b50">[50]</ref>. FATAUVA-Net <ref type="bibr" target="#b6">[7]</ref> performs sequential facial attribute recognition, AU detection, and VA estimation on videos. In <ref type="bibr" target="#b30">[31]</ref> a holistic framework was proposed that jointly learns three facial behavior tasks (recognition of basic emotions, VA estimation and AUs detection) and two simple strategies were used for coupling the tasks during training. Closer to our work, in <ref type="bibr" target="#b58">[58]</ref> a two-level attention with a two-stage MTL (2Att-2Mt) framework was proposed for facial emotion estimation on static images. However, the work was focused mainly on the estimation of VA and the dependencies between the emotion representations was not further explored.</p><p>Apart from MTL, research on the dependencies between the categorical and the dimensional emotion representations is limited. Recently, in CAKE <ref type="bibr" target="#b27">[28]</ref> the link between the two representations was explored and a 3-dimensional representation of emotion learned in a multi-domain fashion was proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR PROPOSED METHOD</head><p>In this section the architecture of the proposed network, the dependent classifiers and regressors and the MTL setting for recognition are introduced. The overall architecture of the proposed Emotion-GCN is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature extraction</head><p>Given an input image I of size 227?227 pixels, we obtain 1024 ? 7 ? 7 feature maps using a Dense Convolutional Network (DenseNet) <ref type="bibr" target="#b26">[27]</ref>. Each layer in DenseNet obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers to ensure maximum information flow between layers. Then, a global max-pooling function is applied to get the feature vector x of the image:</p><formula xml:id="formula_0">x = f gmp (f cnn (I)) ? R D<label>(1)</label></formula><p>where f cnn corresponds to the convolution layers of the DenseNet, f gmp to the the global max-pooling function and D = 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dependent classifiers and regressors</head><p>Given the feature vector x of the image, we want to learn one classifier for each facial expression (classification task) and one regressor for each dimension in the VA space (regression task). Each classifier acts as a weight vector that is multiplied with the feature x and then passes though a softmax activation function assigning a probability score to its emotion category. In parallel, each regressor follows the same procedure without passing through an activation function since its output is a predicted continuous value for either valence or arousal. We denote the expression classifiers by W c ? R 7?D and the VA regressors by W r ? R 2?D where each row of the matrices corresponds to a classifier and a regressor respectively. Traditionally, these vectors are optimized as individual parameters of the deep learning network. To capture the correlation between the categorical and the dimensional model, we propose to learn these vectors using a GCN that maps their word embeddings to dependent classifiers and regressors retaining the shared information between the two tasks.</p><p>1) GCN Overview: Generally, the goal of a GCN is to learn a function f on a graph G = (V, E) that takes as input a feature description for each node of the graph H l ? R n?d (n = |V |) and a correlation matrix A ? R n?n and produces new node-level features H l+1 ? R n?d . The update rule is formulated as follows:</p><formula xml:id="formula_1">H l+1 = h(?H l W l )<label>(2)</label></formula><p>where W l ? R d?d is the weight matrix for the l-th GCN layer,? is the normalized version of matrix A such that all rows sum to one and h(?) is LeakyRELU <ref type="bibr" target="#b41">[41]</ref>. For more information on GCN we refer readers to <ref type="bibr" target="#b29">[30]</ref>. In our case, the nodes of the graph correspond to the seven expression labels and the two VA dimensions i.e. V = {Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, Valence, Arousal}. Hence, the input is Z ? R 9?d that contains the word embedding of each node (d is the dimensionality of the embeddings). Each GCN layer l takes the node representations from the previous layer H l as inputs and outputs new node representations H l+1 using <ref type="bibr">(2)</ref>. Finally, the output representation of the last layer W ? R 9?D contains the dependent classifiers and regressors. The first seven rows of the output matrix W constitute the classification part W c and the rest two the regression part W r .</p><p>2) Adjacency Matrix Design: According to the update rule of a GCN (2) the feature of a node in the graph is the weighted sum of its own feature and the adjacent nodes' features. Since the purpose behind the use of a GCN is to exploit the dependencies between the categorical and the dimensional model, we should design the adjacency matrix A to this direction. As before, we assume that the first seven rows of A correspond to the basic expressions and the last two are the continuous dimensions i.e. valence and arousal. Since we deal with a multi-task and not a multi-label recognition problem, we are interested only in the correlation between the categorical and the dimensional model. Hence, we set the other pairs of A to zero except for the diagonal to allow self-loops. Also, we take the absolute value of the correlation to ignore its type (positive or negative) and focus on its amplitude. The correlation matrix A ? R 9?9 can be written as:</p><formula xml:id="formula_2">A ij = ? ? ? ? ? ? ? ? ? 1, if i = j |c ij |, if i ? Cat ? j ? Dim |c ij |, if j ? Cat ? i ? Dim 0, else<label>(3)</label></formula><p>where Cat and Dim are the set of indices of the categorical and the dimensional labels respectively. As a correlation metric, we use the Spearman's rank correlation coefficient <ref type="bibr" target="#b54">[54]</ref> that for two variables X = {x 1 , ..., x N } and Y = {y 1 , ..., y N } is defined as: </p><formula xml:id="formula_3">c xy = N k=1 x k,r y k,r N k=1 x 2 k,r N k=1 y 2 k,r<label>(4)</label></formula><p>where x k,r and y k,r are the rank transformation of the initial values x k and y k . Following the ideas in ML-GCN <ref type="bibr" target="#b8">[9]</ref>, we use a threshold ? to filter the noisy edges as follows:</p><formula xml:id="formula_4">A ij = 1, if A ij ? ? 0, if A ij &lt; ?<label>(5)</label></formula><p>where ? = 0.1 to enable the propagation of information between weakly correlated nodes. As shown in <ref type="bibr" target="#b33">[34]</ref>, the learned features of each node may be over-smoothed and become indistinguishable when a binary correlation matrix is used. To alleviate the over-smoothing problem, we apply the re-weighted scheme of ML-GCN that is defined as:</p><formula xml:id="formula_5">A ij = ? ? ? (p/ 9 j=1 i =j A ij ) ? A ij , if i = j 1 ? p, if i = j<label>(6)</label></formula><p>where the variable p determines the weights assigned to a node itself and its adjacent nodes. We set p = 0.7 to increase the influence of the neighborhood nodes. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the output adjacency matrix A of Emotion-GCN using the training set of AffectNet. As we expected, pairs like happyvalence and surprise-arousal are strongly connected since the presence of the one usually denotes the presence of the other <ref type="figure" target="#fig_4">(Fig. 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-task learning setting</head><p>After we generate the dependent matrices W c , W r and the feature vector x, we perform FER simultaneously on the categorical and the dimensional model. For the classification task, the predicted scores are computed as:</p><formula xml:id="formula_6">y i = e w c i ?x 7 k=1 e w c k ?x<label>(7)</label></formula><p>where? i is the probability of emotion i and w c i is the classifier of emotion i. The network is trained using a weighted version of the traditional categorical cross entropy loss L c since the dataset is highly imbalanced <ref type="bibr" target="#b44">[44]</ref>. In other words, the network is penalized more for misclassifying samples from under-represented classes than from well-represented classes as follows:</p><formula xml:id="formula_7">L c = ? 7 i=1 w i y i log(? i ) (8) w i = f i f min (9)</formula><p>where y i = 1 if class i is the ground truth expression, f i is the number of samples of the i-th class and f min is the number of samples in the most under-represented class i.e. Disgust. For the regression task, the predicted values are computed as:p</p><formula xml:id="formula_8">i = w r i ? x<label>(10)</label></formula><p>wherep 1 ,p 2 are the predicted values of valence and arousal respectively and w r 1 , w r 2 correspond to the VA regressors. Most studies in the literature use the Mean Squared Error (MSE) as a loss function to train regression models. However, more and more works on emotion recognition use a Correlation-based loss function to measure the agreement between the true emotion dimension and the predicted emotion degree <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The Concordance Correlation Coefficient (CCC) is often used since it takes the bias into Pearson's correlation coefficient <ref type="bibr">[2]</ref>. It is defined as:</p><formula xml:id="formula_9">? c = 2s xy s 2 x + s 2 y + (x ??) 2<label>(11)</label></formula><p>where s x and s y denote the variance of the predicted and ground truth values respectively,x and? are the corresponding mean values and s xy is the respective covariance value. The range of CCC is from -1 (perfect disagreement) to 1 (perfect agreement). Hence, in our case we define L r as:</p><formula xml:id="formula_10">L r = 1 ? ? v + ? a 2<label>(12)</label></formula><p>where ? v and ? a are the respective CCC of valence and arousal. The overall loss function of our network training is defined as L = L c + L r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we present the experimental evaluation of the proposed Emotion-GCN model <ref type="figure" target="#fig_0">(Fig. 2)</ref>. First, we briefly describe the facial expression datasets that we use and the details of our implementations. Then, an ablation study of our models follows and finally a comparison with the current state-of-the-art methods.</p><p>A. Datasets 1) AffectNet: AffectNet is by far the largest database of facial expressions that provides both categorical and VA annotations. It contains more than one million facial images collected from the Internet by querying three major search engines using 1,250 emotion-related keywords in six different languages <ref type="bibr" target="#b44">[44]</ref>. About half of the retrieved images are manually annotated for the presence of seven discrete facial expressions (categorical model) and the intensity of valence and arousal (dimensional model). It is a very challenging database as it contains images of people from different races and ethnic groups as well as high variety in the background, lighting, pose, point of view, etc. To be consistent with the majority of past studies <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b24">[25]</ref>, we exclude the emotion of contempt and train our models on the images with neutral and the 6 basic emotions (approximately 280,000 training samples). Since the test set of AffectNet is not publicly available, we evaluate our approaches on the validation set that contains 500 images for each emotion. The mean class accuracy is used as the evaluation metric for the classification task because the validation set is balanced and the CCC for the regression task.</p><p>2) Aff-Wild2: Aff-Wild2 is the first ever database annotated for valence-arousal estimation, action unit detection and basic expression classification <ref type="bibr" target="#b30">[31]</ref>. It consists of 548 videos collected from YouTube and shows both subtle and extreme human behaviours in real-world settings. We trained and evaluated our models on a subset of the database that contains only the frames with both categorical and VA annotations, as required by our method. As proposed by the authors of Aff-Wild2 a weighted average between the F1 score and mean class accuracy is used as the evaluation metric for the classification task (0.67 ? F1 + 0.33 ? Acc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Before training the network, the face region of each image is cropped using the provided bounding box by the database and scaled to 227 ? 227 pixels. Also, we perform face alignment based on the position of the eyes to obtain a normalized representation of each face. Specifically, we compute the location of the eyes by taking the mean value of the six detected landmarks in each eye. Then, we rotate the image by ? that is defined as ? = tan ?1 ry?ly rx?lx , where (r x , r y ) and (l x , l y ) are the coordinates of the right and the left eye respectively. To augment the data, six types of augmentation techniques are used (flip, rotation and changes in brightness, contrast, hue and saturation) <ref type="bibr" target="#b24">[25]</ref>. The GCN module consists of two layers defined with output dimensionality of 512 and 1024 respectively. For the word representations, we use 300-dimensional GloVe <ref type="bibr" target="#b49">[49]</ref> trained on the Wikipedia dataset. The networks are trained for 10 epochs using a batch size of 35 and a learning rate of 0.001. Stochastic Gradient Descent is adopted as the optimization algorithm with a momentum of 0.9 and PyTorch <ref type="bibr" target="#b48">[48]</ref> is used as our deep learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>First, we investigate the performance of four different networks on the categorical and the dimensional model of affect to present the improvements that Emotion-GCN introduces.  1) Categorical Model: We trained (i) a single-task network for discrete FER using the weighted CE loss of <ref type="bibr" target="#b7">(8)</ref>. Then, two multi-task networks were trained for discrete and continuous FER using a weighted CE loss for the classification task and a (ii) MSE or (iii) CCC loss <ref type="bibr" target="#b11">(12)</ref> for the regression task. Finally, we trained (iv) the proposed Emotion-GCN model that generates dependent classifiers and regressors using a 2-layer GCN as presented in <ref type="figure" target="#fig_0">Fig. 2</ref>. <ref type="table" target="#tab_0">Table I</ref> shows the results of our experiments. In AffectNet we can see that learning to predict the VA values as an additional task in a MTL framework increases the accuracy by 1.32% since the shared representation improves the generalization of the network. Also, in agreement with recent studies, using the CCC loss for the regression task increases the classification accuracy of the model verifying that in MTL a correlationbased regression loss performs better. Finally, Emotion-GCN achieves a total accuracy of 66.46%, since the dependent classifiers manage to effectively capture the dependencies between the two emotion representations. The confusion matrices for these models are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. It can be seen that our proposed method increases the accuracy for most classes while the single-task network performs better only in the neutral class. Similarly, in Aff-Wild2 the total evaluation metric increases by 3.86% indicating that the benefits of Emotion-GCN generalize in more datasets. To investigate how different choices for the parameters of the GCN affect the performance, we perform additional experiments in <ref type="table" target="#tab_0">Table  II</ref>. We observe that the number of GCN layers (L) is the most crucial parameter since using only one layer decreases the accuracy a lot.</p><p>2) Dimensional Model: For the evaluation of our models on the VA space, the networks (ii), (iii) and (iv) are the same since they are trained for both discrete and continuous FER.  Additionally, a single-task network for VA regression was trained using the CCC loss of <ref type="bibr" target="#b11">(12)</ref>. <ref type="table" target="#tab_0">Table III</ref> shows the performance evaluation of our experiments on the dimensional model. In AffectNet we can see that the multi-task network with the CCC loss improves the regression performance along with the classification one verifying that both tasks benefit from the shared feature representation. Actually, we observe that the increase in the performance of arousal prediction is higher that that of valence. This is due to the fact that most emotions have positive value of arousal ( <ref type="figure" target="#fig_4">Fig.  1</ref>) and a simultaneous emotion classification provides more useful information in the task of arousal regression. Finally, our GCN based approach achieves similar performance with that of the multi-task network on the dimensional model. In Aff-Wild2, our proposed model surpasses the performance of both the single-task and multi-task networks. Overall, our method presents significant improvements in both the categorical and the dimensional model. In <ref type="figure" target="#fig_3">Fig. 5</ref> we present some positive and negative results of our Emotion-GCN model on AffectNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization</head><p>To further analyze the effectiveness of our approach, we investigate the similarity between the dependent classifiers and regressors. In <ref type="figure">Fig. 6</ref> the cosine similarity of the learned vectors on AffectNet by our single-task networks, our best multi-task network and our Emotion-GCN are presented. As we can see, learning a shared representation for both tasks through MTL slightly increases the similarity between the expression classifiers and the VA regressors. Our proposed method increases their similarity even more in consistence with the dependencies presented in <ref type="figure" target="#fig_4">Fig. 1</ref>. Specifically, the regressor of valence comes closer to the classifier of happy and the regressor of arousal closer to the classifiers of anger, disgust, fear and surprise. These emotions appear in regions of VA space where the values of valence or arousal are high. Therefore, the proposed network has successfully captured the dependence between the categorical and the dimensional emotion representation.</p><p>We also observe an increase in the similarity between the pairs valence-surprise and happy-surprise while their respective nodes are not adjacent in the graph <ref type="figure" target="#fig_1">(Fig. 3)</ref>. These dependencies are successfully captured by our network since in a 2-layer GCN a node incorporates information from a 2-hop neighborhood <ref type="bibr" target="#b11">[12]</ref>. To further examine whether the dependence between the emotions of happiness and surprise is reasonable, we compute their co-occurrence in a multi-label emotion dataset. The EMOTIC dataset <ref type="bibr" target="#b32">[33]</ref> is a collection of images of people in unconstrained environments annotated according to their apparent emotional states. Each person is annotated for 26 discrete categories, with multiple labels assigned to each image. About 25% of the samples labeled as happy are labeled as surprise too that indicates that these two emotions are strongly related indeed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Dependence between classifiers</head><p>Inspired by the fact that there are dependencies between the basic emotions, we trained a similar GCN based network but we enabled this time the direct propagation of information between the seven emotion classifiers. Instead of setting their correlation to zero as in Emotion-GCN, we compute the conditional probability matrix of the basic emotions like in ML-GCN. The model achieves a total accuracy of 66.23% on the categorical model of AffectNet. Despite the intuition behind this approach, the recognition performance is slightly lower than that of our proposed method. We believe that by enabling the propagation of information between the basic emotions the dependence between the categorical and dimensional model is ignored. Also, we deal with a single-label recognition task and the possible benefits of this approach are more suitable in a multi-label recognition task. To examine which network better exploits the emotional dependencies, we selected samples where the predictions of the networks on the dimensional model are close. In the first four samples our Emotion-GCN model successfully recognizes the depicted emotion while the multi-task network fails indicating that our proposed model effectively captured the dependencies presented in the VA space ( <ref type="figure" target="#fig_4">Fig. 1)</ref>. However, there are cases where our network fails (last two samples).</p><p>(a) Single-task (b) Multi-task + CCC (c) Emotion-GCN <ref type="figure">Fig. 6</ref>: Visualization of the cosine similarity between the learned classifiers and regressors by our models on AffectNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with the State of the Art</head><p>In <ref type="table" target="#tab_0">Table IV</ref>, we compare the performance of our Emotion-GCN model with several state-of-the-art methods for FER on the categorical model of AffectNet. Regarding Aff-Wild2, we only used the subset which contains both categorical and continuous annotations, while methods in the bibliography typically report results on the whole dataset, making direct comparisons not possible. In IPA2LT <ref type="bibr" target="#b61">[61]</ref>, the authors designed LTNet to discover the latent truths from the human annotations and the machine annotations trained from different FER datasets. In gACNN <ref type="bibr" target="#b39">[39]</ref> and OADN <ref type="bibr" target="#b13">[14]</ref> attention networks were proposed for occlusion aware FER. Facial Motion Prior Network in <ref type="bibr" target="#b7">[8]</ref> generates a facial mask so as to focus on facial muscle moving regions. In <ref type="bibr" target="#b20">[21]</ref> deep (CNNs) and handcrafted features (BOVW) were combined and in <ref type="bibr" target="#b24">[25]</ref> a deep Siamese network along with a supervised loss function was used to reduce the intra-class variation of the task. Closer to our work, CAKE <ref type="bibr" target="#b27">[28]</ref> proposed a 3-dimensional representation of emotion learned in a multidomain fashion. Our Emotion-GCN model considerably outperforms these recent state-of-the-art methods, achieving an accuracy of 66.46%.</p><p>In <ref type="bibr" target="#b23">[24]</ref> BReG-NeXt achieved state-of-the-art accuracy in 8-way classification on AffectNet (including contempt) by introducing a residual-based network architecture. To investigate the ability of Emotion-GCN to generalize over other model architectures as well, we replaced our DenseNet backbone network with BReG-NeXt using the publicly available code 1 . Since the provided code does not include the TABLE IV: Comparison with state-of-the-art methods on AffectNet (7-way classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>IPA2LT <ref type="bibr" target="#b61">[61]</ref> 57.31 gACNN <ref type="bibr" target="#b39">[39]</ref> 58.78 Facial Motion Prior Network <ref type="bibr" target="#b7">[8]</ref> 61.52 CAKE <ref type="bibr" target="#b27">[28]</ref> 61.7 OADN <ref type="bibr" target="#b13">[14]</ref> 61.89 CNNs and BOVW + global SVM <ref type="bibr" target="#b20">[21]</ref> 63.31 Siamese <ref type="bibr" target="#b24">[25]</ref> 64 Emotion-GCN (ours) 66.46 data preprocessing strategy, we followed our preprocessing and training pipeline described before (same with DenseNet), which explains the different results compared to <ref type="bibr" target="#b23">[24]</ref>. As we can see in <ref type="table" target="#tab_3">Table V</ref>, our proposed model outperforms the single-task and multi-task models when using BReG-NeXt as the backbone network indicating that Emotion-GCN generalizes across different model architectures as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, a novel GCN based MTL framework is proposed for in-the-wild FER. Specifically, our Emotion-GCN model learns a shared feature representation for both discrete and continuous expression recognition to exploit the dependencies between the categorical and the dimensional model of affect. To further capture these dependencies, the expression classifiers and the VA regressors are learned though a GCN that maps their word representation to dependent vectors inspired by recent work in multi-label image recognition. Experimental results on AffectNet, the largest facial expression database, have demonstrated that our Emotion-GCN outperforms the performance of the recent state-of-the-art methods for discrete FER.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overall architecture of our Emotion-GCN model for FER in the wild. Our graph contains the seven expression labels and the two VA dimensions that are connected to each other based on the adjacency matrix A. Stacked GCNs are learned over the graph to map the word embeddings of the nodes Z into a set of dependent classifiers and regressors, W c and W r respectively. These vectors are then applied to an image representation x extracted from the input image I via a DenseNet f cnn followed by a global max-pooling function f gmp . The whole network is trained end-to-end for both basic expression classification (L c ) and VA regression (L r ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Adjacency matrix of our Emotion-GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Confusion matrices of our models on the validation set of AffectNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Predictions of our models on samples of AffectNet. The first column indicates the ground truth values. The second and the third column present the predictions of the multi-task network trained with CCC loss and our Emotion-GCN respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 1 :</head><label>1</label><figDesc>Adjacency matrix of a GCN that enables the propagation of information between the expression classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 :</head><label>2</label><figDesc>Visualization of the cosine similarity between the learned classifiers and regressors by a GCN that enables the propagation of information between the expression classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Performance of our models on the categorical model of AffectNet and Aff-Wild2.</figDesc><table><row><cell>Method</cell><cell cols="2">AffectNet Aff-Wild2</cell></row><row><cell>Single-task Multi-task + MSE Multi-task + CCC Emotion-GCN</cell><cell>64.37 64.8 65.69 66.46</cell><cell>45.06 43.1 43.33 48.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Classification accuracy of Emotion-GCN on the categorical model of AffectNet using different values for ? , p and L (number of GCN layers). In the first table p = 0.7 and in the second table ? = 0.1.</figDesc><table><row><cell></cell><cell></cell><cell>L</cell><cell>?</cell><cell>0</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 2 3</cell><cell cols="2">53.09 65.29 65.77</cell><cell cols="2">43.85 53.71 65.31 66.46 64.94 64.74</cell><cell>54.94 65.69 65.74</cell><cell></cell></row><row><cell>L</cell><cell>p</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell></row><row><cell>1 2 3</cell><cell></cell><cell cols="2">49.6 65.71 66.14 50.94 65.29 65.54</cell><cell cols="2">52.57 52.66 65.29 66.29 65.89 66.09</cell><cell cols="3">52.86 53.71 59.31 66.09 66.46 65.06 65.54 64.74 65.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance of our models on the dimensional model of AffectNet and Aff-Wild2.</figDesc><table><row><cell>Method</cell><cell cols="4">AffectNet CCC-V CCC-A CCC-V Aff-Wild2 CCC-A</cell></row><row><cell>Single-task Multi-task + MSE Multi-task + CCC Emotion-GCN</cell><cell>0.761 0.752 0.768 0.767</cell><cell>0.628 0.572 0.651 0.649</cell><cell>0.416 0.435 0.408 0.457</cell><cell>0.501 0.378 0.481 0.514</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V :</head><label>V</label><figDesc>Performance of Emotion-GCN using BReG-NeXt as the backbone network on AffectNet.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell>Accuracy</cell></row><row><cell cols="2">Single-task Multi-task+CCC BReG-NeXt BReG-NeXt Emotion-GCN BReG-NeXt</cell><cell>60.49 61.14 61.94</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/behzadhsni/BReG-NeXt</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition</head><p>Panagiotis Antoniadis, Panagiotis P. Filntisis, Petros Maragos</p><p>School of E.C.E., National Technical University of Athens, Greece</p><p>Abstract-In the paper we presented Emotion-GCN, a novel MTL framework that exploits the dependencies between the categorical and the dimensional model using a GCN to recognize facial expressions in-the-wild. In subsection IV. E, we trained a network similar to Emotion-GCN but we enabled this time the direct propagation of information between the seven emotion classifiers. Here, we present some additional figures related to this network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adjacency Matrix</head><p>The part of the adjacency matrix that concerns the correlation between the categorical and the dimensional model is computed like in Emotion-GCN. However, to compute the correlation between the basic emotions a multi-label emotion dataset is required. We used the EMOTIC dataset <ref type="bibr">[1]</ref> and followed the procedure of ML-GCN <ref type="bibr">[2]</ref> to compute the conditional probabilities between the basic emotions. The parameters remained the same as in Emotion-GCN model (? = 0.1 and p = 0.5). The output adjacency matrix of the network is shown in <ref type="figure">Fig. 1</ref>. As expected, some extra edges were created between the nodes of surprise-happy, fear-sad and disgust-anger enabling the propagation of information between these classifiers through training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization</head><p>To explore the emotional dependencies that the network captures, we visualize the cosine similarity between the learned vectors in <ref type="figure">Fig. 2</ref>. We can see that the similarity between the basic emotions that are adjacent in the graph increases while the similarity between the classifiers and the regressors remains the same as in the Emotion-GCN model. Therefore, the network successfully captured the dependencies that are present between the basic emotions. As stated in the paper, the classification accuracy is slightly lower because we deal with a single-label recognition task and the possible benefits of this approach are more suitable in a multi-label recognition task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Covariance pooling for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Pani</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Evaluation of error and correlation-based loss functions for multitask learning dimensional speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Atmaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akagi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10724</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards personalised gaming via facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roijers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AIIDE</title>
		<meeting>AIIDE</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Island loss for learning discriminative features in facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facial expression recognition and histograms of oriented gradients: a comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carcagn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Coco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Distante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SpringerPlus</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fatauva-net: An integrated deep learning framework for facial attribute recognition, action unit detection, and valence-arousal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial motion prior networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VCIP</title>
		<meeting>VCIP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Sim?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The expression of the emotions in man and animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>University of Chicago press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Signed graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Occlusion-adaptive deep network for robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCB</title>
		<meeting>IJCB</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facenet2expnet: Regularizing a deep face recognition net for expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strong evidence for universals in facial expressions: a reply to russell&apos;s mistaken critique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fusing body posture with facial expressions for joint recognition of affect in child-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Filntisis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koutras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local learning with deep and handcrafted features for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM international conference on Multimedia</title>
		<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Breg-next: Facial affect computing using adaptive residual networks with bounded gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial expression recognition using deep siamese neural networks with a supervised loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hayale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning supervised scoring ensemble for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMI</title>
		<meeting>ICMI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cake: Compact and accurate k-dimensional representation of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMI</title>
		<meeting>ICMI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Afew-va database for valence and arousal estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotic: Emotions in context dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional graphical lasso for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Reliable crowdsourcing and deep localitypreserving learning for unconstrained facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep facial expression recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-label image classification with a probabilistic label enhancement model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Occlusion aware facial expression recognition using cnn with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">More evidence for the universality of a contempt expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting ad liking and purchase intent: Large-scale analysis of facial responses to ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">El</forename><surname>Kaliouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Local binary patterns for multi-view facial expression recognition. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A facial-expression monitoring system for improved healthcare in smart cities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alsulaiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoneim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Alhamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Continuous prediction of spontaneous affect from multiple cues and modalities in valencearousal space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of epidemiology</title>
		<imprint>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The first facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Suppressing uncertainties for large-scale facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Twolevel attention with two-stage multi-task learning for facial emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiaohua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Muzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lijuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chunhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMI</title>
		<meeting>ICMI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Aff-wild: valence and arousal&apos;in-the-wild&apos;challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Facial expression recognition with inconsistently annotated datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Emotic: Emotions in context dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
