<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">METAAUDIO: A FEW-SHOT AUDIO CLASSIFICATION BENCHMARK *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-12">April 12, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calum</forename><surname>Heggan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University Of Edinburgh Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Budgett</surname></persName>
							<email>samuel.budgett@uk.thalesgroup.com</email>
							<affiliation key="aff1">
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@sms.ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University Of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Yaghoobi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University Of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">METAAUDIO: A FEW-SHOT AUDIO CLASSIFICATION BENCHMARK *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-12">April 12, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Few-shot classification ? Meta-learning ? Benchmark ? Audio ? Acoustics</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently available benchmarks for few-shot learning (machine learning with few training examples) are limited in the domains they cover, primarily focusing on image classification. This work aims to alleviate this reliance on image-based benchmarks by offering the first comprehensive, public and fully reproducible audio based alternative, covering a variety of sound domains and experimental settings. We compare the few-shot classification performance of a variety of techniques on seven audio datasets (spanning environmental sounds to human-speech). Extending this, we carry out in-depth analyses of joint training (where all datasets are used during training) and cross-dataset adaptation protocols, establishing the possibility of a generalised audio few-shot classification algorithm. Our experimentation shows gradient-based meta-learning methods such as MAML and Meta-Curvature consistently outperform both metric and baseline methods. We also demonstrate that the joint training routine helps overall generalisation for the environmental sound databases included, as well as being a somewhat-effective method of tackling the cross-dataset/domain setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To date, the majority of the breakthroughs seen in machine learning have been in domains or settings where there was an abundance of labelled data, either real or simulated, for example in <ref type="bibr" target="#b0">[1]</ref>. In contrast, the human capability to recognise and discriminate between classes of sensory inputs with few examples, e.g. in visual or acoustic settings, remains unmatched. The development of techniques that can perform such Few-shot Learning (FSL) tasks has seen significant interest within modern machine-learning literature, with particular focus on applying meta-learning (learning to learn) <ref type="bibr" target="#b1">[2]</ref>. These approaches aim to address the setting where classes are rare or labelled data is hard to produce or gather.</p><p>Most work on these types of algorithms focuses on the image domain, with other data modalities and problem settings largely underrepresented. This potentially biases meta-learning algorithmic development towards images, hampering the development of general purpose meta-learners, as well as impeding development of high performance few-shot learning for other types of data or task. Compounding this, the most commonly evaluated datasets, e.g. miniImageNet <ref type="bibr" target="#b2">[3]</ref>, as well as current benchmarks suffer from lack of real-world challenge.</p><p>Acoustic classification and event detection have been well studied in conventional fully supervised machine learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, with many public datasets having a common evaluation protocol that is adhered to by the community, allowing for standardisation and fair comparison. This has however not extended to the few-shot equivalent, where the majority of the works that do exist make little attempt at preserving reproducibility, typically with respect to dataset management and lack of public source code <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. This absence of standardisation poses significant issues when looking to compare novel and existing methods alike.</p><p>In this work, we look to alleviate this gap by contributing the following: 1) Experimental evaluation of some of the most popular few-shot classifiers on a variety of audio datasets, spanning multiple sub-settings from environmental sounds to speech. 2) A fully reproducible few-shot audio classification benchmark with at least one published evaluation split per dataset along with custom data loading allowing for quick plug and play testing in future works. 3) A generalised prescription for dealing with variable length audio datasets in a few-shot setting. 4) Finally, in-depth analyses and evaluation of the joint training and cross-dataset/domain settings. We include all of our code at https: //github.com/CHeggan/MetaAudio-A-Few-Shot-Audio-Classification-Benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Few-Shot Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formulation</head><p>Generally, few-shot learning involves training, validating and testing a model on pairwise disjoint sets of classes (e.g. classes of human non-speech sounds such as sneezing and coughing), C train / ? C val / ? C test . These sets of classes are analogous to the training, validation and test data splits found in traditional machine learning, where splits have non-overlapping samples. In few-shot learning, splits are defined with the addition of non-overlapping classes. The goal of a few-shot classifier is to generalise to a set of N novel classes, given only a few-labelled examples from each class. These episodes (also referred to as tasks throughout) contain a support set S which is used for training and a query set Q where recognition performance is evaluated. Meta-learning can either be trained with episodic training <ref type="bibr" target="#b2">[3]</ref>, where individual few-shot tasks are drawn from C train , or non-episodic training <ref type="bibr" target="#b7">[8]</ref>, where a classifier is trained on all classes contained in C train in order to learn an embedding in the second to last network layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Meta-Learners &amp; Other Approaches</head><p>Due to the number of meta-learning algorithms created and distributed in recent literature, discussed more in Section 3, we restrict our attention to a representative few. Specifically these are; Prototypical Networks <ref type="bibr" target="#b8">[9]</ref>, Model-Agnostic Meta-Learning <ref type="bibr" target="#b9">[10]</ref>, Meta-Curvature <ref type="bibr" target="#b10">[11]</ref>, SimpleShot <ref type="bibr" target="#b7">[8]</ref> and Meta-Baseline <ref type="bibr" target="#b11">[12]</ref>. This selection covers both metric and gradient-based meta-learning, as well as extensions to simpler baseline methods. We leave the specific details of the algorithms to the original papers and instead offer a very high level overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototypical Networks</head><p>The most seminal of the metric learning family of meta-learners, ProtoNets <ref type="bibr" target="#b8">[9]</ref> work by calculating class prototypes as the centroid of the embedded support set during learning, followed by applying a nearest-centroid procedure for classifying queries.</p><p>MAML &amp; Meta-Curvature These gradient-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> aim to learn a transferable initialisation for any model such that it can quickly adapt to a new task ? with only a few steps of gradient descent. At training time, the meta-objective is defined as query set performance after a few steps of gradient descent on the K support samples from the model's initial parameters. Meta-curvature expands on MAML by also learning a transform of the inneroptimisation gradients so as to achieve better generalisation on new tasks. In this work, we experiment and report results with first order variants of these algorithms, as in initial experimentation comparing both variants we observed negligible or negative effects to performance.</p><p>SimpleShot &amp; Meta-Baseline SimpleShot <ref type="bibr" target="#b7">[8]</ref> and Meta-Baseline <ref type="bibr" target="#b11">[12]</ref> are simple baseline methods that aim at lowering computational cost while still achieving strong performance. Both methods train in a conventional way, outputting logits directly from a linear layer of size |C train |, and validate/test using nearest centroid classification. To distinguish themselves, SimpleShot applies additional data normalisations at test time, while Meta-Baseline performs episodic fine-tuning, similar to ProtoNets but with cosine distance and logit scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Few-Shot Classification We review only a small subset of available meta-learners and point the reader to <ref type="bibr" target="#b1">[2]</ref> for a more detailed review. MAML <ref type="bibr" target="#b9">[10]</ref>, Meta-SGD <ref type="bibr" target="#b12">[13]</ref> and Meta-Curvature <ref type="bibr" target="#b10">[11]</ref> are representative gradient-based metalearning (GBML) schemes, designed around the idea of fast adaptation to new learning tasks using additional gradient descent steps. The prototypical networks <ref type="bibr" target="#b8">[9]</ref> that we evaluate here are metric learners, which aim to learn a strong feature embedding space such that support and queries can be compared using nearest neighbour. Other members of this family are Matching Networks <ref type="bibr" target="#b2">[3]</ref>. All of these algorithms have been primarily evaluated in the image domain and their performance in other domains, audio included, is largely unknown.</p><p>Few-Shot Acoustics Currently only a handful of studies exist that look at either few-shot audio classification or event detection. Of these, two are set in event detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> (classification of parts of an audio clip in time) with the other two focused on classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> (classification of an entire audio clip), the focus of this work. Comparing these works, we see a variety of approaches taken toward dataset processing, split formulation and reproducibility. These variations, most importantly the dataset and its associated split used, make comparisons and ranking of the works impossible. Among these, <ref type="bibr" target="#b13">[14]</ref> is distinct in that it provides both a fully reproducible code base and the dataset class-wise splits used for its experiments. Their main contribution is fitting common metric based learners with an attention similarity module, attached to its purely convolutional backbone. This work is currently state-of-the-art for both the ESC-50 <ref type="bibr" target="#b4">[5]</ref> dataset and its proprietary noise injected variant 'noiseESC-50'. As discussed more in Section 4, we use this work as a basis for some of our experiments.</p><p>Benchmarks Most relevant to our work are other few-shot and meta-learning benchmarks. Included in this are works such as Meta-Dataset <ref type="bibr" target="#b15">[16]</ref> (an aggregation of 10 few-shot image based datasets) and MetaCC <ref type="bibr" target="#b16">[17]</ref> (a modifiable set of channel coding tasks). Of the benchmarks currently available for few-shot classifier evaluation, none deal with acoustic classification. This is the primary area that this work aims to fill. Meta-Dataset is of particular relevance to this work as we aim to mimic both the depth and reproducibility achieved by the benchmark. Specifically, both the within and cross dataset evaluations as well as the public leaderboard are components which we find to be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MetaAudio Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setting &amp; Data</head><p>As MetaAudio aims to be a diverse and reproducible benchmark, it covers a variety of experimental settings, algorithms and datasets. Throughout, we mainly consider 5-way 1-shot classification, with some additional analysis of the impact of k-shots and N-ways at test time. We experiment with 7 total datasets, 5 of which are primary datasets which we split for use in training and evaluation, and 2 held-out sets we use exclusively for testing. Among them, 3 have fixed-length and 4 have variable length clips. Additional details about the datasets, including size and settings, can be found in <ref type="table" target="#tab_0">Table 1</ref>. Due to the highly variable sample size of the original dataset and the issues that it presents with reproducibility, we primarily experiment with a pruned version of BirdClef 2020, where samples longer than 180s are removed along with classes with fewer than 50 samples.</p><p>Splits &amp; Labels For every experiment setup, we apply a 7/1/2 train-validation-testing split ratio over all the classes belonging to an individual dataset. These ratios are chosen to be in line with the majority of machine learning and few-shot works. Any conventional sample based train/val/test splits are ignored, and the class splits are applied to all available data. Outside <ref type="bibr" target="#b13">[14]</ref>, from which we can obtain a reproducible split of ESC-50, we have no works with prior dataset splits to follow, and so we define our own. Most simply we assign random splits based on the available classes for a given set. However, we also define within-dataset domain-stratification and shift splits for sets that have additional internal structure and/or accompanying meta-data. Extensive experimentation with these more specific splits is not carried out in this work, however we include them in our code repository. Labels for the datasets vary quite significantly with some having time strong (temporally localised) labels like BirdClef2020 with others having only weak (clip-level) labels. In the interest of consistency, for this work we drop the available strong labels for the datasets that have them and operate exclusively with weak labels. The tradeoff of this approach is that for datasets that have access to strong labels, we expect additional label noise to be present during training, possibly hurting final generalisation performance.</p><p>Pre-Processing Pre-processing is kept minimal, with only the conversion of raw audio samples into spectrograms and some normalisation factors applied during loading. We consider a fixed sample rate and spectrogram parameters over all datasets and contained samples. For normalisation, three techniques were considered; per sample, channel wise and global. Following initial experimentation, global, which uses average statistics across all examples, was used in all experiments due to performance and simplicity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling Strategies</head><p>Throughout MetaAudio, we utilise a variety of sampling strategies for experimentation. The basis of these is our fixed length approach. The steps used for this can be summarised into: 1) Sample a set of N-way classes C N from the necessary split of dataset D and 2) For each class in C N , sample both support and query examples, for support the number will be k-shot.</p><p>We extend this fixed length strategy in order to build a method for dealing with variable length sets. Due to how varied sample length is within some of the considered datasets <ref type="table" target="#tab_0">(Table 1)</ref>, we convert to fixed length representation to avoid the need for specific neural architectures to support variable length inputs and reduce computational requirements. Specifically, we choose to split our variable length samples into L length sub-clips, all sharing the same label. This along with the later conversion of the sub-clips to individual spectrograms is done entirely offline, a decision made to avoid bottlenecking during training.</p><p>Combining a variety of datasets in a joint training and/or evaluation routine has been advocated for in the image space <ref type="bibr" target="#b15">[16]</ref> to evaluate general purpose representation learning, and potentially improve performance through cross-dataset knowledge sharing. We mimic this and expand upon it for the considered acoustic datasets and task. Sampling tasks from the available datasets in this setting can be done in a few distinct ways. We consider this to be an additional area of investigation and compare two variants of task sampling. In Free Dataset Sampling the N classes in an episode can be drawn from multiple source datasets. In Within Dataset Sampling each episode first randomly chooses a dataset, and then draws N random classes within that dataset.</p><p>During these sampling strategies, we largely ignore the class sample imbalance seen in the majority of the datasets we experiment with, we do this for a few reasons. The first of these is that recent works, such as <ref type="bibr" target="#b17">[18]</ref>, suggest it is less detrimental in meta-learning than in conventional learning. The second is that, these imbalances allow algorithms to differentiate themselves with respect to how they handle the more difficult setting. One area in which we do experiment with alleviating the effect of this imbalance is in the re-weighting of the loss functions used in the conventional learning parts of the Meta-baseline and SimpleShot algorithms. To create this dataset custom loss, we employ inverse-frequency class weighting, where the class-wise contribution to the loss function is the inverse of the number of samples present in that class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>Our experimental design follows prior few-shot works, where after one end-to-end training and evaluation procedure, average classification accuracies are reported along with their 95% confidence intervals using 10,000 tasks drawn from the test set. For all experiments we use Adam with a non-adaptive learning rate. Due to the limited tuning performed, we expect it to be fairly easy to obtain a specific result marginally better than those presented, however it is important to note that this does not undermine the comparison and experimental settings investigated in this work.</p><p>Motivated by the performance gap between the commonly used CNNs and other neural architectures currently present in conventional acoustic learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, we briefly investigated the role of the base neural architecture in the fewshot acoustic setting. Due to space restriction, we do not report details here, however our best performing model using MAML and ProtoNets on ESC-50 was a lightweight hybrid CRNN. Due to its relatively low computational cost compared to larger models, we opt for this architecture throughout. Specifically, the CRNN contains a 4-block convolutional backbone (1-64-64-64) with an attached 1-layer non-bidirectional RNN containing 64 hidden units. The number of outputs in the final linear layer is either of size N-way or, in the case of metric learning and baseline methods, 64.</p><p>In the majority of the results presented for variable length datasets, the value of L is set to 5 seconds. We chose this value based on preliminary experiments (not included due to space limitations) where, for Kaggle18, L = 5s performed best when compared against 1 and 10-seconds. Similarly, for the Watkins Mammal Sound Database, L = 5 closely resembles the expected value of the dataset's sample length distribution. Setting a common value of L also allows us to more comfortably facilitate joint training and cross-dataset evaluation without the need for massive padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Within-Dataset Evaluation</head><p>We first benchmark the algorithms and datasets using a within-dataset protocol: Training and evaluating on datasets independently and further evaluating using the held out testing classes. From <ref type="table" target="#tab_1">Table 2</ref>(a), we first remark that on ESC-50 our ProtoNet with a CRNN backbone performs at least as well as the Protonet-CNN considered in <ref type="bibr" target="#b13">[14]</ref> with the same split.</p><p>Comparing the results <ref type="table" target="#tab_1">Table 2</ref>(a), we make the following further observations: (i) Out of the two fixed length sets, ESC-50 appears to be the harder problem, with much lower accuracy than NSynth. This is somewhat expected given the very clean NSynth data compared to the noisier ESC-50 data. (ii) The variable length datasets appear to provide a harder setting in general, with significantly lower performance than the fixed-length sets. (iii) Comparing the metalearners, we see that GBML methods generally perform better, with Meta-Curvature taking first place in 4 out of 5 cases and best average rank; and MAML taking first place on Kaggle18 and second best rank overall. In comparison, our metric and baseline algorithms underperform in accuracy despite their better speed at inference time. We propose that this is due to the GBML methods' adaption mechanism, updating feature representation at each meta-test episode, making them particularly useful for tasks with high inter-class/episode variance. Meanwhile the others must rely on a fixed feature extractor that cannot adapt to each unique episode. Overall the fact that the GMBL methods outperform SimpleShot, in reversal of the widely remarked upon results for miniImageNet in <ref type="bibr" target="#b7">[8]</ref>, shows the value of an audio-based benchmark as a complement to popular image based benchmarks in drawing conclusions about general purpose and domain-specific meta-learner fitness. (iv) Finally, we observe that Meta-Baseline was the most competitive non-adaptive approach. This confirms that episodic meta-learning provides benefit over the conventional supervised representation learning in SimpleShot <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Joint Training &amp; Cross Dataset</head><p>In this section we extend our evaluation to joint training, where a single model is learned on the combined training splits of all source datasets (rather than a per-dataset model as in Section 5.2), and then evaluated on the testing split of each dataset in turn. Furthermore, we now also test on two held-out datasets that were not included during training <ref type="table" target="#tab_0">(Table 1)</ref>, to evaluate cross-dataset few-shot learning performance. We report results for both within-dataset and free-dataset episode sampling (as discussed in Section 4.2) in  <ref type="table" target="#tab_1">Table 2</ref>(a). This difference varies in magnitude between datasets and sampling routines. The mixed results here mirror those observed in <ref type="bibr" target="#b15">[16]</ref> and reflect the tradeoff between two forces: (1) a positive effect of generally increasing the amount of training data available compared to the within-dataset condition, and (2) a negative effect due to the increased difficulty of learning a single model capable of simultaneous high performance on diverse data domains <ref type="bibr" target="#b19">[20]</ref>. This shows that MetaAudio complements <ref type="bibr" target="#b15">[16]</ref> in providing a challenging benchmark to test future meta-learners' ability to fit diverse audio types, as well as enabling few-shot recognition of new categories.</p><p>Moving to the other question of interest, we contrast how the joint training episode sampling routines compare. For our main datasets, we observe 3/5 of the top results were obtained using the free sampling method, with the 2 outliers belonging to VoxCeleb and BirdClef -evidence that their tasks require significantly different and specific model parametrisation, as the within dataset task sampling would allow more opportunity to learn these more specialised features.</p><p>For the held-out cross-dataset tasks (Watkins, SpechCommands), we also see the strongest performance coming from the free sampling routine, where it outperforms its within dataset counterpart by ?2% in both held-out sets. As for the absolute performances obtained on the held-out sets, we see that our joint training transfers somewhat-effectively, with the model in one case attaining a respectable 50-60% and another obtaining accuracies only 5% above random. Finally, comparing meta-learners, we again see GBML approaches Meta-Curvature and MAML performing best overall. However, for this joint training condition, SimpleShot improves to take third place overall by average rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">External Data &amp; Pretraining</head><p>A full meta-learning pipeline for a specific dataset can be expensive. Recent studies in the few-shot and selfsupervision communities have debated whether off-the-shelf models pre-trained on large external datasets may provide a better approach to few-shot than meta-learning <ref type="bibr" target="#b7">[8]</ref>. Transferring a well-trained representation and training a simple classifier for each task could also be cheaper due to amortizing the cost of large-scale pre-training over multiple downstream tasks.</p><p>To this end, we also evaluate our MetaAudio benchmark using pre-trained feature models trained on the large scale Im-ageNet <ref type="bibr" target="#b20">[21]</ref> and AudioSet <ref type="bibr" target="#b21">[22]</ref> datasets. Specifically, we use the SOTA Audio Spectrogram Transformers(ASTs) from <ref type="bibr" target="#b3">[4]</ref>. We experiment with two model variants, the ImageNet only and the ImageNet + AudioSet trained 'base384' transformers provided by the authors, including their suggested AudioSet sample normalisation. We apply both nearestcentroid and SVM linear classification on our output features. <ref type="table">Table 3</ref>: Meta Audio benchmark using a variety of pre-trained spectrogram transformers from <ref type="bibr" target="#b3">[4]</ref>. Models are trained on ImageNet <ref type="bibr" target="#b20">[21]</ref> and AudioSet <ref type="bibr" target="#b21">[22]</ref>. Results show 5-way 1-shot performance using simple classifiers on fixed features. We compare these to the results for SimpleShot using dataset specific training and evaluation. The results in <ref type="table">Table 3</ref> show that the features pre-trained on AudioSet and ImageNet unsurprisingly outperforms those pre-trained on ImageNet alone. However, the small size of this margin is perhaps surprising, showing that imagederived features provide most of the information needed to interpret spectrograms.</p><p>Comparing these results to in-domain training in <ref type="table" target="#tab_1">Table 2</ref>, we see that performance has dropped substantially, with the potential exception of ESC-50 and Kaggle18. In their best cases, NSynth, VoxCeleb and BirdClef all take drops in performance of ?20% due to dataset shift between general purpose pre-training and our specific tasks, such as musical instruments, speech or bird song recognition. While the performance hit due to domain-shift is expected, these results are surprising as AudioSet is a much larger dataset, and the AST transformer is a much larger architecture than the CRNN used in <ref type="table" target="#tab_1">Table 2</ref>. In image modality, analogous experiments show a clear win simply applying larger pre-training datasets and larger-models combined with simple readouts, compared to conducting within-domain meta-learning <ref type="bibr" target="#b22">[23]</ref>. This confirms the value of Meta-Audio as an important benchmark for assessing meta-learning contributions that cannot easily be replicated by larger architectures and more data. Performance on our held-out sets shows a more mixed set of results, with ImageNet only pre-training favouring Watkins, and ImageNet + AudioSet pre-training setting a new SOTA for SpeechCommands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">N-Way k-Shot Analysis</head><p>Although we only trained and evaluated on the task of 5-way 1-shot classification, we are interested in the effect of larger shots and wider ways on algorithm performance. To bridge this gap, we experiment with these components at test time, using our already trained 5-way 1-shot models. We consider all of our primary datasets and algorithms, covering values of N from 5-30, and k from 1-30. Varying N-ways and k-shots are treated separately and not stacked, a decision made to avoid the compounding computational complexity of the problem. For algorithms which have a fixed size output (i.e. GBML methods) we exclude the varying N-ways. Both ESC-50 and Kaggle18 have only 10 and 7 classes belonging to their test sets respectively, and so analysis further than 10/5-way is impossible. We include a sample of our result plots in <ref type="figure">Figure 1</ref>. Varying the numbers of shots, we observe a clear trend of GBML methods 5-Way, k-Shots, N-Way, 1-Shot, <ref type="figure">Figure 1</ref>: Few-shot learning on VoxCeleb1. Varying test K-shots (left) and N-ways (right).</p><p>outperforming baseline and metric learning approaches. This is especially true for large k-shots, where the rise in performance also occurs faster. For both fixed length sets, we see some additional distinction between gradient based methods and the others, where methods without adaptation both stagnate and start to decline in performance after 5-shot. Up to 30-shot, we do not observe this same behaviour in variable length sets, however it is possible that this is simply due to the complexity of the problems. Of the three non-gradient-based methods, which algorithm performs best over k-shots appears to be dataset specific, with each outperforming the others in at least one set. Although we are more limited in varying the number of ways we test over, we still observe some interesting trends. All of our tested algorithms show a non-linear decay in performance, with results at 30-way still reaching ?20-25% for our VoxCeleb and BirdClef sets (approx 7? random). For speed of drop-off, we see a similar story as we saw in increasing k-shot, with all algorithms showing the best performance in at least one set as N-way increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we presented MetaAudio, a new large-scale and diverse few-shot acoustic classification benchmark covering a variety of algorithms, sound domains and experimental settings.</p><p>Our experiments showed that gradient-based meta-learners with feature adaptation capability generally performed better than fixed-representation competitors. This was the case across most of our settings, although the latter algorithms benefitted from faster learning speed.</p><p>We also evaluated the ability of meta-learners to span few-shot learning tasks drawn from a heterogeneous variety of datasets, and their ability to generalise across distribution shift between training and testing. Surprisingly, we also showed that in-domain meta-learning led to substantially better performance than transfer learning from larger architectures trained on larger external datasets, a result that is noticeably different to that from computer vision.</p><p>Going forward, MetaAudio will provide a substantial complement to influential analogous benchmarks <ref type="bibr" target="#b15">[16]</ref> in the vision domain. Besides benefiting few-shot learning in audio domain, we believe that MetaAudio will help to drive meta-learning research overall, ensuring that more generally relevant algorithms and insights are developed, without becoming overly-specific to computer vision (a problem of gaining relevance). This should help ensure meta-learning research benefits data efficient learning demand across society more broadly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>High level details of all datasets considered in MetaAudio Name Setting N o Classes N o Samples Format Sample Length Use</figDesc><table><row><cell>ESC-50</cell><cell>Environmental</cell><cell>50</cell><cell>2,000</cell><cell>Fixed</cell><cell>5s</cell><cell>Meta-train/test</cell></row><row><cell>NSynth</cell><cell>Instrumentation</cell><cell>1006</cell><cell>305,978</cell><cell>Fixed</cell><cell>4s</cell><cell>Meta-train/test</cell></row><row><cell>FDSKaggle18</cell><cell>Mixed</cell><cell>41</cell><cell>11,073</cell><cell>Variable</cell><cell>0.3s -30s</cell><cell>Meta-train/test</cell></row><row><cell>VoxCeleb1</cell><cell>Voice</cell><cell>1251</cell><cell>153,516</cell><cell>Variable</cell><cell>3s -180s</cell><cell>Meta-train/test</cell></row><row><cell>BirdCLEF 2020</cell><cell>Bird Song</cell><cell>960</cell><cell>72,305</cell><cell>Variable</cell><cell>3s -30m</cell><cell>Meta-train/test</cell></row><row><cell>BirdCLEF 2020 (Pruned)</cell><cell>Bird Song</cell><cell>715</cell><cell>63,364</cell><cell>Variable</cell><cell>3s -180s</cell><cell>Meta-train/test</cell></row><row><cell cols="2">Watkins Marine Mammal Sounds Marine Mammals</cell><cell>32</cell><cell>1698</cell><cell>Variable</cell><cell>0.1 -150s</cell><cell>Meta-test</cell></row><row><cell>SpeechCommandsV2</cell><cell>Spoken Word</cell><cell>35</cell><cell>105,829</cell><cell>Fixed</cell><cell>1s</cell><cell>Meta-test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>(b) and Table 2(c) respectively. First, we compare the joint training regime results 2(b,c) against the within-dataset evaluation in Table 2(a). For both ESC-50 and Kaggle18 we obtain new SOTA results with MAML and Meta-Curvature respectively, both from the free dataset sampling routine. For all other datasets, we see a degradation of performance compared to within dataset training in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Main Meta Audio benchmark 5-way 1-shot classification results.Table (a) contains the within-dataset results,where models are trained for each dataset individually and then evaluated with that dataset's test split.Tables (b)and (c) contain results from the joint training scenario, where we train meta-learners over all datasets simultaneously and then evaluate on individual test splits. They differ in that in (b) we only allow training tasks to be sampled using classes from one of the datasets per episode, whereas in (c) we allow cross-dataset task creation. In (b) and (c) the bottom group of 'cross' datasets are held out from training and used only for testing.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">a) Baseline Within Dataset Results</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dataset</cell><cell>FO-MAML</cell><cell>FO-Meta-Curvature</cell><cell>ProtoNets</cell><cell>SimpleShot CL2N</cell><cell>Meta-Baseline</cell></row><row><cell></cell><cell>ESC-50</cell><cell>74.66 ? 0.42</cell><cell>76.17 ? 0.41</cell><cell>68.83 ? 0.38</cell><cell>68.82 ? 0.39</cell><cell>71.72 ? 0.38</cell></row><row><cell></cell><cell>NSynth</cell><cell>93.85 ? 0.24</cell><cell>96.47 ? 0.19</cell><cell>95.23 ? 0.19</cell><cell>90.04 ? 0.27</cell><cell>90.74 ? 0.25</cell></row><row><cell></cell><cell>Kaggle18</cell><cell>43.45 ? 0.46</cell><cell>43.18 ? 0.45</cell><cell>39.44 ? 0.44</cell><cell>42.03 ? 0.42</cell><cell>40.27 ? 0.44</cell></row><row><cell></cell><cell>VoxCeleb1</cell><cell>60.89 ? 0.45</cell><cell>63.85 ? 0.44</cell><cell>59.64 ? 0.44</cell><cell>48.50 ? 0.42</cell><cell>55.54 ? 0.42</cell></row><row><cell></cell><cell>BirdClef (Pruned)</cell><cell>56.26 ? 0.45</cell><cell>61.34 ? 0.46</cell><cell>56.11 ? 0.46</cell><cell>57.66 ? 0.43</cell><cell>57.28 ? 0.41</cell></row><row><cell></cell><cell>Avg Algorithm Rank</cell><cell>2.4</cell><cell>1.2</cell><cell>3.8</cell><cell>4.0</cell><cell>3.6</cell></row><row><cell></cell><cell></cell><cell cols="3">b) Joint Training (Within Dataset Sampling)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ESC-50</cell><cell>68.68 ? 0.45</cell><cell>72.43 ? 0.44</cell><cell>61.49 ? 0.41</cell><cell>59.31 ? 0.40</cell><cell>62.79 ? 0.40</cell></row><row><cell>Trained</cell><cell>NSynth Kaggle18 VoxCeleb1</cell><cell>81.54 ? 0.39 39.51 ? 0.44 51.41 ? 0.43</cell><cell>82.22 ? 0.38 41.22 ? 0.45 51.37 ? 0.44</cell><cell>78.63 ? 0.36 36.22 ? 0.40 50.74 ? 0.41</cell><cell>89.66 ? 0.41 37.80 ? 0.40 40.14 ? 0.41</cell><cell>85.17 ? 0.31 34.04 ? 0.40 39.18 ?0.39</cell></row><row><cell></cell><cell>BirdClef (Pruned)</cell><cell>47.69 ? 0.45</cell><cell>47.39 ? 0.46</cell><cell>46.49 ? 0.43</cell><cell>35.69 ? 0.40</cell><cell>37.40 ? 0.40</cell></row><row><cell>Cross</cell><cell>Watkins SpeechCommands V1</cell><cell>57.75 ? 0.47 25.09 ? 0.40</cell><cell>57.76 ? 0.47 26.33 ? 0.41</cell><cell>49.16 ? 0.43 24.31 ? 0.36</cell><cell>52.73 ? 0.43 24.99 ? 0.35</cell><cell>52.09 ? 0.43 24.18 ? 0.36</cell></row><row><cell></cell><cell>Avg Algorithm Rank</cell><cell>2.0</cell><cell>1.6</cell><cell>4.0</cell><cell>3.4</cell><cell>4.0</cell></row><row><cell></cell><cell></cell><cell cols="3">c) Joint Training (Free Dataset Sampling)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ESC-50</cell><cell>76.24 ? 0.42</cell><cell>75.72 ? 0.42</cell><cell>68.63 ? 0.39</cell><cell>59.04 ? 0.41</cell><cell>61.53 ? 0.40</cell></row><row><cell>Trained</cell><cell>NSynth Kaggle18 VoxCeleb1</cell><cell>77.71 ? 0.41 44.85 ? 0.45 39.52 ? 0.42</cell><cell>83.51 ? 0.37 45.46 ? 0.45 39.83 ? 0.43</cell><cell>79.06 ? 0.36 41.76 ? 0.41 40.74 ? 0.39</cell><cell>90.02 ? 0.27 38.12 ? 0.40 42.66 ? 0.41</cell><cell>85.04 ? 0.31 35.90 ? 0.38 36.63 ? 0.38</cell></row><row><cell></cell><cell>BirdClef (Pruned)</cell><cell>46.76 ? 0.45</cell><cell>46.41 ? 0.46</cell><cell>44.70 ? 0.42</cell><cell>37.96 ? 0.40</cell><cell>32.29 ? 0.38</cell></row><row><cell>Cross</cell><cell>Watkins SpeechCommands V1</cell><cell>60.27 ? 0.47 27.29 ? 0.42</cell><cell>58.19 ? 0.47 26.56 ? 0.42</cell><cell>48.56 ? 0.42 24.30 ? 0.35</cell><cell>54.34 ? 0.43 24.74 ? 0.35</cell><cell>53.23 ? 0.43 23.88 ? 0.35</cell></row><row><cell></cell><cell>Avg Algorithm Rank</cell><cell>2.1</cell><cell>2.1</cell><cell>3.4</cell><cell>3.0</cell><cell>4.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongli</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Augustin??dek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
		</author>
		<title level="m">Meta-Learning in Neural Networks: A Survey. TPAMI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label few-shot learning for sound event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Hsiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Yu</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot acoustic event detection via meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">C</forename><surname>Puvvada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Chi</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearestneighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-baseline: Exploring simple meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to match transient sound events using attentional similarity for few-shot sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Yu</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Hsiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Shing Roger</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study of few-shot audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piper</forename><surname>Wolters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Careaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GHC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A channel coding benchmark for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bohdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeji</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Donald</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Few-shot learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fsd50k: An open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified few-shot classification benchmark to compare transfer and meta learning approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
