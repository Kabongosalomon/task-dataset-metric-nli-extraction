<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Efficient and Layout-Independent Automatic License Plate Recognition System Based on the YOLO detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayson</forename><surname>Laroca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Todt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Robson Schwartz</surname></persName>
							<email>william@dcc.ufmg.br</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Menotti</surname></persName>
							<email>menotti@inf.ufpr.br2gabrielrg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Efficient and Layout-Independent Automatic License Plate Recognition System Based on the YOLO detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1049/itr2.12030)</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an efficient and layout-independent Automatic License Plate Recognition (ALPR) system based on the state-ofthe-art YOLO object detector that contains a unified approach for license plate (LP) detection and layout classification to improve the recognition results using post-processing rules. The system is conceived by evaluating and optimizing different models, aiming at achieving the best speed/accuracy trade-off at each stage. The networks are trained using images from several datasets, with the addition of various data augmentation techniques, so that they are robust under different conditions. The proposed system achieved an average end-to-end recognition rate of 96.9% across eight public datasets (from five different regions) used in the experiments, outperforming both previous works and commercial systems in the Chi-neseLP, OpenALPR-EU, SSIG-SegPlate and UFPR-ALPR datasets. In the other datasets, the proposed approach achieved competitive results to those attained by the baselines. Our system also achieved impressive frames per second (FPS) rates on a high-end GPU, being able to perform in real time even when there are four vehicles in the scene. An additional contribution is that we manually labeled 38,351 bounding boxes on 6,239 images from public datasets and made the annotations publicly available to the research community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic License Plate Recognition (ALPR) became an important topic of research since the appearance of the first works in the early 1990s <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b3">2]</ref>. A variety of ALPR systems and commercial products have been produced over the years due to many practical applications such as automatic toll collection, border control, traffic law enforcement and road traffic monitoring <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b5">4]</ref>.</p><p>ALPR systems typically include three phases, namely: license plate (LP) detection, character segmentation and character recognition, which refer to (i) locating the LP region in the acquired image, (ii) segmenting each character within the detected LP and (iii) classifying each segmented character. The earlier stages require higher accuracy since a failure would probably lead to another failure in the subsequent stages.</p><p>Many authors have proposed approaches with a vehicle detection stage prior to LP detection, aiming to eliminate false positives (FPs) and reduce processing time <ref type="bibr" target="#b6">[5]</ref><ref type="bibr" target="#b7">[6]</ref><ref type="bibr" target="#b8">[7]</ref>. Regarding character This paper is a postprint of a paper accepted by IET Intelligent Transport Systems and is subject to Institution of Engineering and Technology Copyright. The copy of record is available at the Wiley Online Library (DOI: 10.1049/itr2.12030). segmentation, it has become common the use of segmentation-free approaches for LP recognition <ref type="bibr" target="#b9">[8]</ref><ref type="bibr" target="#b10">[9]</ref><ref type="bibr" target="#b11">[10]</ref><ref type="bibr" target="#b12">[11]</ref>, as the character segmentation by itself is a challenging task that is prone to be influenced by uneven lighting conditions, shadows, and noise <ref type="bibr" target="#b13">[12]</ref>.</p><p>Despite the major advances (in terms of both accuracy and efficiency) that have been achieved in computer vision using deep learning <ref type="bibr" target="#b14">[13]</ref>, several solutions are still not robust enough to be executed in real-world scenarios. Such solutions commonly depend on certain constraints such as specific cameras or viewing angles, simple backgrounds, good lighting conditions, search in a fixed region, and certain types of vehicles. Additionally, many authors still propose computationally expensive approaches that are not able to process frames in real time, even when the experiments are performed on a high-end GPU <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b16">15</ref>] (if a system does not perform in real time using a high-end GPU, it is very unlikely to run fast enough on the mid-end GPUs that are often employed in real-world applications). In the literature, generally a system is considered "real-time" if it can process at least 30 frames per second (FPS) since commercial cameras usually record videos at that frame rate <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b18">17]</ref>.</p><p>ALPR systems must also be capable of recognizing multiple LP layouts since there might be various LP layouts in the same country or region. However, as stated in <ref type="bibr" target="#b19">[18]</ref>, most of the existing solutions work only for a specific LP layout. Even though most authors claim that their approaches could be extended with small modifications to detect/segment/recognize LPs of different layouts <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b20">[19]</ref><ref type="bibr" target="#b21">[20]</ref><ref type="bibr" target="#b22">[21]</ref>, this may not be an easy task. For instance, a character segmentation approach designed for LPs with simple backgrounds is likely to fail on LPs with complex backgrounds and logos that touch and overlap some characters (e.g., Florida LPs) <ref type="bibr" target="#b10">[9]</ref>.</p><p>A robust and efficient ALPR system can play a key role in various applications in the context of intelligent transportation systems. For example, vehicle re-identification, which refers to identifying a target vehicle in different cameras with non-overlapping views <ref type="bibr" target="#b23">[22]</ref>, is known to be a very challenging problem since different vehicles with the same model and color are highly similar to each other. Although in these situations the LP information must be considered for precise vehicle search <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b25">24]</ref>, it is generally not explored due to the limitations of existing ALPR systems in unconstrained scenarios <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b27">26]</ref>.</p><p>Considering the above discussion, we propose an end-to-end, efficient and layout-independent ALPR system exploring YOLObased models at all stages. YOLO <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b29">28]</ref> is a real-time object detector that achieved impressive results in terms of speed/accuracy trade-off in the Pascal VOC <ref type="bibr" target="#b30">[29]</ref> and Microsoft COCO <ref type="bibr" target="#b31">[30]</ref> detection tasks. We locate the vehicles in the input image and then their LPs within the vehicle bounding box. Considering that the bottleneck of ALPR systems is the LP recognition stage (see Section 2.3), in this paper we propose a unified approach for LP detection and layout classification to improve the recognition results using postprocessing rules (this is the first time a layout classification stage is proposed to improve LP recognition, to the best of our knowledge). Afterward, all LP characters are recognized simultaneously, i.e., the entire LP patch is fed into the network, avoiding the challenging character segmentation task.</p><p>We eliminate various constraints commonly found in ALPR systems by training a single network for each task using images from several datasets, which were collected under different conditions and reproduce distinct real-world applications. Moreover, we perform several data augmentation tricks and modified the chosen networks (e.g., we explored various models with changes in the input size, as well as in the number of layers, filters, output classes, and anchors) aiming to achieve the best speed/accuracy trade-off at each stage.</p><p>Our experimental evaluation demonstrates the effectiveness of the proposed approach, which outperforms previous works and two commercial systems in the ChineseLP <ref type="bibr" target="#b32">[31]</ref>, OpenALPR-EU <ref type="bibr" target="#b33">[32]</ref>, SSIG-SegPlate <ref type="bibr" target="#b34">[33]</ref> and UFPR-ALPR <ref type="bibr" target="#b18">[17]</ref> datasets, and achieves competitive results to those attained by the baselines in other four public datasets. Our system also achieved an impressive trade-off between accuracy and speed. Specifically, on a high-end GPU (i.e., an NVIDIA Titan XP), the proposed system is able to process images in real time even when there are 4 vehicles in the scene.</p><p>In summary, the main contributions of this work are:</p><p>? A new end-to-end, efficient and layout-independent ALPR system that explores YOLO-based Convolutional Neural Networks (CNNs) at all stages 1 .</p><p>-LP layout classification (along with heuristic rules) greatly improves the recognition results and also enables our system to be easily adjusted for additional/different LP layouts.</p><p>-As the proposed ALPR system processes more than 70 FPS on a high-end GPU, we believe it can be deployed even in mid-end setups/GPUs for several real-world applications.</p><p>? A comparative and detailed evaluation of our approach, previous works in the literature, and two commercial systems in eight publicly available datasets that have been frequently used to train and/or evaluate algorithms in the ALPR context.</p><p>-We are not aware of any work in the literature where so many public datasets were used in the experiments.</p><p>? Annotations regarding the position of the vehicles, LPs and characters, as well as their classes, in each image of the public datasets used in this work that have no annotations or contain labels only for part of the ALPR pipeline. Precisely, we manually labeled 38,351 bounding boxes on 6,239 images.</p><p>-These annotations will considerably assist the development and evaluation of new ALPR approaches, as well as the fair comparison among published works.</p><p>A preliminary version of the system described in this paper was published at the 2018 International Joint Conference on Neural Networks (IJCNN) <ref type="bibr" target="#b18">[17]</ref>. The approach described here differs from that version in several aspects. For instance, in the current version, the LP layout is classified prior to LP recognition (together with LP detection), the recognition of all characters is performed simultaneously (instead of first segmenting and then recognizing each of them) and modifications were made to all networks (e.g., in the input size, number of layers, filters, and anchors, among others) to make them faster and more robust. In this way, we overcome the limitations of the system presented in <ref type="bibr" target="#b18">[17]</ref> and were able to considerably improve both the execution time (from 28ms to 14ms) and the recognition results (e.g., from 64.89% to 90% in the UFPR-ALPR dataset). This version was also evaluated on a broader and deeper manner.</p><p>The remainder of this paper is organized as follows. We review related works in Section 2. The proposed system is presented in Section 3. In Section 4, the experimental setup is thoroughly described. We report and discuss the results in Section 5. Finally, conclusions and future works are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section, we review recent works that use deep learning approaches in the context of ALPR. For relevant studies using conventional image processing techniques, please refer to <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b5">4]</ref>. We first discuss works related to the LP detection and recognition stages, and then conclude with final remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">License Plate Detection</head><p>Many authors have addressed the LP detection stage using object detection CNNs. Silva &amp; Jung <ref type="bibr" target="#b7">[6]</ref> noticed that the Fast-YOLO model <ref type="bibr" target="#b17">[16]</ref> achieved a low recall rate when detecting LPs without prior vehicle detection. Therefore, they used the Fast-YOLO model arranged in a cascaded manner to first detect the frontal view of the cars and then their LPs in the detected patches, attaining high precision and recall rates on a dataset with Brazilian LPs.</p><p>Hsu et al. <ref type="bibr" target="#b35">[34]</ref> customized the YOLO and YOLOv2 models exclusively for LP detection. Despite the fact that the modified versions of YOLO performed better and were able to process 54 FPS on a high-end GPU, we believe that LP detection approaches should be even faster (i.e., 150+ FPS) since the LP characters still need to be recognized. Kurpiel et al. <ref type="bibr" target="#b36">[35]</ref> partitioned the input image in sub-regions, forming an overlapping grid. A score for each region was produced using a CNN and the LPs were detected by analyzing the outputs of neighboring sub-regions. On a GT-740M GPU, it took 230 ms to detect Brazilian LPs in images with multiple vehicles, achieving a recall rate of 83% on a public dataset introduced by them.</p><p>Li et al. <ref type="bibr" target="#b13">[12]</ref> trained a CNN based on characters cropped from general text to perform a character-based LP detection. The network was employed in a sliding-window fashion across the entire image to generate a text salience map. Text-like regions were extracted based on the clustering nature of the characters. Connected Component Analysis (CCA) is subsequently applied to produce the initial candidate boxes. Then, another LP/non-LP CNN was trained to remove FPs. Although the precision and recall rates obtained were higher than those achieved in previous works, such a sequence of methods is too expensive for real-time applications, taking more than 2 seconds to process a single image when running on a Tesla K40c GPU.</p><p>Xie et al. <ref type="bibr" target="#b37">[36]</ref> proposed a YOLO-based model to predict the LP rotation angle in addition to its coordinates and confidence value. Prior to that, another CNN was applied to determine the attention region in the input image, assuming that some distance will inevitably exist between any two LPs. By cascading both models, their approach outperformed all baselines in three public datasets, while still running in real time. Despite the impressive results, it is important to highlight two limitations in their work: (i) the authors simplified the problem by forcing their ALPR system to output only one bounding box per image; (ii) motorcycle LPs might be lost when determining the attention region since, in some scenarios (e.g., traffic lights), they might be very close. Kessentini et al. <ref type="bibr" target="#b19">[18]</ref> detected the LP directly in the input image using YOLOv2 without any change or refinement. However, they also considered only one LP per image (mainly to eliminate false positives in the background), which makes their approach unsuitable for many real-world applications that contain multiple vehicles in the scene <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b36">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">License Plate Recognition</head><p>In <ref type="bibr" target="#b7">[6]</ref>, a YOLO-based model was proposed to simultaneously detect and recognize all characters within a cropped LP. While impressive FPS rates (i.e., 448 FPS on a high-end GPU) were attained in experiments carried out in the SSIG-SegPlate dataset <ref type="bibr" target="#b34">[33]</ref>, less than 65% of the LPs were correctly recognized. According to the authors, the accuracy bottleneck of their approach was letter recognition since the training set of characters was highly unbalanced (in particular, letters). Silva &amp; Jung <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b38">37]</ref> retrained that model with an enlarged training set composed of real and artificially generated images using font-types similar to the LPs of certain regions. In this way, the retrained network became much more robust for the detection and classification of real characters, outperforming previous works and commercial systems in three public datasets.</p><p>Li et al. <ref type="bibr" target="#b13">[12]</ref> proposed to perform character recognition as a sequence labeling problem, also without the character-level segmentation. Sequential features were first extracted from the entire LP patch using a CNN in a sliding window manner. Then, Bidirectional Recurrent Neural Networks (BRNNs) with Long Short-Term Memory (LSTM) were applied to label the sequential features. Lastly, Connectionist Temporal Classification (CTC) was employed for sequence decoding. The results showed that this method attained better recognition rates than the baselines. Nonetheless, only LPs from the Taiwan region were used in their experiments and the execution time was not reported.</p><p>Dong et al. <ref type="bibr" target="#b15">[14]</ref> claimed that the method proposed in <ref type="bibr" target="#b13">[12]</ref> is very fragile to distortions caused by viewpoint change and therefore is not suitable for LP recognition in the wild. Thus, an LP rectification step is employed first in their approach. Afterward, a CNN was trained to recognize Chinese characters, while a shared-weight CNN recognizer was used for digits and English letters, making full use of the limited training data. The recognition rate attained on a private dataset with LPs from mainland China was 89.05%. The authors did not report the execution time of this particular stage.</p><p>Zhuang et al. <ref type="bibr" target="#b39">[38]</ref> proposed a semantic segmentation technique followed by a character count refinement module to recognize the characters of an LP. For semantic segmentation, they simplified the DeepLabV2 (ResNet-101) model by removing the multi-scaling process, increasing computational efficiency. Then, the character areas were generated through CCA. Finally, Inception-v3 and AlexNet were adopted as the character classification and character counting models, respectively. The authors claimed that both an outstanding recognition performance and a high computational efficiency were attained. Nevertheless, they assumed that LP detection is easily accomplished and used cropped patches containing only the LP with almost no background (i.e., the ground truth) as input. Furthermore, their system is not able to process images in real time, especially when considering the time required for the LP detection stage, which is often more time-consuming than the recognition one.</p><p>Some papers focus on deblurring the LPs, which is very useful for LP recognition. Lu et al. <ref type="bibr" target="#b40">[39]</ref> proposed a scheme based on sparse representation to identify the blur kernel, while Svoboda et al. <ref type="bibr" target="#b41">[40]</ref> employed a text deblurring CNN for reconstruction of blurred LPs. Despite achieving exceptional qualitative results, the additional computational cost of a deblurring stage usually is prohibitive for realtime ALPR applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Final Remarks</head><p>The approaches developed for ALPR are still limited in various ways. Many authors only addressed part of the ALPR pipeline, e.g., LP detection <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b42">41]</ref> or character/LP recognition <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b44">43]</ref>, or performed their experiments on private datasets <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b44">43]</ref>, making it difficult to accurately evaluate the presented methods. Note that works focused on a single stage do not consider localization errors (i.e., correct but not so accurate detections) in earlier stages <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b39">38]</ref>. Such errors directly affect the recognition results. As an example, Gon?alves et al. <ref type="bibr" target="#b9">[8]</ref> improved their results by 20% by skipping the LP detection stage, that is, by feeding the LPs manually cropped into their recognition network. In this work, the proposed end-to-end system is evaluated in eight public datasets that present a great variety in the way they were collected, with images of various types of vehicles (including motorcycles) and numerous LP layouts. It should be noted that, in most of the works in the literature, no more than three datasets were used in the experiments (e.g., <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b39">38]</ref>). In addition, despite the fact that motorcycles are one of the most popular transportation means in metropolitan areas <ref type="bibr" target="#b45">[44]</ref>, motorcycle images have not been used in the assessment of most ALPR systems in the literature <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b38">37]</ref>.</p><p>Most of the approaches are not capable of recognizing LPs in real time (i.e., 30 FPS) <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b39">38]</ref>, even running on high-end GPUs, making it impossible for them to be applied in some real-world applications (especially considering that the purchase of high-end setups is not practicable for various commercial applications <ref type="bibr" target="#b46">[45]</ref>). Furthermore, several authors do not report the execution time of the proposed methods or report the time required only for a specific stage <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b44">43]</ref>, making it difficult an accurate analysis of their speed/accuracy trade-off, as well as their applicability. In this sense, we explore different YOLO models at each stage, carefully optimizing and combining them to achieve the best speed/accuracy trade-off. In our experiments, both the accuracy and execution time are reported to enable fair comparisons in future works.</p><p>It is important to highlight that although outstanding results in terms of mean Average Precision (mAP) have been achieved with other object detectors such as Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b47">[46]</ref> and RetinaNet <ref type="bibr" target="#b48">[47]</ref>, in this work we adapt YOLO since it focuses on an extreme speed/accuracy trade-off <ref type="bibr" target="#b48">[47]</ref>, which is essential in intelligent transportation systems <ref type="bibr" target="#b49">[48]</ref>, being able to process more than twice as many FPS as other detectors while still achieving competitive results <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref>.</p><p>Although YOLO has already been employed in the ALPR context, previous works present several limitations (as detailed in sections 2.1 and 2.2), with the authors commonly overlooking many factors that may limit the accuracy or speed (or even both) achieved by YOLO-based models, such as the dimensions of the input images, number of network layers, filters and anchors, and/or using data augmentation strategies that can actually impair the network learning. These factors have not been discussed sufficiently in the literature.</p><p>We consider LP recognition as the current bottleneck of ALPR systems since (i) impressive LP detection results have been reported in recent works <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b50">49]</ref>, both in terms of recall rate and execution time; (ii) Optical Character Recognition (OCR) approaches  <ref type="figure">Fig. 1</ref>. The pipeline of the proposed ALPR system. First, all vehicles are detected in the input image. Then, in a single stage, the LP of each vehicle is detected and its layout is classified (in the example above, the vehicles/LPs are from the Taiwan region). Finally, all characters of each LP are recognized simultaneously, with heuristic rules being applied to adapt the results according to the predicted layout class. must work as close as possible to the optimality (i.e., 100% of character recognition rate) in the ALPR context, as a single mistake may imply in incorrect identification of the vehicle <ref type="bibr" target="#b6">[5]</ref>. Thus, in this work, we propose a unified approach for LP detection and layout classification in order to improve the recognition results using heuristic rules. Additionally, we design and apply data augmentation techniques to simulate LPs of other layouts and also to generate LP images with characters that have few instances in the training set. Hence, unlike <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b44">43]</ref>, we avoid errors in the recognition stage due to highly unbalanced training sets of LP characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED ALPR SYSTEM</head><p>The nature of traffic images might be very problematic to LP detection approaches that work directly on the frames (i.e., without vehicle detection) since (i) there are many textual blocks that can be confused with LPs such as traffic signs and phone numbers on storefronts, and (ii) LPs might occupy very small portions of the original image and object detectors commonly struggle to detect small objects <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b51">50]</ref>. Therefore, we propose to first locate the vehicles in the input image and then detect their respective LPs in the vehicle patches. Afterward, we detect and recognize all characters simultaneously by feeding the entire LP patch into the network. In this way, we do not need to deal with the character segmentation task. Although some approaches with such characteristics (i.e., containing a vehicle detection stage prior to LP detection and/or avoiding character segmentation) have already been proposed in the literature, none of them presented robustness for different LP layouts in both accuracy and processing time. In <ref type="bibr" target="#b7">[6]</ref> and <ref type="bibr" target="#b9">[8]</ref>, for instance, the authors designed real-time ALPR systems able to process more than 50 FPS on high-end GPUs, however, both systems were evaluated only on LPs from a single country and presented poor recognition rates in at least one dataset in which they were evaluated. On the other hand, outstanding results were achieved on different scenarios in some recent works <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b16">15]</ref>, however, the methods presented in these works are computationally expensive and cannot be applied in real time. This makes them unsuitable for use in many real-world applications, especially those where multiple vehicles can appear on the scene at the same time <ref type="bibr" target="#b9">[8]</ref>.</p><p>In order to develop an ALPR system that is robust for different LP layouts, we propose a layout classification stage after LP detection. However, instead of performing both stages separately, we merge the LP detection and layout classification tasks by training an object detection network that outputs a distinct class for each LP layout. In this way, with almost no additional cost, we employ layoutspecific approaches for LP recognition in cases where the LP and its layout are predicted with a confidence value above a predefined threshold. For example, all Brazilian LPs have seven characters: three letters and four digits (in that order), and thus a post-processing method is applied to avoid errors in characters that are often misclassified, such as 'B' and '8', 'G' and '6', 'I' and '1', among others. In cases where the LP and its layout are detected with confidence below the predefined threshold, a generic approach is applied. To the best of our knowledge, this is the first time a layout classification stage is proposed to improve the recognition results.</p><p>It is worth noting that although the LP layout is known a priori in some real-world applications, there are various regions/countries around the world where multiple LP layouts coexist. As an example, Mercosur countries (Argentina, Brazil, Paraguay and Uruguay) are adopting a new standard of LPs for newly purchased vehicles, inspired by the integrated system adopted several years ago by European Union countries <ref type="bibr" target="#b52">[51]</ref>. As changing to the new LP layout is not free of charge <ref type="bibr" target="#b53">[52]</ref> and is not mandatory for used vehicles <ref type="bibr" target="#b54">[53]</ref>, the old and new layouts will coexist for many years in these countries.</p><p>In fact, such a situation will occur in any country/region that adopts a new LP layout without drivers being required to update their current LPs, as occurred in most European Union countries in the past. Hence, in such cases, an ALPR system capable of classifying the LP layout is essential to avoid errors in the number of predicted characters to be considered and also in similar letters and digits, since the number of characters and/or the positions for letters and digits often differ in the old and new LP layouts (e.g., Argentine 'old' LPs consist of exactly 3 letters and 3 digits, whereas the initial pattern adopted in Argentina for Mercosur LPs consists of 2 letters, 3 digits and 2 letters, in that order).</p><p>In this context, although layout-dependent factors can be addressed by developing a tailored ALPR system for the specific subset of LP layouts that coexist in a given region, such systems must be verified/modified if a new LP layout is adopted in that region (or if authorities want to start recognizing LPs from neighboring countries) since some previously used strategies may no longer be applicable. On the other hand, for the proposed approach to work for additional LP layouts, we only need to retrain our network for LP detection and layout classification with images of the new LP layout (in addition to images of the known layouts) and adjust the expected pattern (i.e., the number of characters and fixed positions of digits and letters) in a configuration file. In other words, layout classification (along with heuristic rules) enables the proposed ALPR system to be easily adjusted to work for additional/different LP layouts.</p><p>As great advances in object detection have been achieved using YOLO-inspired models <ref type="bibr" target="#b55">[54]</ref><ref type="bibr" target="#b56">[55]</ref><ref type="bibr" target="#b57">[56]</ref><ref type="bibr" target="#b58">[57]</ref>, we decided to specialize it for ALPR. We use specific models for each stage. Thus, we can tune the parameters separately in order to improve the performance of each task. The models adapted are YOLOv2 <ref type="bibr" target="#b28">[27]</ref>, Fast-YOLOv2 and CR-NET <ref type="bibr" target="#b7">[6]</ref>, which is an architecture inspired by YOLO for character detection and recognition. We explored several data augmentation techniques and performed modifications to each network (e.g., changes in the input size, number of filters, layers and anchors) to achieve the best speed/accuracy trade-off at each stage.</p><p>In this work, unlike <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b59">58]</ref>, for each stage, we train a single network on images from several datasets (described in Section 4.1) to make our networks robust for distinct ALPR applications or scenarios with considerably less manual effort since their parameters are adjusted only once for all datasets.</p><p>This remainder of this section describes the proposed approach and it is divided into three subsections, one for each stage of our endto-end ALPR system: (i) vehicle detection, (ii) LP detection and layout classification and (iii) LP recognition. <ref type="figure">Fig. 1</ref> illustrates the system pipeline, explained throughout this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vehicle Detection</head><p>In this stage, we explored the following models: Fast-YOLOv2, YOLOv2 <ref type="bibr" target="#b28">[27]</ref>, Fast-YOLOv3 and YOLOv3 <ref type="bibr" target="#b29">[28]</ref>. Although the Fast-YOLO variants correctly located the vehicles in most cases, they failed in challenging scenarios such as images in which one or more vehicles are partially occluded or appear in the background. On the other hand, impressive results (i.e., F-measure rates above 98% in the validation set 2 ) were obtained with both YOLOv2 and YOLOv3, which successfully detected vehicles even in those cases where the smaller models failed. As the computational cost is one of our main concerns and YOLOv3 is much more complex than its predecessor, we further improve the YOLOv2 model for vehicle detection.</p><p>First, we changed the network input size from 416 ? 416 to 448 ? 288 pixels since the images used as input to ALPR systems generally have a width greater than height. Hence, our network processes less distorted images and performs faster, as the new input size is 25% smaller than the original. The new dimensions were chosen based on speed/accuracy assessments with different input sizes (from 448 ? 288 to 832 ? 576 pixels). Then, we recalculate the anchor boxes for the new input size as well as for the datasets employed in our experiments using the k-means clustering algorithm. Finally, we reduced the number of filters in the last convolutional layer to match the number of classes. YOLOv2 uses A anchor boxes to predict bounding boxes (we use A = 5), each with four coordinates (x, y, w, h), confidence and C class probabilities <ref type="bibr" target="#b28">[27]</ref>, so the number of filters is given by</p><formula xml:id="formula_0">filters = (C + 5) ? A .<label>(1)</label></formula><p>As we intend to detect cars and motorcycles (two classes), the number of filters in the last convolutional layer must be 35 ((2 + 5) ? 5). According to preliminary experiments, the results were better when using two classes instead of just one regarding both types of vehicles. The modified YOLOv2 architecture for vehicle detection is shown in <ref type="table" target="#tab_0">Table 1</ref>. We exploit various data augmentation strategies, such as flipping, rescaling and shearing, to train our network. Thus, we prevent overfitting by creating many other images with different characteristics from a single labeled one.</p><p>Silva &amp; Jung <ref type="bibr" target="#b8">[7]</ref> slightly modified their pipeline by directly applying their LP detector (i.e., skipping the vehicle detection stage) when dealing with images in which the vehicles are very close to the camera, as their detector failed in several of those cases. We believe this is not the best way to handle the problem. Instead, we do not skip the vehicle detection stage even when only a small part of the vehicle is visible. The entire image is labeled as ground truth in cases where the vehicles are very close to the camera. Therefore, our network also learns to select the Region of Interest (ROI) in such cases.</p><p>In the validation set, we evaluate several confidence thresholds to detect as many vehicles as possible while maintaining a low FP rate. Furthermore, we apply a Non-Maximum Suppression (NMS) algorithm to eliminate redundant detections (those with Intersection over Union (IoU) ? 0.25) since the same vehicle might be detected more than once by the network. A negative recognition result is given in cases where no vehicle is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">License Plate Detection and Layout Classification</head><p>In this work, we detect the LP and simultaneously classify its layout into one of the following classes: American, Brazilian, Chinese (LPs of vehicles registered in mainland China), European or Taiwanese (LPs of vehicles registered in the Taiwan region). These classes were defined based on the public datasets found in the literature <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b32">[31]</ref><ref type="bibr" target="#b33">[32]</ref><ref type="bibr" target="#b34">[33]</ref><ref type="bibr" target="#b60">[59]</ref><ref type="bibr" target="#b61">[60]</ref><ref type="bibr" target="#b62">[61]</ref><ref type="bibr" target="#b63">[62]</ref> and also because there are many ALPR systems designed primarily for LPs of one of those regions <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b63">62]</ref>. It is worth noting that (i) among LPs with different layouts (which may belong to the same class/region) there is a wide variety in many factors, for example, in the aspect ratio, colors, symbols, position of the characters, number of characters, among others; (ii) we consider LPs from different jurisdictions in the United States as a single class; the same is done for LPs from European countries. LPs from the same country or region may look quite different, but still share many characteristics in common. Such common features can be exploited to improve LP recognition. In <ref type="figure" target="#fig_2">Fig. 2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, we show examples of LPs of different layouts and classes.</head><p>Looking for an efficient ALPR system, in this stage we per-  formed experiments with the Fast-YOLOv2 and Fast-YOLOv3 models. In the validation set, Fast-YOLOv2 obtained slightly better results than its successor. This is due to the fact that YOLOv3 and Fast-YOLOv3 have relatively high performance on small objects (which is not the case since we first detect the vehicles), but comparatively worse performance on medium and larger size objects <ref type="bibr" target="#b29">[28]</ref>. Accordingly, here we modified the Fast-YOLOv2 model to adapt it to our application and to achieve even better results. First, we changed the kernel size of the next-to-last convolutional layer from 3 ? 3 to 1 ? 1. Then, we added a 3 ? 3 convolutional layer with twice the filters of that layer. In this way, the network reached better results (F-measure ? 1% higher, from 97.97% to 99.00%) almost without increasing the number of floating-point operations (FLOP) required, i.e., from 5.35 to 5.53 billion floatingpoint operations (BFLOP), as alternating 1 ? 1 convolutional layers between 3 ? 3 convolutions reduce the feature space from preceding layers <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b28">27]</ref>. Finally, we recalculate the anchors for our data and make adjustments to the number of filters in the last layer. The modified architecture is shown in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref>. Fast-YOLOv2 modified for LP detection and layout classification. First, we reduced the kernel size of layer #13 from 3 ? 3 to 1 ? 1, and added layer #14. Then, we reduced the number of filters in layer #15 from 425 to 50, as we use 5 anchor boxes to detect 5 classes (see <ref type="bibr">Equation 1</ref>). In <ref type="table">Table 2</ref>, we also list the number of FLOP required in each layer to highlight how small the modified network is compared to others, e.g., YOLOv2 and YOLOv3. For this task, our network requires 5.53 BFLOP while YOLOv2 and YOLOv3 require 29.35 and 66.32 BFLOP, respectively. It is noteworthy that we only need to increase the number of filters (following Equation 1) in the last convolutional layer so that the network can detect/classify additional LP layouts.</p><p>For LP detection and layout classification, we also use data augmentation strategies to generate many other images from a single labeled one. However, horizontal flipping is not performed at this stage, as the network leverages information such as the position of the characters and symbols on the LP to predict its layout (besides the aspect ratio, colors, and other characteristics).</p><p>Only the detection with the highest confidence value is considered in cases where more than one LP is predicted, as each vehicle has only one LP. Then, we classify as 'undefined layout' every LP that has its position and class predicted with a confidence value below 0.75, regardless of which class the network predicted (note that such LPs are not rejected, instead, a generic approach is used in the recognition stage). This threshold was chosen based on experiments performed in the validation set, in which approximately 92% of the LPs were predicted with a confidence value above 0.75. In each of these cases, the LP layout was correctly classified. A negative result is given in cases where no LP is predicted by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">License Plate Recognition</head><p>Once the LP has been detected and its layout classified, we employ CR-NET <ref type="bibr" target="#b7">[6]</ref> for LP recognition (i.e., all characters are recognized simultaneously by feeding the entire LP patch into the network). CR-NET is a model that consists of the first eleven layers of YOLO and four other convolutional layers added to improve nonlinearity. This model was chosen for two main reasons. First, it was capable of detecting and recognizing LP characters at 448 FPS in <ref type="bibr" target="#b7">[6]</ref> Second, very recently, it yielded the best recognition results in the context of image-based automatic meter reading <ref type="bibr" target="#b64">[63]</ref>, outperforming two segmentation-free approaches based on deep learning.</p><p>The CR-NET architecture is shown in <ref type="table" target="#tab_2">Table 3</ref>. We changed its input size, which was originally defined based on Brazilian LPs, from 240 ? 80 to 352 ? 128 pixels taking into account the average aspect ratio of the LPs in the datasets used in our experiments, in addition to results obtained in the validation set, where several input sizes were evaluated (e.g., 256 ? 96 and 384 ? 128 pixels). As the same model is employed to recognize LPs of various layouts, we enlarge all LP patches (in both the training and testing phases) so that they have aspect ratios (w/h) between 2.5 and 3.0, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, considering that the input image has an aspect ratio of 2.75. The network is trained to predict 35 classes (0-9, A-Z, where the letter 'O' is detected/recognized jointly with the digit '0') using the LP patch as well as the class and coordinates of each character as inputs.   It is worth to mention that the first character in Chinese LPs (see <ref type="figure" target="#fig_2">Fig. 2</ref>) is a Chinese character that represents the province in which the vehicle is affiliated <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b65">64]</ref>. Following <ref type="bibr" target="#b16">[15]</ref>, our network was not trained/designed to recognize Chinese characters, even though Chinese LPs are used in the experiments. In other words, only digits and English letters are considered. The reason is threefold: (i) there are less than 400 images in the ChineseLP dataset <ref type="bibr" target="#b32">[31]</ref> (only some of them are used for training), which is employed in the experiments, and some provinces are not represented; (ii) labeling the class of Chinese characters is not a trivial task for non-Chinese people (we manually labeled the position and class of the LP characters in the ChineseLP dataset); and (iii) to fairly compare our system with others trained only on digits and English letters. We remark that in the literature the approaches capable of recognizing Chinese characters, digits and English letters were evaluated, for the most part, on datasets containing only LPs from mainland China <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b65">64]</ref>.</p><p>As the LP layout is classified in the previous stage, we design heuristic rules to adapt the results produced by CR-NET according to the predicted class. Based on the datasets employed in this work, we defined the minimum and the maximum number of characters to be considered in LPs of each layout. Brazilian and Chinese LPs have a fixed number of characters, while American, European and Taiwanese LPs do not (see <ref type="table" target="#tab_3">Table 4</ref>). Initially, we consider all characters predicted with a confidence value above a predefined threshold. Afterward, as in the vehicle detection stage, an NMS algorithm is applied to remove redundant detections. Finally, if necessary, we discard the characters predicted with lower confidence values or consider others previously discarded (i.e., ignoring the confidence threshold) so that the number of characters considered is within the range defined for the predicted class. We consider that the LP has between 4 and 8 characters in cases where its layout was classified with a low confidence value (i.e., undefined layout). Additionally, inspired by Silva &amp; Jung <ref type="bibr" target="#b7">[6]</ref>, we swap digits and letters on Brazilian and Chinese LPs, as there are fixed positions for digits or letters in those layouts. In Brazilian LPs, the first three characters correspond to letters and the last four to digits; while in Chinese LPs the second character is a letter that represents a city in the province in which the vehicle is affiliated. This swap approach is not employed for LPs of other layouts since each character position can be occupied by either a letter or a digit in American, European and Taiwanese LPs. The specific swaps are given by <ref type="bibr">[</ref></p><formula xml:id="formula_1">1 ? I; 2 ? Z; 4 ? A; 5 ? S; 6 ? G; 7 ? Z; 8 ? B] and [A ? 4; B ? 8; D ? 0; G ? 6; I ? 1; J ? 1; Q ? 0; S ? 5; Z ? 7].</formula><p>In this way, we avoid errors in characters that are often misclassified.</p><p>The LP characters might also be arranged in two rows instead of one. We distinguish such cases based on the predictions of the vehicle type, LP layout, and character coordinates. In our experiments, only two datasets have LPs with the characters arranged in two rows. These datasets were captured in Brazil and Croatia. In Brazil, car and motorcycle LPs have the characters arranged in one and two rows, respectively. Thus, we look at the predicted class in the vehicle detection stage in those cases. In Croatia, on the other hand, cars might also have LPs with two rows of characters. Therefore, for European LPs, we consider that the characters are arranged in two rows in cases where the bounding boxes of half or more of the predicted characters are located entirely below another character. In our tests, this simple rule was sufficient to distinguish LPs with one and two rows of characters even in cases where the LP is considerably inclined. We emphasize that segmentation-free approaches (e.g., <ref type="bibr" target="#b9">[8]</ref><ref type="bibr" target="#b10">[9]</ref><ref type="bibr" target="#b11">[10]</ref><ref type="bibr" target="#b12">[11]</ref>) cannot recognize LPs with two rows of characters, contrarily to YOLO-based approaches, which are better suited to recognize them thanks to YOLO's versatility and ability to learn general component features, regardless of their positions <ref type="bibr" target="#b19">[18]</ref>.</p><p>In addition to using the original LP images, we design and apply data augmentation techniques to train the CR-NET model and improve its robustness. First, we double the number of training samples by creating a negative image of each LP, as we noticed that in some cases negative LPs are very similar to LPs of other layouts. This is illustrated with Brazilian and American LPs in <ref type="figure" target="#fig_4">Fig. 4</ref>. We also generate many other images by randomly rescaling the LP patch and adding a margin to it, simulating more or less accurate detections of the LP in the previous stage.  The datasets for ALPR are generally very unbalanced in terms of character classes due to LP allocation policies. It is well-known that unbalanced data is undesirable for neural network classifiers since the learning of some patterns might be biased. To address this issue, we permute on the LPs the characters overrepresented in the training set by those underrepresented. In this way, as in <ref type="bibr" target="#b9">[8]</ref>, we are able to create a balanced set of images in which the order and frequency of the characters on the LPs are chosen to uniformly distribute them across the positions. We maintain the initial arrangement of letters and digits of each LP so that the network might also learn the positions of letters and digits in certain LP layouts. <ref type="figure">Fig. 5</ref> shows some artificially generated images by permuting the characters on LPs of different layouts. We also perform random variations of brightness, rotation and cropping to increase even more the diversity of the generated images. The parameters were empir-ically adjusted through visual inspection, i.e., brightness variation of the pixels [0.85; 1.15], rotation angles between ?5?and 5?and cropping from ?2% to 8% of the LP size. Once these ranges were established, new images were generated using random values within those ranges for each parameter. <ref type="figure" target="#fig_9">Figure 6</ref>: Examples of LP images generated using the data augmentation technique proposed in <ref type="bibr" target="#b11">[10]</ref>. The images in the first row are the originals and the others were generated automatically. 9 <ref type="figure">Fig. 5</ref>. Examples of LP images generated by permuting the characters on the LPs. The images in the first row are the originals and the others were generated automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head><p>All experiments were performed on a computer with an AMD Ryzen Threadripper 1920X 3.5GHz CPU, 32 GB of RAM (2400 MHz), HDD 7200 RPM, and an NVIDIA Titan Xp GPU. The Darknet framework <ref type="bibr" target="#b66">[65]</ref> was employed to train and test our networks. However, we used Alexey Bochkovskiy's version of Darknet <ref type="bibr" target="#b67">[66]</ref>, which has several improvements over the original, including improved neural network performance by merging two layers into one (convolutional and batch normalization), optimized memory allocation during network resizing, and many other code fixes. For more details on this repository, refer to <ref type="bibr" target="#b67">[66]</ref>.</p><p>We also made use of the Darknet's built-in data augmentation, which creates a number of randomly cropped and resized images with changed colors (hue, saturation, and exposure). We manually implemented the flip operation only for the vehicle detection stage, as this operation would probably impair the layout classification and the LP recognition tasks. Similarly, we disabled the colorrelated data augmentation for the LP detection and layout classification stage (further explained in Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The experiments were carried out in eight publicly available datasets: Caltech Cars <ref type="bibr" target="#b60">[59]</ref>, EnglishLP <ref type="bibr" target="#b61">[60]</ref>, UCSD-Stills <ref type="bibr" target="#b62">[61]</ref>, Chi-neseLP <ref type="bibr" target="#b32">[31]</ref>, AOLP <ref type="bibr" target="#b63">[62]</ref>, OpenALPR-EU <ref type="bibr" target="#b33">[32]</ref>, SSIG-SegPlate <ref type="bibr" target="#b34">[33]</ref> and UFPR-ALPR <ref type="bibr" target="#b18">[17]</ref>. These datasets are often used to evaluate ALPR systems, contain multiple LP layouts and were collected under different conditions/scenarios (e.g., with variations in lighting, camera position and settings, and vehicle types). An overview of the datasets is presented in <ref type="table" target="#tab_4">Table 5</ref>. It is noteworthy that in most of the works in the literature, including some recent ones <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b39">38]</ref>, no more than three datasets were used in the experiments.</p><p>The datasets collected in the United States (i.e., Caltech Cars and UCSD-Stills) and in Europe (i.e., EnglishLP and OpenALPR-EU) are relatively simple and have certain characteristics in common, for example, most images were captured with a hand-held camera and there is only one vehicle (generally well-centered) in each image. There are only a few cases in which the LPs are not well aligned. The ChineseLP and AOLP datasets, on the other hand, also contain images where the LP is inclined/tilted, as well as images with more than one vehicle, which may be occluded by others. Lastly, the SSIG-SegPlate and UFPR-ALPR datasets are composed of highresolution images, enabling LP recognition from distant vehicles. In both datasets, there are several frames of each vehicle and, therefore, redundant information may be used to improve the recognition results. Among the eight datasets used in our experiments, we consider the UFPR-ALPR dataset the most challenging, as three different non-static cameras were used to capture images from different types of vehicles (cars, motorcycles, buses and trucks) with complex backgrounds and under different lighting conditions <ref type="bibr" target="#b18">[17]</ref>. Note that both the vehicles and the camera (inside another vehicle) were moving and most LPs occupy a very small region of the image. Most datasets have no annotations or contain labels for a single stage only (e.g., LP detection), despite the fact that they are often used to train/evaluate algorithms in the ALPR context. Therefore, in all images of these datasets, we manually labeled the position of the vehicles (including those in the background where the LP is also legible), LPs and characters, as well as their classes.</p><p>In addition to using the training images of the datasets, we downloaded and labeled more 772 images from the internet to train all stages of our ALPR system. This procedure was adopted to eliminate biases from the datasets employed in our experiments. For example, the Caltech Cars and UCSD-Stills datasets have similar characteristics (e.g., there is one vehicle per image, the vehicle is centered and occupies a large portion of the image, and the resolutions of the images are not high), which are different from those of the other datasets. Moreover, there are many more examples of Brazilian and Taiwanese LPs in our training data (note that the exact number of images used for training, testing and validation in each dataset is detailed in the next section). Therefore, we downloaded images containing vehicles with American, Chinese and European LPs so that there are at least 500 images of LPs of each class/region to train our networks. Specifically, we downloaded 257, 341, and 174 images containing American, Chinese and European LPs, respectively <ref type="bibr" target="#b4">3</ref> .</p><p>In our experiments, we did not make use of two datasets proposed recently: AOLPE <ref type="bibr" target="#b35">[34]</ref> (an extension of the AOLP dataset) and Chinese City Parking Dataset (CCPD) <ref type="bibr" target="#b68">[67]</ref>. The former has not yet been made available by the authors, who are collecting more data to make it even more challenging. The latter, although already available, does not provide the position of the vehicles and the characters in its 250,000 images and it would be impractical to label them to train/evaluate our networks (Xu et al. <ref type="bibr" target="#b68">[67]</ref> used more than 100,000 images for training in their experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Protocol</head><p>To evaluate the stages of (i) vehicle detection and (ii) LP detection and layout classification, we report the precision and recall rates achieved by our networks. Each metric has its importance since, for system efficiency, all vehicles/LPs must be detected without many false positives. Note that the precision and recall rates are equal in the LP detection and layout classification stage because we consider only one LP per vehicle.</p><p>We consider as correct only the detections with IoU greater than 0.5 with the ground truth. This bounding box evaluation, defined in the PASCAL VOC Challenge <ref type="bibr" target="#b30">[29]</ref> and employed in previous works <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b22">21]</ref>, is interesting since it penalizes both overand under-estimated objects. In the LP detection and layout classification stage, we assess only the predicted bounding box on LPs classified as undefined layout (see Section 3.2). In other words, we consider as correct the predictions when the LP position is correctly predicted but not its layout, as long as the LP (and its layout) has not been predicted with a high confidence value (i.e., below 0.75).</p><p>In the LP recognition stage, we report the number of correctly recognized LPs divided by the total number of LPs in the test set. A correctly recognized LP means that all characters on the LP were correctly recognized, as a single character recognized incorrectly may imply in incorrect identification of the vehicle <ref type="bibr" target="#b6">[5]</ref>.</p><p>According to <ref type="table" target="#tab_4">Table 5</ref>, only three of the eight datasets used in this work contain an evaluation protocol (defined by the respective authors) that can be reproduced perfectly: UCSD-Stills, SSIG-SegPlate and UFPR-ALPR. Thus, we split their images into training, validation, and test sets according to their own protocols. We randomly divided the other five datasets using the protocols employed in previous works, aiming at a fair comparison with them. In the next paragraph, such protocols (which we also provide for reproducibility purposes) are specified.</p><p>We used 80 images of the Caltech Cars dataset for training and 46 for testing, as in <ref type="bibr" target="#b69">[68]</ref><ref type="bibr" target="#b70">[69]</ref><ref type="bibr" target="#b71">[70]</ref>. Then, we employed 16 of the 80 training images for validation (i.e., 20%). The EnglishLP dataset was divided in the same way as in <ref type="bibr" target="#b59">[58]</ref>, with 80% of the images being used for training and the remainder for testing. Also in this dataset, 20% of the training images were employed for validation. Regarding the ChineseLP dataset, we did not find any previous work in which it was split into training/test sets, that is, all its images were used either to train or to test the methods proposed in <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b72">71,</ref><ref type="bibr" target="#b73">72]</ref>, often jointly with other datasets. Thus, we adopted the same protocol of the SSIG-SegPlate and UFPR-ALPR datasets, in which 40% of the images are used for training, 40% for testing and 20% for validation. The AOLP dataset is categorized into three subsets, which represent three major ALPR applications: access control (AC), traffic law enforcement (LE), and road patrol (RP). As this dataset has been divided in several ways in the literature, we divided each subset into training and test sets with a 2:1 ratio, following <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b39">38]</ref>. Then, 20% of the training images were employed for validation. Lastly, all images belonging to the OpenALPR-EU dataset were used for testing in <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b74">73]</ref>, while other public or private datasets were employed for training. Therefore, we also did not use any image of this dataset for training or validation, only for testing. An overview of the number of images used for training, testing and validation in each dataset can be seen in <ref type="table" target="#tab_5">Table 6</ref>.</p><p>We discarded a few images from the Caltech Cars, UCSD-Stills, and ChineseLP datasets <ref type="bibr" target="#b5">4</ref> . Although most images in these datasets are reasonable, there are a few exceptions where (i) it is impossible  <ref type="figure" target="#fig_9">Fig. 6</ref>. Such images were also discarded in <ref type="bibr" target="#b74">[73]</ref>.  It is worth noting that we did not discard any image from the test set of the UCSD-Stills dataset and used the same number of test images in the Caltech Cars dataset. In this way, we can fairly compare our results with those obtained in previous works. In fact, we used fewer images from those datasets to train and validate our networks. In the ChineseLP dataset, on the other hand, we first discard the few images with problems and then split the remaining ones using the same protocol as the SSIG-SegPlate and UFPR-ALPR datasets (i.e., 40/20/40% for training, validation and testing, respectively) since, in the literature, a division protocol has not yet been proposed for the ChineseLP dataset, to the best of our knowledge.</p><p>To avoid an overestimation or bias in the random division of the images into the training, validation and test subsets, we report in each stage the average result of five runs of the proposed approach (note that most works in the literature, including recent ones <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b39">38]</ref>, report the results achieved in a single run only). Thus, at each run, the images of the datasets that do not have an evaluation protocol were randomly redistributed into each subset (training/validation/test). In the UCSD-Stills, SSIG-SegPlate and UFPR-ALPR datasets, we employed the same division (i.e., the one proposed along with the respective dataset) in all runs.</p><p>As pointed out in Section 4.1, we manually labeled the vehicles in the background of the images in cases where their LPs are legible. Nevertheless, in the testing phase, we considered only the vehicles/LPs originally labeled in the datasets that have annotations to perform a fair comparison with previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND DISCUSSION</head><p>In this section, we report the experiments carried out to verify the effectiveness of the proposed ALPR system. We first assess the detection stages separately since the regions used in the LP recognition stage are from the detection results, rather than cropped directly from the ground truth. This is done to provide a realistic evaluation of the entire ALPR system, in which well-performed vehicle and LP detections are essential for achieving outstanding recognition results. Afterward, our system is evaluated in an end-to-end manner and the results achieved are compared with those obtained in previous works and by commercial systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Vehicle Detection</head><p>In this stage, we employed a confidence threshold of 0.25 (defined empirically) to detect as many vehicles as possible, while avoiding high FP rates and, consequently, a higher cost of the proposed ALPR system. The following parameters were used for training the network: 60K iterations (max batches) and learning rate = [10 -3 , 10 -4 , 10 -5 ] with steps at 48K and 54K iterations.</p><p>The vehicle detection results are presented in <ref type="table" target="#tab_6">Table 7</ref>. In the average of five runs, our approach achieved a recall rate of 99.92% and a precision rate of 98.37%. It is remarkable that the network was able to correctly detect all vehicles (i.e., recall = 100%) in 5 of the 8 datasets used in the experiments. Some detection results are shown in <ref type="figure" target="#fig_8">Fig. 7</ref>. As can be seen, well-located predictions were attained on vehicles of different types and under different conditions.   Observe that vehicles of different types were correctly detected regardless of lighting conditions (daytime and nighttime), occlusion, camera distance, and other factors.</p><p>To the best of our knowledge, with the exception of the preliminary version of this work <ref type="bibr" target="#b18">[17]</ref>, there is no other work in the ALPR context where both cars and motorcycles are detected at this stage. This is of paramount importance since motorcycles are one of the most popular transportation means in metropolitan areas, especially in Asia <ref type="bibr" target="#b45">[44]</ref>. Although motorcycle LPs may be correctly located by LP detection approaches that work directly on the frames, they can be detected with fewer false positives if the motorcycles are detected first <ref type="bibr" target="#b75">[74]</ref>.</p><p>The precision rates obtained by the network were only not higher due to unlabeled vehicles present in the background of the images, especially in the AOLP and SSIG-SegPlate datasets. Three examples are shown in <ref type="figure" target="#fig_10">Fig. 8a. In Fig. 8b</ref>, we show some of the few cases where our network failed to detect one or more vehicles in the image. As can be seen, such cases are challenging since only a small part of each undetected vehicle is visible.   As can be seen in (a), the predicted FPs are mostly unlabelled vehicles in the background. In (b), one can see that the vehicles not predicted by the network (i.e., the FNs) are predominantly those occluded or in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">License Plate Detection and Layout Classification</head><p>In <ref type="table" target="#tab_7">Table 8</ref>, we report the results obtained by the modified Fast-YOLOv2 network in the LP detection and layout classification stage.</p><p>As we consider only one LP per vehicle image, the precision and recall rates are identical. The average recall rate obtained in all datasets was 99.51% when disregarding the vehicles not detected in the previous stage and 99.45% when considering the entire test set. This result is particularly impressive since we considered as incorrect the predictions in which the LP layout was incorrectly classified with a high confidence value, even in cases where the LP position was predicted correctly. According to <ref type="figure" target="#fig_13">Fig. 9</ref>, the proposed approach was able to successfully detect and classify LPs of various layouts, including those with few examples in the training set such as LPs issued in the U.S. states of Connecticut and Utah, or LPs of motorcycles registered in the Taiwan region. <ref type="figure" target="#fig_16">Figure 10</ref>: LPs correctly detected and classified by the proposed approach. Observe the robustness for this task regardless of vehicle type, lighting conditions, camera distance, and other factors. <ref type="figure" target="#fig_13">Fig. 9</ref>. LPs correctly detected and classified by the proposed approach. Observe the robustness for this task regardless of vehicle type, lighting conditions, camera distance, and other factors.</p><p>It should be noted that (i) the LPs may occupy a very small portion of the original image and that (ii) textual blocks (e.g., phone numbers) on the vehicles or in the background can be confused with LPs. Therefore, as can be seen in <ref type="figure" target="#fig_16">Fig. 10</ref>, the vehicle detection stage is crucial for the effectiveness of our ALPR system, as it helps to prevent both FPs and FNs. Some images where our network failed either to detect the LP or to classify the LP layout are shown in <ref type="figure" target="#fig_18">Fig. 11</ref>. As can be seen in <ref type="figure" target="#fig_18">Fig. 11a</ref>, our network failed to detect the LP in cases where there is a textual block very similar to an LP in the vehicle patch, or even when the LP of another vehicle appears within the patch (a single case in our experiments). This is due to the fact that one vehicle can be almost totally occluded by another. Regarding the errors in which the LP layout was misclassified, they occurred mainly in cases where the LP is considerably similar to LP of other layouts. For example, the left image in <ref type="figure" target="#fig_18">Fig. 11b</ref> shows a European LP (which has exactly the same colors and number of characters as standard Chinese LPs) incorrectly classified as Chinese.</p><p>It is important to note that it is still possible to correctly recognize the characters in some cases where our network has failed at this stage. For example, in the right image in <ref type="figure" target="#fig_18">Fig. 11a</ref>, the detected region contains exactly the same text as the ground truth (i.e., the LP).     Moreover, a Brazilian LP classified as European (e.g., the middle image in <ref type="figure" target="#fig_18">Fig. 11b</ref>) can still be correctly recognized in the next stage since the only post-processing rule we apply to European LPs is that they have between 5 and 8 characters.</p><p>As mentioned earlier, in this stage we disabled the color-related data augmentation of the Darknet framework. In this way, we eliminated more than half of the layout classification errors obtained when the model was trained using images with changed colors. We believe this is due to the fact that the network leverages color information (which may be distorted with some data augmentation approaches) for layout classification, as well as other characteristics such as the position of the characters and symbols on the LP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">License Plate Recognition (end-to-end)</head><p>As in the vehicle detection stage, we first evaluated different confidence threshold values in the validation set in order to miss as few characters as possible, while avoiding high FP rates. We adopted a 0.5 confidence threshold for all LPs except European ones, where a higher threshold (i.e., 0.65) was adopted since European LPs can have up to 8 characters and several FPs were predicted on LPs with fewer characters when using a lower confidence threshold.</p><p>We considered the '1' and 'I' characters as a single class in the assessments performed in the SSIG-SegPlate and UFPR-ALPR datasets, as those characters are identical but occupy different positions on Brazilian LPs. The same procedure was done in <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b18">17]</ref>.</p><p>For each dataset, we compared the proposed ALPR system with state-of-the-art methods that were evaluated using the same protocol <ref type="table">Table 9</ref>. Recognition rates (%) obtained by the proposed system, modified versions of our system, previous works, and commercial systems in all datasets used in our experiments. To the best of our knowledge, in the literature, only algorithms for LP detection and character segmentation were evaluated in the Caltech Cars, UCSD-Stills and ChineseLP datasets. Therefore, our approaches are compared only with the commercial systems in these datasets. 96.9 ? 1.0 * A modified version of our approach in which the LPs are detected (and their layouts classified) directly in the original image (i.e., without vehicle detection). ? The proposed ALPR system assuming that all LP layouts were classified as undefined (i.e., without layout classification and heuristic rules). ? The LP patches for the LP recognition stage were cropped directly from the ground truth in <ref type="bibr" target="#b39">[38]</ref>.</p><p>as the one described in Section 4.2. In addition, our results are compared with those obtained by Sighthound <ref type="bibr" target="#b74">[73]</ref> and OpenALPR [75], which are two commercial systems often used as baselines in the ALPR literature <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b38">37]</ref>. According to the authors, both systems are robust for the detection and recognition of LPs of different layouts. It is important to emphasize that although the commercial systems were not tuned specifically for the datasets employed in our experiments, they are trained in much larger private datasets, which is a great advantage, especially in deep learning approaches.</p><p>OpenALPR contains specialized solutions for LPs from different regions (e.g., mainland China, Europe, among others) and the user must enter the correct region before using its API, that is, it requires prior knowledge regarding the LP layout. Sighthound, on the other hand, uses a single model/approach for LPs from different countries/regions, as well as the proposed system.</p><p>The remainder of this section is divided into two parts. First, in Section 5.4, we conduct an overall evaluation of the proposed method across the eight datasets used in our experiments. The time required for our system to process an input image is also presented. Afterward, in Section 5.5, we briefly present and discuss the results achieved by both the baselines and our ALPR system on each dataset individually. Such an analysis is very important to find out where the proposed system fails and the baselines do not, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Overall Evaluation</head><p>The results obtained in all datasets by the proposed ALPR system, previous works, and commercial systems are shown in <ref type="table">Table 9</ref>. In the average of five runs, across all datasets, our end-to-end system correctly recognized 96.9% of the LPs, outperforming Sighthound and OpenALPR by 9.1% and 6.2%, respectively. More specifically, the proposed system outperformed both previous works and commercial systems in the ChineseLP, OpenALPR-EU, SSIG-SegPlate and UFPR-ALPR datasets, and yielded competitive results to those attained by the baselines in the other datasets.</p><p>The proposed system attained results similar to those obtained by OpenALPR in the Caltech Cars dataset (98.7% against 99.1%, which represents a difference of less than one LP per run, on average, as there are only 46 testing images), even though our system does not require prior knowledge. Regarding the EnglishLP dataset, our system performed better than the best baseline <ref type="bibr" target="#b59">[58]</ref> in 2 of the 5 runs (this evaluation highlights the importance of executing the proposed method five times and then averaging the results). Although we used the same number of images for testing, in <ref type="bibr" target="#b59">[58]</ref> the dataset was divided only once and the images used for testing were not specified. In the UCSD-Stills dataset, both commercial systems reached a recognition rate of 98.3% while our system achieved 98% on average (with a standard deviation of 1.4%). Lastly, in the AOLP dataset, the proposed approach obtained similar results to those reported by <ref type="bibr" target="#b39">[38]</ref>, even though in their work the LP patches used as input in the LP recognition stage were cropped directly from the ground truth (simplifying the problem, as explained in Section 2); in other words, they did not take into account vehicles or LPs not detected in the earlier stages, nor background noise in the LP patches due to less accurate LP detections.</p><p>To further highlight the importance of the vehicle detection stage, we included in <ref type="table">Table 9</ref> the results achieved by a modified version of our approach in which the LPs are detected (and their layouts classified) directly in the original image (i.e., without vehicle detection). Although comparable results were achieved on datasets where the images were acquired on well-controlled scenarios, the modified version failed to detect/classify LPs in various images captured under less controlled conditions (as illustrated in <ref type="figure" target="#fig_16">Fig. 10b</ref>), e.g. with vehicles far from the camera and shadows on the LPs, which explains the low recognition rate achieved by that approach in the challenging UFPR-ALPR dataset -where the images were taken from inside a vehicle driving through regular traffic in an urban environment, and most LPs occupy a very small region of the image <ref type="bibr" target="#b18">[17]</ref>.</p><p>Similarly, to evaluate the impact of classifying the LP layout prior to LP recognition (i.e., our main proposal), we also report in <ref type="table">Table 9</ref> the results obtained when assuming that all LP layouts were classified as undefined and that a generic approach (i.e., without heuristic rules) was employed in the LP recognition stage. The mean recognition rate was improved by 2.1%. We consider this strategy (layout classification + heuristic rules) essential for accomplishing outstanding results in datasets that contain LPs with fixed positions for letters and digits (e.g., Brazilian and Chinese LPs), as the recognition rates attained in the ChineseLP, SSIG-SegPlate and UFPR-ALPR datasets were improved by 3.6% on average.</p><p>The robustness of our ALPR system is remarkable since it achieved recognition rates higher than 95% in all datasets except UFPR-ALPR (where it outperformed the best baseline by 7.8%). The commercial systems, on the other hand, achieved similar results only in the Caltech Cars and UCSD-Stills datasets, which contain exclusively American LPs, and performed poorly (i.e., recognition rates below 85%) in at least two datasets. This suggests that the commercial systems are not so well trained for LPs of other layouts and highlights the importance of carrying out experiments on multiple datasets (with different characteristics) and not just on one or two, as is generally done in most works in the literature.</p><p>Although OpenALPR achieved better results than Sighthound (on average across all datasets), the latter system can be seen as more robust than the former since it does not require prior knowledge regarding the LP layout. In addition, OpenALPR does not support LPs from the Taiwan region. In this sense, we tried to employ OpenALPR solutions designed for LPs from other regions (including mainland China) in the experiments performed in the AOLP dataset; however, very low detection and recognition rates were obtained. <ref type="figure" target="#fig_2">Fig. 12</ref> shows some examples of LPs that were correctly recognized by the proposed approach. As can be seen, our system can generalize well and correctly recognize LPs of different layouts, even when the images were captured under challenging conditions. It is noteworthy that, unlike <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b59">58]</ref>, the exact same networks were applied to all datasets; in other words, no specific training procedure was used to tune the networks for a given dataset or layout class. Instead, we use heuristic rules in cases where the LP layout is classified with a high confidence value.  Some LPs in which our system failed to correctly detect/recognize all characters are shown in <ref type="figure" target="#fig_1">Fig. 13</ref>. As one may see, the errors occurred mainly in challenging LP images, where even humans can make mistakes since, in some cases, one character might become very similar to another due to the inclination of the LP, the LP frame, shadows, blur, among other factors. Note that, in this work, we did not apply preprocessing techniques to the LP image in order not to increase the overall cost of the proposed system.</p><p>In <ref type="table" target="#tab_0">Table 10</ref>, we report the time required for each network in our system to process an input. As in <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b38">37]</ref>, the reported time is the average time spent processing all inputs in each stage, assuming that the network weights are already loaded and that there is a single vehicle in the scene. Although a relatively deep model is explored <ref type="figure" target="#fig_1">Figure 13</ref>: Examples of LPs that were incorrectly recognized by the proposed ALPR system. The ground truth is shown in parentheses.</p><formula xml:id="formula_2">AB0416 (AR0416) 2MFE674 (2MFF674) HOR8361 (HDR8361) AK04I3 (AK0473) AYH5087 (AXH5087) 430463TC (30463TC) YB8096 (Y88096) DJ9A4AE (DJ944AE) RL0020- (L0020I) ATT4026 (ATT4025) ZG594TSH (ZG594TS) 4NTU770 (4NIU770)</formula><p>28 <ref type="figure" target="#fig_1">Fig. 13</ref>. Examples of LPs that were incorrectly recognized by the proposed ALPR system. The ground truth is shown in parentheses.</p><p>for vehicle detection, our system is still able to process 73 FPS using a high-end GPU. In this sense, we believe that it can be employed for several real-world applications, such as parking and toll monitoring systems, even in cheaper setups (e.g., with a mid-end GPU). It should be noted that practically all images from the datasets used in our experiments contain only one labeled vehicle. However, to perform a more realistic analysis of the execution time, we listed in <ref type="table" target="#tab_0">Table 11</ref> the time required for the proposed system to process images assuming that there is a certain number of vehicles in every image (note that vehicle detection is performed only once, regardless of the number of vehicles in the image). According to the results, our system can process more than 30 FPS even when there are 4 vehicles in the scene. This information is relevant since some ALPR approaches, including the one proposed in our previous work <ref type="bibr" target="#b18">[17]</ref>, can only run in real time if there is at most one vehicle in the scene. The proposed approach achieved an outstanding trade-off between accuracy and speed, unlike others recently proposed in the literature. For example, the methods proposed in <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b9">8]</ref> are capable of processing more images per second than our system but reached poor recognition rates (i.e., below 65%) in at least one dataset in which they were evaluated. On the other hand, impressive results were achieved on different scenarios in <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b16">15]</ref>. However, the methods presented in these works are computationally expensive and cannot be applied in real time. The Sighthound and OpenALPR commercial systems do not report the execution time.</p><p>We remark that real-time processing may be affected by many factors in practice. For example, we measured our system's execution time when there was no other process consuming machine resources significantly. This is the standard procedure in the literature since it enables/facilitates the comparison of different approaches, despite the fact that it may not accurately represent some real-world applications, where other tasks must be performed simultaneously. Some other factors that may affect real-time processing are the time it takes to transfer the image from the camera to the processing unit, hardware characteristics (e.g., CPU architecture, read/write speeds, and data transfer time between CPU and GPUs), and the versions of the frameworks and libraries used (e.g., OpenCV, Darknet and CUDA).</p><p>It is important to emphasize that, according to our experiments, the proposed ALPR system is robust under different conditions while being efficient essentially due to the meticulous way in which we designed, optimized and combined its different parts, always seeking the best trade-off between accuracy and speed. All strategies adopted are very important in some way for the robustness and/or efficiency of the proposed approach, and no specific part contributes more than the others in every scenario. For example, as shown in <ref type="table">Table 9</ref> and <ref type="figure" target="#fig_16">Fig. 10</ref>, vehicle detection mainly helps to prevent false positives and false negatives on complex scenarios, while layout classification (along with heuristic rules) mainly improves the recognition of LPs with a fixed number of characters and/or fixed positions for letters and digits. In the same way, both tasks and also LP recognition would not have been accomplished so successfully, or so efficiently, if not for careful modifications to the networks and exploration of data augmentation techniques (all details were given in Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluation by Dataset</head><p>In this section, we briefly discuss the results achieved by both the baselines and our ALPR system on each dataset individually, striving to clearly identify what types of errors are generally made by each system. For each dataset, we show some qualitative results obtained by the commercial systems and the proposed approach, since we know exactly which images/LPs these systems recognized correctly or not. In the OpenALPR-EU, SSIG-SegPlate and UFPR-ALPR datasets, we also show some predictions obtained by the methods introduced in <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b18">17]</ref>, as their architectures and pre-trained weights were made publicly available by the respective authors. Note that, as we are comparing different ALPR systems, the LP images shown in this section were cropped directly from the ground truth. We focus on the recognition stage for visualization purposes and also because we consider this stage as the current bottleneck of ALPR systems. However, we pointed out cases where one or more systems did not return any predictions on multiple images from a given dataset, which may indicate that the LPs were not properly detected.</p><p>Caltech Cars <ref type="bibr" target="#b60">[59]</ref>: this is the dataset with fewer images for testing (only 46). Hence, a single image recognized incorrectly reduces the accuracy of the system being evaluated by more than 2%. By carefully analyzing the results, we found out that there is a challenging image in this dataset that neither the commercial systems nor the proposed system could correctly recognize. Note that, in some executions, this image was not in the test subset, which explains the mean recognition rates above 98% attained by both our system and OpenALPR. As illustrated in <ref type="figure" target="#fig_4">Fig. 14,</ref> while OpenALPR only made mistakes in that image, the proposed system failed in another image as well (where an 'F' looks like an 'E' due to the LP's frame), and Sighthound failed in some other LPs due to very similar characters (e.g., '1' and 'I') or false positives. <ref type="bibr" target="#b74">[73]</ref> EnglishLP <ref type="bibr" target="#b61">[60]</ref>: this dataset has several LP layouts and different types of vehicles such as cars, buses and trucks. Panahi &amp; Gholampour <ref type="bibr" target="#b59">[58]</ref> reported a recognition rate of 97.0% in this dataset, however, their method was executed only once and the images used for testing were not specified. As can be seen in <ref type="table" target="#tab_0">Table 12</ref>, using the same number of test images, our method achieved recognition rates above 97% in two out of five executions (Sighthound also surpassed 97% in one run). In this sense, we consider that our system is as robust as the one presented in <ref type="bibr" target="#b59">[58]</ref>. According to <ref type="figure">Fig. 15</ref>, neither the commercial systems nor the proposed system had difficulty in recognizing LPs with two rows of characters in this dataset. Instead, as there are many different LP layouts in Europe and thus the number of characters on each LP is not fixed, most errors refer to a character being lost (i.e., false negatives) or, conversely, a non-existent character being predicted (i.e., false positives). The low recognition rates achieved by OpenALPR are due to the fact that it did not return any predictions in some cases (as if there were no vehicles/LPs in the image). In this sense, we conjecture that OpenALPR only returns predictions obtained with a high confidence value and that it is not as well trained for European LPs as it is for American/Brazilian ones. very well trained for American LPs. Also very robustly, our system failed in just two images over 5 runs, remarkably recognizing all 60 images correctly in one of them. All images in which at least one system failed, as well as other representative ones, are shown in <ref type="figure" target="#fig_9">Fig. 16</ref>. <ref type="figure" target="#fig_8">Fig. 17</ref>. Some qualitative results obtained on ChineseLP <ref type="bibr" target="#b32">[31]</ref> by Sighthound <ref type="bibr" target="#b74">[73]</ref>, OpenALPR [75] and the proposed system.</p><p>AOLP <ref type="bibr" target="#b63">[62]</ref>: this dataset has images collected in the Taiwan region from front/rear views of vehicles and various locations, time, traffic, and weather conditions. In our experiments, 683 images were used for testing in each run. As OpenALPR does not support LPs from the Taiwan region (as pointed out in Section 5.4), here we compare the results obtained by Sighthound (87.1%) and the proposed system (99.2%). As shown in <ref type="figure" target="#fig_10">Fig. 18</ref>, different from what we expected, both systems dealt well with inclined LPs in this dataset. While our system failed mostly in challenging cases, such as very similar characters ('E' and 'F', 'B' and '8', etc.), Sighthound also failed in simpler cases where our system had no difficulty in correctly recognizing all LP characters.  OpenALPR-EU <ref type="bibr" target="#b33">[32]</ref>: this dataset consists of 108 testing images, generally with the vehicle well centered and occupying a large portion of the image. Therefore, both our ALPR system and the baselines performed well on this dataset. Over five executions, the proposed system (97.8%) failed in just 3 different images, while the baselines failed in a few more. Surprisingly, as can be seen in <ref type="figure" target="#fig_13">Fig. 19</ref>, the systems made distinct recognition errors and we were unable to find an explicit pattern among the incorrect predictions made by each of them. In this sense, we believe that the errors in this dataset are mainly due to the great variability in the fonts of the characters in different LP layouts. As an example, note in <ref type="figure" target="#fig_13">Fig. 19</ref> that the 'W' character varies considerably depending on the LP layout. <ref type="figure" target="#fig_13">Fig. 19</ref>. Some qualitative results obtained on OpenALPR-EU <ref type="bibr" target="#b33">[32]</ref> by Sighthound <ref type="bibr" target="#b74">[73]</ref>, OpenALPR [75], Silva &amp; Jung <ref type="bibr" target="#b8">[7]</ref>, and the proposed system. <ref type="bibr" target="#b34">[33]</ref>: this dataset contains 800 images for testing. All images were taken with a static camera on the campus of a Brazilian university. Here, the proposed system achieved a high recognition rate of 98.2%, outperforming the best baseline by 6.2%. As shown in <ref type="figure" target="#fig_2">Fig. 20</ref>, as well as in other datasets, our system failed mostly in challenging cases where one character becomes very similar to another due to motion blur, the position of the camera, and other factors. This was also the reason for most of the errors made by OpenALPR and the system designed by Silva &amp; Jung <ref type="bibr" target="#b8">[7]</ref>. However, these systems also struggled to correctly recognize degraded LPs in which some characters are distorted or erased. In addition to such errors, Sighthound predicted 6 characters instead of 7 on several occasions, probably because it does not take advantage of information regarding the LP layout. Lastly, the preliminary version of our approach <ref type="bibr" target="#b18">[17]</ref>, where the LP characters are first segmented and then individually recognized, had difficulty segmenting the characters 'I' and '1' in some cases, which resulted in recognition errors. <ref type="figure" target="#fig_2">Fig. 20</ref>. Some qualitative results obtained on SSIG-SegPlate <ref type="bibr" target="#b34">[33]</ref> by Sighthound <ref type="bibr" target="#b74">[73]</ref>, OpenALPR [75], Silva &amp; Jung <ref type="bibr" target="#b8">[7]</ref>, the preliminary version of our approach <ref type="bibr" target="#b18">[17]</ref>, and the proposed system. <ref type="bibr" target="#b18">[17]</ref>: this challenging dataset includes 1,800 testing images acquired from inside a vehicle driving through regular traffic in an urban environment, that is, both the vehicles and the camera (inside another vehicle) were moving and most LPs occupy a very small region of the image. In this sense, the commercial systems did not return any prediction in some images from this dataset where the vehicles are far from the camera. Regarding the recognition errors, they are very similar to those observed in the SSIG-SegPlate dataset. Sighthound often confused similar letters and digits, while segmentation failures impaired the results obtained by the approach proposed in our previous work <ref type="bibr" target="#b18">[17]</ref>. According to <ref type="figure" target="#fig_2">Fig. 21</ref>, the images were collected under different lighting conditions and the four ALPR systems found it difficult to correctly recognize certain LPs with shadows or high exposure. It should be noted that motorcycle LPs (those with two rows of characters) are challenging in nature, as the characters are smaller and closely spaced. In this context, some authors have evaluated their methods, which do not work for motorcycles or for LPs with two rows of characters, exclusively in images containing cars, overlooking those with motorcycles <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b38">37]</ref>. Final remarks: while being able to process in real time, the proposed system is also capable of correctly recognizing LPs from several countries/regions in images taken under different conditions. In general, our ALPR system failed in challenging cases where one character becomes very similar to another due to factors such as shadows and occlusions (note that some of the baselines also failed in most of these cases). We believe that vehicle information, such as make and model, can be explored in our system's pipeline in order to make it even more robust and prevent errors in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSIG-SegPlate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UFPR-ALPR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this work, as our main contribution, we presented an end-to-end, efficient and layout-independent ALPR system that explores YOLObased models at all stages. The proposed system contains a unified <ref type="figure" target="#fig_2">Fig. 21</ref>. Some qualitative results obtained on UFPR-ALPR <ref type="bibr" target="#b18">[17]</ref> by Sighthound <ref type="bibr" target="#b74">[73]</ref>, OpenALPR [75], the preliminary version of our approach <ref type="bibr" target="#b18">[17]</ref>, and the proposed system. approach for LP detection and layout classification to improve the recognition results using post-processing rules. This strategy proved essential for reaching outstanding results since, depending on the LP layout, we avoided errors in characters that are often misclassified and also in the number of predicted characters to be considered.</p><p>Our system achieved an average recognition rate of 96.9% across eight public datasets used in the experiments, outperforming Sighthound and OpenALPR by 9.1% and 6.2%, respectively. More specifically, the proposed system outperformed both previous works and commercial systems in the ChineseLP, OpenALPR-EU, SSIG-SegPlate and UFPR-ALPR datasets, and yielded competitive results to those attained by the baselines in the other datasets.</p><p>We also carried out experiments to measure the execution time. Compared to previous works, our system achieved an impressive trade-off between accuracy and speed. Specifically, even though the proposed approach achieves high recognition rates (i.e., above 95%) in all datasets except UFPR-ALPR (where it outperformed the best baseline by 7.8%), it is able to process images in real time even when there are 4 vehicles in the scene. In this sense, we believe that our ALPR system can run fast enough even in mid-end setups/GPUs.</p><p>Another important contribution is that we manually labeled the position of the vehicles, LPs and characters, as well as their classes, in all datasets used in this work that have no annotations or that contain labels only for part of the ALPR pipeline. Note that the labeling process took a considerable amount of time since there are several bounding boxes to be labeled on each image (precisely, we manually labeled 38,351 bounding boxes on 6,239 images). These annotations are publicly available to the research community, assisting the development and evaluation of new ALPR approaches as well as the fair comparison among published works.</p><p>We remark that the proposed system can be exploited in several applications in the context of intelligent transportation systems. For example, it can clearly help re-identify vehicles of the same model and color in non-overlapping cameras through LP recognition <ref type="bibr" target="#b25">[24]</ref> -note that very similar vehicles can be easily distinguished if they have different LP layouts. Considering the impressive results achieved for LP detection, it can also be explored for the protection of privacy in images obtained in urban environments by commercial systems such as Mapillary and Google Street View <ref type="bibr" target="#b76">[76]</ref>.</p><p>As future work, we intend to design new CNN architectures to further optimize (in terms of speed) vehicle detection. We also plan to correct the alignment of the detected LPs and also rectify them in order to achieve even better results in the LP recognition stage. Finally, we want to investigate the impact of various factors (e.g., concurrent processes, hardware characteristics, frameworks/libraries used, among others) on real-time processing thoroughly. Such an investigation is of paramount importance for real-world applications, but it has not been done in the ALPR literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples of LPs of different layouts and classes. Observe the wide variety in different ways on different LP layouts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of LPs of different layouts and classes (from top to bottom: American, Brazilian, Chinese, European and Taiwanese). Observe the wide variety in different ways on different LP layouts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Vertical Enlargement (b) Horizontal Enlargement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Two illustrations of enlargement of the LPs detected in the previous stage. In this way, a single network is trained to recognize LPs of different layouts, regardless of their aspect ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Two illustrations of enlargement of the LPs detected in the previous stage. In this way, a single network is trained to recognize LPs of different layouts, regardless of their aspect ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )Figure 5 :</head><label>a5</label><figDesc>Gray ? Red (BR) (b) Red ? Gray (BR) (c) Black ? White (USA) (d) White ? Black (USA) Examples of negative images created to simulate LPs of other layouts. In Brazil, private vehicles have gray LPs, while buses, taxis and other transportation vehicles have red LPs. In the United States, old California LPs featured gold characters on a black background. Currently, they have blue characters on a white background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of negative images created to simulate LPs of other layouts. In Brazil, private vehicles have gray LPs, while buses, taxis and other transportation vehicles have red LPs. In the United States, old California LPs featured gold characters on a black background. Currently, they have blue characters on a white background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Examples of images discarded in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Examples of images discarded in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Some detection results achieved by the YOLOv2 model in different datasets. Observe that vehicles of different types were correctly detected regardless of lighting conditions (daytime and nighttime), occlusion, camera distance, and other factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Some vehicle detection results achieved in distinct datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) FPs predicted by the network (dashed bounding boxes).(b) Vehicles not predicted by the network (dashed bounding boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>FP and false negative (FN) predictions obtained in the vehicle detection stage. As can be seen in (a), the predicted FPs are mostly unlabelled vehicles in the background. In (b), one can see that the vehicles not predicted by the network (i.e., the FNs) are predominantly those occluded or in the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>23 Fig. 8 .</head><label>238</label><figDesc>FP and FN predictions obtained in the vehicle detection stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(a) Examples of results obtained by detecting the LPs directly in the original image. (b) Examples of results obtained by detecting the LPs in the vehicle patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 10 .</head><label>10</label><figDesc>Comparison of the results achieved by detecting/classifying the LPs directly in the original image (a) and in the vehicle regions predicted in the vehicle detection stage (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Some images in which the LP position was predicted incorrectly.(b) Examples of images in which the position of the LP was predicted correctly, but not the layout. In the left image, the LP is European. In the middle and right ones, the LPs are Brazilian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Some images in which our network failed either to detect the LP or to classify the LP layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>25 Fig. 11 .</head><label>2511</label><figDesc>Some images in which our network failed either to detect the LP or to classify the LP layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 12 :</head><label>12</label><figDesc>Examples of LPs that were correctly recognized by the proposed ALPR system. In the rows, LPs of different layout classes are shown. From top to bottom: American, Brazilian, Chinese, European and Taiwanese LPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 12 .</head><label>12</label><figDesc>Examples of LPs that were correctly recognized by the proposed ALPR system. From top to bottom: American, Brazilian, Chinese, European and Taiwanese LPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 18 .</head><label>18</label><figDesc>Some qualitative results obtained on the AOLP [62] dataset by Sighthound [73] and the proposed system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The YOLOv2 architecture, modified for vehicle detection. The input size was changed from 416 ? 416 to 448 ? 288 pixels and the number of filters in the last layer was reduced from 425 to 35.</figDesc><table><row><cell>#</cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>32</cell><cell>3 ? 3/1</cell><cell>448 ? 288 ? 3</cell><cell>448 ? 288 ? 32</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 448 ? 288 ? 32 224 ? 144 ? 32</cell></row><row><cell>2</cell><cell>conv</cell><cell>64</cell><cell cols="3">3 ? 3/1 224 ? 144 ? 32 224 ? 144 ? 64</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 224 ? 144 ? 64</cell><cell>112 ? 72 ? 64</cell></row><row><cell>4</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>112 ? 72 ? 64</cell><cell>112 ? 72 ? 128</cell></row><row><cell>5</cell><cell>conv</cell><cell>64</cell><cell cols="2">1 ? 1/1 112 ? 72 ? 128</cell><cell>112 ? 72 ? 64</cell></row><row><cell>6</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>112 ? 72 ? 64</cell><cell>112 ? 72 ? 128</cell></row><row><cell>7</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 112 ? 72 ? 128</cell><cell>56 ? 36 ? 128</cell></row><row><cell>8</cell><cell>conv</cell><cell>256</cell><cell>3 ? 3/1</cell><cell>56 ? 36 ? 128</cell><cell>56 ? 36 ? 256</cell></row><row><cell>9</cell><cell>conv</cell><cell>128</cell><cell>1 ? 1/1</cell><cell>56 ? 36 ? 256</cell><cell>56 ? 36 ? 128</cell></row><row><cell>10</cell><cell>conv</cell><cell>256</cell><cell>3 ? 3/1</cell><cell>56 ? 36 ? 128</cell><cell>56 ? 36 ? 256</cell></row><row><cell>11</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell>56 ? 36 ? 256</cell><cell>28 ? 18 ? 256</cell></row><row><cell>12</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>28 ? 18 ? 256</cell><cell>28 ? 18 ? 512</cell></row><row><cell>13</cell><cell>conv</cell><cell>256</cell><cell>1 ? 1/1</cell><cell>28 ? 18 ? 512</cell><cell>28 ? 18 ? 256</cell></row><row><cell>14</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>28 ? 18 ? 256</cell><cell>28 ? 18 ? 512</cell></row><row><cell>15</cell><cell>conv</cell><cell>256</cell><cell>1 ? 1/1</cell><cell>28 ? 18 ? 512</cell><cell>28 ? 18 ? 256</cell></row><row><cell>16</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>28 ? 18 ? 256</cell><cell>28 ? 18 ? 512</cell></row><row><cell>17</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell>28 ? 18 ? 512</cell><cell>14 ? 9 ? 512</cell></row><row><cell>18</cell><cell>conv</cell><cell>1024</cell><cell>3 ? 3/1</cell><cell>14 ? 9 ? 512</cell><cell>14 ? 9 ? 1024</cell></row><row><cell>19</cell><cell>conv</cell><cell>512</cell><cell>1 ? 1/1</cell><cell>14 ? 9 ? 1024</cell><cell>14 ? 9 ? 512</cell></row><row><cell>20</cell><cell>conv</cell><cell>1024</cell><cell>3 ? 3/1</cell><cell>14 ? 9 ? 512</cell><cell>14 ? 9 ? 1024</cell></row><row><cell>21</cell><cell>conv</cell><cell>512</cell><cell>1 ? 1/1</cell><cell>14 ? 9 ? 1024</cell><cell>14 ? 9 ? 512</cell></row><row><cell>22</cell><cell>conv</cell><cell>1024</cell><cell>3 ? 3/1</cell><cell>14 ? 9 ? 512</cell><cell>14 ? 9 ? 1024</cell></row><row><cell>23</cell><cell>conv</cell><cell>1024</cell><cell>3 ? 3/1</cell><cell>14 ? 9 ? 1024</cell><cell>14 ? 9 ? 1024</cell></row><row><cell>24</cell><cell>conv</cell><cell>1024</cell><cell>3 ? 3/1</cell><cell>14 ? 9 ? 1024</cell><cell>14 ? 9 ? 1024</cell></row><row><cell>25</cell><cell>route [16]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>26</cell><cell>reorg</cell><cell></cell><cell>/2</cell><cell>28 ? 18 ? 512</cell><cell>14 ? 9 ? 2048</cell></row><row><cell cols="2">27 route [26, 24]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>28</cell><cell>conv</cell><cell>1024</cell><cell>3 ? 3/1</cell><cell>14 ? 9 ? 3072</cell><cell>14 ? 9 ? 1024</cell></row><row><cell>29</cell><cell>conv</cell><cell>35</cell><cell>1 ? 1/1</cell><cell>14 ? 9 ? 1024</cell><cell>14 ? 9 ? 35</cell></row><row><cell>30</cell><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The CR-NET model. We increased the input size from 240?80 to 352?128 pixels. The number of filters in the last convolutional layer (#14) was defined following Equation 1 (using A = 5).</figDesc><table><row><cell>#</cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell><cell>BFLOP</cell></row><row><cell>0</cell><cell>conv</cell><cell>32</cell><cell>3 ? 3/1</cell><cell>352 ? 128 ? 3</cell><cell>352 ? 128 ? 32</cell><cell>0.078</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 352 ? 128 ? 32</cell><cell>176 ? 64 ? 32</cell><cell>0.001</cell></row><row><cell>2</cell><cell>conv</cell><cell>64</cell><cell>3 ? 3/1</cell><cell>176 ? 64 ? 32</cell><cell>176 ? 64 ? 64</cell><cell>0.415</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell>176 ? 64 ? 64</cell><cell>88 ? 32 ? 64</cell><cell>0.001</cell></row><row><cell>4</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>88 ? 32 ? 64</cell><cell>88 ? 32 ? 128</cell><cell>0.415</cell></row><row><cell>5</cell><cell>conv</cell><cell>64</cell><cell>1 ? 1/1</cell><cell>88 ? 32 ? 128</cell><cell>88 ? 32 ? 64</cell><cell>0.046</cell></row><row><cell>6</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>88 ? 32 ? 64</cell><cell>88 ? 32 ? 128</cell><cell>0.415</cell></row><row><cell>7</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell>88 ? 32 ? 128</cell><cell>44 ? 16 ? 128</cell><cell>0.000</cell></row><row><cell>8</cell><cell>conv</cell><cell>256</cell><cell>3 ? 3/1</cell><cell>44 ? 16 ? 128</cell><cell>44 ? 16 ? 256</cell><cell>0.415</cell></row><row><cell>9</cell><cell>conv</cell><cell>128</cell><cell>1 ? 1/1</cell><cell>44 ? 16 ? 256</cell><cell>44 ? 16 ? 128</cell><cell>0.046</cell></row><row><cell>10</cell><cell>conv</cell><cell>256</cell><cell>3 ? 3/1</cell><cell>44 ? 16 ? 128</cell><cell>44 ? 16 ? 256</cell><cell>0.415</cell></row><row><cell>11</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>44 ? 16 ? 256</cell><cell>44 ? 16 ? 512</cell><cell>1.661</cell></row><row><cell>12</cell><cell>conv</cell><cell>256</cell><cell>1 ? 1/1</cell><cell>44 ? 16 ? 512</cell><cell>44 ? 16 ? 256</cell><cell>0.185</cell></row><row><cell>13</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>44 ? 16 ? 256</cell><cell>44 ? 16 ? 512</cell><cell>1.661</cell></row><row><cell>14</cell><cell>conv</cell><cell>200</cell><cell>1 ? 1/1</cell><cell>44 ? 16 ? 512</cell><cell>44 ? 16 ? 200</cell><cell>0.144</cell></row><row><cell cols="2">15 detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The minimum and maximum number of characters to be considered in LPs of each layout class.</figDesc><table><row><cell cols="6">Characters American Brazilian Chinese European Taiwanese</cell></row><row><cell>Minimum</cell><cell>4</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>5</cell></row><row><cell>Maximum</cell><cell>7</cell><cell>7</cell><cell>6</cell><cell>8</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>An overview of the datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Year Images</cell><cell>Resolution</cell><cell>LP Layout</cell><cell>Evaluation Protocol</cell></row><row><cell>Caltech Cars</cell><cell>1999</cell><cell>126</cell><cell>896 ? 592</cell><cell>American</cell><cell>No</cell></row><row><cell>EnglishLP</cell><cell>2003</cell><cell>509</cell><cell>640 ? 480</cell><cell>European</cell><cell>No</cell></row><row><cell>UCSD-Stills</cell><cell>2005</cell><cell>291</cell><cell>640 ? 480</cell><cell>American</cell><cell>Yes</cell></row><row><cell>ChineseLP</cell><cell>2012</cell><cell>411</cell><cell>Various</cell><cell>Chinese</cell><cell>No</cell></row><row><cell>AOLP</cell><cell>2013</cell><cell>2,049</cell><cell>Various</cell><cell>Taiwanese</cell><cell>No</cell></row><row><cell cols="2">OpenALPR-EU 2016</cell><cell>108</cell><cell>Various</cell><cell>European</cell><cell>No</cell></row><row><cell>SSIG-SegPlate</cell><cell>2016</cell><cell>2,000</cell><cell>1,920 ? 1,080</cell><cell>Brazilian</cell><cell>Yes</cell></row><row><cell>UFPR-ALPR</cell><cell>2018</cell><cell>4,500</cell><cell>1,920 ? 1,080</cell><cell>Brazilian</cell><cell>Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>An overview of the number of images used for training, testing and validation in each dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Training Validation Testing Discarded Total</cell></row><row><cell>Caltech Cars</cell><cell>62</cell><cell>16</cell><cell>46</cell><cell>2</cell><cell>126</cell></row><row><cell>EnglishLP</cell><cell>326</cell><cell>81</cell><cell>102</cell><cell>0</cell><cell>509</cell></row><row><cell>UCSD-Stills</cell><cell>181</cell><cell>39</cell><cell>60</cell><cell>11</cell><cell>291</cell></row><row><cell>ChineseLP</cell><cell>159</cell><cell>79</cell><cell>159</cell><cell>14</cell><cell>411</cell></row><row><cell>AOLP</cell><cell>1,093</cell><cell>273</cell><cell>683</cell><cell>0</cell><cell>2,049</cell></row><row><cell>OpenALPR-EU</cell><cell>0</cell><cell>0</cell><cell>108</cell><cell>0</cell><cell>108</cell></row><row><cell>SSIG-SegPlate</cell><cell>789</cell><cell>407</cell><cell>804</cell><cell>0</cell><cell>2,000</cell></row><row><cell>UFPR-ALPR</cell><cell>1,800</cell><cell>900</cell><cell>1,800</cell><cell>0</cell><cell>4,500</cell></row><row><cell cols="6">to recognize the vehicle's LP due to occlusion, lighting or image ac-</cell></row><row><cell cols="6">quisition problems, etc.; (ii) the image does not represent real ALPR</cell></row><row><cell cols="6">scenarios, for example, a person holding an LP. Three examples are</cell></row><row><cell>shown in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Vehicle detection results achieved across all datasets.</figDesc><table><row><cell>Dataset</cell><cell>Precision (%)</cell><cell>Recall (%)</cell></row><row><cell>Caltech Cars</cell><cell cols="2">100.00 ? 0.00 100.00 ? 0.00</cell></row><row><cell>EnglishLP</cell><cell>99.04 ? 0.96</cell><cell>100.00 ? 0.00</cell></row><row><cell>UCSD-Stills</cell><cell>97.42 ? 1.40</cell><cell>100.00 ? 0.00</cell></row><row><cell>ChineseLP</cell><cell>99.26 ? 1.00</cell><cell>99.50 ? 0.52</cell></row><row><cell>AOLP</cell><cell>96.92 ? 0.37</cell><cell>99.91 ? 0.08</cell></row><row><cell>OpenALPR-EU</cell><cell>99.27 ? 0.76</cell><cell>100.00 ? 0.00</cell></row><row><cell>SSIG-SegPlate</cell><cell>95.47 ? 0.62</cell><cell>99.98 ? 0.06</cell></row><row><cell>UFPR-ALPR</cell><cell>99.57 ? 0.07</cell><cell>100.00 ? 0.00</cell></row><row><cell>Average</cell><cell>98.37 ? 0.65</cell><cell>99.92 ? 0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Results attained in the LP detection and layout classification stage. The recall rates achieved in all datasets when disregarding the vehicles not detected in the previous stage are presented in (a), while the recall rates obtained when considering the entire test set are listed in (b).</figDesc><table><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell>Dataset</cell><cell>Recall (%)</cell><cell>Dataset</cell><cell>Recall (%)</cell></row><row><cell>Caltech Cars</cell><cell>99.13 ? 1.19</cell><cell>Caltech Cars</cell><cell>99.13 ? 1.19</cell></row><row><cell>EnglishLP</cell><cell>100.00 ? 0.00</cell><cell>EnglishLP</cell><cell>100.00 ? 0.00</cell></row><row><cell>UCSD-Stills</cell><cell>100.00 ? 0.00</cell><cell>UCSD-Stills</cell><cell>100.00 ? 0.00</cell></row><row><cell>ChineseLP</cell><cell>100.00 ? 0.00</cell><cell>ChineseLP</cell><cell>99.63 ? 0.34</cell></row><row><cell>AOLP</cell><cell>99.94 ? 0.08</cell><cell>AOLP</cell><cell>99.85 ? 0.10</cell></row><row><cell>OpenALPR-EU</cell><cell>98.52 ? 0.51</cell><cell>OpenALPR-EU</cell><cell>98.52 ? 0.51</cell></row><row><cell>SSIG-SegPlate</cell><cell>99.83 ? 0.26</cell><cell>SSIG-SegPlate</cell><cell>99.80 ? 0.24</cell></row><row><cell>UFPR-ALPR</cell><cell>98.67 ? 0.25</cell><cell>UFPR-ALPR</cell><cell>98.67 ? 0.25</cell></row><row><cell>Average</cell><cell>99.51 ? 0.29</cell><cell>Average</cell><cell>99.45 ? 0.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>The time required for each network in our system to process an input on an NVIDIA Titan Xp GPU.</figDesc><table><row><cell>ALPR Stage</cell><cell cols="3">Adapted Model Time (ms) FPS</cell></row><row><cell>Vehicle Detection</cell><cell>YOLOv2</cell><cell>8.5382</cell><cell>117</cell></row><row><cell>LP Detection and Layout Classification</cell><cell>Fast-YOLOv2</cell><cell>3.0854</cell><cell>324</cell></row><row><cell>LP Recognition</cell><cell>CR-NET</cell><cell>1.9935</cell><cell>502</cell></row><row><cell>End-to-end</cell><cell>-</cell><cell>13.6171</cell><cell>73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>Execution times considering that there is a certain number of vehicles in every image.</figDesc><table><row><cell cols="3"># Vehicles Time (ms) FPS</cell></row><row><cell>1</cell><cell>13.6171</cell><cell>73</cell></row><row><cell>2</cell><cell>18.6960</cell><cell>53</cell></row><row><cell>3</cell><cell>23.7749</cell><cell>42</cell></row><row><cell>4</cell><cell>28.8538</cell><cell>35</cell></row><row><cell>5</cell><cell>33.9327</cell><cell>29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Fig. 14. Some qualitative results obtained on Caltech Cars [59] by Sighthound [73], OpenALPR [75] and the proposed system.</figDesc><table><row><cell>: L3WAZ301</cell><cell>[73]: IKCM356</cell><cell>[73]: 2MFF674</cell><cell cols="2">[73]: VZW818</cell></row><row><cell>[75]: 3WAZ301</cell><cell>[75]: 1KCM356</cell><cell>[75]: 2MFF674</cell><cell cols="2">[75]: VZW818</cell></row><row><cell>Ours: 3WAZ301</cell><cell>Ours: 1KCM356</cell><cell>Ours: 2MFE674</cell><cell cols="2">Ours: VZW818</cell></row><row><cell>[73]: 997JDG</cell><cell>[73]: 4CY2275</cell><cell>[73]: VFY818</cell><cell cols="2">[73]: F118</cell></row><row><cell>[75]: 997JDG</cell><cell>[75]: 4CYE275</cell><cell>[75]: VFY818</cell><cell>[75]:</cell><cell>n/a</cell></row><row><cell>Ours: 997JDG</cell><cell>Ours: 4CYE275</cell><cell>Ours: VFY818</cell><cell cols="2">Ours: IR69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 .</head><label>12</label><figDesc>Recognition rates (%) achieved by Panahi &amp; Gholampour<ref type="bibr" target="#b59">[58]</ref>, Sighthound<ref type="bibr" target="#b74">[73]</ref>, OpenALPR [75], and our system on En-glishLP<ref type="bibr" target="#b61">[60]</ref>.UCSD-Stills<ref type="bibr" target="#b62">[61]</ref>: as Caltech Cars, the UCSD-Stills dataset also has few test images (only 60). Despite containing LPs from distinct U.S. states (i.e., different LP layouts) and under several lighting conditions, all ALPR systems evaluated by us achieved excellent results in this dataset. More specifically, both Sighthound and OpenALPR failed in just one image (interestingly, not in the same one). This is another indication that these commercial systems areFig. 15. Some qualitative results obtained on EnglishLP<ref type="bibr" target="#b61">[60]</ref> by Sighthound<ref type="bibr" target="#b74">[73]</ref>, OpenALPR [75] and the proposed system.</figDesc><table><row><cell>Run</cell><cell>[58]</cell><cell>[73]</cell><cell>[75]</cell><cell>Proposed</cell></row><row><cell># 1</cell><cell>?</cell><cell>98.0</cell><cell>82.4</cell><cell>96.1</cell></row><row><cell># 2</cell><cell>?</cell><cell>94.1</cell><cell>79.4</cell><cell>97.1</cell></row><row><cell># 3</cell><cell>?</cell><cell>91.2</cell><cell>76.5</cell><cell>98.0</cell></row><row><cell># 4</cell><cell>?</cell><cell>91.2</cell><cell>73.5</cell><cell>95.1</cell></row><row><cell># 5</cell><cell>?</cell><cell>88.2</cell><cell>81.4</cell><cell>92.2</cell></row><row><cell>Average</cell><cell>97.0</cell><cell>92.5</cell><cell>78.6</cell><cell>95.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The entire ALPR system, i.e., the architectures and weights, along with all annotations made by us are publicly available at https://web.inf.ufpr.br/vri/ publications/layout-independent-alpr/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The division of the images of each dataset into training, test and validation sets is detailed in Section 4.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The images were downloaded from www.platesmania.com. We also made their download links and annotations publicly available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The list of discarded images can be found at https://web.inf. ufpr.br/vri/publications/layout-independent-alpr/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Some qualitative results obtained on UCSD-Stills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fig</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">handled tilted/inclined LPs well and mostly failed in cases where one character become very similar to another due to the LP frame, shadows, blur, etc. It should be noted that Sighthound (90.4%) misclassified the Chinese character (see Section 3.3 for details) as an English letter on some occasions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chineselp</surname></persName>
		</author>
		<idno>73]: ALA8-- [75]: AIA82I Ours: ALA82I [73]: ADT444 [75]: ADI444 Ours: ADI444 [73]: BABII57 [75]: ABII57 Ours: ABII57 [73]: AK0473 [75]: AK0473 Ours: AK04I3 [73]: C44444 [75]: G44444 Ours: C44444 [73]: AEI4LI [75]: AEI4II Ours: AEI4II [73]: IA6ITII [75]: A6ITI7 Ours: A6ITII [73]: RL0020I [75]: L0020N Ours: RL0020- [73]: BSE5579 [75]: BSE5579 [7]: BSE5579 Ours: BSE5579 [73]: BA228IM [75]: BA268IM [7]: 3A268IM Ours: BA268IM [73]: VW4X4WP [75]: VW474WP [7]: VW4X4WP Ours: VW4X4WP [73]: GWAGEN [75]: -WAGEN [7]: GVNAGEN Ours: GVNAGEN [73]: WSQ3021 [75]: WS03021 [7]: WSQ302- Ours: WSQ3021 [73]: RK600AB [75]: RK605AB [7]: RK605AB Ours: RK605AB [73]: 1Z7 5233 [75]: 1Z7 5233 [7]: 1Z7 5233 Ours: 1Z785233 [73]: RK161AG [75]: BK161AG [7]: RK161AG Ours: RK161AG [73]: HIM2848 [75]: HIM2848 [7]: HIM2848 [17]: HIM2848 Ours: HIM2848 [73]: OQD541D [75]: DQO5410 [7]: OOO5410 [17]: OOO5410 Ours: OOO5410 [73]: HOR8361 [75]: HDR8361 [7]: HDR8361 [17]: HDR8361 Ours: HOR8361 [73]: HJN208- [75]: RJN2081 [7]: HLN2081 [17]: HJN208- Ours: HJN2081 [73]: DGQ6370 [75]: OGO6370 [7]: OGO6370 [17]: OGO6370 Ours: OGO6370 [73]: DXH86J7 [75]: OXH8617 [7]: OXH8617 [17]: OXH86-7 Ours: OXH8617 [73]: -TV7556 [75]: KTV7556 [7]: 4TV7556 [17]: AIV7556 Ours: ATV7556 [73]: GMF-862 [75]: GMF2862 [7]: GNF2862 [17]: GNF2862 Ours: GMF2862 [73]: ABN8528 [75]: ABN8528 [17]: ABN8528 Ours: ABN8528 [73]: AMDD663 [75]: AMD0663 [17]: AMO0663 Ours: AMO0663 [73]: ATT4025 [75]: ATT4025 [17]: ATU4025 Ours: ATT4026 [73]: AUG-936 [75]: ADG0936 [17]: AUS0936 Ours: AUG0936 [73]: BBO851- [75]: BBO8514 [17]: BBO85-4 Ours: BBO8514 [73]: IO23616 [75]: IOZ3616 [17]: IOZ3616 Ours: IOZ3616 [73]: AOW1379 [75]: NQW1379 [17]: -OW7379 Ours: AOW1379 [73]: AIQ-Q56 [75]: ATQ1056 [17]: AUC1056 Ours: ATQ1056</idno>
	</analytic>
	<monogr>
		<title level="m">this dataset contains both images captured by the authors and downloaded from the Internet. We used 159 images for testing in each run. An important feature of ChineseLP is that it has several images in which the LPs are tilted or inclined, as shown in Fig. 17. In fact, most of the prediction errors obtained by commercial systems were in such images. Our system, on the other hand</title>
		<imprint/>
	</monogr>
	<note>This kind of recognition error was rarely made by the proposed system (97.5%) and OpenALPR (92.6%)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic number-plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lotufo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Colloquium on Image Analysis for Transport Applications</title>
		<imprint>
			<date type="published" when="1990-02" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Development of vehicle-license number recognition system using real-time image processing and its application to travel-time measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vehicular Technology Conference</title>
		<imprint>
			<date type="published" when="1991-05" />
			<biblScope unit="page" from="798" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">License plate recognition from still images and video sequences: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Psoroulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Loumos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="391" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic license plate recognition (ALPR): A state-of-the-art review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Badawy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2013-02" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="311" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">License plate recognition based on temporal redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2577" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time brazilian license plate detection and recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">License plate detection and recognition in unconstrained scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="593" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Realtime automatic license plate recognition through deep multi-task networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmentation-and annotationfree license plate recognition with deep localization and failure identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bulan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kozitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shreve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2351" to="2363" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Holistic recognition of low quality license plates by CNN using track annotated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?pa?hel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jur?nek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mar??k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zem??k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task learning for low-resolution license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberoamerican Congress on Pattern Recognition (CIARP)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reading car license plates using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A CNN-based approach for automatic license plate recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward end-to-end car license plate detection and recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A robust real-time automatic license plate recognition based on the YOLO detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A two-stage deep neural network for multi-norm license plate detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kessentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Besbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chabbouh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="159" to="170" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A two-stage character segmentation method for chinese license plate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="539" to="553" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vehicle license plate recognition based on extremal regions and restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1096" to="1107" />
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A robust and efficient approach to license plate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1102" to="1114" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey of advances in vision-based vehicle reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="50" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PROVID: Progressive and multimodal vehicle reidentification for large-scale urban surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="658" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Vehicle re-identification: exploring feature fusion using multi-stream convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V O</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minetto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05541</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Part-regularized near-duplicate vehicle reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3992" to="4000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep quadruplet appearance learning for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="8512" to="8522" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1804.02767</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Principal visual word discovery for automatic license plate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4269" to="4279" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">OpenALPR-EU dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openalpr</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://github.com/openalpr/benchmarks/tree/master/endtoend/eu" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Benchmark for license plate character segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P G</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust license plate detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ambikapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for license plate detection in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Kurpiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Nassu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="3395" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new CNN-based method for multi-directional car license plate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="507" to="517" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time license plate detection and recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="page">102773</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards human-level license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="314" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust blur kernel estimation for license plate images from fast moving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2311" to="2323" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CNN for license plate motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mar??k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemc?k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="3832" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved license plate localisation algorithm based on morphological operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yepez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Intelligent Transport Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="542" to="549" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Vehicle license plate recognition with random convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chiachia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falc?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J O</forename><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="298" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Chinese vehicle license plate recognition using kernel-based extreme learning machine with deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Intelligent Transport Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A comparison study on real-time tracking motorcycle license plates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)</title>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">License plate segmentation and recognition system using deep learning and openvino</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Castro-Zunti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Y?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Intelligent Transport Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="126" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intern. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Driver activity recognition for intelligent vehicles: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Velenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5379" to="5390" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ensemble of adaboost cascades of 3L-LBPs classifiers for license plates detection with low quality images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Al-Shemarry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="216" to="235" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Infobae</surname></persName>
		</author>
		<ptr target="https://www.infobae.com/2016/04/01/1801043-entro-vigencia-la-nueva-patente-del-mercosur/" />
		<title level="m">Mercosur finally agrees: unified number plates for new cars beginning 2016</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Mercosur vehicle plates should cost R$138.24, says S?o Paulo&apos;s traffic department</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>The Rio Times</surname></persName>
		</author>
		<ptr target="https://riotimesonline.com/brazil-news/mercosur/mercosur-vehicle-plates-should-cost-r138-24-says-sao-paulos-traffic-department/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mercosur licence plate model is postponed to 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>The Rio Times</surname></persName>
		</author>
		<ptr target="https://riotimesonline.com/brazil-news/miscellaneous/mercosur-licence-plate-model-is-deferred-to-2020/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatially supervised recurrent convolutional neural networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SqueezeDet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">LCDet: Lowcomplexity fully-convolutional neural networks for object detection in embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhaskaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A benchmark for iris location and a deep learning detector evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Bezerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weingaertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Accurate detection and recognition of dirty vehicle plate numbers for high-speed applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gholampour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Caltech Cars dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<ptr target="http://www.vision.caltech.edu/Image_Datasets/cars_markus/cars_markus.tar" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">EnglishLP database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srebri?</surname></persName>
		</author>
		<ptr target="http://www.zemris.fer.hr/projects/LicensePlates/english/baza_slika.zip" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">UCSD dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dlagnekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<ptr target="http://vision.ucsd.edu/belongie-grp/research/carRec/car_data.html" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Application-oriented license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="552" to="561" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for automatic meter reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13023</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Convolutional neural networks-based intelligent recognition of Chinese license plates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2403" to="2419" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Darknet: Open source neural networks in C</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="http://pjreddie.com/darknet" />
		<imprint>
			<biblScope unit="page" from="2013" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">YOLOv4, v3 and v2 for Windows and Linux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards end-to-end license plate detection and recognition: A large dataset and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="261" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">License plate detection based on fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Lightweight fully convolutional network for license plate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Vehicle license plate detection and recognition using deep neural networks and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robust chinese traffic sign detection and recognition with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Coenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Computation (ICNC)</title>
		<imprint>
			<date type="published" when="2015-08" />
			<biblScope unit="page" from="791" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">License plate detection in an open environment by density-based boundary clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">License plate detection and recognition using deeply learned convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A comparison study on motorcycle license plate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Privacy protection in street-view panoramas using depth and multi-view imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Uittenbogaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vijverberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H N</forename><surname>De With</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="10573" to="10582" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
