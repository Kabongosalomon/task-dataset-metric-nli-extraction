<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimSwap: An Efficient Framework For High Fidelity Face Swapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renwang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhong</forename><surname>Chen</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
							<email>nibingbing@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Source Target Result Source Target Result Source Target Result Source Target Result</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimSwap: An Efficient Framework For High Fidelity Face Swapping</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published">October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413630</idno>
					<note>ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 ACM Reference Format: Renwang Chen, Xuanhong Chen, Bingbing Ni and Yanhao Ge. 2020. Sim-Swap: An Efficient Framework For High Fidelity Face Swapping. In Pro-ceedings of the 28th ACM International Conference on Multimedia (MM &apos;20), October 12-16, 2020, Seattle, WA, USA. ACM, New York, NY, USA, 13 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>generative adversarial network</term>
					<term>face swapping</term>
					<term>image translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Face swapping results generated by SimSwap. We replace the face in the target image with the face in the source image. More results can be found in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>We propose an efficient framework, called Simple Swap (Sim-Swap), aiming for generalized and high fidelity face swapping. In contrast to previous approaches that either lack the ability to generalize to arbitrary identity or fail to preserve attributes like facial expression and gaze direction, our framework is capable of transferring the identity of an arbitrary source face into an arbitrary target face while preserving the attributes of the target face. We overcome the above defects in the following two ways. First, we present the ID Injection Module (IIM) which transfers the identity information of the source face into the target face at feature level. By using this module, we extend the architecture of an identityspecific face swapping algorithm to a framework for arbitrary face swapping. Second, we propose the Weak Feature Matching Loss * Equal contribution. ? Corresponding author: Bingbing Ni.</p><p>which efficiently helps our framework to preserve the facial attributes in an implicit way. Extensive experiments on wild faces demonstrate that our SimSwap is able to achieve competitive identity performance while preserving attributes better than previous state-of-the-art methods. The code is already available on github: https://github.com/neuralchen/SimSwap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Image-based rendering; Appearance and texture representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Face swapping is a promising technology that transfers the identity of a source face into a target face while keeping the attributes (e.g. expression, posture, lighting etc.) of the target face unchanged. It has been widely used in film industry to produce nonexistent twins. The industrial face swapping method utilizes advanced equipment to reconstruct the actor's face model and rebuild the scene's lighting condition, which is beyond the reach of most people. Recently, face swapping without high-end equipment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b23">26]</ref> has attracted the researcher's attention.</p><p>The main difficulties in face swapping can be concluded as follows: 1). A face swapping framework with a strong generalization ability should be adapted to arbitrary faces; 2). The identity of the result face should be close to the identity of the source face; 3). The attributes(e.g. expression, posture, lighting etc.) of the result face should be consistent with the attributes of the target face.</p><p>There are mainly two types of face swapping methods, including source-oriented methods that work on the source face at image level and target-oriented methods that work on the target face at feature level. Source-oriented methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b24">27]</ref> transfer attributes(like expression and posture) from the target face to the source face and then blend the source face into the target image. These methods are sensitive to the posture and lighting of the source image and are not able to reproduce the target's expression accurately. Targetoriented approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b17">20]</ref> directly modify the features of the target image and can be well adapted to the variation of the source face. The open-source algorithm [7] is able to generate face swapping results between two specific identities, but lacks the ability for generalization. The GAN-based work <ref type="bibr" target="#b1">[2]</ref> combines the source's identity and the target's attributes at the feature level and extends the application to arbitrary identity. A recent work <ref type="bibr" target="#b17">[20]</ref> utilizes a two-stage framework and achieves high fidelity results. However, these methods focus too much on identity modification. They apply weak constrain on attribute preservation and often encounter mismatch in expression or posture.</p><p>To overcome the defects in generalization and attribute preservation, we propose an efficient face swapping framework, called SimSwap. We analyze the architecture of an identity-specific face swapping algorithm <ref type="bibr">[7]</ref> and find out the lack of generalization is caused by the integration of identity information into the Decoder so the Decoder can be only applied to one specific identity. To avoid such integration, we present the ID Injection Module. Our module conducts modifications on the features of the target image by embedding the identity information of the source face, so the relevance between identity information and the weights of Decoder can be removed and our architecture can be applied to arbitrary identities. Furthermore, identity and attribute information are highly coupled at feature level. A direct modification on the whole features will lead to a decrease in attribute performance and we need to use training losses to alleviate the effect. While explicitly constraining each attribute of the result image to match that of the target image is too complicated, we propose the Weak Feature Matching Loss. Our Weak Feature Matching Loss aligns the generated result with the input target at high semantic level and implicitly helps our architecture to preserve the target's attributes. By using this term, our SimSwap is capable of achieving competitive identity performance while possessing a better attribute preservation skill than previous state-of-the-art methods. Extensive experiments demonstrate the generalization and effectiveness of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Face swapping has been studied for a long period. The methods can be mainly divided into two types including source-oriented methods that work on the source face at image level and targetoriented methods that work on the target face at feature level. Source-oriented Methods. Source-oriented methods transfer attributes from the target face to the source face and then blend the source face into the target image. Early method <ref type="bibr" target="#b3">[4]</ref> used 3D models to transfer postures and lighting but required manual intervention. An automatic method <ref type="bibr" target="#b2">[3]</ref> was proposed but could only swap faces with identities in a specific face library. Nirkin et al. <ref type="bibr" target="#b24">[27]</ref> utilized a 3D face dataset <ref type="bibr" target="#b26">[29]</ref> to transfer the expression and posture and then used Poisson Blending <ref type="bibr" target="#b27">[30]</ref> to merge the source face into the target image. However, owing to the limited expressiveness of the 3D face dataset, methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">27]</ref> replying on 3D models often failed to reproduce the expressions accurately. Recently, FSGAN <ref type="bibr" target="#b23">[26]</ref> proposed a two-stage architecture which first conduct the expression and posture transfer with a face reenactment network and then used another face inpainting network to blend the source face into the target image. A common problem for source-oriented methods is that they are sensitive to the input source image. Exaggerated expression or large posture of the source face will strongly affect the performance of the face swapping result. Target-oriented Methods. Target-oriented methods use neural network to extract the features of the target image, then conduct modifications on the features and restore the features to the output face swapping image. Korshunova et al. <ref type="bibr" target="#b15">[18]</ref> trained a generator and was able to swap faces with one specific identity. The famous algorithm DeepFakes [7] utilized an Encoder-Decoder architecture. Once trained, it was able to swap faces between two specific identities but lacked the ability for generalization. Methods like <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b22">25]</ref> combined the latent representations from the source face area and target non-face area to produce the result but failed to keep target's expression. IPGAN <ref type="bibr" target="#b1">[2]</ref> extracted the identity vector from the source image and the attribute vector from the target image before sending them to the Decoder. The generated outputs were good at transferring the identity of source face but often failed to preserve the expression or posture of the target face. The recently proposed method FaceShifter <ref type="bibr" target="#b17">[20]</ref> was able to produce high fidelity face swapping results. FaceShifter leveraged a sophisticated two-stage framework and achieved the-state-of-art identity performance. However, like previous methods, FaceShifter imposed too weak constrain on the attributes so their results often suffered from expression mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Given a source image and a target image, we present a framework that transfers the identity of the source face into the target face while keeping the attributes of target face unchanged. Our framework extends from an identity-specific face swapping architecture [7] and can be adapted to arbitrary identities. We first discuss the limitation of the original architecture(Sec 3.1). We show how to extend it to a framework for arbitrary identity(Sec 3.2). Then we present the Weak Feature Matching Loss which helps to preserve target's attributes(Sec 3.3). Finally, we give out our loss function(Sec 3.4).  . The Decoder restores the modified features to the result image. We use Identity Loss to encourage our network to generate results with similar identity as the source face. We apply the Weak Feature Matching Loss to make sure that our network can preserve the attributes of the target face while not harming the identity modification performance too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Limitation of the DeepFakes</head><p>The architecture of DeepFakes contains two parts, a common Encoder and two identity-specific Decoders , . In the training stage, the -architecture takes in the warped source images and restore them to the original unwarped source images. The same procedure will be conducted with the target images using the -architecture. In the test stage, a target image will be sent to the -architecture. The architecture will mistake it for a warped source image and produce an image with the source's identity and the target's attributes.</p><p>During this process, the Encoder Enc extracts the target's features which contain both identity and attribute information of the target face. Since the Decoder manages to convert the target's features to an image with source's identity, the identity information of the source face must have been integrated into the weights of . So the Decoder in DeepFakes can be only applied to one specific identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalization to Arbitrary Identity</head><p>To overcome such limitation, we are seeking a way to separate the identity information from the Decoder so that the whole architecture can be generalized to arbitrary identity. We improve the architecture by adding an additional ID Injection Module between the Encoder and the Decoder. Our framework is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Given a target image , we pass it through our Encoder to extract its features . Since our task is to swap the target face with the source face, we have to replace the identity information in with the identity information of source face while keeping the attribute information in unchanged. However, the identity and attribute information in are highly coupled and difficult to tell apart. So we directly conduct modifications on the whole and we are using the training loss to encourage the network to learn implicitly which part of should be changed and which part should be preserved.</p><p>Our ID Injection Module works on changing the identity information in towards the identity information of the source face. The module is composed of two parts, the identity extraction part and the embedding part. In the identity extraction part, we deal with the input source image which contains both identity and attribute information of source face. Since we only need the former, we use a face recognition network <ref type="bibr" target="#b6">[8]</ref> to extract the identity vector from . In the embedding part, we are using the ID-Blocks to inject the identity information into the features. Our ID-Block is a modified version of the Residual Block <ref type="bibr" target="#b9">[12]</ref> and we are using the Adaptive Instance Normalization(AdaIN) <ref type="bibr" target="#b10">[13]</ref> to replace the original Batch Normalization <ref type="bibr" target="#b11">[14]</ref>. The formulation of AdaIN in our task can be written as:</p><formula xml:id="formula_0">( , ) = ? ( ) ( ) +<label>(1)</label></formula><p>Here, ( ) and ( ) is the channel-wise mean and standard deviation of the input feature . and are two variables generated from using full connected layers. To guarantee enough identity embedding, we are using a total of 9 ID-Blocks.</p><p>After the injection of identity information, we pass the modified features through the Decoder to generate the final result . Since source images from different identities are involved in the training, the weights of the Decoder should be unrelated to any specific identity. Our Decoder will just focus on restoring the image from the features and leave the identity modification mission to the ID Injection Module, so we can apply our architecture to arbitrary identities.</p><p>During the training process, we extract the identity vector from the generated result and we use the Identity Loss to minimize the distance between and . However, the minimization of the Identity Loss can make the network overfitted and only generate front face images with the source's identity while losing all the target's attributes. To avoid such phenomena, we utilize the idea of adversarial training <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b25">28]</ref> and use the Discriminator to distinguish results with apparent error. The Adversarial Loss also plays an import role in improving the quality of the generated result. We use the patchGAN <ref type="bibr" target="#b12">[15]</ref> version of the Discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Preserving the Attributes of the Target</head><p>In the face swapping task, the modification should be only conducted in the identity part and the attributes (e.g. expression, posture, lighting etc.) of the target face should remain unchanged. However, since we are directly conducting modifications on the whole which contains both identity and attribute information of target face, the attribute information is likely to be affected by the identity embedding. To prevent the attribute mismatch, we are using the training loss to constrain them. However, if we choose to constrain all the attributes explicitly, we will have to train one network for each attribute. The whole process should be impractical since there are too many attributes should be considered. So we propose to use the Weak Feature Matching Loss to do the constraining in an implicit way.</p><p>The idea of Feature Matching originated in pix2pixHD <ref type="bibr" target="#b31">[34]</ref> which used the Discriminator to extract multiple layers of features from the Ground Truth image and the generated output. The original Feature Matching Loss is written as:</p><formula xml:id="formula_1">( ) = ?? =1 1 ? ( ) ( ) ? ( ) ( )? 1<label>(2)</label></formula><p>Here ( ) denotes the -th layer feature extractor of Discriminator and denotes the number of elements in the -th layer. is the total number of layers. is the generated output and is its corresponding Ground Truth image.</p><p>In our architecture, since there's no Ground Truth in face swapping task, we are using the input target image to replace its position. We remove the first few layers and only use the last few layers to calculate our Weak Feature Matching Loss, which can be written as:</p><formula xml:id="formula_2">( ) = ?? = 1 ? ( ) ( ) ? ( ) ( )? 1<label>(3)</label></formula><p>Here is the layer where we start to calculate the Weak Feature Matching Loss. Although the original Feature Matching Loss and Weak Feature Matching Loss share similar formulations, their objectives are totally different. The original Feature Matching Loss is proposed to stabilize the training and the generator is required to produce natural statistics at multiple levels. The features of shallow layers will play the key role since they mainly contain texture information and are able to constrain the results at pixel level. However, in our face swapping task, introducing too much texture information from the input target image will make the result similar to target face and cause difficulty in identity modification, so we remove the first few layers in the original Feature Matching term. Our objective is to constrain the attribute performance. Since the attribute is high semantic information which mainly lies in deep features, we are requiring the result image to align with the input target at deep level, and our Weak Feature Matching Loss is only using the last few layers of the Discriminator to calculate the Feature Matching term. By using such a loss function, even if we are not explicitly constraining the network on any specific attribute, it will implicitly learn how to preserve the attributes of the input target face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Loss Function</head><p>Our Loss function has 5 components, including Identity Loss, Reconstruction Loss, Adversarial Loss, Gradient Penalty and Weak Feature Matching Loss. Identity Loss Identity Loss is used to constrain the distance between and . We are using the cosine similarity to calculate the distance, which can be written as:</p><formula xml:id="formula_3">= 1 ? ? ? ? 2 ? ? 2<label>(4)</label></formula><p>Reconstruction Loss If the source face and the target face are from the same identity, the generated result should look the same as target face. We are using the Reconstruction Loss as a regularization term, which can be written as:</p><formula xml:id="formula_4">= ? ? ? 1<label>(5)</label></formula><p>We set this term to 0 if the source and target faces are from different identities. Adversarial Loss and Gradient Penalty We are using the Hinge version <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b25">28]</ref> of the Adversarial Loss. We use multi-scale Discriminators <ref type="bibr" target="#b31">[34]</ref> for better performance under large postures. We also utilize the Gradient Penalty term <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">11]</ref> to prevent the Discriminators from gradient explosion. Weak Feature Matching Loss Since we are using mutli-scale Discriminator, the Weak Feature Matching Loss should be calculated using all Discriminators, which can be written as:</p><formula xml:id="formula_5">_ = 2 ?? =1 ( )<label>(6)</label></formula><p>The overall Loss can be written as:</p><formula xml:id="formula_6">+ + + + _<label>(7)</label></formula><p>Here = 10, = 10, = 10 ?5 , = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Implementation Detail Since we are working on face swapping for arbitrary identities, we choose a large face dataset VG-GFace2 <ref type="bibr" target="#b5">[6]</ref> as our training set. To improve the quality of our training set, we remove images with size smaller than 250 ? 250. We align and crop the images to a standard position with size 224 ? 224. As for the face recognition model in the ID Injection Module, we use a pretrained Arcface [8] model on <ref type="bibr">[9]</ref>. We train our network using the Adam optimizer <ref type="bibr" target="#b14">[17]</ref> with 1 = 0 and 2 = 0.999. We train one batch for image pairs with the same identity and another batch for image pairs with different identities alternately. We train our networks for more than 500 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative Face Swapping Results</head><p>We are presenting a face matrix to show our face swapping results. We pick 8 face images from the movie scenes as target Target Source R e s u l t s <ref type="figure">Figure 3</ref>: Face matrix generated by SimSwap. The target images are picked from the movie scenes and the source images are downloaded from the Internet. All the target and source images are excluded from the training set. Source images with a front posture or neutral expression are not necessary since we are only using their identity vectors. The results show that SimSwap is able to change the identity into the source face while preserving the attributes of the target face.</p><p>images. We download 10 face images from the Internet as source images. The source images are not required to have a front posture or neutral expression since we are only using their identity vectors. All these images are excluded from our training set. We conduct face swapping for all source and target pairs. As shown in <ref type="figure">Figure 3</ref>, SimSwap is capable of transferring the identity of the source face into the target face while preserving the attributes (like expression, gaze direction, posture and lighting condition) of the target face. Our method can well handle various identities. Even given difficult target conditions like exaggerated expression(row 4), face stripe(row 4), large face rotations(row 7), SimSwap can still produce high fidelity face swapping results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Other Methods</head><p>Many face swapping methods have been proposed in recent years. We choose three of them, including the source-oriented method FSGAN <ref type="bibr" target="#b23">[26]</ref> and target-oriented methods DeepFakes [7], FaceShfiter <ref type="bibr" target="#b17">[20]</ref>. We show the comparison between these methods and our SimSwap. Comparison on FaceForensics++ The FaceForensics++ <ref type="bibr" target="#b28">[31]</ref> dataset contains 1,000 face videos downloaded from Internet and 1,000 face swapping videos generated by DeepFakes. We compare Sim-Swap with DeepFakes and FaceShifter on FaceForensics++. Since FaceShifter doesn't release their code, we directly crop the images from their paper for fair comparison. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>,  and FaceShifter <ref type="bibr" target="#b17">[20]</ref> on FaceForensics++ <ref type="bibr" target="#b28">[31]</ref>. The results of SimSwap achieve competitive identity performance while preserving better attributes (like expression, posture, lighting).</p><p>the results of DeepFakes suffer from severe lighting and posture mismatch. FaceShifter manages to produce decent face swapping results but the expression and gaze direction of the result faces do not fully respect those of the target faces. Our SimSwap generates plausible face swapping results while achieving better performance in attribute preservation. Additional Comparison with FaceShifter We further compare more results with FaceShifter in <ref type="figure">Figure 5</ref>. As we can see, FaceShifter exhibits a strong identity modification ability and it is able to change the face shape of the result towards that of the source face. However, it focus too much on the identity part and often fail in keeping attributes like expression and gaze direction. In <ref type="figure">Figure 5</ref> row 2, the target face is narrowing his eyes. SimSwap can generate result that reproduces such subtle expression while FaceShifter fails. Furthermore, although FaceShifter is using a second network to combine its face swapping results with the background, we still manage to produce a slightly better lighting condition(row 3&amp;4) than FaceShifter.</p><p>Comparison with FSGAN We compare with FSGAN in <ref type="figure">Figure  6</ref>. The results of FSGAN fail to reproduce the expression(row 1), gaze direction(row 1&amp;4) of the target face and there are apparent differences in lighting conditions between their results and target images. Our SimSwap achieves better performance in attribute preservation. Besides, FSGAN is very sensitive to input source image. As shown in row 2, the target face has a clear eye area but FSGAN brings in the shadows from the source face. Similar problem Source Target FaceShifter SimSwap <ref type="figure">Figure 5</ref>: More comparison results with FaceShifter <ref type="bibr" target="#b17">[20]</ref>. We have better attribute performance in expression and lighting condition.</p><p>also occurs in row 4 around the nose area. SimSwap is much more robust to the input source image and produce more convincing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of SimSwap</head><p>In this section, we will first give an analysis of our identity modification ability. Then we will conduct several ablation tests to show how to keep a balance between identity and attribute performance in the face swapping task. Efficient Id Embedding The architecture of SimSwap uses the ID Injection Module to do the identity embedding so that we can separate the identity information from the weights of Decoder and generalize our architecture to arbitrary identity. To verify the effectiveness of our architecture, we conduct the same quantitative experiment on FaceForensics++ <ref type="bibr" target="#b28">[31]</ref> using the criterion proposed by <ref type="bibr" target="#b17">[20]</ref>. We randomly pick 10 frames from each face video in Face-Forensics++. We conduct face swapping using SimSwap following the same source and target pairs in FaceForensics++. We use another face recognition network <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b30">33]</ref> to extract the identity vectors of the generated frames and original frames. For each generated frame, we search for the nearest face in the original frames and check whether that face is from the correct source video. The accuracy rate is named as ID retrieval and serves as a representation of the method's identity performance. We also use the pose estimator <ref type="bibr" target="#b29">[32]</ref> to estimate the postures in the generated frames and the original frames, and we calculate their averaged L2 distance. We neglect the facial expression part because we are not able to find a valid reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target FSGAN SimSwap <ref type="figure">Figure 6</ref>: Comparison with FSGAN <ref type="bibr" target="#b23">[26]</ref>. SimSwap can preserve attributes(like expression, gaze direction, lighting) better than FSGAN. Our results are less likely to be affected by the attributes of the input source images.</p><p>For further comparison, we train another 2 networks, called SimSwap-oFM which uses the original Feature Matching formulation and SimSwap-nFM which uses no Feature Matching term. We conduct the same quantitative experiments on these 2 networks. We also test on the frames generated by DeepFakes. The comparison results are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>As we can see, SimSwap-oFM has the lowset ID retrieval since it aligns the results at shallow levels. Meanwhile, by removing the constrains at all levels, SimSwap-nFM has a very close identity performance as Faceshifter. Our SimSwap is a little behind in identity but achieves a relatively good posture performance. Combining with the results in <ref type="figure" target="#fig_2">Figure 4</ref> and 5, our SimSwap presents a slightly weaker identity performance than FaceShifter but possess a better attribute preservation ability. Keeping a Balance between Identity and Attribute Since our IIM directly works on the whole feature extracted from target image, the embedding of identity will inevitably influence the performance of attribute preservation. So we need to find a balance between identity modification and attribute preservation.</p><p>In our framework, there are two ways to adjust the balance between identity and attribute. The first is to explicitly set a heavier  weight for to encourage a stronger modification skill. The second is to select more or less features in the Feature Matching term. The combination of these two approaches can result in a wide range of results.</p><p>Apart from SimSwap-oFM and SimSwap-nFM, we train another 4 networks, called SimSwap-, SimSwap-oFM-FM-, SimSwap-oFM-id+ and SimSwap-wFM-id+. For , we keep the first few layers in the original Feature Matching term while removing the last few. For oFM-FM-, we utilize the original Feature Matching formulation and decrease to 5. For oFM-id+, we utilize the original Feature Matching formulation and increase to 20. For wFM-id+, we keep the same Weak Feature Matching Loss as SimSwap and increase to 20. We train all the above networks following the same training strategy as SimSwap. We test the ID retrieval of all the networks.</p><p>We conduct additional quantitative experiments on CelebAMask-HQ <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b19">22]</ref>. First, we randomly pick 1,000 source and target pairs with different identities and generate face swapping results. We use the average Identity Loss between the result and the source to measure the identity modification skill. Then we randomly pick 1,000 images and use each image as both source and target to do self-swapping. We use the average Reconstruction Loss to measure how much attribute information from target has been lost during the face swapping process. The comparison results are shown in <ref type="figure" target="#fig_3">Figure 7</ref>.</p><p>As we can see, the oFM, , oFM-FM-and oFM-id+ all have lower ID retrieval than SimSwap. This indicates that keeping the last few layers in the Feature Matching term has less effect on the identity performance. Meanwhile, although nFM has the highest ID retrieval, its Reconstruction Loss is the highest among all, which shows that a strong modification ability will cause difficulty in attribute preservation. SimSwap is achieving a relatively high ID</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target oFM oFM-id+ SimSwap wFM-id+ nFM  retrieval while keeping a medium Reconstruction Loss, which keeps a good balance between the identity and attribute performance.</p><p>To further validate the effectiveness of our Weak Feature Matching Loss, results generated by different networks are shown in <ref type="figure" target="#fig_4">Figure 8</ref>. We notice that results(col 3&amp;4, col 5&amp;6) with the same Feature Matching term present relatively small difference, which indicates that the increase of has limited influence on visual looking. As we compare the results between SimSwap (col 5) and SimSwap-oFM (col 3), SimSwap is presenting a better identity performance without losing much attributes. The results of SimSwap-nFM (col 7) have the best identity performance, and the shapes of the result faces have been modified towards the shape of the source face. However, SimSwap-nFM is apparently losing attributes since gaze directions tend to deviate from those in the target faces.</p><p>As for SimSwap and wFM-id+, at most times they produce very similar visual outputs. However, when comparing the values of wFM-id+ and nFM in <ref type="figure" target="#fig_3">Figure 7</ref> and the results in <ref type="figure" target="#fig_4">Figure 8</ref>, we notice that although wFM-id+ has a smaller Identity Loss, the ID retrieval and visual looking show that the nFM achieves better identity performance indeed. This indicates that wFM-id+ has been overfitted for the Identity Loss. Besides, wFM-id+ is more likely to introduce in hair from the source face (as shown in <ref type="figure" target="#fig_5">Figure 9</ref>). This is unwanted since we are only replacing the faces. So we are choosing SimSwap for more stable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose SimSwap, an efficient framework aiming for generalized and high fidelity face swapping. Our ID Injection Module which transfers the identity information at feature level and extends the identity-specific face swapping to arbitrary face swapping. The Weak Feature Matching Loss helps our framework to possess a good attribute preservation ability. Extensive results have shown that we are able to generate visually appealing results and our method can preserve attributes better than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Target</head><p>DeepFakes SimSwap Source Target DeepFakes SimSwap <ref type="figure">Figure 10</ref>: More comparison results on FaceForensics++ <ref type="bibr" target="#b28">[31]</ref>. Even though we have not trained on source and target faces, we manage to produce better results than DeepFakes <ref type="bibr">[7]</ref>. and Y stands for the target identity. We put the source image on the upper left corner and the target image on the bottoem left corner. Please check our videos for details. The video is available on our github project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL RESULTS FOR SIMSWAP</head><p>We show more face matrix generated by SimSwap in <ref type="figure">Figure  11</ref>,12,13. We even include some fictional characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target R e s u l t <ref type="figure">Figure 11</ref>: Male Face Matrix generated by SimSwap. The identities in row 1 and row 7 are fictional characters. Our method manages to generate high fidelity face swapping result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target R e s u l t <ref type="figure" target="#fig_0">Figure 12</ref>: Female Face Matrix generated by SimSwap. The identities in row 4 and row 7 are fictional characters. Our method manages to generate high fidelity face swapping result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target R e s u l t <ref type="figure">Figure 13</ref>: Expression Face Matrix generated by SimSwap. Some identities in this matrix have extraggerated expressions. Our method is still to generate decent face swapping result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The framework of SimSwap. Our generator consists of three parts, including the Encoder part, the ID Injection Module(IIM) and the Decoder part. The Encoder extracts features from the target image . The ID Injection Module transfers the identity information from into</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison with DeepFakes [7]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Comparison between different Feature Matching term and Id weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Face swapping results generated by different networks. The results with original Feature Matching formulation(col 3&amp;4) exhibit a shortage of identity performance. The results with no Feature Matching term(col 7) have the best identity performance but suffer from attribute mismatch. The results with Weak Feature Matching Loss(col 5&amp;6) show a good balance between identity modification and attribute preservation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Comparison between SimSwap and SimSwap-wFM-id+. SimSwap-wFM-id+ is more likely to introduce in hair from the source face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Qualitative Experiments on FaceForensics++</figDesc><table><row><cell>Method</cell><cell>ID retrieval</cell><cell>Posture</cell></row><row><cell>DeepFakes [7]</cell><cell>77.65%</cell><cell>4.59</cell></row><row><cell>FaceShifter [20]</cell><cell>97.38%</cell><cell>2.96</cell></row><row><cell>SimSwap-oFM</cell><cell>73.64%</cell><cell>1.22</cell></row><row><cell>SimSwap</cell><cell>92.83%</cell><cell>1.53</cell></row><row><cell>SimSwap-nFM</cell><cell>96.57%</cell><cell>2.47</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by National Science Foundation of China (61976137, U1611461, U19B2035) and STCSM(18DZ1112300).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL COMPARISON WITH DEEPFAKES</head><p>We show more comparison with DeepFakes [7] on FaceForen-sics++ <ref type="bibr" target="#b28">[31]</ref> in <ref type="figure">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B VIDEO RESULTS</head><p>We provide 5 face swapping videos generated by SimSwap. The name format is s_X_t_Y.avi, where X stands for the source identity</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<ptr target="http://arxiv.org/abs/1701.07875" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards Open-Set Identity Preserving Face Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00702</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00702" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6713" to="6722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face swapping: automatically replacing faces in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Bitouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samreen</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<idno type="DOI">10.1145/1360612.1360638</idno>
		<ptr target="https://doi.org/10.1145/1360612.1360638" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exchanging Faces in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Volker Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidel</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2004.00799.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-8659.2004.00799.x" />
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="669" to="676" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1xsqj09Fm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">VGGFace2: A Dataset for Recognising Faces across Pose and Age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2018.00020</idno>
		<ptr target="https://doi.org/10.1109/FG.2018.00020" />
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition, FG 2018</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-05-15" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00482</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00482" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<editor>Neil D. Lawrence, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada, Zoubin Ghahramani, Max Welling, Corinna Cortes</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
	<note>Ulrike von Luxburg, Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.167</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.167" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.632</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.632" />
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="5967" to="5976" />
			<date type="published" when="2017-07-21" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00453</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00453" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast Face-Swap Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.397</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.397" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="3697" to="3705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MaskGAN: Towards Diverse and Interactive Facial Image Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11922</idno>
		<ptr target="http://arxiv.org/abs/1907.11922" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13457</idno>
		<ptr target="http://arxiv.org/abs/1912.13457" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Few-Shot Unsupervised Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.01065</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.01065" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="10550" to="10559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Learning Face Attributes in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.425</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.425" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mugglewang</surname></persName>
		</author>
		<ptr target="https://github.com/MuggleWang/CosFace_pytorch" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FSNet: An Identity-Aware Generative Model for Image-Based Face Swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Yatagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20876-9_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-20876-9_8" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018 -14th Asian Conference on Computer Vision</title>
		<editor>C. V. Jawahar, Hongdong Li, Greg Mori, and Konrad Schindler</editor>
		<meeting><address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-12-02" />
			<biblScope unit="volume">11366</biblScope>
			<biblScope unit="page" from="117" to="132" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part VI (Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RSGAN: face swapping and editing using face and hair representation in latent spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Yatagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<idno type="DOI">10.1145/3230744.3230818</idno>
		<ptr target="https://doi.org/10.1145/3230744.3230818" />
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on Computer Graphics and Interactive Techniques Conference, SIGGRAPH 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-12" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1" to="69" />
		</imprint>
	</monogr>
	<note>Posters Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FSGAN: Subject Agnostic Face Swapping and Reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00728</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00728" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On Face Segmentation, Face Swapping, and Face Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2018.00024</idno>
		<ptr target="https://doi.org/10.1109/FG.2018.00024" />
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition, FG 2018</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-05-15" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic Image Synthesis With Spatially-Adaptive Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00244</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00244" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A 3D Face Model for Pose and Illumination Invariant Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="DOI">10.1109/AVSS.2009.58</idno>
		<ptr target="https://doi.org/10.1109/AVSS.2009.58" />
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<editor>Stefano Tubaro and Jean-Luc Dugelay</editor>
		<meeting><address><addrLine>Genova, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009-09-04" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<idno type="DOI">10.1145/882262.882269</idno>
		<ptr target="https://doi.org/10.1145/882262.882269" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faceforensics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2074" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00917</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00917" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
