<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Expanding Language-Image Pretrained Models for General Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Expanding Language-Image Pretrained Models for General Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Video Recognition, Contrastive Language-Image Pretraining</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive language-image pretraining has shown great success in learning visual-textual joint representation from web-scale data, demonstrating remarkable "zero-shot" generalization ability for various image tasks. However, how to effectively expand such new languageimage pretraining methods to video domains is still an open problem. In this work, we present a simple yet effective approach that adapts the pretrained language-image models to video recognition directly, instead of pretraining a new model from scratch. More concretely, to capture the long-range dependencies of frames along the temporal dimension, we propose a cross-frame attention mechanism that explicitly exchanges information across frames. Such module is lightweight and can be plugged into pretrained language-image models seamlessly. Moreover, we propose a video-specific prompting scheme, which leverages video content information for generating discriminative textual prompts. Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios. In particular, under fully-supervised settings, our approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using 12? fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot experiments, our approach surpasses the current state-ofthe-art methods by +7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In few-shot scenarios, our approach outperforms previous best methods by +32.1% and +23.1% when the labeled data is extremely limited. Code and models are available at aka.ms/X-CLIP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Throughput (clip/s)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics-400</head><p>Top-1 Accuracy (%) <ref type="bibr" target="#b29">30</ref>  are pre-defined. Such method is unrealistic for many real-world applications, such as automatic tagging of web videos, where information regarding new video categories is not available during training. It is thus very challenging for closed-set methods to train a classifier for recognizing unseen or unfamiliar categories. Fortunately, recent work in large-scale contrastive language-image pretraining, such as CLIP <ref type="bibr" target="#b36">[37]</ref>, ALIGN <ref type="bibr" target="#b19">[20]</ref>, and Florence <ref type="bibr" target="#b54">[55]</ref>, has shown great potentials in addressing this challenge. The core idea is to learn visual or visual-language representation with natural language supervision using web-scale image-text data. After pretraining, natural language is used to reference learned visual concepts (or describe new ones), thus enabling zero/few-shot transfer of the models to downstream tasks. Inspired by these works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b54">55]</ref>, we consider to use text as the supervision signals to learn a new video representation for general recognition scenarios, including zero-shot, few-shot, and fully-supervised.</p><p>However, directly training a language-video model is unaffordable for many of us, because it requires large-scale video-text pretraining data as well as a massive number of GPU resources (e.g., thousands of GPU days). A feasible solution is to adapt the pretrained language-image models to video domain. Very recently, there are several studies exploring how to transfer the knowledge from the pretrained language-image models to other downstream tasks, e.g., point cloud understanding <ref type="bibr" target="#b58">[59]</ref> and dense prediction <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b59">60]</ref>. However, the transfer and adaptation to video recognition is not well explored. When adapting the pretrained cross-modality models from image to video domain, there are two key issues to be solved: 1) how to leverage the temporal information contained in videos, and 2) how to acquire discriminative text representation for a video.</p><p>For the first question, we present a new architecture for video temporal modeling. It consists of two key components: a cross-frame communication transformer and a multi-frame integration transformer. Specifically, the crossframe communication transformer takes raw frames as input and provides a framelevel representation using a pretrained language-image model, while allowing information exchange between frames with message tokens. Each message token not only depicts the semantics of the current frame, but also communicates with other frames to model their dependencies. The multi-frame integration transformer then simply transfer the frame-level representations to video-level.</p><p>For the second question, we employ the text encoder pretrained in the languageimage models and expand it with a video-specific prompting scheme. The key idea is to leverage video content information to enhance text prompting. The intuition behind is that appropriate contextual information can help the recognition. For example, if there is extra video content information about "in the water", the actions "swimming" and "running" will be much easier to be distinguished. In contrast to prior work manually designing a fixed set of text prompts, this work proposes a learnable prompting mechanism, which integrates both semantic labels and representation of videos for automatic prompt generation.</p><p>With the above two issues addressed, we can smoothly adapt existing imagelevel cross-modality pretrained models to video domains. Without loss of generality, here we choose the CLIP <ref type="bibr" target="#b36">[37]</ref> and Florence <ref type="bibr" target="#b54">[55]</ref> models and eXpand them for general video recognition, forming new model families called X-CLIP and X-Florence, respectively. Comprehensive experiments demonstrate our expanded models are generally effective. In particular, under the fully-supervised setting, X-CLIP-L/14 achieves competitive performance on Kinetics-400/600 with top-1 accuracies of 87.1%/88.3%, surpassing ViViT-H [3] by 2.3%/2.5% while using 12? fewer FLOPs, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In zero-shot experiments, X-Florence surpasses the state-of-the-art ActionCLIP <ref type="bibr" target="#b48">[49]</ref> by +7.6% and +14.9% under two popular protocols. In few-shot experiments, X-CLIP outperforms other prevailing methods by +32.1% and +23.1% when the data is extremely limited.</p><p>In summary, our contributions are three-fold:</p><p>-We propose a new cross-frame communication attention for video temporal modeling. This module is light and efficient, and can be seamlessly plugged into existing language-image pretrained models, without undermining their original parameters and performance. -We design a video-specific prompting technique to yield instance-level textual representation automatically. It leverages video content information to enhance the textual prompt generation. -Our work presents a new way of expanding existing large-scale languageimage pretrained models for general video recognition and other potential video tasks. Extensive experiments demonstrate the superiority and good generalization ability of our method under various learning configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual-language Pretraining. Visual-language pretraining has achieved remarkable progress over the past few years <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b61">62]</ref>. In particular, contrastive language-image pretraining demonstrates very impressive "zero-shot" transfer and generalization capacities <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b54">55]</ref>. One of the most representative works is the recent CLIP <ref type="bibr" target="#b36">[37]</ref>. A large amount of follow-up works have been proposed to leverage the pretrained models for downstream tasks. For example, CoOp <ref type="bibr" target="#b60">[61]</ref>, CLIP-Adapter <ref type="bibr" target="#b15">[16]</ref> and Tip-Adapter <ref type="bibr" target="#b57">[58]</ref> use the pretrained CLIP for improving the few-shot transfer, while PointCLIP <ref type="bibr" target="#b58">[59]</ref> and DenseCLIP <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b59">60]</ref> transfer the knowledge to point cloud understanding and dense prediction, respectively. VideoCLIP <ref type="bibr" target="#b51">[52]</ref> extends the image-level pretraining to video by substituting the image-text data with video-text pairs <ref type="bibr" target="#b31">[32]</ref>. However, such video-text pretraining is computationally expensive and requires a large amount of curated video-text data which is not easy to acquire. In contrast, our method directly adapts the existing pretrained model to video recognition, largely saving the training cost.</p><p>There are two concurrent works mostly related to ours. One is ActionCLIP <ref type="bibr" target="#b48">[49]</ref>, while the other is <ref type="bibr" target="#b20">[21]</ref>. Both of them introduce visual-language pretrained models to video understanding. ActionCLIP proposes a "pretrain, prompt and finetune" framework for action recognition, while <ref type="bibr" target="#b20">[21]</ref> proposes to optimize a few random vectors for adapting CLIP to various video understanding tasks. In contrast, our method is more general. It supports adapting various languageimage models, such as CLIP and Florence <ref type="bibr" target="#b54">[55]</ref>, from image to video. Moreover, we propose a lightweight and efficient cross-frame attention module for video temporal modeling, while presenting a new video-specific text prompting scheme.</p><p>Video Recognition. One key factor to build a robust video recognition model is to exploit the temporal information. Among many methods, 3D convolution is widely used <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51]</ref>, while it suffers from high computational cost. For efficiency purposes, some studies <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51]</ref> factorize convolutions across spatial and temporal dimensions, while others insert the specific temporal modules into 2D CNNs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. Nevertheless, the limited receptive field of CNNs gives the rise of transformer-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b53">54]</ref>, which achieve very promising performance recently. However, these transformer-based methods are either computationally intensive or insufficient in exploiting the temporal information. For example, ViViT <ref type="bibr" target="#b2">[3]</ref> disregards the temporal information in the early stage. Video Swin <ref type="bibr" target="#b29">[30]</ref> utilizes 3D attention while having high computational cost.</p><p>The temporal modeling scheme in our method shares a similar spirit with the recent proposed video transformers, i.e., VTN <ref type="bibr" target="#b32">[33]</ref>, ViViT <ref type="bibr" target="#b2">[3]</ref>, and AVT <ref type="bibr" target="#b17">[18]</ref>. They all use a frame-level encoder followed by a temporal encoder, but our method has two fundamental differences. 1) In <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>, each frame is encoded separately, resulting in no temporal interaction before final aggregation. This late fusion strategy does not fully make use of the temporal cues. By contrast, our method replaces the spatial attention with the proposed cross-frame attention, which allows global spatio-temporal modeling for all frames. 2) Similar to previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>, both ViViT <ref type="bibr" target="#b2">[3]</ref> and VTN <ref type="bibr" target="#b32">[33]</ref> adopt a dense temporal sampling strategy and ensemble the predictions of multiple views at inference, which is time-consuming. On the contrary, we empirically analyze different sampling methods for late fusion, and demonstrate that a sparse sampling is good enough, achieving better performance with fewer FLOPs than the dense strategy, as verified in Sec. 4.5 (Analysis).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random initialization Random initialization Partially pretrained initialization Partially pretrained initialization Pretrained initialization Pretrained initialization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we present our proposed framework in detail. First, we briefly overview our video-text framework in Sec. 3.1. Then, we depict the architecture of the video encoder, especially for the proposed cross-frame attention in Sec. 4.1. Finally, we introduce a video-specific prompting scheme in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Most prior works in video recognition learn discriminative feature embeddings supervised by a one-hot label <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b47">48]</ref>. While in this work, inspired by the recent contrastive language-image pretraining <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b54">55]</ref>, we propose to use text as the supervision, since the text provides more semantic information. As shown in <ref type="figure" target="#fig_3">Fig. 2</ref>, our method learns to align the video representation and its corresponding text representation by jointly training a video encoder and a text encoder. Rather than pretraining a new video-text model from scratch, our method is built upon prior language-image models and expands them with video temporal modeling and video-adaptive textual prompts. Such a strategy allows us to fully take advantage of existing large-scale pretrained models while transferring their powerful generalizability from image to video in a seamless fashion.</p><p>Formally, given a video clip V ? V and a text description C ? C, where V is a set of videos and C is a collection of category names, we feed the video V into the video encoder f ?v and the text C into the text encoder f ?c to obtain a video representation v and a text representation c respectively, where</p><formula xml:id="formula_0">v = f ?v (V ), c = f ?c (C).<label>(1)</label></formula><p>Then, a video-specific prompt generator f ?p is employed to yield instance-level textual representation for each video. It takes the video representation v and text representation c as inputs, formulated a?  Finally, a cosine similarity function sim(v,?) is utilized to compute the similarity between the visual and textual representations:</p><formula xml:id="formula_1">c = f ?p (c, v).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Frame Diffusion Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Frame Diffusion Attention (b) Various Space-Time Attentions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Frame Diffusion Attention</head><formula xml:id="formula_2">sim(v,?) = ?v,?? ?v? ??? .<label>(3)</label></formula><p>The goal of our method is to maximize the sim(v,?) if V and C are matched and otherwise minimize it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video Encoder</head><p>Our proposed video encoder is composed of two cascaded vision transformers: a cross-frame communication transformer and a multi-frame integration transformer. The cross-frame transformer takes raw frames as input and provides a framelevel representation using a pretrained language-image model, while allowing information exchange between frames. The multi-frame integration transformer then simply integrates the frame-level representations and outputs video features. Specifically, given a video clip V ? R T ?H?W ?3 of T sampled frames with H and W denote the spatial resolution, following ViT <ref type="bibr" target="#b10">[11]</ref>, the t-th frame is</p><formula xml:id="formula_3">divided into N non-overlapping patches {x t,i } N i=1 ? R P 2 ?3 with each of size P ? P pixels, where t ? {1, ? ? ? , T } denotes the temporal index, and N =HW/P 2 . The patches {x t,i } N i=1</formula><p>are then embedded into patch embeddings using a linear projection E ? R 3P 2 ?D . After that, we prepend a learnable embedding x class to the sequence of embedded patches, called [class] token. Its state at the output of the encoder serves as the frame representation. The input of the cross-frame communication transformer at the frame t is denoted as:</p><formula xml:id="formula_4">z (0) t = [x class , Ex t,1 , Ex t,2 , ? ? ? , Ex t,N ] + e spa ,<label>(4)</label></formula><p>where e spa represents the spatial position encoding. Then we feed the patch embeddings into an L c -layer Cross-frame Communication Transformer (CCT) to obtain the frame-level representation h t :</p><formula xml:id="formula_5">z (l) t = CCT (l) (z (l?1) t ), l = 1, ? ? ? , L c h t = z (Lc) t,0 ,<label>(5)</label></formula><p>where l denotes the block index in CCT, z </p><formula xml:id="formula_6">v = AvgPool(MIT(H + e temp )),<label>(6)</label></formula><p>where AvgPool and e temp denote the average pooling and temporal position encoding, respectively. We use standard learnable absolute position embeddings <ref type="bibr" target="#b46">[47]</ref> for e spa and e temp . The multi-frame integration transformer is constructed by the standard multi-head self-attention and feed-forward networks <ref type="bibr" target="#b46">[47]</ref>.</p><p>Cross-frame Attention. To enable a cross-frame information exchange, we propose a new attention module. It consists of two types of attentions, i.e., cross-frame fusion attention (CFA) and intra-frame diffusion attention (IFA), with a feed-forward network (FFN). We introduce a message token mechanism for each frame to abstract, send and receive information, thus enabling visual information to exchange across frames, as shown in <ref type="figure" target="#fig_5">Fig. 3(a)</ref>. In detail, the message token m (l) t for the t-th frame at the l-th layer is obtained by employing a linear transformation on the [class] token z (l?1) t,0 . This allows message tokens to abstract the visual information of the current frame.</p><p>Then, the cross-frame fusion attention (CFA) involves all message tokens to learn the global spatio-temporal dependencies of the input video. Mathematically, this process at l-th block can be expressed as:</p><p>Note that the message token is dropped before the FFN layer and does not pass through the next block, since it is generated online and used for frames communication within each block. Alternating the fusion and diffusion attentions through L c blocks, the cross-frame communication transformer (CCT) can encode the global spatial and temporal information of video frames. Compared to other space-time attention mechanisms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30]</ref>, as presented in <ref type="figure" target="#fig_5">Fig. 3(b)</ref>, our proposed cross-frame attention models the global spatio-temporal information while greatly reducing the computational cost.</p><p>Initialization. When adapting the pretrained image encoder to the video encoder, there are two key modifications. 1) The intra-frame diffusion attention (IFA) inherits the weights directly from the pretrained models, while the cross-frame fusion attention (CFA) is randomly initialized. 2) The multi-frame integration transformer is appended to the pretrained models with random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text Encoder</head><p>We employ the pretrained text encoder and expand it with a video-specific prompting scheme. The key idea is to use video content to enhance the text representation. Given a description C about a video, the text representation c is obtained by the text encoder, where c = f ?c (C). For video recognition, how to generate a good text description C for each video is a challenging problem. Previous work, such as CLIP <ref type="bibr" target="#b36">[37]</ref>, usually defines textual prompts manually, such as "A photo of a {label}". However, in this work, we empirically show that such manually-designed prompts do not improve the performance for video recognition (as presented in Tab. 9). In contrast, we just use the "{label}" as the text description C and then propose a learnable text prompting scheme.</p><p>Video-specific prompting. When understanding an image or a video, human can instinctively seek helps from discriminative visual cues. For example, the extra video semantic information of "in the water" will make it easier to distinguish "swimming" from "running". However, it is difficult to acquire such visual semantics in video recognition tasks, because 1) the datasets only provide the category names, such as "swimming" and "running", which are pre-defined and fixed; and 2) the videos in the same class share the identical category name, but their visual context and content are different. To address these issues, we propose a learnable prompting scheme to generate textual representation automatically. Concretely, we design a video-specific prompting module, which takes the video content representationz and text representation c as inputs. Each block in the video-specific prompting module is consisting of a multi-head selfattention (MHSA) <ref type="bibr" target="#b46">[47]</ref> followed by a feed-forward network to learn the prompts,</p><formula xml:id="formula_7">c = c + MHSA(c,z), c =c + FFN(c),<label>(10)</label></formula><p>where c is the text embedding,z ? R N ?d is the average of {z (Lc) t } T t=1 along the temporal dimension, andc is the video-specific prompts. We use text representa- tion c as query and the video content representationz as key and value. This implementation allow the text representation to extract the related visual context from videos. We then enhance the text embedding c with the video-specific promptsc as follows,? = c + ?c, where ? is a learnable parameter with an initial value of 0.1. The? is finally used for classification in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments on different settings, i.e., fully-supervised, zero-shot and few-shot, followed by the ablation studies of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Architectures and Datasets. We expand CLIP and Florence to derive four variants: X-CLIP-B/32, X-CLIP-B/16, X-CLIP-L/14 and X-Florence, respectively. In detail, there are three parts in our framework: a cross-frame communication transformer followed by a multi-frame integration transformer and a text encoder.  all X-CLIP variants, we use a simple 1-layer multi-frame integration transformer. For X-Florence, we stack a 4-layer multi-frame integration transformer. The number of the video-specific prompting blocks is set to 2 for all variants. We evaluate the efficacy of our method on four benchmarks: Kinetics-400&amp;600 <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref>, UCF-101 <ref type="bibr" target="#b41">[42]</ref> and HMDB-51 <ref type="bibr" target="#b23">[24]</ref>. More details about architectures and datasets are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fully-supervised Experiments</head><p>Training and Inference. We sample 8 or 16 frames with a sparse sampling method in fully-supervised experiments. All the expanded models are trained with 32 NVIDIA 32G V100 GPUs. The detailed hyperparameters are showed in the supplementary materials.</p><p>Results. In Tab. 1, we report the results on Kinetics-400 and compare our method with state-of-the-art under different pretraining, including random initialization, ImageNet-1k/21k <ref type="bibr" target="#b9">[10]</ref> pretraining, web-scale image and languageimage pretraining. We develop a family of models with different FLOPs by setting the number of sampled frames to 8 or 16.</p><p>Compared to the methods pretrained on ImageNet-21k <ref type="bibr" target="#b9">[10]</ref>, our X-CLIP-B/16 8f (with 8 sampled frames) surpasses Swin-L <ref type="bibr" target="#b28">[29]</ref> by +0.7% with 4? fewer FLOPs and running 5? faster(as presented in <ref type="figure" target="#fig_0">Fig. 1</ref>). The underlying reason is that the 3D shift-window attention in Swin is inefficient. Also, our X-CLIP-L/14 8f outperforms MViTv2-L <ref type="bibr" target="#b26">[27]</ref> by +1.0% with 5? fewer FLOPs. In addition, when pretraining the video encoder only on IN-21k, our method achieves higher performance than the recent TimeSformer-L <ref type="bibr" target="#b4">[5]</ref> with fewer computation cost.</p><p>When compared to the methods using web-scale image pretraining, our X-CLIP is also competitive. For example, X-CLIP-L/14 8f achieves +2.3% higher accuracy than ViViT-H <ref type="bibr" target="#b2">[3]</ref> with 12? fewer FLOPs. MTV-H <ref type="bibr" target="#b53">[54]</ref> achieves better results than ours, but it uses much more pretraining data. Specifically, MTV-H  uses a 70M video-text dataset with about 17B images, which are much larger than the 400M image-text data used in CLIP pretraining.</p><p>Moreover, compared to ActionCLIP <ref type="bibr" target="#b48">[49]</ref>, which also adopts CLIP as the pretrained model, our X-CLIP-L/14 8f is still superior, getting +3.3% higher accuracy with fewer FLOPs. There are two factors leading to the smaller FLOPs of our method. One is that X-CLIP does not use 3D attention like <ref type="bibr" target="#b29">[30]</ref> and has fewer layers. The other factor is that X-CLIP samples fewer frames for each video clip, such as 8 or 16 frames, while ActionCLIP <ref type="bibr" target="#b48">[49]</ref> using 32 frames.</p><p>In addition, we report the results on Kinetics-600 in Tab. 2. Using only 8 frames, our X-CLIP-B/16 8f achieves a higher top-1 accuracy compared to ViViT-L, while using 27? fewer FLOPs. More importantly, our X-CLIP-L/14 8f achieves 88.3% top-1 accuracy while using 5? fewer FLOPs compared to the current state-of-the-art method MTV-H <ref type="bibr" target="#b53">[54]</ref>.</p><p>From the above fully-supervised experiments, we can observe that, our X-CLIP method achieves very competitive performance compared to prevailing video transformer models <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b20">21]</ref>. This mainly attributes to two factors. 1) The proposed cross-frame attention can effectively model temporal dependencies of video frames. 2) The joint language-image representation is successfully transferred to videos, unveiling its powerful generalization ability for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero-shot Experiments</head><p>Training and Inference. We pretrain X-CLIP-B/16 on Kinetics-400 with 32 frames. The single view inference is adopted. More details about the evaluation protocols are provided in the supplementary materials.</p><p>Results. Zero-shot video recognition is very challenging, because the categories in the test set are unseen to the model during training. We report the results in Tab. 3 and Tab. 4. On HMDB-51 <ref type="bibr" target="#b23">[24]</ref> and UCF-101 <ref type="bibr" target="#b41">[42]</ref> benchmarks, our X-CLIP outperforms the previous best results by +3.8% and +13.7% in terms of top-1 accuracy respectively, as reported in Tab. 3. On Kinetics-600 <ref type="bibr" target="#b6">[7]</ref> as presented in Tab. 4, our X-CLIP outperforms the state-of-the-art ER-ZSAR <ref type="bibr" target="#b7">[8]</ref> by +23.1%. Such remarkable improvements can be attributed to the proposed videotext learning framework, which leverages the large-scale visual-text pretraining and seamlessly integrates temporal cues and textual prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Few-shot Experiments</head><p>Training and Inference. A general K-shot setting is considered, i.e., K examples are sampled from each category randomly for training. We compare with some representative methods, i.e., TSM <ref type="bibr" target="#b27">[28]</ref>, TimeSformer <ref type="bibr" target="#b4">[5]</ref> and Swin <ref type="bibr" target="#b29">[30]</ref>. More details about the comparison methods and evaluation protocols are provided in the supplementary materials. Results. Tab. 5 presents the results of K-shot learning. For the extreme case where K=2, we observe that for those single-modality methods, the performance drops significantly, demonstrating that over-fitting occurs due to the serious lack of data. In contrast, X-CLIP shows robustness by surpassing them with large margins. For example, X-CLIP-B/16 outperforms Swin-B by +32.1% and +23.1% in terms of top-1 accuracy on HMDB-51 and UCF-101 with K=2, respectively. Such large improvements are mainly due to the exploitation of the semantics in text representation. It further verifies the efficacy of transferring the knowledge of the pretrained language-image models to the few-shot models. We also observe that the performance gap between our method and others decreases as the sample size increases. It demonstrates increasing data can mitigate the over-fitting for other methods. Besides, it is noteworthy that the comparison of methods with CLIP pretraining and ImageNet pretraining is not fair enough. Hence, in Sec. 4.5, we provide an additional ablation analysis and verify the performance gains mainly comes from the use of textual information, rather than the CLIP pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation and Analysis</head><p>Unless stated otherwise, the fully-supervised experiments are performed on Kinectics-400, while the few-shot experiments are conducted on HMDB-51 with K=2. The zero-shot evaluation is on the first split of the validation set of UCF-101. We use X-CLIP-B/16 8f with single-view inference in all experiments. Ablation.The effects of the proposed components. Tab. 6 shows the performance evolution from the pretrained image CLIP to our expanded video X-CLIP. First,    we design a simple baseline that averages the CLIP features of all video frames for classification, called CLIP-Mean. It uses the text supervision but does not utilize prompting technique. We can observe that equipping the original transformer in CLIP with our proposed cross-frame communication mechanism, i.e. Eq. (7-9), can improve the accuracy by +1.2%. Then, appending 1-layer multi-frame integration transformer (MIT) can further improve the accuracy by +0.5%. This illustrates that our X-CLIP framework can effectively leverage temporal cues in video clips.</p><p>With the proposed video-specific prompting, X-CLIP can surpass the CLIP-Mean baseline by +2.3%. It demonstrates that the video-specific prompting scheme can generate more discriminative textual representation. Meanwhile, additionally using multi-view inference can boost the performance by +1.5%. Overall, with our proposed methods and all the techniques mentioned above, X-CLIP can boost the top-1 accuracy of the CLIP-Mean baseline from 80.0% to 83.8%. Which branch to finetune? In order to demonstrate which branch should be finetuned when transferred to different downstream tasks, we separately freeze the parameters of the pretrained image and text encoder. Note that the randomly initialized parameters are always finetuned. From Tab. 7, we summarize the following observations. 1) For fully-supervised setting, finetuning the image encoder brings +3.0% improvements, while freezing the text encoder reduces the CUDA memory from 22G to 6G with minor performance loss. 2) For few-shot setting, we find the top-2 results are achieved by finetuning the text encoder. We conjecture the reason is that with few samples, the text encoder suffers less from the over-fitting than the over-parameterized image model. 3) For zero-shot setting, finetuning both the image and the text encoder achieves the best results.</p><p>The effects of text. To evaluate the impact of text, we replace the text encoder with a randomly initialized fully-connected layer as the classification head. From Tab. 8, we can observe that, without the text branch, the model cannot adapt to  zero-shot setting, because there is no data to initialize the head. For the few-shot and fully-supervised experiments, text information can bring +18.8% and +0.7% gains, respectively. This indicates the semantic information involved in text representation is beneficial to classification, especially for low-shot learning.</p><p>The effects of pretraining. We further investigate the effects of pretraining when expanding the language-image models to video. We use ViT-B/16 pretrained on ImageNet-1k/21k as the video encoder in our framework. As represented in Tab. 10, though the pretrained image encoder and text encoder are not in a joint embedding space, the model with IN-21k and IN-1k pretraining still achieve 79.8% and 75.9% top-1 accuracy on Kinectics-400, yet much inferior to the original CLIP large-scale pretraining (82.3%). Analysis.Comparison with other prompting methods. We compare with two existing methods in Tab. 9: prompt ensembling <ref type="bibr" target="#b36">[37]</ref> with 16 handcraft templates and learnable vectors <ref type="bibr" target="#b60">[61]</ref> with length 16. It can be seen that our video-specific prompts outperforms others, especially in zero-shot setting (+6.1%). This demonstrates the efficacy of our method, which generates more adaptive prompts and better textual representation for unseen videos.</p><p>Dense v.s. sparse sampling. We further explore what is the best sampling strategy for our method in Tab. 11. We find that the dense sampling does not perform well as in previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref>. In contrast, the sparse sampling best matches our method. Regardless of the number of frames and views, using sparse sampling both in training and inference achieves the best performance.</p><p>Single-view v.s. multi-view inference. Although it can improve performance, multi-view inference takes relatively high computational cost, because the cost grows linearly with the number of views. In Tab. 11, we show that our multimodality models with sparse sampling is robust to the number of views, i.e., single-view can achieve comparable performance to 10 temporal views. The underlying reason is the language-image models provide robust representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present a simple approach that adapts the pretrained languageimage models to video recognition. To capture the temporal information, we propose a cross-frame attention mechanism that explicitly exchanges information across frames. A video-specific prompting technique is designed to yield instancelevel discriminative textual representation. Extensive experiments under three different learning scenarios demonstrate the effectiveness of our method. In future work, we plan to extend our method to different video tasks beyond classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Hyperparameters</head><p>Tab. 1 presents the hyperparameters for our experiments, corresponding to Section 4.2-4.4 of the main manuscript. It is noteworthy that the learning rate of the randomly initialized parameters is 10? higher than the base learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hand-craft Prompt Templates</head><p>In Tab. 9 of the main manuscript, we compare our video-specific prompting scheme with the existing prompt ensemble method <ref type="bibr" target="#b36">[37]</ref> and demonstrate the superiority of our method. We construct 16 hand-craft templates <ref type="bibr" target="#b48">[49]</ref> totally. We randomly choose one template in each training iteration, and the result in inference is the average result of all templates. The complete list of templates is as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and Evaluation Protocols</head><p>In this section, we overview the four datasets briefly in Sec. 3.1. Then, we provide the evaluation protocols for different experiment settings, i.e., zero-shot, few-shot and fully-supervised in Sec. 3.2-3.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets Overview</head><p>-Kinetics-400&amp;600. The Kinetics <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref> dataset consists of 10-second video clips collected from YouTube. In particular, Kinetics-400 <ref type="bibr" target="#b22">[23]</ref> consists of ?240k training videos and ?20k validation videos with 400 classes, while Kinetics-600 <ref type="bibr" target="#b6">[7]</ref> consists of ?410k training videos and ?29k validation videos from 600 classes. -UCF-101 <ref type="bibr" target="#b41">[42]</ref>. UCF-101 is a video recognition dataset for realistic actions, collected from YouTube, including 13, 320 video clips with 101 action categories in total. There are three splits of the training and test data. -HMDB-51 <ref type="bibr" target="#b23">[24]</ref>. It has around 7, 000 videos with 51 classes, which is relatively small compared to UCF-101 and Kinetics. HMDB-51 has three splits of the training and test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fully-supervised Experiments</head><p>We conduct the fully-supervised experiments on Kinetics-400&amp;600. We use the complete training and validation sets for training and inference, respectively. During training, a sparse sampling strategy <ref type="bibr" target="#b47">[48]</ref> is used. The number of frames is set to 8 or 16. We spatially scale the shorter side of each frame to 256 and take a 224 center crop. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>, we adopt the multi-view inference with 3 spatial crops and 4 temporal clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Few-shot Experiments</head><p>We randomly sample 2, 4, 8 and 16 videos from each class on UCF-101 and HMDB-51 for constructing the training set. For evaluation, we use the first split of the test set on UCF-101 and HMDB-51. We report the results with a single view of 32 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Zero-shot Experiments</head><p>We train X-CLIP-B/16 with 32 frames on Kinetics-400. The single-view inference is adopted for our method. The same as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37]</ref>, we apply the following two evaluation protocols in our zero-shot experiments. 1) Evaluation for HMDB-51 and UCF-101. Following <ref type="bibr" target="#b36">[37]</ref>, the prediction is conducted on the three splits of the test data, and we report the average top-1 accuracy and standard deviation.</p><p>2) Evaluation for Kinetics-600. Following <ref type="bibr" target="#b7">[8]</ref>, the 220 new categories outside Kinetics-400 <ref type="bibr" target="#b22">[23]</ref> in Kinetics-600 are used for evaluation. The evaluation is conducted three times. For each iteration, we randomly sampled 160 categories for evaluation from the 220 categories in Kinetics-600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional Experiments Analysis</head><p>In this section, we further compare different methods of adapting an image encoder to a video encoder in Sec. 4.1. Besides, we provide an analysis of aligning  <ref type="bibr" target="#b36">[37]</ref>. The fully-supervised experiment is conducted on Kinetics-400 <ref type="bibr" target="#b22">[23]</ref>. The few-shot(2-shot) experiment is conducted on HMDB-51, and zero-shot experiment is conducted on UCF-101 <ref type="bibr" target="#b41">[42]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with other video encoders adapted from images</head><p>Researchers have proposed several ways of adapting an image encoder to a video encoder <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. We compare with two existing methods in Tab. 2. The first method is named "CLIP-One", in which we randomly sample one frame and feed it to the pretrained image encoder. The second method is named "CLIP-Joint", where we apply the joint space-time attention <ref type="bibr" target="#b2">[3]</ref> that simply forwards all spatiotemporal tokens extracted from the video through the image encoder. Although the CLIP-Joint also considers global spatio-temporal information in videos, it takes more computational overhead than our proposed X-CLIP. What is more, our method surpasses the CLIP-Joint by +0.2% and +9.5% in the fully-supervised and few-shot experiments, respectively. We conjecture the reasons are two-fold. 1) CLIP-Joint considers the joint spatio-temporal tokens and therefore breaks the customary input pattern of the pretrained image encoder, which may impede the representation ability. In contrast, our method maintains the input pattern of the pretrained image encoder via modeling frame-level information, thus leveraging the strong representation ability of the pretrained image encoder.</p><p>2) The joint space-time attention requires more training data and training time to converge than our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Can ImageNet pretrained video encoder align with CLIP pretrained text encoder?</head><p>We have demonstrated that the video encoder with ImageNet pretraining still achieves competitive performance on the fully-supervised experiment in Tab. 10 of the main manuscript. However, the two embedding spaces of the ImageNet pretrained visual encoder and CLIP pretrained text encoder are not well aligned, which raises a question: can we align the two embedding spaces without the webscale joint pretraining, and then transfer the knowledge to zero-shot experiments?</p><p>To answer this question, we build an ImageNet pretrained video encoder and separate the text encoder from the pretrained CLIP. Then, we finetune the video encoder with the text supervision on Kinetics-400 to align the two embedding spaces. As a comparison, we also finetune a same video encoder but supervised by the discrete one-hot labels. Finally, we conduct the few-shot and zero-shot experiments using the two finetuned models to verify the transfer ability. The categories in few-shot and zero-shot experiments are not seen in finetuning. From Tab. 3, we can observe that the aligned model, i.e., the model supervised by text information, achieves superior performance and surpasses the unaligned model by a large margin. It indicates that the ImageNet pretrained video encoder can still align with the CLIP pretrained text encoder by an acquired finetuning process using limited samples. The results also show the generality and flexibility of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of the proposed architectures in the single-modality framework</head><p>We further evaluate the proposed cross-frame communication transformer and multi-frame integration transformer on a simple classification setting, i.e., training from scratch with a single-modality framework on Kinetics-400. We use ViT-B/32 8f as the backbone and adopt a fully-connected layer as the classification head. ViT-B/32-Mean averages the representation of all frames, while our method uses the cross-frame attention and stacks 1-layer multi-frame integration transformer on the top. We train both models 100 epochs with a learning rate 1 ? 10 ?4 , and all the other hyperparameters are the same as in Tab. 1. From Tab. 4, it can be seen that our method outperforms the baseline +2.5% in terms of top-1 accuracy, which illustrates that our proposed architecture does not rely on pretraining and can help general video classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art methods on Kinetics-400<ref type="bibr" target="#b22">[23]</ref> in terms of throughput, the number of views, and FLOPs. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>An overview of our framework. The details are elaborated in Sec. 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Cross-frame Attention. (b) compares different space-time attention mechanisms used in existing video transformer backbones [3,5,30].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>represents the final output of the [class] token. CCT is built-up with the proposed cross-frame attention, as will be elaborated later. At last, the L m -layer Multi-frame Integration Transformer (MIT) takes all frame representation H = [h 1 , h 2 , ? ? ? , h T ] as input and outputs the video-level representation v as following:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>X-CLIP-B/32 adopts ViT-B/32 as parts of the cross-frame communication transformer, X-CLIP-B/16 uses ViT-B/16, while X-CLIP-L/14 employs ViT-L/14. For</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>a photo of action {label}; a picture of action {label}; Human action of {label}; {label}, an action; {label}, this is an action; {label}, a video of action; Playing action of {label}; {label}; Playing a kind of action, {label}; Doing a kind of action, {label}; Look, the human is {label}; Can you recognize the action of {label}; Video classification of {label}; A video of {label}; The man is {label}; The woman is {label}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art on Kinetics-400. We report the FLOPs and throughput per view. Throughput is measured using the GitHub repository of<ref type="bibr" target="#b28">[29]</ref> on a V100 GPU. * indicates pretraining with a video-text collection.</figDesc><table><row><cell>Method</cell><cell cols="7">Pretrain Frames Top-1 Top-5 Views FLOPs(G) Throughput</cell></row><row><cell cols="2">Methods with random initialization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv1-B, 64?3 [12]</cell><cell>-</cell><cell>64</cell><cell cols="3">81.2 95.1 3 ? 3</cell><cell>455</cell><cell>7</cell></row><row><cell cols="2">Methods with ImageNet pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uniformer-B [25]</cell><cell>IN-1k</cell><cell>32</cell><cell cols="3">83.0 95.4 4 ? 3</cell><cell>259</cell><cell>-</cell></row><row><cell>TimeSformer-L [5]</cell><cell>IN-21k</cell><cell>96</cell><cell cols="3">80.7 94.7 1 ? 3</cell><cell>2380</cell><cell>3</cell></row><row><cell>Mformer-HR [34]</cell><cell>IN-21k</cell><cell>16</cell><cell cols="3">81.1 95.2 10 ? 3</cell><cell>959</cell><cell>-</cell></row><row><cell>Swin-L [30]</cell><cell>IN-21k</cell><cell>32</cell><cell cols="3">83.1 95.9 4 ? 3</cell><cell>604</cell><cell>6</cell></row><row><cell>Swin-L (384?) [30]</cell><cell>IN-21k</cell><cell>32</cell><cell cols="3">84.9 96.7 10 ? 5</cell><cell>2107</cell><cell>-</cell></row><row><cell>MViTv2-L (312?) [27]</cell><cell>IN-21k</cell><cell>40</cell><cell cols="3">86.1 97.0 5 ? 3</cell><cell>2828</cell><cell>-</cell></row><row><cell cols="3">Methods with web-scale image pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViViT-H/16x2 [3]</cell><cell>JFT-300M</cell><cell>32</cell><cell cols="3">84.8 95.8 4 ? 3</cell><cell>8316</cell><cell>-</cell></row><row><cell cols="2">TokenLearner-L/10 [40] JFT-300M</cell><cell>-</cell><cell cols="3">85.4 96.3 4 ? 3</cell><cell>4076</cell><cell>-</cell></row><row><cell>CoVeR [56]</cell><cell>JFT-3B</cell><cell>-</cell><cell>87.2</cell><cell>-</cell><cell>1 ? 3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Methods with web-scale language-image pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ActionCLIP-B/16 [49] CLIP-400M 32</cell><cell cols="3">83.8 96.2 10 ? 3</cell><cell>563</cell><cell>-</cell></row><row><cell>A6 [21]</cell><cell cols="2">CLIP-400M 16</cell><cell cols="2">76.9 93.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MTV-H [54]</cell><cell>WTS  *</cell><cell>32</cell><cell cols="3">89.1 98.2 4 ? 3</cell><cell>3705</cell><cell>-</cell></row><row><cell>X-Florence (384?)</cell><cell>FLD-900M</cell><cell>8</cell><cell cols="3">86.2 96.6 4 ? 3</cell><cell>2114</cell><cell>6</cell></row><row><cell>X-Florence</cell><cell>FLD-900M</cell><cell>32</cell><cell cols="3">86.5 96.9 4 ? 3</cell><cell>2822</cell><cell>2</cell></row><row><cell>X-CLIP-B/16</cell><cell>IN-21k</cell><cell>8</cell><cell cols="3">81.1 94.7 4 ? 3</cell><cell>145</cell><cell>33</cell></row><row><cell>X-CLIP-B/32</cell><cell></cell><cell>8</cell><cell cols="3">80.4 95.0 4 ? 3</cell><cell>39</cell><cell>136</cell></row><row><cell>X-CLIP-B/32 X-CLIP-B/16 X-CLIP-B/16 X-CLIP-L/14 X-CLIP-L/14 (336?)</cell><cell>CLIP-400M</cell><cell>16 8 16 8 16</cell><cell cols="3">81.1 95.5 4 ? 3 83.8 96.7 4 ? 3 84.7 96.8 4 ? 3 87.1 97.6 4 ? 3 87.7 97.4 4 ? 3</cell><cell>75 145 287 658 3086</cell><cell>69 33 17 8 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art on Kinetics-600.</figDesc><table><row><cell>Method</cell><cell cols="7">Pretrain Frames Top-1 Top-5 Views FLOPs Throughput</cell></row><row><cell cols="2">Methods with random initialization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-B-24, 32?3 [12]</cell><cell>-</cell><cell>32</cell><cell cols="4">83.8 96.3 5 ? 1 236</cell><cell>-</cell></row><row><cell cols="2">Methods with ImageNet pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-L (384?) [30]</cell><cell>IN-21k</cell><cell>32</cell><cell cols="4">86.1 97.3 10 ? 5 2107</cell><cell>-</cell></row><row><cell cols="2">Methods with web-scale pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ViViT-L/16x2 320 [3] JFT-300M</cell><cell>32</cell><cell cols="4">83.0 95.7 4 ? 3 3992</cell><cell>-</cell></row><row><cell>ViViT-H/16x2 [3]</cell><cell>JFT-300M</cell><cell>32</cell><cell cols="4">85.8 96.5 4 ? 3 8316</cell><cell>-</cell></row><row><cell cols="2">TokenLearner-L/10 [40] JFT-300M</cell><cell>-</cell><cell cols="4">86.3 97.0 4 ? 3 4076</cell><cell>-</cell></row><row><cell>Florence (384?) [55]</cell><cell>FLD-900M</cell><cell>-</cell><cell cols="3">87.8 97.8 4 ? 3</cell><cell>-</cell><cell>-</cell></row><row><cell>CoVeR [56]</cell><cell>JFT-3B</cell><cell>-</cell><cell>87.9</cell><cell>-</cell><cell>1 ? 3</cell><cell>-</cell><cell>-</cell></row><row><cell>MTV-H [54]</cell><cell>WTS  *</cell><cell>32</cell><cell cols="4">89.6 98.3 4 ? 3 3705</cell><cell>-</cell></row><row><cell>X-CLIP-B/16</cell><cell></cell><cell>8</cell><cell cols="4">85.3 97.1 4 ? 3 145</cell><cell>74</cell></row><row><cell>X-CLIP-B/16</cell><cell>CLIP-400M</cell><cell>16</cell><cell cols="4">85.8 97.3 4 ? 3 287</cell><cell>40</cell></row><row><cell>X-CLIP-L/14</cell><cell></cell><cell>8</cell><cell cols="4">88.3 97.7 4 ? 3 658</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Zero-shot performances on HMDB51<ref type="bibr" target="#b23">[24]</ref> and UCF101<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell>Method</cell><cell>HMDB-51</cell><cell>UCF-101</cell></row><row><cell>MTE [53]</cell><cell cols="2">19.7 ? 1.6 15.8 ? 1.3</cell></row><row><cell>ASR [50]</cell><cell cols="2">21.8 ? 0.9 24.4 ? 1.0</cell></row><row><cell>ZSECOC [35]</cell><cell cols="2">22.6 ? 1.2 15.1 ? 1.7</cell></row><row><cell>UR [64]</cell><cell cols="2">24.4 ? 1.6 17.5 ? 1.6</cell></row><row><cell>TS-GCN [15]</cell><cell cols="2">23.2 ? 3.0 34.2 ? 3.1</cell></row><row><cell>E2E [6]</cell><cell>32.7</cell><cell>48</cell></row><row><cell>ER-ZSAR [8]</cell><cell cols="2">35.3 ? 4.6 51.8 ? 2.9</cell></row><row><cell cols="3">ActionCLIP [49] 40.8 ? 5.4 58.3 ? 3.4</cell></row><row><cell>X-CLIP-B/16</cell><cell cols="2">44.6 ? 5.2 72.0 ? 2.3 (+3.8) (+13.7)</cell></row><row><cell>X-Florence</cell><cell cols="2">48.4 ? 4.9 73.2 ? 4.2 (+7.6) (+14.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot performance on Kinetics-600<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc. Top-5 Acc.</cell></row><row><cell cols="2">DEVISE [14] 23.8 ? 0.3 51.0 ? 0.6</cell></row><row><cell>ALE [1]</cell><cell>23.4 ? 0.8 50.3 ? 1.4</cell></row><row><cell>SJE [2]</cell><cell>22.3 ? 0.6 48.2 ? 0.4</cell></row><row><cell cols="2">ESZSL [39] 22.9 ? 1.2 48.3 ? 0.8</cell></row><row><cell>DEM [57]</cell><cell>23.6 ? 0.7 49.5 ? 0.4</cell></row><row><cell>GCN [17]</cell><cell>22.3 ? 0.6 49.7 ? 0.6</cell></row><row><cell cols="2">ER-ZSAR [8] 42.1 ? 1.4 73.1 ? 0.3</cell></row><row><cell>X-CLIP-B/16</cell><cell>65.2 ? 0.4 86.1 ? 0.8 (+23.1) (+13.0)</cell></row><row><cell>X-Florence</cell><cell>68.8 ? 0.9 88.4 ? 0.6 (+26.7) (+15.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Few-shot results. Top-1 accuracy is reported with 32 frames.</figDesc><table><row><cell>Method</cell><cell cols="6">HMDB-51 K=2 K=4 K=8 K=16 K=2 K=4 K=8 K=16 UCF-101</cell></row><row><cell>TSM [28]</cell><cell>17.5</cell><cell>20.9</cell><cell>18.4</cell><cell>31.0</cell><cell>25.3</cell><cell>47.0 64.4 61.0</cell></row><row><cell cols="2">TimeSformer [5] 19.6</cell><cell>40.6</cell><cell>49.4</cell><cell>55.4</cell><cell>48.5</cell><cell>75.6 83.7 89.4</cell></row><row><cell>Swin-B [30]</cell><cell>20.9</cell><cell>41.3</cell><cell>47.9</cell><cell>56.1</cell><cell>53.3</cell><cell>74.1 85.8 88.7</cell></row><row><cell>X-CLIP-B/16</cell><cell cols="6">53.0 57.3 62.8 64.0 76.4 83.4 88.3 91.4 (+32.1) (+16.0) (+13.4) (+7.9) (+23.1) (+7.8) (+2.5) (+2.0)</cell></row><row><cell>X-Florence</cell><cell cols="6">51.6 57.8 64.1 64.2 84.0 88.5 92.5 94.8 (+30.7) (+16.5) (+14.7) (+8.1) (+30.7) (+12.9) (+6.7) (+5.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Component-wise analysis of our X-CLIP and other techniques.</figDesc><table><row><cell>Components</cell><cell>Top-1.(%)</cell></row><row><cell>Baseline(CLIP-Mean)</cell><cell>80.0</cell></row><row><cell cols="2">+ Cross-frame Communication 81.2(+1.2)</cell></row><row><cell>+ Multi-frame Integration</cell><cell>81.7(+1.7)</cell></row><row><cell>+ Video-specific Prompt</cell><cell>82.3(+2.3)</cell></row><row><cell>Techniques</cell><cell></cell></row><row><cell>+ 4?3-views Inference</cell><cell>83.8(+3.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on which part to finetune. ?means finetuning. The CUDA memory is calculated on 2 video inputs, each containing 8 frames.</figDesc><table><row><cell cols="3">Visual Text Zero. Few. Fully. Mem.(G)</cell></row><row><cell>?</cell><cell>? 72.9 54.6 82.4</cell><cell>22</cell></row><row><cell>?</cell><cell>? 70.0 50.8 82.3</cell><cell>6</cell></row><row><cell>?</cell><cell>? 66.8 53.4 79.3</cell><cell>20</cell></row><row><cell>?</cell><cell>? 64.2 47.3 79.1</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on the effect of the text information.</figDesc><table><row><cell cols="3">Method Zero-shot Few-shot</cell><cell>Fully.</cell></row><row><cell>w/o text</cell><cell>/</cell><cell>32.0</cell><cell>81.6</cell></row><row><cell>w/ text</cell><cell cols="3">70.0 50.8(+18.8) 82.3(+0.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparison with different prompting methods.</figDesc><table><row><cell>Method</cell><cell>Fully.</cell><cell>Few.</cell><cell>Zero.</cell></row><row><cell>w/o prompt</cell><cell>81.7</cell><cell>49.6</cell><cell>63.2</cell></row><row><cell>Ensemble. [37]</cell><cell>81.7</cell><cell>49.6</cell><cell>63.9</cell></row><row><cell>Vectors. [61]</cell><cell>82.0</cell><cell>49.9</cell><cell>63.2</cell></row><row><cell>Ours</cell><cell cols="3">82.3(+0.3) 50.8(+0.9) 70.0(+6.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Ablation study on the different pretraining.</figDesc><table><row><cell>Pretrain</cell><cell>Top-1. Top-5. (%) (%)</cell></row><row><cell cols="2">ImageNet-1k 75.9 90.2</cell></row><row><cell cols="2">ImageNet-21k 79.8 94.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Comparison of two sampling methods.</figDesc><table><row><cell cols="2">Test #F Train</cell><cell>multi-view ? single-view Dense Sparse</cell></row><row><cell>8</cell><cell cols="2">Dense 81.9 ? 77.8(-4.1) 82.4 ? 81.1(-1.3) Sparse 82.2 ? 77.3(-4.9) 83.4 ? 82.3(-1.1)</cell></row><row><cell>32</cell><cell cols="2">Dense 82.8 ? 78.8(-4.0) 83.2 ? 83.0(-0.2) Sparse 83.0 ? 77.9(-5.1) 84.4 ? 84.2(-0.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 :</head><label>1</label><figDesc>The training hyperparameters for all the experiments.</figDesc><table><row><cell></cell><cell cols="3">Fully-sup. Few-shot Zero-shot</cell></row><row><cell>Optimisation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell></cell><cell>AdamW</cell><cell></cell></row><row><cell>Optimizer betas</cell><cell></cell><cell>(0.9, 0.98)</cell><cell></cell></row><row><cell>Batch size</cell><cell>256</cell><cell>64</cell><cell>256</cell></row><row><cell>Learning rate schedule</cell><cell></cell><cell>cosine</cell><cell></cell></row><row><cell>Linear warmup epochs</cell><cell></cell><cell>5</cell><cell></cell></row><row><cell>Base learning rate</cell><cell>8e-6</cell><cell>2e-6</cell><cell>8e-6</cell></row><row><cell>Minimal learning rate</cell><cell>8e-8</cell><cell>2e-8</cell><cell>8e-8</cell></row><row><cell>Epochs</cell><cell>30</cell><cell>50</cell><cell>10</cell></row><row><cell>Data augmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RandomFlip</cell><cell></cell><cell>0.5</cell><cell></cell></row><row><cell>MultiScaleCrop</cell><cell cols="3">(1, 0.875, 0.75, 0.66)</cell></row><row><cell>ColorJitter</cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>GrayScale</cell><cell></cell><cell>0.2</cell><cell></cell></row><row><cell>Label smoothing</cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell>Mixup</cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>Cutmix</cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell>Other regularisation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Weight decay</cell><cell></cell><cell>0.001</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 :</head><label>2</label><figDesc>Comparison with different video encoders. The video encoders are adapted from ViT-B/16</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 3 :</head><label>3</label><figDesc>Comparison between the multi-modal framework and single-modal framework under ImageNet pretraining. ImageNet pretrained video encoder and the CLIP pretrained text encoder in Sec. 4.2. Last, we further evaluate our proposed cross-frame communication transformer and multi-frame integration transformer on a simple single-modality classification setting in Sec. 4.3.</figDesc><table><row><cell cols="3">Method Zero-shot Few-shot</cell><cell cols="3">Method Zero-shot Few-shot</cell></row><row><cell>w/o text</cell><cell>/</cell><cell>39.4</cell><cell>w/o text</cell><cell>/</cell><cell>10.8</cell></row><row><cell>w/ text</cell><cell>62.8</cell><cell>50.7(+11.3)</cell><cell>w/ text</cell><cell>58.0</cell><cell>46.0(+35.2)</cell></row><row><cell cols="3">(a) ImageNet-21k pretraining.</cell><cell cols="3">(b) ImageNet-1k pretraining.</cell></row></table><note>the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 4 :</head><label>4</label><figDesc>Evaluating the proposed architecture in the single-modality framework.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1(%) Top-5(%)</cell></row><row><cell>ViT-B/32-Mean</cell><cell>45.3</cell><cell>68.5</cell></row><row><cell cols="3">ViT-B/32 (Ours) 47.8(+2.5) 71.8(+3.3)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">M (l) = M (l) + CFA(LN(M (l) )),<ref type="bibr" target="#b6">(7)</ref> whereM(l)  = [m (l) 1 ,m (l) 2 , ? ? ? ,m(l)T ] and LN indicates layer normalization<ref type="bibr" target="#b3">[4]</ref>. Next, the intra-frame diffusion (IFA) takes the frame tokens with the associated message token to learn visual representation, while the involved message token could also diffuse global spatio-temporal dependencies for learning. Mathematically, this process at l-th block can be formulated as:[? (l) t ,m (l) t ] = [z (l?1) t ,m (l) t ] + IFA(LN([z (l?1) t ,m (l) t ])),(8)where [?, ?] concatenates the features of frame tokens and message tokens. Finally, the feed-forward network(FFN) performs on the frame tokens as:z (l) t =? (l) t + FFN(LN(? (l) t )).(9)</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expanding Language-Image Pretrained Models</head><p>for General Video Recognition ---Supplementary Material---This supplementary material contains additional details of the main manuscript, and provides more experiment analysis. In Sec. 1, we present the details of our proposed architectures and the comparison methods. Next, we elaborate the hyperparameters in Sec. 2. Then, we overview the four datasets and provide the evaluation protocols of our experiments in Sec. 3. Finally, we provide more experiment analysis in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Architecture Details</head><p>In this section, we elaborate the details of the proposed architectures in Sec. 1.1 and the compared architectures in the few-shot experiments in Sec. 1.2. , where L c denotes the layers, N h refers to the number of attention heads, d represents the embedding dimension and p is the patch size. We use a simple 1-layer multi-frame integration transformer for all three X-CLIP variants (L m =1, N h =8 for X-CLIP-B while N h =12 for X-CLIP-L). The text encoder is the same as in CLIP <ref type="bibr" target="#b36">[37]</ref>. For Florence, we replace the cross-frame communication transformer with the pretrained CoSwin-H <ref type="bibr" target="#b54">[55]</ref> visual encoder. We stack a 4-layer multi-frame integration transformer on top of CoSwin-H. The text encoder is the same as in Florence <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The proposed architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Other compared architectures</head><p>In few-shot experiments, we implemented the Video Swin <ref type="bibr" target="#b29">[30]</ref>, TSM <ref type="bibr" target="#b27">[28]</ref> and TimeSformer <ref type="bibr" target="#b4">[5]</ref> using MMAction2 <ref type="bibr" target="#b8">[9]</ref> library with the default hyperparameters. The TSM-R50 is initialized with ImageNet-1k pretraining, while the Video Swin-B and TimeSformer are initialized with ImageNet-21k pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hyperparameter Details</head><p>In this section, we present the elaborated training hyperparameters in Sec. 2.1 and the hand-craft prompt templates in the Tab. 9 of the main manuscript in Sec. 2.2.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding? In: ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="813" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking zero-shot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chalupka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4613" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<title level="m">A short note about kinetics-600</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Elaborative rehearsal for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13638" to="13647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Openmmlab&apos;s next generation video understanding toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmaction2" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="2121" to="2129" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8303" to="8311" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04544</idno>
		<title level="m">Clip-adapter: Better vision-language models with feature adapters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">All about knowledge graphs for actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12432</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Anticipative video transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Prompting visual-language models for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Uniformer: Unifying convolution and self-attention for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">Video swin transformer</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tam: Temporal adaptive module for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13708" to="13718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Asselmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Zero-shot action recognition with error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2833" to="2842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Denseclip: Language-guided dense prediction with context-aware prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tokenlearner: Adaptive space-time tokenization for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clap?s</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05991</idno>
		<title level="m">Video transformers: A survey</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning video representations using contrastive bidirectional transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<title level="m">Actionclip: A new paradigm for video action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Alternative semantic representations for zero-shot human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multi-task zero-shot action recognition with prioritised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Multiview transformers for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">Florence: A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Co-training transformer with videos and images improves action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07175</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03930</idno>
		<title level="m">Tipadapter: Training-free clip-adapter for better vision-language modeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pointclip: Point cloud understanding by clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01071</idno>
		<title level="m">Denseclip: Extract free dense labels from clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06567</idno>
		<title level="m">A comprehensive study of deep video action recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9436" to="9445" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
