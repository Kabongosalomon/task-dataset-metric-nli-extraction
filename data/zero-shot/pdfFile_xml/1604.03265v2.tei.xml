<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-theart methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multiresolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Statement</head><p>We consider volumetric representations of 3D point clouds or meshes as input to the 3D object classification problem. This is primarily inspired by recent advances in real-time scanning technology, which use volumetric data representations. We further assume that the input data is already pre-segmented by 3D bounding boxes. In practice, these bounding boxes can be extracted using the sliding windows, object proposals, or background subtraction. The output of the method is the category label of the volumetric data instance.</p><p>Approach We provide a detailed analysis over factors that influence the performance of volumetric CNNs, including network architecture and volumn resolution. Based upon our analysis, we strive to improve the performance of volumetric CNNs. We propose two volumetric CNN network architectures that signficantly improve state-of-the-art of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding 3D environments is a vital element of modern computer vision research due to paramount relevance in many vision systems, spanning a wide field of application scenarios from self-driving cars to autonomous robots. Recent advancements in real-time SLAM techniques and crowd-sourcing of virtual 3D models have additionally facilitated the availability of 3D data. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2]</ref>. This development has encouraged the lifting of 2D to 3D for deep learning, opening up new opportunities with the additional information of 3D data; e.g., aligning models is easier in 3D Euclidean space. In this paper, we specifically focus on the object classification task on 3D data obtained from both CAD models and commodity RGB-D sensors. In * indicates equal contributions. addition, we demonstrate retrieval results in the supplemental material.</p><p>While the extension of 2D convolutional neural networks to 3D seems natural, the additional computational complexity (volumetric domain) and data sparsity introduces significant challenges; for instance, in an image, every pixel contains observed information, whereas in 3D, a shape is only defined on its surface. Seminal work by Wu et al. <ref type="bibr" target="#b32">[33]</ref> propose volumetric CNN architectures on volumetric grids for object classification and retrieval. While these approaches achieve good results, it turns out that training a CNN on multiple 2D views achieves a significantly higher performance, as shown by Su et al. <ref type="bibr" target="#b31">[32]</ref>, who augment their 2D CNN with pre-training from ImageNet RGB data <ref type="bibr" target="#b5">[6]</ref>. These results indicate that existing 3D CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this work, we analyze these observations and evaluate the design choices. Moreover, we show how to reduce the gap between volumetric CNNs and multi-view CNNs by efficiently augmenting training data, introducing new CNN architectures in 3D. Finally, we examine multiview CNNs; our experiments show that we are able to improve upon state of the art with improved training data augmentation and a new multi-resolution component.</p><p>volumetric CNNs on 3D shape classification. This result has also closed the gap between volumetric CNNs and multi-view CNNs, when they are provided with 3D input discretized at 30 ? 30 ? 30 3D resolution. The first network introduces auxiliary learning tasks by classifying part of an object, which help to scrutize details of 3D objects more deeply. The second network uses long anisotropic kernels to probe for long-distance interactions. Combining data augmentation with a multi-orientation pooling, we observe significant performance improvement for both networks. We also conduct extensive experiments to study the influence of volume resolution, which sheds light on future directions of improving volumetric CNNs.</p><p>Furthermore, we introduce a new multi-resolution component to multi-view CNNs, which improves their already compelling performance.</p><p>In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of realworld 3D data, constructed using dense 3D reconstruction taken with <ref type="bibr" target="#b24">[25]</ref>. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Shape Descriptors A large variety of shape descriptors has been developed in the computer vision and graphics community. For instance, shapes can be represented as histograms or bag-of-feature models which are constructed from surface normals and curvatures <ref type="bibr" target="#b12">[13]</ref>. Alternatives include models based on distances, angles, triangle areas, or tetrahedra volumes <ref type="bibr" target="#b25">[26]</ref>, local shape diameters measured at densely-sampled surface points <ref type="bibr" target="#b2">[3]</ref>, Heat kernel signatures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, or extensions of SIFT and SURF feature descriptors to 3D voxel grids <ref type="bibr" target="#b17">[18]</ref>. The spherical harmonic descriptor (SPH) <ref type="bibr" target="#b16">[17]</ref> and the Light Field descriptor (LFD) <ref type="bibr" target="#b3">[4]</ref> are other popular descriptors. LFD extracts geometric and Fourier descriptors from object silhouettes rendered from several different viewpoints, and can be directly applied to the shape classification task. In contrast to recently developed feature learning techniques, these features are handcrafted and do not generalize well across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Neural Networks Convolutional Neural</head><p>Networks (CNNs) <ref type="bibr" target="#b20">[21]</ref> have been successfully used in different areas of computer vision and beyond. In particular, significant progress has been made in the context of learning features. It turns out that training from large RGB image datasets (e.g., ImageNet <ref type="bibr" target="#b5">[6]</ref>) is able to learn general purpose image descriptors that outperform handcrafted features for a number of vision tasks, including object detection, scene recognition, texture recognition and classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>. This significant improve-  ment in performance on these tasks has decidedly moved the field forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs on Depth and 3D Data</head><p>With the introduction of commodity range sensors, the depth channel became available to provide additional information that could be incorporated into common CNN architectures. A very first approach combines convolutional and recursive neural networks for learning features and classifying RGB-D images <ref type="bibr" target="#b29">[30]</ref>. Impressive performance for object detection from RGB-D images has been achieved using a geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity <ref type="bibr" target="#b10">[11]</ref>. Recently, a CNN architecture has been proposed where the RGB and depth data are processed in two separate streams; in the end, the two streams are combined with a late fusion network <ref type="bibr" target="#b7">[8]</ref>. All these descriptors operate on single RGB-D images, thus processing 2.5D data.</p><p>Wu et al. <ref type="bibr" target="#b32">[33]</ref> lift 2.5D to 3D with their 3DShapeNets approach by categorizing each voxel as free space, surface or occluded, depending on whether it is in front of, on, or behind the visible surface (i.e., the depth value) from the depth map. The resulting representation is a 3D binary voxel grid, which is the input to a CNN with 3D filter banks. Their method is particularly relevant in the context of this work, as they are the first to apply CNNs on a 3D representation. A similar approach is VoxNet <ref type="bibr" target="#b23">[24]</ref>, which also uses binary voxel grids and a corresponding 3D CNN architecture. The advantage of these approaches is that it can process different sources of 3D data, including LiDAR point clouds, RGB-D point clouds, and CAD models; we likewise follow this direction.</p><p>An alternative direction is to exploit established 2D CNN architectures; to this end, 2D data is extracted from the 3D representation. In this context, DeepPano <ref type="bibr" target="#b27">[28]</ref> converts 3D shapes into panoramic views; i.e., a cylinder projection around its principle axis. Current state-of-the-art uses multiple rendered views, and trains a CNN that can process all views jointly <ref type="bibr" target="#b31">[32]</ref>. This multi-view CNN (MVCNN) is pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref> and uses view-point pooling to combine all streams obtained from each view. A similar idea on stereo views has been proposed earlier <ref type="bibr" target="#b21">[22]</ref>. Multi-View CNN (standard rendering) <ref type="figure">Figure 2</ref>. Classification accuracy. Yellow and blue bars: Performance drop of multi-view CNN due to discretization of CAD models in rendering. Blue and green bars: Volumetric CNN is significantly worse than multi-view CNN, even though their inputs have similar amounts of information. This indicates that the network of the volumetric CNN is weaker than that of the multiview CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis of</head><p>Two representations of generic 3D shapes are popularly used for object classification, volumetric and multi-view ( <ref type="figure" target="#fig_1">Fig 1)</ref>. The volumetric representation encodes a 3D shape as a 3D tensor of binary or real values. The multi-view representation encodes a 3D shape as a collection of renderings from multiple viewpoints. Stored as tensors, both representations can easily be used to train convolutional neural networks, i.e., volumetric CNNs and multi-view CNNs.</p><p>Intuitively, a volumetric representation should encode as much information, if not more, than its multi-view counterpart. However, experiments indicate that multiview CNNs produce superior performance in object classification. <ref type="figure">Fig 2 reports</ref> the classification accuracy on the ModelNet40 dataset by state-of-the-art volumetric/multiview architectures <ref type="bibr" target="#b0">1</ref> . A volumetric CNN based on voxel occupancy (green) is 7.3% worse than a multi-view CNN (yellow).</p><p>We investigate this performance gap in order to ascertain how to improve volumetric CNNs. The gap seems to be caused by two factors: input resolution and network architecture differences. The multi-view CNN downsamples each rendered view to 227 ? 227 pixels (Multiview Standard Rendering in <ref type="figure" target="#fig_1">Fig 1)</ref>; to maintain a similar computational cost, the volumetric CNN uses a 30?30?30 occupancy grid (Volumetric Occupancy Grid in <ref type="figure" target="#fig_1">Fig 1)</ref> 2 . As shown in <ref type="figure" target="#fig_1">Fig 1,</ref> the input to the multi-view CNN captures more detail. However, the difference in input resolution is not the primary reason for this performance gap, as evidenced by further experiments. We compare the two networks by providing them with data containing similar level of detail. To this end, we feed the multi-view CNN with renderings of the 30 ? 30 ? 30 occupancy grid using sphere rendering 3 , i.e., for each occupied voxel, a ball is placed at its center, with radius equal to the edge length of a voxel (Multi-View Sphere Rendering in <ref type="figure" target="#fig_1">Fig 1)</ref>. We train the multi-view CNN from scratch using these sphere renderings. The accuracy of this multi-view CNN is reported in blue.</p><p>As shown in <ref type="figure">Fig 2,</ref> even with similar level of object detail, the volumetric CNN (green) is 4.8% worse than the multi-view CNN (blue). That is, there is still significant room to improve the architecture of volumetric CNNs. This discovery motivates our efforts in Sec 4 to improve volumetric CNNs. Additionally, low-frequency information in 3D seems to be quite discriminative for object classification-it is possible to achieve 89.5% accuracy (blue) at a resolution of only 30 ? 30 ? 30. This discovery motivates our efforts in Sec 5 to improve multi-view CNNs with a 3D multi-resolution approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Volumetric Convolutional Neural Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>We improve volumetric CNNs through three separate means: 1) introducing new network structures; 2) data augmentation; 3) feature pooling.</p><p>Network Architecture We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data. The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multiview CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.</p><p>In order to mitigate overfitting from too many parameters, we adopt the mlpconv layer from <ref type="bibr" target="#b22">[23]</ref> as our basic building block in both network variations.  . The main innovation is that we add auxiliary tasks to predict class labels that focus on part of an object, intended to drive the CNN to more heavily exploit local discriminative features. An mlpconv layer is a composition of three conv layers interleaved by ReLU layers. The five numbers under mlpconv are the number of channels, kernel size and stride of the first conv layer, and the number of channels of the second and third conv layers, respectively. The kernel size and stride of the second and third conv layers are 1. For example, mlpconv(48, 6, 2; 48; 48) is a composition of conv(48, 6, 2), ReLU, conv(48, 1, 1), ReLU, conv(48, 1, 1) and ReLU layers. Note that we add dropout layers with rate=0.5 after fully connected layers.</p><p>Data Augmentation Compared with 2D image datasets, currently available 3D shape datasets are limited in scale and variation. To fully exploit the design of our networks, we augment the training data with different azimuth and elevation rotations. This allows the first network to cover local regions at different orientations, and the second network to relate distant points at different relative angles.</p><p>Multi-Orientation Pooling Both of our new networks are sensitive to shape orientation, i.e., they capture different information at different orientations. To capture a more holistic sense of a 3D object, we add an orientation pooling stage that aggregates information from different orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network 1: Auxiliary Training by Subvolume Supervision</head><p>We observe significant overfitting when we train the volumetric CNN proposed by <ref type="bibr" target="#b32">[33]</ref> in an end-to-end fashion (see supplementary). When the volumetric CNN overfits to the training data, it has no incentive to continue learning. We thus introduce auxiliary tasks that are closely correlated with the main task but are difficult to overfit, so that learning continues even if our main task is overfitted.</p><p>These auxiliary training tasks also predict the same object labels, but the predictions are made solely on a local subvolume of the input. Without complete knowledge of the object, the auxiliary tasks are more challenging, and can thus better exploit the discriminative power of local regions. This design is different from the classic multitask learning setting of hetergenous auxiliary tasks, which inevitably requires collecting additional annotations (e.g., conducting both object classification and detection <ref type="bibr" target="#b8">[9]</ref>).</p><p>We implement this design through an architecture shown in <ref type="figure" target="#fig_3">Fig 3.</ref> The first three layers are mlpconv (multilayer perceptron convolution) layers, a 3D extension of the 2D mlpconv proposed by <ref type="bibr" target="#b22">[23]</ref>. The input and output of our mlpconv layers are both 4D tensors. Compared with the standard combination of linear convolutional layers and max pooling layers, mlpconv has a three-layer structure and is thus a universal function approximator if enough neurons are provided in its intermediate layers. Therefore, mlpconv is a powerful filter for feature extraction of local patches, enhancing approximation of more abstract representations. In addition, mlpconv has been validated to be more discriminative with fewer parameters than ordinary convolution with pooling <ref type="bibr" target="#b22">[23]</ref>.</p><p>At the fourth layer, the network branches into two. The lower branch takes the whole object as input for traditional classification. The upper branch is a novel branch for auxiliary tasks. It slices the 512 ? 2 ? 2 ? 2 4D tensor (2 grids along x, y, z axes and 512 channels) into 2?2?2 = 8 vectors of dimension 512. We set up a classification task for each vector. A fully connected layer and a softmax layer are then appended independently to each vector to construct classification losses. Simple calculation shows that the receptive field of each task is 22 ? 22 ? 22, covering roughly 2/3 of the entire volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network 2: Anisotropic Probing</head><p>The success of multi-view CNNs is intriguing. multiview CNNs first project 3D objects to 2D and then make use of well-developed 2D image CNNs for classification. Inspired by its success, we design a neural network archi- Image-based CNN (Network In Network) <ref type="figure">Figure 4</ref>. CNN with Anisotropic Probing kernels. We use an elongated kernel to convolve the 3D cube and aggregate information to a 2D plane. Then we use a 2D NIN (NIN-CIFAR10 <ref type="bibr" target="#b22">[23]</ref>) to classify the 2D projection of the original 3D shape. tecture that is also composed of the two stages. However, while multi-view CNNs use external rendering pipelines from computer graphics, we achieve the 3D-to-2D projection using network layers in a manner similar to 'X-ray scanning'.</p><p>Key to this network is the use of an elongated anisotropic kernel which helps capture the global structure of the 3D volume. As illustrated in <ref type="figure">Fig 4,</ref> the neural network has two modules: an anisotropic probing module and a network in network module. The anisotropic probing module contains three convolutional layers of elongated kernels, each followed by a nonlinear ReLU layer. Note that both the input and output of each layer are 3D tensors.</p><p>In contrast to traditional isotropic kernels, an anisotropic probing module has the advantage of aggregating longrange interactions in the early feature learning stage with fewer parameters. As a comparison, with traditional neural networks constructed from isotropic kernels, introducing long-range interactions at an early stage can only be achieved through large kernels, which inevitably introduce many more parameters. After anisotropic probing, we use an adapted NIN network <ref type="bibr" target="#b22">[23]</ref> to address the classification problem.</p><p>Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.</p><p>In addition, this architecture is scalable to higher resolutions, since all its layers can be viewed as 2D. While 3D convolution involves computation at locations of cubic resolution, we maintain quadratic compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Data Augmentation and Multi-Orientation Pooling</head><p>The two networks proposed above are both sensitive to model orientation. In the subvolume supervision method, different model orientations define different local subvolumes; in the anisotropic probing method, only voxels of the same height and along the probing direction can have interaction in the early feature extraction stage. Thus it is helpful to augment the training data by varying object orientation and combining predictions through orientation pooling.</p><p>Similar to Su-MVCNN <ref type="bibr" target="#b31">[32]</ref> which aggregates information from multiple view inputs through a view-pooling layer and follow-on fully connected layers, we sample 3D input from different orientations and aggregate them in a multi-orientation volumetric CNN (MO-VCNN) as shown in <ref type="figure" target="#fig_6">Fig 5.</ref> At training time, we generate different rotations of the 3D model by changing both azimuth and elevation angles, sampled randomly. A volumetric CNN is firstly trained on single rotations. Then we decompose the network to CNN 1 (lower layers) and CNN 2 (higher layers) to construct a multi-orientation version. The MO-VCNN's weights are initialized by a previously trained volumetric CNN with CNN 1 's weights fixed during fine-tuning. While a common practice is to extract the highest level features (features before the last classification linear layer) of multiple orientations, average/max/concatenate them, and train a linear SVM on the combined feature, this is just a special case of the MO-VCNN.</p><p>Compared to 3DShapeNets <ref type="bibr" target="#b32">[33]</ref> which only augments data by rotating around vertical axis, our experiment shows that orientation pooling combined with elevation rotation  can greatly increase performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Multi-View Convolutional Neural Networks</head><p>The multi-view CNN proposed by <ref type="bibr" target="#b31">[32]</ref> is a strong alternative to volumetric representations. This multi-view representation is constructed in three steps: first, a 3D shape is rendered into multiple images using varying camera extrinsics; then image features (e.g. conv5 feature in VGG or AlexNet) are extracted for each view; lastly features are combined across views through a pooling layer, followed by fully connected layers.</p><p>Although the multi-view CNN presented by <ref type="bibr" target="#b31">[32]</ref> produces compelling results, we are able to improve its performance through a multi-resolution extension with improved data augmentation. We introduce multi-resolution 3D filtering to capture information at multiple scales. We perform sphere rendering (see Sec 3) at different volume resolutions. Note that we use spheres for this discretization as they are view-invariant. In particular, this helps regularize out potential noise or irregularities in real-world scanned data (relative to synthetic training data), enabling robust performance on real-world scans. Note that our 3D multiresolution filtering is different from classical 2D multiresolution approaches, since the 3D filtering respects the distance in 3D.</p><p>Additionally, we also augment training data with variations in both azimuth and elevation, as opposed to azimuth only. We use AlexNet instead of VGG for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate our volumetric CNNs and multi-view CNNs along with current state of the art on the ModelNet dataset <ref type="bibr" target="#b32">[33]</ref> and a new dataset of real-world reconstructions of 3D objects.</p><p>For convenience in following discussions, we define 3D resolution to be the discretization resolution of a 3D shape. That is, a 30 ? 30 ? 30 volume has 3D resolution 30. The sphere rendering from this volume also has 3D resolution 30, though it may have higher 2D image resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>ModelNet We use ModelNet <ref type="bibr" target="#b32">[33]</ref> for our training and testing datasets. ModelNet currently contains 127, 915 3D CAD models from 662 categories. ModelNet40, a subset including 12, 311 models from 40 categories, is well annotated and can be downloaded from the web. The authors also provide a training and testing split on the website, in which there are 9, 843 training and 2, 468 test models <ref type="bibr" target="#b3">4</ref> . We <ref type="bibr" target="#b3">4</ref> VoxNet <ref type="bibr" target="#b23">[24]</ref> uses the train/test split provided on the website and report average class accuracy on the 2, 468 test split. 3DShapeNets <ref type="bibr" target="#b32">[33]</ref> and MVCNN <ref type="bibr" target="#b31">[32]</ref> use another train/test split comprising the first 80 shapes of each category in the "train" folder (or all shapes if there are fewer than 80) and the first 20 shapes of each category in the "test" folder, respectively.  use this train/test split for our experiments. By default, we report classification accuracy on all models in the test set (average instance accuracy). For comparisons with previous work we also report average class accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-world Reconstructions</head><p>We provide a new realworld scanning dataset benchmark, comprising 243 objects of 12 categories; the geometry is captured with an ASUS Xtion Pro and a dense reconstruction is obtained using the publicly-available VoxelHashing framework <ref type="bibr" target="#b24">[25]</ref>. For each scan, we have performed a coarse, manual segmentation of the object of interest. In addition, each scan is aligned with the world-up vector. While there are existing datasets captured with commodity range sensors -e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b30">31]</ref> -this is the first containing hundreds of annotated models from dense 3D reconstructions. The goal of this dataset is to provide an example of modern real-time 3D reconstructions; i.e., structured representations more complete than a single RGB-D frame but still with many occlusions. This dataset is used as a test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison with State-of-the-Art Methods</head><p>We compare our methods with state of the art for shape classification on the ModelNet40 dataset. In the following, we discuss the results within volumetric CNN methods and within multi-view CNN methods. Data augmentation is applied as described in Sec 6.4 (azimuth and elevation rotations). For clarity, we use MOto denote that both networks are trained with an additional multi-orientation pooling step (20 orientations in practice). For reference of multi-view CNN performance at the same  3D resolution, we also include Ours-MVCNN-Sphere-30, the result of our multi-view CNN with sphere rendering at 3D resolution 30. More details of setup can be found in the supplementary.</p><p>As can be seen, both of our proposed volumetric CNNs significantly outperform state-of-the-art volumetric CNNs. Moreover, they both match the performance of our multiview CNN under the same 3D resolution. That is, the gap between volumetric CNNs and multi-view CNNs is closed under 3D resolution 30 on ModelNet40 dataset, an issue that motivates our study (Sec 3).  study the effect of 3D resolution for both types of networks. <ref type="figure">Fig 9</ref> shows the performance of our volumetric CNN and multi-view CNN at different 3D resolutions (defined at the beginning of Sec 6). Due to computational cost, we only test our volumetric CNN at 3D resolutions 10 and 30. The observations are: first, the performance of our volumetric CNN and multi-view CNN is on par at tested 3D resolutions; second, the performance of multiview CNN increases as the 3D resolution grows up. To further improve the performance of volumetric CNN, this experiment suggests that it is worth exploring how to scale volumetric CNN to higher 3D resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view CNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Effect of 3D Resolution over Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">More Evaluations Data Augmentation and Multi-Orientation Pooling</head><p>We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets <ref type="bibr" target="#b32">[33]</ref>, to train and test on three variations of augmented data (  <ref type="table" target="#tab_2">Table 1</ref>. Effects of data augmentations on multi-orientation volumetric CNN. We report numbers of classification accuracy on ModelNet40, with (Multi-Ori) or without (Single-Ori) multiorientation pooling described in Sec 4.4.</p><p>When combined with multi-orientation pooling, applying both azimuth rotation (AZ) and elevation rotation (EL) augmentations is extremely effective. Using only azimuth augmentation (randomly sampled from 0 ? to 360 ? ) with orientation pooling, the classification performance is increased by 86.1% ? 84.7% = 1.4%; combined with eleva-Network Single-Ori Multi-Ori E2E- <ref type="bibr" target="#b32">[33]</ref> 83.0 87.8 VoxNet <ref type="bibr" target="#b23">[24]</ref> 83.8 85.9 3D-NIN 86.1 88.5 Ours-SubvolumeSup 87.2 89.2 Ours-AniProbing 85.9 89.9 <ref type="table">Table 2</ref>. Comparison of performance of volumetric CNN architectures. Numbers reported are classification accuracy on Model-Net40. Results from E2E- <ref type="bibr" target="#b32">[33]</ref> (end-to-end learning version) and VoxNet <ref type="bibr" target="#b23">[24]</ref> are obtained by ourselves. All experiments are using the same set of azimuth and elevation augmented data. tion augmentation (randomly sampled from ?45 ? to 45 ? ), the improvement becomes more significant -increasing by 87.8% ? 83.0% = 4.8%. On the other hand, translation jittering (randomly sampled shift from 0 to 6 voxels in each direction) provides only marginal influence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Volumetric CNN Architectures</head><p>The architectures in comparison include VoxNet <ref type="bibr" target="#b23">[24]</ref>, E2E- <ref type="bibr" target="#b32">[33]</ref> (the end-to-end learning variation of <ref type="bibr" target="#b32">[33]</ref> implemented in Caffe <ref type="bibr" target="#b15">[16]</ref> by ourselves), 3D-NIN (a 3D variation of Network in Network <ref type="bibr" target="#b22">[23]</ref> designed by ourselves as in <ref type="figure" target="#fig_3">Fig 3  without</ref> the "Prediction by partial object" branch), Subvol-umeSup (Sec 4.2) and AniProbing (Sec 4.3). Data augmentation of AZ+EL (Sec 6.4) are applied.</p><p>From <ref type="table">Table 2</ref>, first, the two volumetric CNNs we propose, SubvolumeSup and AniProbing networks, both show superior performance, indicating the effectiveness of our design; second, multi-orientation pooling increases performance for all network variations. This is especially significant for the anisotropic probing network, since each orientation usually only carries partial information of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Multi-view Methods</head><p>We compare different methods that are based on multi-view representations in <ref type="table">Table 3</ref>  <ref type="table">Table 3</ref>. Comparison of multi-view based methods. Numbers reported are classification accuracy (class average and instance average) on ModelNet40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Classification Retrieval MAP E2E- <ref type="bibr" target="#b32">[33]</ref> 69.6 -Su-MVCNN <ref type="bibr" target="#b31">[32]</ref> 72 of ModelNet40 containing 3,183 training samples. They are provided for reference. Also note that the MVCNNs in the second group are our implementations in Caffe with AlexNet instead of VGG as in Su-MVCNN <ref type="bibr" target="#b31">[32]</ref>.</p><p>We observe that MVCNNs are superior to methods by SVMs on hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on the Real-World Reconstruction Dataset</head><p>We further assess the performance of volumetric CNNs and multi-view CNNs on real-world reconstructions in <ref type="table" target="#tab_4">Table 4</ref>. All methods are trained on CAD models in ModelNet40 but tested on real data, which may be highly partial, noisy, or oversmoothed ( <ref type="figure" target="#fig_8">Fig 6)</ref>. Our networks continue to outperform state-of-the-art results. In particular, our 3D multiresolution filtering is quite effective on real-world data, possibly because the low 3D resolution component filters out spurious and noisy micro-structures. Example results for object retrieval can be found in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future work</head><p>In this paper, we have addressed the task of object classification on 3D data using volumetric CNNs and multi-view CNNs. We have analyzed the performance gap between volumetric CNNs and multi-view CNNs from perspectives of network architecture and 3D resolution. The analysis motivates us to propose two new architectures of volumetric CNNs, which outperform state-of-the-art volumetric CNNs, achieving comparable performance to multi-view CNNs at the same 3D resolution of 30 ? 30 ? 30. Further evalution over the influence of 3D resolution indicates that 3D resolution is likely to be the bottleneck for the performance of volumetric CNNs. Therefore, it is worth exploring the design of efficient volumetric CNN architectures that scale up to higher resolutions.</p><p>[34] J. Xiao, A. Owens, and A. Torralba. Sun3d: A database of big spaces reconstructed using sfm and object labels. In ICCV 2013, pages 1625-1632. IEEE, 2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this section, we present positive effects of two addson modules -volumetric batch normalization (Sec A.1) and spatial transformer networks (Sec A.2). We also provide more details on experiments in the main paper (Sec A.3) and real-world dataset construction (Sec A.4). Retrieval results can also be found in Sec A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Batch Normalization</head><p>We observe that using batch normalization <ref type="bibr" target="#b13">[14]</ref> can accelerate the training process and also improve final performance. Taking our subvolume supervision model (base network is 3D-NIN) for example, the classification accuracy from single orientation is 87.2% and 88.8% before and after using batch normalization, respectively. Complete results are in <ref type="table">Table 5</ref>.</p><p>Specifically, compared with the model described in the main paper, we add batch normalization layers after each convolution and fully connected layers. We also add dropout layers after each convolutional layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Spatial Transformer Networks</head><p>One disadvantage of multi-view/orientation method is that one needs to prepare multiple views/orientations of the 3D data, thus computationally more expensive. It would be ideal if we can achieve similar performance with just a single input. In this section we show how a Spatial Transformer Network (STN) <ref type="bibr" target="#b14">[15]</ref> can help boost our model's performance on single-orientation input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Single-Ori Ours-SubvolSup + BN 88.8 Ours-SubvolSup + BN + STN 89.1 <ref type="table">Table 6</ref>. Spatial transformer network helps improve single orientation classification accuracy.</p><p>The spatial transformer network has three components: (1) a regressor network which takes occupancy grid as input and predicts transformation parameters. (2) a grid generator that outputs a sampling grid based on the transformation and (3) a sampler that transforms the input volume to a new volume based on the sampling grid. We include a spatial transfomer network directly after the data layer and before the original volumetric CNN (see <ref type="table">Table 6</ref> for results).</p><p>In <ref type="figure" target="#fig_1">Fig 10,</ref> we visualize the effect of spatial transformer network on some exemplar input occupancy grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input occupancy grid:</head><p>Output from STN: <ref type="figure" target="#fig_1">Figure 10</ref>. Each row is a input and output pair of the spatial transformer netowrk ('table' category). Each point represents an occupied voxel and color is determined by depth. We see STN tends to align all the tables to a canonical viewpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Details on Model Training</head><p>Training for Our Volumetric CNNs To produce occupancy grids from meshes, the faces of a mesh are subdivided until the length of the longest edge is within a single voxel; then all voxels that intersect with a face are marked as occupied. For 3D resolution 10,30 and 60 we generate voxelizations with central regions 10, 24, 54 and padding 0, 3, 3 respectively. This voxelization is followed by a hole filling step that fills the holes inside the models as occupied voxels.</p><p>To augment our training data with azimuth and elevation rotations, we generate 60 voxelizations for each model, with azimuth uniformly sampled from [0, 360] and elevation uniformly sampled from [?45, 45] (both in degrees).</p><p>We use a Nesterov solver with learning rate 0.005 and weight decay 0.0005 for training. It takes around 6 hours to train on a K40 using Caffe <ref type="bibr" target="#b15">[16]</ref> for the subvolume supervision CNN and 20 hours for the anisotropic probing CNN. For multi-orientation versions of them, Subvolume-Sup splits at the last conv layer and AniProbing splits at the second last conv layer. Volumetric CNNs trained on single orientation inputs are then used to initialize their multiorientation version for fine tuning.</p><p>During testing time, 20 orientations of a CAD model occupancy grid (equally distributed azimuth and uniformly sampled elevation from [?45, 45]) are input to MO-VCNN to make a class prediction.</p><p>Training for Our MVCNN and Multi-resolution MVCNN We use Blender to render 20 views of each (either ordinary or spherical) CAD model from azimuth angles in 0, 36, 72, ..., 324 degrees and elevation angles in ?30 and 30 degrees. For sphere rendering, we convert voxelized CAD models into meshes by replacing each voxel with an approximate sphere with 50 faces and diameter length of the voxel size. Four fixed point light sources are used for the ray-tracing rendering.</p><p>We first finetune AlexNet with rendered images for ordinary rendering and multi-resolutional sphere renderings separately. Then we use trained AlexNet to initialize the MVCNN and fine tune on multi-view inputs.</p><p>Other Volumetric Data Representations Note that while we present our volumetric CNN methods using occupancy grid representations of 3D objects, our approaches easily generalize to other volumetric data representations. In particular, we have also used Signed Distance Functions and (unsigned) Distance Functions as input (also 30 ? 30 ? 30 grids). Signed distance fields were generated through virtual scanning of synthetic training data, using volumetric fusion (for our real-world reconstructed models, this is the natural representation); distance fields were generated directly from the surfaces of the models. Performance was not affected significantly by the different representations, differing by around 0.5% to 1.0% for classification accuracy on ModelNet test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Real-world Reconstruction Test Data</head><p>In order to evaluate our method on real scanning data, we obtain a dataset of 3D models, which we reconstruct using data from a commodity RGB-D sensor (ASUS Xtion Pro). To this end, we pick a variety of real-world objects for which we record a short RGB-D frame sequence (several hundred frames) for each instance. For each object, we use the publicly-available Voxel Hashing framework in order to obtain a dense 3D reconstruction. In a semi-automatic post-processing step, we segment out the object of interest's geometry by removing the scene background. In addition, we align the obtained model with the world up direction. Overall, we obtained scans of 243 objects, comprising of a total of over XYZ thousand RGB-D input frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. More Retrieval Results</head><p>For model retrieval, we extract CNN features (either from 3D CNNs or MVCNNs) from query models and find nearest neighbor results based on L2 distance. Similar to MVCNN (Su et al.) <ref type="bibr" target="#b31">[32]</ref>, we use a low-rank Mahalanobis metric to optimize retrieval performance. <ref type="figure" target="#fig_1">Figure 11</ref> and <ref type="figure" target="#fig_1">Figure 12</ref> show more examples of retrieval from real model queries.    <ref type="table" target="#tab_2">Table  18</ref> Toilet 17 <ref type="bibr">Figure 13</ref>. Our real-world reconstruction test dataset, comprising 12 categories and 243 models. Each row lists a category along with the number of objects and several example reconstructed models in that category.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>3D shape representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Auxiliary Training by Subvolume Supervision (Sec 4.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Left: Volumetric CNN (single orientation input). Right: Multi-orientation volumetric CNN (MO-VCNN), which takes in various orientations of the 3D input, extracts features from shared CNN1 and then pass pooled feature through another network CNN2 to make a prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Example models from our real-world dataset. Each model is a dense 3D reconstruction, annotated, and segmented from the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Volumetric CNNs Fig 7</head><label>7</label><figDesc>summarizes the performance of volumetric CNNs. Ours-MO-SubvolumeSup is the subvolume supervision network in Sec 4.2 and Ours-MO-AniProbing is the anistropic probing network in Sec 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Classification acurracy on ModelNet40 (multi-view representation). The 3D multi-resolution version is the strongest. It is worth noting that the simple baseline HoGPyramid-LFD performs quite well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig 8 summarizes the performance of multi-view CNNs. Ours-MVCNN-MultiRes is the result by training an SVM over the concatenation of fc7 features from Ours-MVCNN-Sphere-30, 60, and Ours-MVCNN. HoGPyramid-LFD is the result by training an SVM over a concatenation of HoG features at three 2D resolutions. Here LFD (lightfield descriptor) simply refers to extracting features from renderings. Ours-MVCNN-MultiRes achieves state-of-the-art.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Sec 6 .Figure 9 .</head><label>69</label><figDesc>2 shows that our volumetric CNN and multi-view CNN performs comparably at 3D resolution 30. Here we Top: sphere rendering at 3D resolution 10, 30, 60, and standard rendering. Bottom: performance of image-based CNN and volumetric CNN with increasing 3D resolution. The two rightmost points are trained/tested from standard rendering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>More retrieval results. Left column: queries, real reconstructed meshes. Right five columns: retrieved models from ModelNet40 Test800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .</head><label>12</label><figDesc>More retrieval results (samples with mistakes). Left column: queries, real reconstructed meshes. Right five columns: retrieved models from ModelNet40 Test800. Red bounding boxes denote results from wrong categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 )</head><label>1</label><figDesc>. Similar trend is observed for other volumetric CNN variations.</figDesc><table><row><cell>Data Augmentation</cell><cell cols="2">Single-Ori Multi-Ori</cell><cell>?</cell></row><row><cell>Azimuth rotation (AZ)</cell><cell>84.7</cell><cell>86.1</cell><cell>1.4</cell></row><row><cell>AZ + translation</cell><cell>84.8</cell><cell>86.1</cell><cell>1.3</cell></row><row><cell>AZ + elevation rotation</cell><cell>83.0</cell><cell>87.8</cell><cell>4.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. Methods in the second group are trained on the full ModelNet40 train set. Methods in the first group, SPH, LFD, FV, and Su-MVCNN, are trained on a subset</figDesc><table><row><cell>Method</cell><cell>#Views</cell><cell>Accuracy (class)</cell><cell>Accuracy (instance)</cell></row><row><cell>SPH (reported by [33])</cell><cell>-</cell><cell>68.2</cell><cell>-</cell></row><row><cell>LFD (reported by [33])</cell><cell>-</cell><cell>75.5</cell><cell>-</cell></row><row><cell>FV (reported by [32])</cell><cell>12</cell><cell>84.8</cell><cell>-</cell></row><row><cell>Su-MVCNN [32]</cell><cell>80</cell><cell>90.1</cell><cell>-</cell></row><row><cell>PyramidHoG-LFD</cell><cell>20</cell><cell>87.2</cell><cell>90.5</cell></row><row><cell>Ours-MVCNN</cell><cell>20</cell><cell>89.7</cell><cell>92.0</cell></row><row><cell>Ours-MVCNN-MultiRes</cell><cell>20</cell><cell>91.4</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Classification accuracy and retrieval MAP on reconstructed meshes of 12-class real-world scans.</figDesc><table><row><cell>.4</cell><cell>35.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table 5. Positive effect of adding batch normalization at convolutional layers. Numbers reported are classification (instace average) on ModelNet40 test set.</figDesc><table><row><cell>Model</cell><cell cols="2">Single-Ori Multi-Ori</cell></row><row><cell>Ours-SubvolSup</cell><cell>87.2</cell><cell>89.2</cell></row><row><cell>Ours-AniProbing</cell><cell>85.9</cell><cell>89.9</cell></row><row><cell>Ours-SubvolSup + BN</cell><cell>88.8</cell><cell>90.1</cell></row><row><cell>Ours-AniProbing + BN</cell><cell>87.5</cell><cell>90.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Class # of Models Bathtub 7 Bed 27 Bench 19 Chair 17 Cup 18 Desk 16 Dresser 12 Monitor 45 Night- stand 21 Sofa 26</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We train models by replicating the architecture of<ref type="bibr" target="#b32">[33]</ref> for volumetric CNNs and<ref type="bibr" target="#b31">[32]</ref> for multi-view CNNs. All networks are trained in an endto-end fashion. All methods are trained/tested on the same split for fair comparison. The reported numbers are average instance accuracy. See Sec 6 for details.<ref type="bibr" target="#b1">2</ref> Note that 30 ? 30 ? 30 ? 227 ? 227.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It is computationally prohibitive to match the volumetric CNN resolution to multi-view CNN, which would be 227 ? 227 ? 227.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape google: Geometric words and expressions for invariant shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data-driven suggestions for creativity support in 3d modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">183</biblScope>
			<date type="published" when="2010" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On visual similarity based 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGF</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2009</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for robust rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extended gaussian images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1671" to="1686" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3 d shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SGP 2003</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hough transform and 3d surf for robust three dimensional classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="589" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bronstein. Intrinsic shape context descriptors for deformable shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2012</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time 3d reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="807" to="832" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
	<note>2014 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeppano: Deep panoramic representation for 3-d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
