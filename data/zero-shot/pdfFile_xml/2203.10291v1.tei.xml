<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SmartMore Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiaoguang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SmartMore Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video frame interpolation</term>
					<term>motion ambiguity</term>
					<term>cross-scale alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For video frame interpolation (VFI), existing deeplearning-based approaches strongly rely on the ground-truth (GT) intermediate frames, which sometimes ignore the nonunique nature of motion judging from the given adjacent frames. As a result, these methods tend to produce averaged solutions that are not clear enough. To alleviate this issue, we propose to relax the requirement of reconstructing an intermediate frame as close to the GT as possible. Towards this end, we develop a texture consistency loss (TCL) upon the assumption that the interpolated content should maintain similar structures with their counterparts in the given frames. Predictions satisfying this constraint are encouraged, though they may differ from the predefined GT. Without the bells and whistles, our plug-and-play TCL is capable of improving the performance of existing VFI frameworks. On the other hand, previous methods usually adopt the cost volume or correlation map to achieve more accurate image/feature warping. However, the O(N 2 ) (N refers to the pixel count) computational complexity makes it infeasible for highresolution cases. In this work, we design a simple, efficient (O(N )) yet powerful cross-scale pyramid alignment (CSPA) module, where multi-scale information is highly exploited. Extensive experiments justify the efficiency and effectiveness of the proposed strategy.</p><p>Compared to state-of-the-art VFI algorithms, our method boosts the PSNR performance by 0.66dB on the Vimeo-Triplets dataset and 1.31dB on the Vimeo90K-7f dataset. In addition, our method is easily extended to the video frame extrapolation task. Surprisingly, our extrapolation model has achieved a 0.91dB PSNR gain over FLAVR under the same experimental setting, while being 2? times smaller in terms of the model size. At last, we show that our high-quality interpolated frames are also beneficial to the development of the video super-resolution task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO frame interpolation (VFI) plays a critical role in computer vision with numerous applications, such as video editing and novel view synthesis. Unlike other vision tasks that heavily rely on human annotations, VFI benefits from the abundant off-the-shelf videos to generate high-quality training data. The recent years have witnessed the rapid development of VFI empowered by the success of deep neural networks. The popular approaches can be roughly divided into two categories: 1) optical-flow-based methods <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b18">[19]</ref> and 2) kernel-regression-based algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref>.</p><p>The optical-flow-based methods typically warp the images/features based on a linear or quadratic motion model and then complete the interpolation by fusing the warped results. Nevertheless, it is not flexible enough to model the real-world motion under the linear or quadratic assumption, especially for cases with long-range correspondence or complex motion. Besides, occlusion reasoning is a challenging problem for pixel-wise optical flow estimation. Without the prerequisites above, the kernel-based methods handle the reasoning and aggregation in an implicit way, which adaptively aggregate neighboring pixels from the images/features to generate the target pixel. However, this line stands the chance of failing to tackle the high-resolution frame interpolation or large motion due to the limited receptive field. Thereafter, deformable convolutional networks, a variant of kernel-based methods, are adopted to aggregate the long-term correspondence <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, achieving better performance. Despite many attempts, some challenging issues remain unresolved.</p><p>First, the deep-learning-based VFI works focus on learning the predefined ground truth (GT) and ignore the inherent motion diversity across a sequence of frames. As illustrated in <ref type="figure">Fig. 1 (a)</ref>, given the positions of a ball in frames I ?1 and I 1 , we conduct a user study of choosing its most possible position in the intermediate frame I 0 . The obtained probability distribution map clearly clarifies the phenomenon of motion ambiguity in VFI. Without considering this point, existing methods that adopt the pixel-wise L1 or L2 supervision possibly generate blurry results, as shown in <ref type="figure">Fig. 1 (b)</ref>. To resolve this problem, we propose a novel texture consistency loss (TCL) that relaxes the rigid supervision of GT while ensuring texture consistency across adjacent frames. Specifically, for an estimated patch, apart from the predefined GT, we look for another texture-matched patch from the input frames as a pseudo label to jointly optimize the network. In this case, predictions satisfying the texture consistency are also encouraged. From the visualization comparison of SepConv <ref type="bibr" target="#b0">[1]</ref> and our model with/without TCL 1 in <ref type="figure">Fig. 1 (b)</ref>, we observe that the proposed TCL leads to clearer results. Besides, as shown in <ref type="figure">Fig. 1 (c)</ref>, it is seen that our TCL brings about considerable PSNR improvement on Vimeo-Triplets <ref type="bibr" target="#b1">[2]</ref> and Middlebury <ref type="bibr" target="#b2">[3]</ref> benchmarks for both two methods. More visual examples are available in our appendix A.</p><p>Second, the cross-scale aggregation during alignment is not fully exploited in VFI. For example, PDWN <ref type="bibr" target="#b20">[21]</ref> conducts an image-level warping using the gradually refined offsets. However, the single-level alignment may not take full advantage of the cross-scale information, which has been proven useful in many low-level tasks <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>. In this work, we propose <ref type="bibr" target="#b0">1</ref> The four models are trained on Vimeo-Triplets <ref type="bibr" target="#b1">[2]</ref> dataset.</p><p>Input (Overlay) <ref type="bibr" target="#b0">[1]</ref> w/o TCL <ref type="bibr" target="#b0">[1]</ref>  a novel cross-scale pyramid alignment (CSPA) module, which performs bidirectional temporal alignment from low-resolution stages to higher ones. In each step, the previously aligned low-scale features are regarded as a guidance for the currentlevel warping. To aggregate the multi-scale information, we design an efficient fusion strategy rather than building the time-consuming cost volume or correlation map. Extensive quantitative and qualitative experiments verify the effectiveness and efficiency of the proposed method. In a nutshell, our contributions are summarized as follows:</p><p>? Texture consistency loss: Inspired by the motion ambiguity in VFI, we design a novel texture consistency loss to allow the diversity of interpolated content, producing clearer results. ? Cross-scale pyramid alignment: The proposed alignment strategy utilizes the multi-scale information to conduct a more accurate and robust motion compensation while requiring few computational resources. ? State-of-the-art performance: The extensive experiments including single-frame and multi-frame interpolation have demonstrated the superior performance of the proposed algorithm. ? Extension to other tasks: Based on the same architecture, our model is easily tailored to the video frame extrapolation task. Moreover, we take advantage of our well-trained model to generate high-quality intermediate frames, which can be naturally utilized in the video superresolution task. We show that the synthesized images further bring the existing video super-resolution methods to new heights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optical-Flow-Based Methods</head><p>A large group of methods utilize optical flow to build pixelwise correspondences, thereafter, they warp the given neighboring frames to the target frame. For example, TOFlow <ref type="bibr" target="#b1">[2]</ref> designs a task-oriented optical flow module and achieves favorable results compared with approaches using off-the-shelf optical flow. Jiang et al. <ref type="bibr" target="#b6">[7]</ref> propose an end-to-end convolutional neural network to interpret the bidirectional motion based on the optical flow meanwhile reasoning the occlusions from a visibility map. With the help of the linear combination of bidirectional warped features, they can interpolate frames at an arbitrary time. In <ref type="bibr" target="#b8">[9]</ref>, Niklaus and Liu present a contextaware frame interpolation approach by introducing additional warped deep features to provide rich contextual information. Huang et al. <ref type="bibr" target="#b3">[4]</ref> devise a lightweight sub-module named IFNet to predict the optical flow and train it in a supervised way. Choi et al. <ref type="bibr" target="#b29">[30]</ref> propose a tridirectional motion estimation method to obtain more accurate optical flow fields. To resolve the conflict of mapping multiple pixels to the same target location in the forward mapping, Niklaus et al. <ref type="bibr" target="#b4">[5]</ref> develop a differential softmax splatting method achieving a new state of the art. However, these optical flow-based methods generally have a poor performance when facing some challenging cases, such as large occlusion and complex motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Kernel-Regression-Based Methods</head><p>In addition to optical-flow-based methods above, learning adaptive gathering kernels <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> has also received intensive attention. Niklaus et al. <ref type="bibr" target="#b23">[24]</ref> regard the video frame interpolation as a local convolution over the input frames. A U-Net is designed to regress a pair of kernels that are applied on the input frames to handle the alignments and occlusions simultaneously. To reduce the model parameters while maintaining a comparable receptive field, Niklaus et al. <ref type="bibr" target="#b0">[1]</ref> propose another separable convolution network by combining two 1D kernels into a 2D adaptive kernel. However, both of these methods obtain limited performance when dealing with large displacements due to the restricted kernel size. To enlarge the receptive field, Peleg et al. <ref type="bibr" target="#b30">[31]</ref> develop a multi-scale feature extraction module to capture long-distance correspondences. Recently, deformable convolution networks (DCN) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref> have shown a great success in the field of video frame interpolation. In PDWN <ref type="bibr" target="#b20">[21]</ref>, the authors design a pyramid deformable network to warp the contents of input frames to the target frame. While these kernel-based VFI approaches show high flexibility</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>Cross-Scale Pyramid Alignment Attention-based Fusion Reconstruction</p><formula xml:id="formula_0">1 I 1 I ? {0,1,2} 1 F 1 0 F ? 1 0 F ? ? 0 F 0 I TCL L1 Loss 0 I {0,1,2} 1 F ? 1 I 1 I ? Fig. 2:</formula><p>Overview of the proposed VFI architecture. There are four components including a feature extraction module, a crossscale pyramid alignment module, an attention-based fusion module, and a reconstruction module. In addition to the L1 loss for supervision, we propose a texture consistency loss (TCL) to encourage the diversity of objects' motion.</p><p>and good performance, they neglect the essential cross-scale information from input frames. In this work, we devise a crossscale pyramid alignment to fuse multiple features in different resolutions, achieving better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Beyond Linear Motion Model</head><p>Another line of studies focuses on investigating the physical motion from more input frames. For instance, Yin et al. <ref type="bibr" target="#b7">[8]</ref> present a quadratic video interpolation method that takes the acceleration information into consideration, performing a better approximation of the complex motion in the real world. Later on, Zhang et al. <ref type="bibr" target="#b33">[34]</ref> develop a well-generalized model to analyze the complex motion patterns, which further boosts the interpolation quality. Though more input frames can be used to better understand motion properties, this kind of methods need to carefully handle the abundant clues for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Census Transform</head><p>A lot of works have exploited image/feature matching for many computer vision tasks, including optical flow estimation, stereo and correspondence matching. Due to the illumination robustness of Census Transform (CT), it has been widely used in those research fields. For example, Stein et al. <ref type="bibr" target="#b34">[35]</ref> use the Census Transform to convert the image patches from the RGB space to the CT-based feature space and verify its robustness of image matching on real-world cases. M?ller et al. <ref type="bibr" target="#b35">[36]</ref> propose an illumination-invariant census transform for optical flow estimation. Hermann et al. <ref type="bibr" target="#b36">[37]</ref> adopt a census cost function for 3D medical image registration. In this work, we incorporate the robustness of census transform into our texture consistency loss to generate finer details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Temporal Consistency</head><p>Some previous studies <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref> have exploited temporal consistency for deep-learning models. Lai et al. <ref type="bibr" target="#b37">[38]</ref> present a short-and long-term temporal loss to enforce the model to learn consistent results over time. Recently, Zhang et al. <ref type="bibr" target="#b38">[39]</ref> conduct extensive experiments to study spatial-temporal tradeoff for video super-resolution. Dwibedi et al. <ref type="bibr" target="#b39">[40]</ref> utilize a temporal cycle-consistency loss for self-supervised representation learning. In this work, we propose a novel temporal supervision which improves the quality of frame interpolation by considering adjacent frames. Following <ref type="bibr" target="#b38">[39]</ref>, we manually search for a balancing factor to achieve appropriate spatialtemporal tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED VIDEO INTERPOLATION SCHEME</head><p>In this section, we first give an overview of the proposed algorithm for video frame interpolation (VFI) in Sec. III-A. In Sec. III-B, we explain our texture consistency loss for supervision. Then, we elaborate on the cross-scale pyramid alignment and adaptive fusion in Sec. III-C. At last, we describe the configurations of our network in detail in Sec III-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Frame interpolation aims at synthesizing an intermediate frame (e.g., I 0 ) in the middle of two adjacent frames (e.g., I ?1 and I 1 ). As illustrated in <ref type="figure">Fig. 2</ref>, our framework completes the interpolation in a four-step process. First, we obtain the feature pyramids F {0,1,2} ?1 and F {0,1,2} 1 of frames I ?1 and I 1 using a feature extraction module. After that, the extracted features are passed through a cross-scale pyramid alignment module to perform a bidirectional alignment towards the middle point in time. Then, we develop an attention-based fusion module to fuse aligned features F ?1?0 and F 1?0 , resulting in F 0 . Finally, a sequence of residual blocks are applied on F 0 to synthesize the intermediate frame? 0 .</p><p>The existing methods usually strongly penalize the predicted frame? 0 when it does not exactly match the predefined ground truth (GT) I 0 . However, due to the non-uniqueness of movement between I ?1 and I 1 , there may exist many plausible solutions in terms of I 0 . Relaxing the rigid requirement of synthesizing the intermediate frame as close as possible to GT I 0 , we allow the prediction to be supervised by not only the GT but also the corresponding patterns in I ?1 and I 1 . In this case, our learning target is formulated a? where L 1 (? 0 , I 0 ) is the commonly adopted data term and L p (? 0 , I ?1 , I 1 ) is the proposed texture consistency loss detailed in Sec. III-B. The scaling parameter ? is to balance the importance of the two items.</p><formula xml:id="formula_1">I 0 = arg min I0 ( L 1 (? 0 , I 0 ) + ?L p (? 0 , I ?1 , I 1 )),<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Texture Consistency Loss</head><p>The proposed texture consistency loss is illustrated in <ref type="figure" target="#fig_0">Fig. 3</ref>. For the patchf x centrally located at position x on the predicted frame? 0 , we first seek for its best matching f t * y * from the input frames {I ?1 , I 1 }, where y * and t * are obtained from y * , t * = arg min</p><formula xml:id="formula_2">y,t L2(f x , f t y ),<label>(2)</label></formula><p>where y * and t * ? {?1, 1} refer to the optimal position and the optimal frame index, respectively. Then f t * y * is adopted as an additional pseudo label for estimationf x .</p><p>In our implementation, to avoid the interference of illumination in the RGB space across frames, we first apply a census transform <ref type="bibr" target="#b40">[41]</ref> to the query and the matching candidates before matching:</p><formula xml:id="formula_3">v x (x + x n ) = 0, f x (x) &gt; f x (x + x n ) 1, f x (x) ? f x (x + x n ) , x n ? R,<label>(3)</label></formula><p>where f x (x) is the pixel value at centeral position x and the patch field x n is defined as</p><formula xml:id="formula_4">R = {(?1, ?1), (?1, 0), . . . , (0, 1), (1, 1)}.<label>(4)</label></formula><p>To accelerate the matching process and maintain a reasonable receptive field, we further define the searching area as</p><formula xml:id="formula_5">?(x) = {y| |y ? x| ? d} ,<label>(5)</label></formula><p>where y is the position of patch candidates and d indicates the maximum displacement. In this case, the matching process is represented as</p><formula xml:id="formula_6">y * , t * = arg min y??(x),t?{?1,1} L2(v x , v t y ),<label>(6)</label></formula><p>wherev x and v t y are the representations of patchesf x and f t y after census transform. Noticing that the operation of census transform is non-differentiable, our TCL is performed on the original RGB space as</p><formula xml:id="formula_7">L p (? 0 , I ?1 , I 1 )(x) = L1(f x , f t * y * ).<label>(7)</label></formula><p>C. Cross-Scale Pyramid Alignment</p><p>As aforementioned in Sec. I, most VFI methods utilize the optical flow to perform a two-step synthesis, image-level alignment and deep-learning-based interpolation. However, these approaches face challenges in handling occluded or textureless areas. Consequently, the inaccurate alignment may degrade the performance of the latter processing phases. By contrast, kernel-based works formulate the interpolation as an adaptive convolution over input frames, which typically use a deep network to regress a pair of pixel-wise kernels and apply them on the input frames. However, this single-scale aggregation at the image level may not make full use of information of input frames. To cope with this problem, some approaches have exploited multi-scale aggregation strategies by building dense correlation maps. Nevertheless, the computational complexity increases dramatically with the growth of image resolution.</p><p>In this work, we develop a cross-scale pyramid alignment (CSPA) at the feature level aided by deformable convolution networks <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Compared with the previous multi-scale aggregation strategies, CSPA has the following advantages: (1) the previous aligned low-resolution results are regarded as a guidance for the alignment of higher-resolution features, which ensures more accurate warping; (2) aggregating cross-scale information is beneficial to restoring more details; (3) without constructing a cost volume or correlation map, our CSPA is more computationally efficient.</p><p>In detail, the feature pyramids F from low resolution (i.e., F 2 ?1 ) to high resolution (i.e., F 0 ?1 ), as illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. At first, referring to the other endpoint F 2 1 , the alignment of F 2 ?1 is conducted to handle the large motion as</p><formula xml:id="formula_8">F 2 ?1?0 = Align(F 2 ?1 , F 2 1 ).<label>(8)</label></formula><p>Next, at the higher resolution level, we bilinearly upsample F 2 ?1?0 to F 2,?2 ?1?0 by a factor of 2 and aggregate the crossscale (level 2 and level 1) information as</p><formula xml:id="formula_9">F 1 ?1?0 = F use(F 2,?2 ?1?0 , F 1 ?1 ).<label>(9)</label></formula><p>in a fusion module ("CSF" in <ref type="figure" target="#fig_2">Fig. 4a</ref>), which is implemented as a concatenation followed by a convolution operation. Later on, we feed the fusionF 1 ?1?0 and F 1 1 at the other endpoint into the alignment block and obtain the aligned result:</p><formula xml:id="formula_10">F 1 ?1?0 = Align(F 1 ?1?0 , F 1 1 ).<label>(10)</label></formula><p>Following the same pipeline, we perform the alignment at the highest resolution level to handle subtle motion. Specifically, the cross-scale fusion module takes three-level inputs as</p><formula xml:id="formula_11">F 0 ?1?0 = F use(F 2,?4 ?1?0 , F 1,?2 ?1?0 , F 0 ?1 ).<label>(11)</label></formula><p>It is noted that the alignment of I 1 ? I 0 is completed symmetrically. The alignment block is zoomed up in <ref type="figure" target="#fig_2">Fig. 4b</ref>. In terms of the l-th level alignemnt for frame I ?1 , the block first concatenates the fused cross-scale featureF l ?1?0 and F l 1 , and conducts a 3 ? 3 convolution. Five sequential residual blocks and another</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment Block</head><p>Cross-scale Fusion Bilinear Upsampling convolution are used to predict a weight map W l ?1?0 and an offset map O l ?1?0 . Finally, the aligned feature F l ?1?0 (x) at position x is calculated by</p><formula xml:id="formula_12">0 1 F Conv 3?3 Res. block ?5 Conv 3?3 Deformable Convolution 1 l F (a) Cross-scale Pyramid Alignment (b) Alignment Block (AB) Concatenation + + 1 1 F 2 1 F 2 1 F ? 1 1 F ? 0 1 F ? 0 1 0 F ? ? 1 0 l F ? ? 1 0 l W ? ? 1 0 l O ? ? AB AB AB CSF CSF AB CSF 2 1 F ? CSF 2 : l ? 2 : l =</formula><formula xml:id="formula_13">F l ?1?0 (x) = iF l ?1?0 (x + O l ?1?0,i (x)) * W l ?1?0,i (x),<label>(12)</label></formula><p>where the subscript i means the i-th element in the receptive field of convolution.</p><p>As detailed in the Appendix B, the proposed CSPA module is of O(N ) computational complexity where N is the number of pixels. In <ref type="figure" target="#fig_3">Fig. 5</ref>, we compare the running time (on NVIDIA RTX 2080Ti) of three alignment models: single-scale (Model-1), cross-scale using a cost volume (Model-2) and the proposed CSPA (Model-3). The complexities of Model-1 and Model-2 are O(N ) and O(N 2 ). It is observed that our CSPA costs much less time than the cost-volume-based Model-2 for the largescale input. Especially, CSPA obtains comparable efficiency compared to the single-scale Model-1. In terms of the GPU memory cost for 340 ? 340 input, our model only requires 4.0 GBytes that is close to the Model-1 (2.3 GBytes), much smaller than the Model-2 (10.0 GBytes). To quantitatively evaluate the performance of three strategies, we train three models under a fair experimental setting 2 . The PSNR results shown in <ref type="figure" target="#fig_3">Fig. 5</ref> further demonstrate the effectiveness of our CSPA approach (Model-3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attention-Based Fusion</head><p>After the bidirectional alignment, we obtain a pair of aligned features F 0 ?1?0 and F 0 1?0 . In order to determine whether the information is useful or not in a spatially variant way, we employ an attention mechanism to aggregate these two <ref type="bibr" target="#b1">2</ref> For a quick comparison, we construct a small training set that contains 5000 samples from Vimeo-Triplets <ref type="bibr" target="#b1">[2]</ref> and randomly select 100 testing samples from Vimeo-Triplets-Test (denoted as "Vimeo-100S"). We also test the generalization ability of three models by assessing their performance on the out-of-domain Middlebury set <ref type="bibr" target="#b2">[3]</ref>. features. First, the attention map is calculated by a convolution followed by a sigmoid operation as</p><formula xml:id="formula_14">0HWKRG 9LPHR6 0LGGOHEXU\ 0RGHO G% G% 0RGHO G% G% 0RGHO 3DWFK6L]H 5XQWLPHPV G% G% 0RGHO 0RGHO 0RGHO</formula><formula xml:id="formula_15">M = Sigmoid(Conv(F 0 ?1?0 , F 0 1?0 )).<label>(13)</label></formula><p>Then, the final aggregated result F 0 is obtained by</p><formula xml:id="formula_16">F 0 = M * F 0 ?1?0 + (1 ? M ) * F 0 1?0 .<label>(14)</label></formula><p>E. Other Configurations a) Common Settings: As illustrated in <ref type="figure">Fig. 2</ref>, there are four modules in our framework: feature extraction, crossscale pyramid alignment, attention-based feature fusion, and reconstruction. The number of parameters are 4.28M, 12.52M, 0.29M and 11.80M, respectively, in a total of 28.89M. Following <ref type="bibr" target="#b43">[44]</ref>, we adopt the residual block <ref type="bibr" target="#b44">[45]</ref> as the basic component (shorted as "RB"), which is detailed in <ref type="table" target="#tab_2">Table I</ref>. In our network, the channel number of convolutions is set to 128. We use ? to point out the output of a layer in tables I to III.  b) Feature Extraction: The structure of the feature extraction module is shown in <ref type="table" target="#tab_2">Table II</ref>. For a given input frame <ref type="figure">1})</ref>, we first utilize a convolution to change its channel dimension to 128. Then the feature maps are passed through five residual blocks, resulting in the 0-th level feature F 0 i of the pyramid representation. Finally, we use two convolutional layers with strides of 2 to generate the downsampled features F 1 i and F 2 i , respectively.</p><formula xml:id="formula_17">I i ? R C?H?W (i = {?1,</formula><formula xml:id="formula_18">Input I i Layer1</formula><p>Conv <ref type="formula">(</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Implementation Details</head><p>All experiments are conducted on the NVIDIA GeForce RTX 2080Ti GPUs. We use two adjacent frames to interpolate the middle frame. An Adam optimizer is adopted and the learning rate decays from 5 ? 10 ?4 to 0 by a cosine annealing strategy. We set the batch size to 64. The training lasts 600K iterations, during which we adopt random 64 ? 64 cropping, vertical or horizontal flipping, and 90 ? rotation augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets and Evaluation Metrics a) Vimeo-Triplets [2]</head><p>: It contains 51,312 and 3,782 triplet frames with a resolution of 256 ? 448 for training and testing, respectively. Following the most commonly used protocols <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>, we train a model for single-frame interpolation on the training split (Vimeo-Triplets-Train) while evaluating the results on the testing part (Vimeo-Triplets-Test). b) Middlebury <ref type="bibr" target="#b2">[3]</ref>: It is a widely used evaluation benchmark for video frame interpolation. In total, there are 12 challenging cases where each of them contains three video frames. The central frame serves as the ground truth while the others are used as the input.   <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>, we assess our approach on this benchmark without finetuning.</p><p>d) UCF101-E <ref type="bibr" target="#b7">[8]</ref>: Since UCF101 can only be used for evaluating single-frame interpolation, QVI <ref type="bibr" target="#b7">[8]</ref> further collects the UCF101-E benchmark for multi-frame interpolation. UCF101-E is composed of 100 samples and each sample has 5 consecutive frames with a resolution of 225 ? 225. e) Vimeo90K-7f <ref type="bibr" target="#b1">[2]</ref>: It is widely used in the video super-resolution task. There are 64,612 training and 7,824 validation video sequences containing 7 consecutive frames with a resolution of 256 ? 448. In this work, we train our multi-frame interpolation model using the training split of Vimeo90K-7f. In detail, we randomly sample 5 consecutive frames at a time where the central frame is the interpolation target and the others are used as the input. f) Metrics: We adopt PSNR and SSIM <ref type="bibr" target="#b51">[52]</ref> as the quantitative evaluation metrics. The higher values indicate the better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with SOTA Methods</head><p>To verify the effectiveness of the proposed method, we make a comparison with state-of-the-art methods under single-frame and multi-frame interpolation settings. a) Single-Frame Interpolation: As illustrated in Table IV, it is clear that our model achieves a new state of the  <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 6</ref>. Compared with other methods, our model successfully handles complicated motion and produces more plausible structures. In terms of the second example in <ref type="figure">Fig. 7</ref>, all other methods fail to restore the right hand of the human, while ours interpolates the hand that is closest to the ground truth. As for the first example in <ref type="figure">Fig. 6</ref>, both SepConv <ref type="bibr" target="#b0">[1]</ref> and RIFE-L [4] cannot synthesize regular balls. Although Context-Syn <ref type="bibr" target="#b8">[9]</ref> and Soft-Splat <ref type="bibr" target="#b4">[5]</ref> generate visually correct balls, they produce some annoying artifacts on the hand. On the contrary, our method can interpolate the contents well without generating notable artifacts. The error maps further demonstrate the effectiveness of our method. b) Multi-Frame Interpolation: To compare with methods that utilize multiple input frames on both sides, e.g., generating I 0 from {I ?2 , I ?1 , I 1 , I 2 }, we follow <ref type="bibr" target="#b7">[8]</ref> to train a model on the Vimeo90k-7f training subset.</p><p>In <ref type="table" target="#tab_11">Table V</ref>, we quantitatively evaluate our method on the Vimeo90K-7f <ref type="bibr" target="#b1">[2]</ref> and UCF101-E <ref type="bibr" target="#b50">[51]</ref> benchmarks. It is observed that the performance of our method surpasses all competing methods including kernel-based models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>, optical flow-based models <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and motion model <ref type="bibr" target="#b7">[8]</ref>. Especially, our method brings about nearly 1.3dB PSNR improvement over the second-best QVI <ref type="bibr" target="#b7">[8]</ref> on the Vimeo90K-7f testing set. Besides, it is noteworthy that our model with only two input frames (denoted as"Ours-triplet" ) still outperforms other methods that take four frames as input. <ref type="figure">Fig. 8</ref> shows the qualitative comparison between our method and the other two approaches. For cases with large motion, the kernel-based SepConv <ref type="bibr" target="#b0">[1]</ref> suffers from the restricted receptive field thus is difficult to handle the lone-range correspondences. As a result, blurry contents are sometimes produced. As for the flow-based method QVI <ref type="bibr" target="#b7">[8]</ref>, it may generate artifacts for some challenging cases. By contrast, our approach interpolates higher-quality frames with sharper edges, demonstrating the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>In this section, we make a comprehensive analysis of the contribution of each proposed component under the single-   <ref type="figure">Fig. 8</ref>: Visual comparison of multi-frame VFI algorithms.</p><p>frame interpolation setting. The Vimeo-Triplets-Test is adopted as the evaluation benchmark. a) Texture Consistency Loss (TCL): The proposed TCL is designed to alleviate the over-constrained issue of the predefined ground truth, which actually is just one of many possible solutions given observed input frames. To verify this claim, we quantitatively compare the conventional L1 loss and the proposed TCL. As illustrated in <ref type="table" target="#tab_2">Table VI</ref>, the baseline trained with the additional TCL achieves a better performance in terms of PSNR and SSIM compared to the baseline. We also give some visual examples to illustrate the impact of the proposed TCL in <ref type="figure" target="#fig_6">Fig. 9a</ref>. The model trained with TCL is able to preserve the structures of interpolated contents.   b) Hyperparamter ? in Eq. 1: The hyperparameter ? is used to balance the predefined ground truth and our proposed pseudo label. From <ref type="table" target="#tab_2">Table VII</ref>, we notice that ? = 0.1 is the best setting in our experiments (may not be optimal), and a large ? harms the performance of TCL. Especially, the model trained with ? = 10.0 fails to converge. We think of the proposed pseudo label better serving as auxiliary supervision apart from the L1 loss.   of the model. It is reasonable since the increase in patch size brings more difficulties in matching correctly to the candidates on neighboring frames and the inaccurate supervision signals bring negative impacts during training.  We analyze the effect of adopting census transform in our TCL. We train another model by performing patch matching in the RGB space directly (denoted as "TCL-RGB"). The results are described in <ref type="table" target="#tab_2">Table IX</ref>. It is observed that "TCL-RGB" leads to a lower interpolation quality in terms of PSNR and SSIM, which supports the claim that census transform is useful in eliminating the interference of illumination in Sec. III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Vimeo-  e) Cross-Scale Pyramid Alignment: Different from existing works that apply temporal alignment on a specific scale or multiple scales individually, we propose a crossscale pyramid alignment module (CSPA) that enables a more accurate alignment. As shown in <ref type="table" target="#tab_2">Table VI</ref>, the model with the proposed CSPA leads to a 0.66dB improvement on PSNR compared with the baseline. Furthermore, we also give some   visual results for qualitative evaluation in <ref type="figure" target="#fig_6">Fig. 9b</ref>. The CSPA benefits our model in restoring the structure of the car and patterns on the clothes more clearly. f) Temporal Consistency: Apart from the quantitative evaluation of PSNR and SSIM, temporal consistency <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> is also an important measure within the realm of video frame interpolation. We compare our method with two representative methods including SepConv <ref type="bibr" target="#b0">[1]</ref> and RIFE-L <ref type="bibr" target="#b3">[4]</ref> in <ref type="figure" target="#fig_8">Fig. 10</ref>. It is observed that SepConv and RIFE-L generate blurry and inconsistent patterns along the time axis, while our method successfully restores the correct and consistent patterns compared with the ground truth. E. Extension a) Video Frame Extrapolation: We make an exploration to extend our method to the video frame extrapolation task. Unlike video frame interpolation, extrapolation aims to synthesize the future frames based on the observed historical frames. All the flow-based methods are heavily dependent on the pre-defined displacements, making them unsuitable for extrapolation. In this case, we compare our method with SepConv <ref type="bibr" target="#b0">[1]</ref> and FLAVR <ref type="bibr" target="#b54">[55]</ref> as they do not require opticalflow information. SepConv adopts a U-Net to regress a pair of separable 1D kernels to perform convolutional operations on the two input frames. FLAVR proposes an efficient 3D convolutional neural network for reconstruction. For a fair comparison, we retrain the three models from scratch under the same experimental setting on Vimeo-Triplets. More specifically, we need to predict a future frame I 3 from historical {I 1 , I 2 }. We use fewer residual blocks (18 residual blocks) in the reconstruction module, which makes our model has comparable parameters (22.4M) with the SepConv (21.7M) and much fewer parameters than FLAVR (42.1M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLAVR SepConv</head><p>Ours-extra. GT Inputs (Overlay) <ref type="figure">Fig. 11</ref>: Visual comparison of our method and two recent methods for video frame extrapolation. <ref type="table" target="#tab_18">Table X</ref>, compared with SepConv and FLAVR, our model boosts PSNR by 1.63dB and 0.91dB, respectively. In addition, our model is nearly 2 times smaller than the FLAVR. As depicted in <ref type="figure">Fig. 11</ref>, our method produces sharper edges and fewer artifacts. Especially in the last image, our model can produce recognizable characters. In a nutshell, both the quantitative and qualitative results demonstrate that our model is capable of generating high-quality extrapolated frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>b) Synthetic Frames for Video Super-Resolution: We illustrate that the interpolated frames by our model also benefit video super-resolution methods. A well-trained recurrent framework BasicVSR <ref type="bibr" target="#b55">[56]</ref> is adopted due to its strong performance as well as the flexibility to allow an arbitrary number of input frames. In the basic setting, we take three consecutive low-resolution frames as input: </p><formula xml:id="formula_19">I HR 0 = f vsr (I LR ?1 , I LR 0 , I LR +1 ),<label>(15)</label></formula><p>The quantitative comparison is shown in <ref type="table" target="#tab_2">Table XI</ref>. It suggests that the two additional frames generated by our model can boost the performance of BasicVSR by 0.1dB and 0.012 on PSNR and SSIM. Besides, <ref type="figure" target="#fig_9">Fig. 12</ref> shows that the results with more input frames (denoted as "BasicVSR + VFI") have more   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Natural Phenomena</head><p>In this paper, following the most widely used setting in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b49">[50]</ref>, we also focus on studying the problem of frame interpolation   <ref type="figure" target="#fig_0">Fig. 13</ref>, we observe that our method recovers a better and more natural texture. It is noteworthy that, without any physical priors, e.g., the diffusion of smoke, nor fine-tuning on a specific texture class <ref type="bibr" target="#b56">[57]</ref>, our approach still produces relatively plausible contents in these examples. All these observations demonstrate the generalization ability of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Limitations and Disscussions</head><p>Though our model can generate high-quality results for most cases, it still faces a challenge for some complicated samples. We show two failure cases of our methods in <ref type="figure" target="#fig_2">Fig. 14.</ref> For the first example, the woman's right arm is occluded by the wall in frame I 1 , making it difficult to estimate accurate correspondence. As a result, our method tends to generate overly smooth content. In terms of the second sample, the man in the red box has an extremely large motion and the man in the green box turns around. Our method fails to hallucinate the corresponding objects in the middle frame. These challenging scenarios often require specific knowledge, e.g., physics-based priors, to enhance the model capability.</p><p>For example, skeletal constraints (if exploited well) could be strong clues for interpolating human bodies. This can be a potential direction for future study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We present a novel and effective video frame interpolation approach. The proposed texture consistency loss relaxes the strict contraint of the pre-defined ground truth and the crossscale pyramid alignment is able to make better use of multiscale information, making it possible to generate much clearer details. Comprehensive experiments have demonstrated the effectiveness of our method to interpolate high-quality frames. Also, we show that our model is easily tailored to the video extrapolation task. And our interpolated frames are proven to be useful in improving the performance of existing video super-resolution methods. In the future, we plan to study the potential of our method on other video restoration problems, such as video deblurring and video denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A MORE VISUAL COMPARISON OF TCL</head><p>As shown in <ref type="figure" target="#fig_3">Fig. 15a</ref> and <ref type="figure" target="#fig_3">Fig. 15b</ref>, we give more visual examples of SepConv <ref type="bibr" target="#b0">[1]</ref> and our method with/without TCL. It is clear that the proposed TCL is benefical in hallucinating more plausible structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B DISCUSSION OF MODEL-1,2,3 IN SECTION III-C</head><p>Except for the alignment module, the Model-1,2,3 in Sec. III-C share the same framework architecture as shown in <ref type="figure">Fig. 2</ref>. The Model-1 adopts a single-scale alignment of which computational complexity is</p><formula xml:id="formula_21">C M odel?1 = k 1 * N,<label>(17)</label></formula><p>where N is the number of pixels on the highest resolution and k 1 denotes the number of operations applied on a single pixel. As for Model-2, the cost volume is obtained by calculating pixel-to-pixel correlations. Taking the alignment of highestresolution features for example, apart from the current scale, it also requires to compute the cross-scale correlations with respect to the 1/2 and 1/4 resolution features, resulting in C M odel?2 = (1 + 1/4 + 1/16) * k 2 * N 2 ,</p><p>where k 2 represents the pixel-wise operations of Model-2. In contrast, the Model-3 (our CSPA) performs the cross-scale alignment with computational complexity as C M odel?3 = (1 + 1/4 + 1/16) * k 3 * N,</p><p>where k 3 counts the number of pixel-wise operations of Model-3. Considering k 1 , k 2 and k 3 are much smaller than the pixel number N , our CSPA shares a comparable computational complexity of O(N ) with the Model-1, much smaller than Model-2 of O(N 2 ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input (Overlay)</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of our texture consistency loss (TCL). The best matched f t * y * is served as a pseudo label for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>frames I ?1 and I 1 are aligned in a bidirectional way. Taking the direction of I ?1 ? I 0 for example, we gradually align F {0,1,2} ?1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>The framework of the cross-scale pyramid alignment (CSPA) module. (a) The 3-level pyramid alignment from I ?1 to I 0 . (b) The detailed structure of the alignment block at the l-th level. The source feature (in light yellow) and the other endpoint feature F l 1 (in blue) are fed through the alignment block to generate an aligned feature F l ?1?0 . The source feature is F 2 ?1 when l = 2, while representing the fused result of CSF for other cases. More details can be found in Section III-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Efficiency of three alignment models. Model-1 refers to the single-scale alignment. Model-2 denotes the cross-scale alignment using a cost volume. Model-3 is our proposed cross-scale pyramid alignment (CSPA). Our CSPA achieves the most promising results, while only requiring comparable computational complexity with the single-scale alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(a) 30</head><label>30</label><figDesc>.06dB (b) 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) 36.56 36.76 36.69 36.69 36.54 -SSIM 0.976 0.980 0.979 0.979 0.978 -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Ablation study of TCL and CSPA in visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>) 36.76 36.64 36.56 36.50 SSIM 0.980 0.979 0.978 0.978</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Temporal consistency analysis of three single-frame VFI approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 :</head><label>12</label><figDesc>Visual results of BasicVSR and "BasicVSR + VFI" for the video super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 :</head><label>13</label><figDesc>Visual results for natural phenomena. (a) refers to the overlayed input frames. (b-f) represent the results of SepConv [1], FeFlow [50], RIFE-L [4], ours-triplets and groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 :</head><label>14</label><figDesc>Two failure cases of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>The structure of the residual block ("RB").</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>The structure of our feature extraction module.Table IIIshows the details of the reconstruction module. The fused intermediate feature F 0 is firstly passed to a sequence of residual blocks for refinement. At last, we use a single convolution without activation to generate the final result I 0 .</figDesc><table><row><cell>Input</cell><cell>F 0</cell></row><row><cell>Layer1</cell><cell>40?RB(128)</cell></row><row><cell>Layer2</cell><cell>Conv(128,3,3,1) ?? 0</cell></row><row><cell>Params.</cell><cell>11.80M</cell></row></table><note>c) Reconstruction:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>The structure of the reconstruction module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>It is a large scale dataset for human action recognition, where Liu et al. [6] sample 379 triplets (256 ? 256) for single-frame interpolation evaluation. Unlike the aforementioned datasets, UCF101 has heavy compression noises. Following recent methods</figDesc><table><row><cell>06dB (c) 34.86dB</cell><cell>(d) 33.16dB (e) 34.97dB</cell></row><row><cell>(a) 40.10dB (b) 44.28dB (c) 46.30dB</cell><cell>(d) 46.04dB (e) 46.</cell></row></table><note>53dB Fig. 6: Visualized results for single-frame interpolation on Middlebury [3]. (a-e) represents SepConv [1], CtxSyn [9], Softmax-Splatting [5], RIFE-L [4] and ours. The first and third rows show some cropped regions of the interpolated images, while the second and fourth rows are the corresponding error maps normalized between [0, 1] for best view. The best results are highlighted in bold.c) UCF101 [51]:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative comparison of single-frame VFI algorithms. The numbers in red and blue refer to the best and second-best PSNR(dB)/SSIM results. Runtime of each model is also reported with an input size of 2 ? 480 ? 640. " ?" means using self-ensembling during evaluation.</figDesc><table><row><cell>SepConv</cell><cell>QVI</cell></row><row><cell>Inputs (Overlay)</cell><cell></cell></row><row><cell>Ours</cell><cell>GT</cell></row><row><cell>SepConv</cell><cell>QVI</cell></row><row><cell>Inputs (Overlay)</cell><cell></cell></row><row><cell>Ours</cell><cell>GT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE V :</head><label>V</label><figDesc>Quantitative comparison of multi-frame VFI algorithms. The numbers in red and blue refer to the best and second-best PSNR(dB)/SSIM results.</figDesc><table><row><cell>Method</cell><cell>PSNR (dB)</cell><cell>SSIM</cell></row><row><cell>Baseline</cell><cell>35.90</cell><cell>0.969</cell></row><row><cell>Baseline w/ TCL</cell><cell cols="2">36.21(+0.31) 0.977(+0.08)</cell></row><row><cell cols="3">Baseline w/ CSPA 36.56(+0.66) 0.976(+0.07)</cell></row><row><cell>Full</cell><cell cols="2">36.76(+0.86) 0.980(+0.11)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VI :</head><label>VI</label><figDesc></figDesc><table /><note>Ablation studies of the proposed components on the Vimeo-Triplets-Test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VII :</head><label>VII</label><figDesc>We explore the influence of the patch size K ? {3, 5, 7, 9} used in the TCL. As shown inTable VIII, a larger patch size may degrade the performance</figDesc><table><row><cell>Inputs (Overlay)</cell><cell>w/o TCL 33.1dB</cell><cell>w/ TCL 33.8dB</cell><cell>GT</cell></row><row><cell>Inputs (Overlay)</cell><cell>w/o TCL 34.5dB</cell><cell>w/ TCL 35.7dB</cell><cell>GT</cell></row><row><cell cols="4">(a) Visual comparison of results with/without TCL.</cell></row><row><cell>Inputs (Overlay)</cell><cell cols="2">w/o CSPA 29.1dB w/ CSPA 32.6dB</cell><cell>GT</cell></row><row><cell>Inputs (Overlay)</cell><cell cols="2">w/o CSPA 27.5dB w/ CSPA 38.8dB</cell><cell>GT</cell></row><row><cell cols="4">(b) Visual comparison of results with/without CSPA.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Inputs(Overlay)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>w/ CSPA 28.8dB</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GT</cell><cell>w/o CSPA 27.5dB</cell></row></table><note>Analysis of ? in Eq. 1 on the Vimeo-Triplets-Test benchmark.c) Patch size K of TCL:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VIII :</head><label>VIII</label><figDesc>Analysis of different patch sizes in TCL on the Vimeo-Triplets-Test benchmark.</figDesc><table /><note>d) Census Transform:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE IX :</head><label>IX</label><figDesc>Analysis of cencus transform in TCL. We adopt PSNR(dB) and SSIM as the evaluation metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE X</head><label>X</label><figDesc></figDesc><table><row><cell>: Quantitative comparison of SepConv, FLAVR, and</cell></row><row><cell>our method for video frame extrapolation. The models are</cell></row><row><cell>trained on the Vimeo-Triplets-Train dataset and evaluated on</cell></row><row><cell>the Vimeo-Triplets-Test set and Middlebury benchmarks. The</cell></row><row><cell>best PSNR(dB)/SSIM results are highlighted in bold.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE XI :</head><label>XI</label><figDesc>The quantitative analysis of our VFI method for the video super-resolution task. plausible structures. The experiment clarifies that the highquality interpolated frames obtained from our VFI model are beneficial for the VSR method.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Bicubic</cell><cell>w/o Ours-triplets</cell><cell>w/ Ours-tri</cell></row><row><cell></cell><cell></cell><cell></cell><cell>23.77dB</cell><cell>26.00dB</cell><cell>26.28dB</cell></row><row><cell>LR</cell><cell>Bicubic 23.19dB</cell><cell>BasicVSR 24.89dB</cell><cell>BasicVSR+FI 25.16dB</cell><cell>GT</cell></row><row><cell>LR</cell><cell>Bicubic 23.77dB</cell><cell>BasicVSR 26.00dB</cell><cell>BasicVSR+VFI 26.28dB</cell><cell>GT</cell></row><row><cell></cell><cell>Bicubic 19.85dB</cell><cell>BasicVSR 22.54dB</cell><cell>BasicVSR+VFI 22.71dB</cell><cell>GT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE XII :</head><label>XII</label><figDesc>The quantitative comparison on the collected cases of natural phenomena. in general scenes. Taking a further step, we evaluate the generalization ability of our model (trained on the Vimeo-Triplets) by transferring it to out-of-domain natural scenarios. To this end, we first collect over 100 video clips from the Internet (we will make this newly collected dataset publicly available for research uses later), containing dynamic textures of natural phenomena. As shown in theTable XII, our method outperforms others by a large margin, yielding a 1.37dB gain. Besides, from the visual examples in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rife: Realtime intermediate flow estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5437" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A flexible recurrent residual pyramid network for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="474" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="41" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep animation video interpolation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6587" to="6595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ea-net: Edge-aware network for flow-based video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07673</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fastrife: Optimization of real-time intermediate flow estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kubas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sarwas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13482</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A low complexity motion compensated frame interpolation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4927" to="4930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lap-based video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jayashankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gilliam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4195" to="4199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bwin: A bilateral warping method for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">All at once: Temporally adaptive multi-frame interpolation with advanced motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="107" to="123" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVII 16</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flow-aware synthesis: A generic motion model for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video frame interpolation via deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">614</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pdwn: Pyramid deformable warping network for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adacof: adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5316" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video frame interpolation via generalized deformable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple video frame interpolation via enhanced deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-scale cost aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1590" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mucan: Multicorrespondence aggregation network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bestbuddy gans for highly detailed image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15295</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-quality frame interpolation via tridirectional inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="596" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Im-net for high resolution video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sendik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2398" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lapar: Linearly-assembled pixel-adaptive regression network for single image super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting temporal alignment for video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video frame interpolation without temporal priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/9a11883317fde3aef2e2432a58c86779-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient computation of optical flow using the census transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Illumination-robust dense optical flow using census signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rannacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="236" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tv-l 1-based 3d medical image registration with the census cost function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Symposium on Image and Video Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="149" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Is there tradeoff between spatial and temporal in video super-resolution?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06141</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal cycle-consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1801" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A non-parametric approach to visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12622</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Depthaware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="663" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting adaptive convolutions for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1099" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Featureflow: Robust video interpolation via structure-to-texture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>Center for Research in Computer Vision</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Flavr: Flowagnostic video representations for fast frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08512</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4947" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Texture-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Danier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 Picture Coding Symposium (PCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
