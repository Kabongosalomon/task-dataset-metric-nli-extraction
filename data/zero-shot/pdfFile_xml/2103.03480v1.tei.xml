<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IAFA: Instance-aware Feature Aggregation for 3D Object Detection from a Single Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory of Deep Learning Technology and Application</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory of Deep Learning Technology and Application</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feixiang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory of Deep Learning Technology and Application</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory of Deep Learning Technology and Application</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IAFA: Instance-aware Feature Aggregation for 3D Object Detection from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection from a single image is an important task in Autonomous Driving (AD), where various approaches have been proposed. However, the task is intrinsically ambiguous and challenging as single image depth estimation is already an ill-posed problem. In this paper, we propose an instance-aware approach to aggregate useful information for improving the accuracy of 3D object detection with the following contributions. First, an instance-aware feature aggregation (IAFA) module is proposed to collect local and global features for 3D bounding boxes regression. Second, we empirically find that the spatial attention module can be well learned by taking coarse-level instance annotations as a supervision signal. The proposed module has significantly boosted the performance of the baseline method on both 3D detection and 2D bird-eye's view of vehicle detection among all three categories. Third, our proposed method outperforms all single image-based approaches (even these methods trained with depth as auxiliary inputs) and achieves stateof-the-art 3D detection performance on the KITTI benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate perception of surrounding environment is particularly important for Autonomous Driving <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and robot systems. In AD pipeline, the perception 3D positions and orientation of surrounding obstacles (e.g., vehicle, pedestrian, and cyclist) are essential for the downstream navigation and control modules. 3D object detection with depth sensors (e.g., RGBD camera, LiDAR) is relatively easy and has been well studied recently. Especially, with the development of deep learning techniques in 3D point cloud, a wide variety of 3D object detectors have sprung up including point-based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, voxel-based methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref> and hybrid-point-voxel-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Although depth sensors have been widely used in different scenarios, their drawbacks are also obvious: expensive prices, high-energy consumption, and less structure information. Recently, 3D object detection from passive sensors such as monocular or stereo cameras has attracted many researchers' attention and some of them achieved impressive detection performance. Compared with the active sensors, the most significant bottleneck of 2D image-based approaches <ref type="bibr" target="#b9">[10]</ref> is how to recover the depth of these obstacles. For stereo rig, the depth (or disparity) map can be recovered by traditional geometric matching <ref type="bibr" target="#b10">[11]</ref> or learned by deep neural networks <ref type="bibr" target="#b11">[12]</ref>. By using traditional geometric techniques, it's really difficult to estimate the depth map from a single image without any prior information while this problem has been partly solved with deep learning based methods, <ref type="bibr" target="#b12">[13]</ref>. With the estimated depth map, the 3D point cloud (pseudo LiDAR point cloud) can be easily reconstructed via pre-calibrated intrinsic (or extrinsic) camera parameters. Any 3D detectors designed for LiDAR point cloud can be use directly on pseudo LiDAR point cloud <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref>. Furthermore, in <ref type="bibr" target="#b13">[14]</ref>, the depth estimation and 3D object detection network has been integrated together in an end-to-end manner.</p><p>Recently, center-based (anchor-free) frameworks become popular for 2D object detection. Two representative frameworks are "CenterNet" <ref type="bibr" target="#b17">[18]</ref> and "FCOS" <ref type="bibr" target="#b18">[19]</ref>. Inspired by these 2D detectors, some advanced anchor-free 3D object detectors have been proposed such as "SMOKE" <ref type="bibr" target="#b19">[20]</ref> and "RTM3D" <ref type="bibr" target="#b20">[21]</ref>. In the center-based frameworks, an object is represented as a center point and object detection has been transferred as a problem of point classification and its corresponding attributes (e.g., size, offsets, depth, etc.) regression.</p><p>Although the center-based representation is very compact and effective, it also has some drawbacks. In the left sub-figure of <ref type="figure" target="#fig_0">Fig. 1</ref>, the vehicles are represented with center points, where the red points are the projected 3D object centers on the 2D image plane. The white numbers in the blue text boxes represent the "ID" of the vehicles and the white numbers in the red text boxes are the vehicle "ID"s that these points belong to. From this image, we can easily find that the projected 3D centers of vehicle "2" is on the surface of vehicle "1". Similarly, the projected 3D centers of vehicle "3" is on the surface of vehicle "2". Particularly, this kind of misalignment commonly happens in the AD scenario in the case of occlusion. Taking vehicle "2" as an example, its projected 3D center is on the surface of vehicle "1" and most of its surrounding pixels are from vehicle "1". During the training, the network may be confused about which pixels (or features) should be used for this center classification and its attributes regression. This problem becomes much more serious for the depth regression because the real depth of the 2D center point (on the surface of vehicle "1") is totally different from its ground truth value-the the depth of its 3D Bounding Box's (BBox's) center.</p><p>In order to well handle this kind of misalignment or to alleviate this kind of confusion during the network learning process, we propose to learn an additional attention map for each center point during training and explicitly tell the network that which pixels belong to this object and they should contribute more for the center classification and attributes regression. Intuitively, the learning of the attention map can be guided by the instance mask of the object. By adding this kind of attention map estimation, we can achieve the following advantages: first of all, the occluded objects, attention map can guide the network to use the features on the corresponding objects and suppress these features that belong to the other object; second, for these visible objects, the proposed module is also able to aggregate the features from different locations to help the object detection task. The contributions of our work can be summarized as follows:</p><p>1. First, we propose a novel deep learning branch termed as Instance-Aware</p><p>Features Aggregation (IAFA) to collect all the pixels belong to the same object for contributing to the object detection task. Specifically, the proposed branch explicitly learns an attention map to automatically aggregate useful information for each object. 2. Second, we empirically find that the coarse instance annotations from other instance segmentation networks can provide good supervision to generate the features aggregation attention maps. 3. The experimental results on the public benchmark show that the performance of the baseline can be significantly improved by adding the proposed branch. In addition, the boosted framework outperforms all other monocularbased 3D object detectors among all the three categories ("Easy", "Moderate" and "Hard").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>LiDAR-based 3D Detection: 3D object detection in traffic scenario becomes popular with the development of range sensor and the AD techniques. Inspired by 2D detection, earlier 3D object detectors project point cloud into 2D (e.g., bird-eye-view <ref type="bibr" target="#b21">[22]</ref> or front-view <ref type="bibr" target="#b22">[23]</ref>) to obtain the 2D object bounding boxes first and then re-project them into 3D. With the development of 3D convolution techniques, the recently proposed approaches can be generally divided into two categories: volumetric convolution-based methods and points-based methods. Voxel-net <ref type="bibr" target="#b4">[5]</ref> and PointNet <ref type="bibr" target="#b23">[24]</ref> are two pioneers for these methods, respectively. How to balance the GPU memory consumption and the voxel's resolution is one bottleneck of voxel-based approach. At the beginning, the voxel resolution is relative large as 0.4m ? 0.2m ? 0.2m due the limitation of GPU memory. Now this issue has been almost solved due to the development of GPU hardware and some sparse convolution techniques, e.g., SECOND <ref type="bibr" target="#b24">[25]</ref> and PointPillars <ref type="bibr" target="#b5">[6]</ref>. At the same time, points-based methods <ref type="bibr" target="#b25">[26]</ref> also have been well explored and achieved good performance on the public benchmarks.</p><p>Camera-based 3D object Detection: due to the cheaper price and less power consumption, many different approaches have been proposed recently for 3D object detection from camera sensor. A simple but effective idea is to reconstruct the 3D information of the environment first and then any point cloud-based detectors can be employed to detect objects from the reconstructed point clouds (which is also called "Pseudo-LiDAR") directly. For depth estimation (or 3D reconstruction), either classical geometric-based approaches or deep-learning based approaches can be used. Based on this idea, many approaches have been proposed for either monocular <ref type="bibr" target="#b15">[16]</ref> or stereo cameras <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Rather than transforming the depth map into point clouds, many approaches propose using the depth estimation map directly in the framework to enhance the 3D object detection. In M3D-RPN <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b28">[29]</ref>, the pre-estimated depth map has been used to guide the 2D convolution, which is called as "Depth-Aware Convolution". In addition, in order to well benefit the prior knowledge, some approaches are also proposed to integrate the shape information into the object detection task via sparse key-points <ref type="bibr" target="#b29">[30]</ref> or dense 2D and 3D mapping .</p><p>Direct Regression-based Methods: although these methods mentioned above achieved impressive results, they all need auxiliary information to aid the object detection, such as "Depth Map" or "Pseudo Point Cloud". Other approaches are direct regression-based methods. Similar to the 2D detectors, the direct regression based methods can be roughly divided into anchor-based or anchor-free methods. Anchor-based methods such as <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> [33], <ref type="bibr" target="#b33">[34]</ref> need to detect 2D object bounding boxes first and then ROI align technique is used to crop the related information in both original image domain or extracted feature domain for corresponded 3D attributes regression. Inspired by the development of centerpoint-based (anchor free) methods in 2D object detection <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and instance segmentation <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, some researchers have proposed center-point-based methods for 3D object detection task <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b33">[34]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, the object has been represented as a center point with associated attributes (e.g., object's size, category class, etc.). In addition, they extend this representation into 3D and achieve reasonable performance. Based on this framework, Liu et.al <ref type="bibr" target="#b19">[20]</ref> modify the baseline 3D detector by adding the group-normalization in backbone network and propose a multi-step disentangling approach for constructing the 3D bounding box. With these modifications, the training speed and detection performances have been significantly improved. Instead of representing the object as a single point, Li et.al., <ref type="bibr" target="#b20">[21]</ref> propose to use nine points which are center point plus eight vertexes of the 3D bounding box. First, the network is designed to detect all the key-points and a post-processing step is required for solving the object pose as an optimization problem.</p><p>Attention-based Feature Aggregation: recently, attention-based feature aggregation strategies have proven their effectiveness in many areas, such as image super-resolution <ref type="bibr" target="#b36">[37]</ref>, image translation <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, GAN based methods <ref type="bibr" target="#b39">[40]</ref>, semantic segmentation <ref type="bibr" target="#b40">[41]</ref>. According to previous work, the attention strategies can efficiently enhance extracted features in several manners, including: channel attention aggregation <ref type="bibr" target="#b41">[42]</ref> and spatial attention based aggregation <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref>. The channel attention based aggregation strategy aims to learn the weight of each channel of feature maps to aggregate the features in channel-level, while spatial attention based aggregation aims to learn the weight of each pixel in feature maps to aggregate the features in pixel-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definition and Baseline Method</head><p>Before the introduction of the proposed approach, the 3D object detection problem and the baseline center-based framework will be discussed first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>For easy understanding, the camera coordinate is set as the reference coordinate and all the objects are defined based on it. In deep-learning-based approaches, an object is generally represented as a rotated 3D BBox as</p><formula xml:id="formula_0">c, d, r = (c x , c y , c z ), (l, w, h), (r x , r y , r z ),<label>(1)</label></formula><p>in which c, d, r represent the centroid, dimension and orientation of the BBox respectively. In AD scenario, the road surface that the objects lie on is almost flat locally, therefore the orientation parameters are reduced from three to one by keeping only the yaw angle r y around the Y-axis. In this case, the BBox is simply represented as (c x , c y , c z , l, w, h, r y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Center-based 3D Object Detector</head><p>Center-based (anchor-free) approaches have been widely employed for 2D object detection and instance segmentation recently. In these methods, an object is represented as a center with associated attributes (e.g., dimensions and center offsets) which are obtained with a classification and regression branches simultaneously. Based on the 2D centernet, Liu et al. <ref type="bibr" target="#b19">[20]</ref> modified and improved it for 3D object detection, where the object center is the projection of 3D BBox's centroid and the associated attributes are 3d dimensions, depth and object's orientation etc.</p><p>A sketch of baseline 3D object detector is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>  classification and regressions are generated densely for all points of the feature map. In center classification branch, a point is classified as positive if its response is higher than a certain threshold. At the same time, its associated attributes can be obtained correspondingly according to its location index. 3D BBox Recovery: assuming that a point (x i , y i ) is classified as an object's center and its associated attributes usually includes (x offset , y offset ), depth, (l, w, h) and (sin?, cos?), where d is the depth of object, (l, w, h) is 3D BBox's dimension. Similar to <ref type="bibr" target="#b43">[44]</ref>, ? is an alternative representation of r y for easy regression and (x offset , y offset ) is estimated discretization residuals due to feature map downsampling operation. Based on the 2D center and its attributes, the 3D centroid (c x , c y , c z ) of the object can be recovered via</p><formula xml:id="formula_1">[c x , c y , c z ] T = K ?1 * [x i + x offset , y i + y offset , 1] T * depth,<label>(2)</label></formula><p>where K is the camera intrinsic parameter. Loss Function: during training, for each ground truth center p k of class j, its corresponding low-resolution pointp j in the down-sampled feature map is computed first. To increase the positive sample ratio, all the ground truth centers are splat onto a heatmap h ? [0, 1] with the size of W 4 ? H 4 ? C using a Gaus-</p><formula xml:id="formula_2">sian kernel h xyj = exp(? (x?px) 2 +(y?py) 2 2? 2 p</formula><p>, where ? p is an object size-adaptive standard deviation. If two Gaussians of the same class overlap, the elementwise maximum is employed here. The training loss for center point classification branch is defined as</p><formula xml:id="formula_3">L center-ness = ?1 M xyj (1 ?? xyj ) ? log(? xyj ) if h xy = 1, (1 ? h xyj ) ? (? xyj ) ? log(1 ?? xy ) otherwise.<label>(3)</label></formula><p>where ? and ? are hyper-parameters of the focal loss <ref type="bibr" target="#b44">[45]</ref> and M is the number of all positive center points.</p><p>Although the attributes regression is computed densely for each location in the feature map, the loss function is only defined sparsely on the ground truth centers. Usually, a general expression of regression loss is defined as</p><formula xml:id="formula_4">L reg = 1 N N i=1 (1 pi l reg ), 1 pi = 1 if p i is an object center, 0 otherwise.<label>(4)</label></formula><p>where l reg is a general definition of regression loss which can be defined as L 1 or smooth ? L 1 loss defined on the prediction directly, corners loss <ref type="bibr" target="#b19">[20]</ref> on the vertex of the recovered 3D BBox, IoU loss <ref type="bibr" target="#b45">[46]</ref> on 3D BBoxes or disentangling detection loss <ref type="bibr" target="#b46">[47]</ref> etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Approach</head><p>We propose the IAFA network to gather all the useful information related to a certain object for 3D pose regression. It generates a pixel-wise spatial attention map to aggregate all the features belongs to the objects together for contributing the center classification and its attribution regression. The proposed branch is a light-weight and plug-and-play module, which can be integrated into any onestage based 3D object detection framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IAFA Module</head><p>The proposed IAFA branch is highlighted with dotted box in <ref type="figure" target="#fig_1">Fig. 2</ref>, which aims at collecting all the useful information (e.g., related to a certain object) together to help 3D object detection task. Specifically, for a feature map F s in a certain level, with the size of W s ? H s ? C s , the IAFA module will generate a high-dimension matrix G with the size of W s ? H s ? D, where D = W s ? H s . For a certain location (i, j) of G, the vector G ij ? R 1?D encodes a dense relationship map of the target point p(i, j) with all the other locations (including itself). Intuitively, these pixels belonging to the same object should have closer relationship than those pixels that don't belong to the object and therefore they should give more contribution for the 3D object detection task. To achieve this purpose, we propose to use the corresponding object instance mask as a supervised signal for learning this attention map G. For efficient computation, this supervision signal is only sparsely added to object's centers. Some learned attention vectors G(i, j) (reshaped as images with the size of W s ? H s for easy understanding) are displayed in <ref type="figure" target="#fig_2">Fig. 4</ref>, in which three maps correspond to three objects' centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detailed IAFA Structure</head><p>The detailed structure of the proposed IAFA branch is illustrated in <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B:</head><p>Matrix Multiplication Element-wise Sum <ref type="figure">Fig. 3</ref>. The proposed IAFA module. The input F backbone will be enhanced by collecting the features from the corresponding instance to generate the output F enhanced .</p><p>R W s ?H s ?C which has the same dimension with the input feature map. The enhanced feature F enhanced can be obtained as</p><formula xml:id="formula_5">F enhanced = F backbone + ? * F aggregated ,<label>(5)</label></formula><p>where F aggregated is the aggregated features from other locations and ? is a learnable parameter initialized with zero to balance the importance of F aggregated and F backbone . The building of the F aggregated has been highlighted with red dotted rectangle in <ref type="figure">Fig. 3</ref>. If needed, the input F backbone can be downsampled to an appropriate size for saving the GPU memory and upsampled to the same size of F backbone after aggregation. For general representation, we assume the input features size is w?h?C. First of all, two new feature maps {F 1 , F 2 } ? R w?h?4C are generated with a series of convolutions operations which are represented with "Operation" A and B in short. Here A and B share the same structure with different parameters. Specifically, both of them contain two 1 ? 1 convolution layers, one non-linear activation layer (e.g., ReLU) and one group normalization layer. Detailed convolution kernel information and group size information are given in right top of <ref type="figure">Fig. 3</ref>. Then both of them are reshaped to R d?4C , where d = w ? h is the number of the pixels in F 1 or F 2 . Assuming the two reshaped tensors are F 1 and F 2 , then the high-dimension relation map G can be obtained as</p><formula xml:id="formula_6">G = N orm{Sigmoid(F 1 ? (F 2 ) T )},<label>(6)</label></formula><p>where ? represent the matrix multiplication, "Sigmoid" represent the Sigmoid function to re-scale the element's value from (??, +?) to (0, +1) and Norm represent the normalization operation along the row dimension. Then we reshape this relationship map G from R d?d to R W s ?H s ?d and each vector of G(i, j) gives the relationship of current pixel (i, j) with all other pixels. With the estimated G, F aggregated can be computed as</p><formula xml:id="formula_7">F aggregated = G ? F {F backbone },<label>(7)</label></formula><p>here F{.} operation is used for transforming the downsampled F backbone from the shape of w ? h ? C to the shape of d ? C. Finally, the F aggregated can be upsampled to W s ? H s , the same size as F backbone .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss Function for Instance Mask</head><p>Three loss functions are used for training the framework which are L center-ness , L reg and L mask . Here, we choose the smooth-L1 loss on the 3D BBox's 8 corners for regression loss L reg . Consequently, the whole loss function is formulated as</p><formula xml:id="formula_8">L = ? 0 L center-ness + ? 1 L reg + ? 2 L mask ,<label>(8)</label></formula><p>where ? 0 , ? 1 and ? 3 are hype-parameters for balancing the contributions of different parts. As shown in the green dotted box in <ref type="figure">Fig. 3</ref>, the loss for mask is only activated sparsely on the center points. Due to the unbalance between the foreground and background pixels, focal loss is also applied here. Similar to <ref type="formula" target="#formula_3">(3)</ref>, the L mask is defined with focal loss as</p><formula xml:id="formula_9">L mask = ? 1 N N j=0 1 M j Mj i=0 (1 ?? i ) ? log(? i )<label>(9)</label></formula><p>where? i is the predicted probability that a pixel i belongs to a certain instance j, N is the number of instance per batch and M j is the number of pixels for instance j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Coarse Instance Annotation Generation</head><p>For training the mask attention module, dense pixel-wise instance segmentation annotation is needed. However, for most of the 3D object detection dataset (e.g., KITTI <ref type="bibr" target="#b0">[1]</ref>), only the 2D/3D bounding boxes are provided and the instance-level segmentation annotation is not provided. In our experiment, we just used the output of the commonly used instance segmentation framework "Mask R-CNN <ref type="bibr" target="#b47">[48]</ref>" as the coarse label. Surprisingly, we find that the performance can also be boosted evenly with this kind of noise label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We implement our approach and evaluate it on the public KITTI [1] 3D object detection benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Implementation Details</head><p>Dataset: the KIITI data is collected from the real traffic environment in Europe streets. The whole dataset has been divided into training and testing two subsets, which consist of 7, 481 and 7, 518 frames, respectively. Since the ground truth for the testing set is not available, we divide the training data into a training and validation set as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5]</ref>, and obtain 3, 712 data samples for training and 3, 769 data samples for validation to refine our model. On the KITTI benchmark, the objects have been categorized into "Easy", "Moderate" and "Hard" based on their height in the image and occlusion ratio, etc. For each frame, both the camera image and the LiDAR point cloud have been provided, while only RGB image has been used for object detection and the point cloud is only used for visualization purposes.</p><p>Evaluation Metric: we focus on the evaluation on "Car" category because it has been considered most in the previous approaches. In addition, the number of the training data for "Pedestrain" and "Cyclist" is too small for training a stable model. For evaluation, the average precision (AP) with Intersection over Union (IoU) is used as the metric for evaluation. Specifically, before October 8, 2019, the KITTI test sever used the 11 recall positions for comparison. After that the test sever change the evaluation criterion from 11-points to 40-points because the latter one is proved to be more stable than the former <ref type="bibr" target="#b46">[47]</ref>. Therefore, we use the 40-points criterion on the test sever, while we keep the 11-points criterion on validation dataset because most of previous methods only report their performance using 11-points criterion.</p><p>Implementation Details: for each original image, we pad it symmetrically to 1280 ? 384 for both training and inference. Before training, these ground truth BBoxes whose depth larger than 50 m or whose 2D projected center is out of the image range are eliminated and all the rest are used for training our model. Similar to <ref type="bibr" target="#b19">[20]</ref>, three types of data-augmentation strategies have been applied here: random horizontal flip, random scale and shift. The scale ratio is set to 9 steps from 0.6 to 1.4, and the shift ratio is set to 5 steps from ?0.2 to 0.2. To be clear, the scale and shift augmentation haven't been used for the regression branch because the 3D information is inconsistent after these transformations. Parameters setting: for each object, the "depth" prediction is defined as depth = a 0 + b 0 x, where a 0 , b 0 are two predefined constants and x is the output of the network. Here, we set a 0 = b 0 = 12.5 experimentally and re-scale the output x ? [?1.0, 1.0]. For the focal loss in <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula" target="#formula_9">(9)</ref>, we set ? = 2.0 and ? = 4.0 for all the experiments. The group number for normalization in IAFA module is set to 8. For decreasing the GPU consumption, we set w = 1 2 W s and h = 1 2 H s in the IAFA module. Training: Adam <ref type="bibr" target="#b48">[49]</ref> together with L1 weights regularization is used for optimizing our model. The network is trained with a batch size of 42 on 7 Tesla V100 for 160 epochs. The learning rate is set at 2.5 ? 10 ?4 and drops at 80 and 120 epochs by a factor of 10. The total training process requires about 12 hours. During testing, top 100 center points with response above 0.25 are chosen as valid detection. No data augmentation is applied during inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on the "test" Split</head><p>First of all, we evaluate our methods with other monocular based 3D object detectors on the KITTI testing benchmark. Due the limited space, we only list the results with public publications. For fair comparison, all the numbers are collected directly from the official benchmark website 5 . Here, we show the Bird-Eye-View (BEV) and 3D results with threshold of 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Modality  <ref type="table">Table 1</ref>. Comparison with other public methods on the KITTI testing sever for 3D "Car" detection. For the "direct" methods, we represent the " Modality" with "Mono" only. For the other methods, we use * , ? to indicate that the "depth" or "3D model" have been used by these methods during training or inference procedure. For each column, we have highlighted the top numbers in bold and the second best is shown in blue. The numbers in red represent the absolute improvements compared with the baseline.</p><p>Comparison with SOTA methods: we make our results public on the KITTI benchmark sever and the comparison with other methods are listed in Tab. 1. For fair comparison, the monocular-based methods can also be divided into two groups, which are illustrated in the top and middle rows of Tab. 1, respectively. The former is called the "direct"-based method, which only uses a single image during the training and inference. In the latter type, other information such as depth or 3D model is used as an auxiliary during the training or inference. Our proposed method belongs to the former.</p><p>Similar to the official benchmark website, all the results are displayed in ascending order based on the values of "Moderate" AP 3D 70. From the table, we can find that the proposed method outperforms all the "direct"-based method with a big margin among all the three categories. For example, for "Easy" AP BEV 70, our method achieved 5.05 points improvements than the best method of SMOKE <ref type="bibr" target="#b19">[20]</ref>. The minimum improvement happens on "Moderate" AP 3D 70, even so, we also obtained 1.67 points of improvements than RTM3D <ref type="bibr" target="#b20">[21]</ref>.</p><p>Based on the evaluation criterion defined by KITTI (ranking based on values of "Moderate" AP 3D 70 ), our method achieved the first place among all the monocular-based 3D object detectors 6 up to the submission of this manuscript (Jul. 8, 2020), including these models trained with depth or 3D models. Specifically, the proposed method achieved four first places, two second places among all the six sub-items. The run time of different methods is also provided in the last column of Tab. 1. Compared with other methods, we also show superiority on efficiency. By using the DAL34 as the backbone network, our methods can achieve 29 fps on Tesla V-100 with a resolution of 384 ? 1280.</p><p>Comparison with baseline: from the table, we also can find that the proposed method significantly boosts the baseline method on both the BEV and 3D evaluation among all the six sub-items. Especially, for "Easy" category, we have achieved 3.78 and 5.05 points improvements for AP 3D and AP BEV respectively. For the other four sub-items, the proposed method achieves more 2.0 points improvement. The minimal improvement we have achieved is for "Moderate" AP 3D 70, while it also provides 2.25 points of improvement.  <ref type="table">Table 2</ref>. Comparison with other public methods on the KITTI "val" split for 3D "Car" detection, where "-" represent the values are not provided in their papers. For easy understanding, we have highlighted the top number in bold for each column and the second best is shown in blue. The numbers in red represent the absolute improvements compared with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on "validation" Split</head><p>We also evaluate our proposed method on the validation split. The detailed comparison is given in Tab. 2. As mentioned in <ref type="bibr" target="#b14">[15]</ref>, the 200 training images of KITTI stereo 2015 overlap with the validation images of KITTI object detection. That is to say, some LiDAR point cloud in the object detection validation split has been used for training the depth estimation networks. That is the reason why some 3D object detectors (with depth for training) achieved very good performances while obtained unsatisfactory results on the test dataset. Therefore, we only list the "direct"-based methods for comparison here. Compared with the baseline method, the proposed method achieves significant improvements among all the six sub-items. Especially, we achieve more than 3.0 points improvement in four items and the improvements for all the items are above 2.0 points. Comparison with other methods, we achieve 2 first places, 2 second places and 2 third places among all the 6 sub-items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>In addition, we also have designed a set of ablation experiments to verify the effectiveness of each module of our proposed method.  <ref type="table">Table 3</ref>. Ablation studies on the KITTI "val" split for 3D "Car" detection with/without instance mask supervision. From the table, we can easily find that the performances have been largely improved by adding the mask supervision signal.</p><p>Supervision of the instance mask: self-attention strategy is commonly used for in semantic segmentation <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b40">[41]</ref> and object detection <ref type="bibr" target="#b57">[58]</ref> etc. To highlight the influence of the supervision of the instance mask, we compare the performance of the proposed module with and without the supervision signal. From the table, we can easily found that the supervision signal is particularly useful for training IAFA module. Without the supervision, the detection performance nearly unchanged. The positive effect of the instance supervision signal is obvious. Furthermore, we also illustrated some examples of the learned attention maps in <ref type="figure" target="#fig_2">Fig. 4</ref>, where the bottom sub-figures are the corresponding attention maps for the three instances respectively. From the figure, we can see that the maximum value is at the center of object and it decreases gradually with the increasing of its distance to the object center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Results</head><p>Some qualitative detection results on the test split are displayed in <ref type="figure" target="#fig_3">Fig. 5</ref>. For better visualization and comparison, we also draw the 3D BBoxes in point cloud and BEV-view images. The results clearly demonstrate that the proposed framework can recover objects' 3D information accurately.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Works</head><p>In this paper, we have proposed a simple but effective instance-aware feature aggregation (IAFA) module to collect all the related information for the task of single image-based 3D object detection. The proposed module is an easily implemented plug-and-play module that can be incorporated into any one-stage object detection framework. In addition, we find out that the IAFA module can achieve satisfactory performance even though the coarsely annotated instance masks are used as supervision signals.</p><p>In the future, we plan to implement the proposed framework for real-world AD applications. Our proposed framework can also be extended to a multicamera configuration to handle the detection from 360?-viewpoints. In addition, extending the detector to multi-frame is also an interesting direction, which can boost the detection performances of distant instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of center-point-based object representation. The red points in the left sub-image are the projected 3D center of the objects on the 2D image. The numbers in the blue text box represent the "ID" of vehicles. The red points are the projected 3D vehicle centers onto the 2D image plane. The number in the red text boxes represents which vehicle that the center point belongs to.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A sketch description of the proposed Instance-Aware Feature Aggregation (IAFA) module integrated with the baseline 3D object detector. In which, the structure inside the dotted bordered rectangle is the proposed IAFA and "C" and "N" at the end of the frameworks are the number of "categories" and regression parameters respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>An example of feature map for IAFA module. Different brightness reveals different importance related to the target point (red dot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Three examples of 3D detection results. The "top", "Middle" and "Bottom" are results are drawn in RGB image, 3D Point cloud and BEV-view respectively. The point cloud is only used for visualization purposes here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. By passing the backbone network (e.g., DLA34<ref type="bibr" target="#b42">[43]</ref>), a feature map F backbone with the size of W 4 ? H 4 ? 64 is generated from the input image I (W ? H ? 3). After a specific 1 ? 1 ? 256 convolution layer, two separate branches are designed for center classification and corresponding attributes regression. Due to anchor-free, the</figDesc><table><row><cell>H * W * 3</cell><cell cols="2">H/4 * W/4 * 64</cell><cell>H/4 * W/4 * 256</cell><cell>H/4 * W/4 * C</cell></row><row><cell></cell><cell>DLA34 Backbone</cell><cell></cell><cell>1 x 1 Conv (256)</cell><cell>Center-ness Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>H/4 * W/4 * N</cell></row><row><cell></cell><cell>Instance Mask Loss</cell><cell cols="2">Instance-Aware Feature Aggregation (IAFA)</cell><cell>Regression Loss</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.</head><label></label><figDesc>The input of IAFA is the feature map F backbone ? R W s ?H s ?C from the backbone network and the output of the module is the enhanced feature F enhanced ?</figDesc><table><row><cell>H s</cell><cell cols="2">F 1 : (w*h)*4C</cell><cell>F 2 : (w*h)*4C</cell><cell>A:</cell><cell>Conv 1x1x2C</cell><cell>ReLU</cell><cell>Group Norm, 8</cell><cell>Conv 1x1x4C</cell></row><row><cell>W s</cell><cell>A F 1</cell><cell></cell><cell>B T F 2</cell><cell></cell><cell>Conv 1x1x2C</cell><cell>ReLU</cell><cell>Group Norm, 8</cell><cell>Conv 1x1x4C</cell></row><row><cell>C</cell><cell>C*(w*h)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F_backbone</cell><cell>Reshape &amp; Transform</cell><cell>Sigmoid, Norm, Reshape</cell><cell></cell><cell cols="2">Reshape</cell><cell>h</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(w*h)*(w*h)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? F enhanced</cell><cell cols="3">Reshape &amp; Transform F aggregated w*h*C</cell><cell></cell><cell cols="2">Mask Loss w*h</cell><cell>w</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Mono 16.48 20.40 13.34 21.15 26.86 17.14 Baseline [20] Mono 12.85 14.76 11.50 15.61 19.99 15.28 Proposed Mono 14.96 18.95 14.84 19.60 22.75 19.21 Improvements +2.11 +4.19 +3.34 +3.998 +2.76 +3.94</figDesc><table><row><cell>Methods</cell><cell>Modality</cell><cell cols="6">AP3D70 (%) Mod Easy Hard Mod APBEV70 (%) Easy Hard</cell></row><row><cell cols="2">CenterNet [18] Mono</cell><cell>1.06</cell><cell>0.86</cell><cell>0.66</cell><cell>3.23</cell><cell>4.46</cell><cell>3.53</cell></row><row><cell cols="2">Mono3D [22] Mono</cell><cell>2.31</cell><cell>2.53</cell><cell>2.31</cell><cell>5.19</cell><cell>5.22</cell><cell>4.13</cell></row><row><cell cols="2">OFTNet [57] Mono</cell><cell>3.27</cell><cell>4.07</cell><cell>3.29</cell><cell>8.79</cell><cell cols="2">11.06 8.91</cell></row><row><cell cols="2">GS3D [31] Mono</cell><cell cols="3">10.51 11.63 10.51</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MonoGRNet [52] Mono</cell><cell cols="3">10.19 13.88 7.62</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ROI-10D [53] Mono</cell><cell>6.63</cell><cell>9.61</cell><cell>6.29</cell><cell>9.91</cell><cell cols="2">14.50 8.73</cell></row><row><cell cols="2">MonoDIS [47] Mono</cell><cell cols="6">14.98 18.05 13.42 18.43 24.26 16.95</cell></row><row><cell>M3D-RPN [28]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Mod Easy Hard Mod Easy Hard Baseline 12.85 14.76 11.50 15.61 19.99 15.28 w/o supervision 12.98 14.59 11.76 15.79 20.12 14.98 w supervision 14.96 18.95 14.84 19.60 22.75 19.21</figDesc><table><row><cell>Methods</cell><cell>AP3D70 (%) APBEV70 (%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Only these methods with publications have been listed for comparison here.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The apolloscape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichuan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05784</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lidarbased online 3d video object detection with graph-based message passing and spatiotemporal transformer attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11495" to="11504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Std: Sparse-todense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moving object detection and segmentation in urban environments from a moving platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Fr?mont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Quost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="76" to="87" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiview photometric stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="554" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03080</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Refinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean Marie Uwabeza</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhra</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09712</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Smoke: Single-stage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>T?th</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10111</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03343</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint 3d instance segmentation and object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1839" to="1849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5452" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskil</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08070</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00504</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13906" to="13915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Centermask: single shot instance segmentation with point representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9313" to="9321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution via attention based back projection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yui-Lam</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3517" to="3525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multichannel attention selection gan with cascaded semantic guidance for cross-view image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Channel attention networks for image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mateen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="95751" to="95761" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Channel attention based iterative residual learning for depth map superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5631" to="5640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-view reprojection architecture for orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Min</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoa</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsuk</forename><surname>Hyun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andretti</forename><surname>Naiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongmo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongmoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monofenet: Monocular 3d object detection with feature enhancement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with decoupled structured polygon estimation and height-guided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="395" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
