<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Homography estimation between multiple aerial images can provide relative pose estimation for collaborative autonomous exploration and monitoring. The usage on a robotic system requires a fast and robust homography estimation algorithm. In this study, we propose an unsupervised learning algorithm that trains a Deep Convolutional Neural Network to estimate planar homographies. We compare the proposed algorithm to traditional feature-based and direct methods, as well as a corresponding supervised learning algorithm. Our empirical results demonstrate that compared to traditional approaches, the unsupervised algorithm achieves faster inference speed, while maintaining comparable or better accuracy and robustness to illumination variation. In addition, our unsupervised method has superior adaptability and performance compared to the corresponding supervised deep learning method. Our image dataset and a Tensorflow implementation of our work are available at htt ps</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A homography is a mapping between two images of a planar surface from different perspectives. They play an essential role in robotics and computer vision applications such as image mosaicing <ref type="bibr" target="#b0">[1]</ref>, monocular SLAM <ref type="bibr" target="#b1">[2]</ref>, 3D camera pose reconstruction <ref type="bibr" target="#b2">[3]</ref> and virtual touring <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. For example, homographies are applicable in scenes viewed at a far distance by an arbitrary moving camera <ref type="bibr" target="#b5">[6]</ref>, which are the situations encountered in UAV imagery. However, to work well in the aerial multi-robot setting, the homography estimation algorithm needs to be reliable and fast.</p><p>The two traditional approaches for homography estimation are direct methods and feature-based methods <ref type="bibr" target="#b6">[7]</ref>. Direct methods, such as the seminal Lucas-Kanade algorithm <ref type="bibr" target="#b7">[8]</ref>, use pixel-to-pixel matching by shifting or warping the images relative to each other and comparing the pixel intensity values using an error metric such as the sum of squared differences (SSD). They initialize a guess for the homography parameters and use a search or optimization technique such as gradient descent to minimize the error function <ref type="bibr" target="#b8">[9]</ref>. The robustness of direct methods can be improved by using different performance criterion such as the enhanced correlation coefficient (ECC) <ref type="bibr" target="#b9">[10]</ref>, integrating feature-based methods with direct methods <ref type="bibr" target="#b10">[11]</ref>, or by representing the images in the Fourier domain <ref type="bibr" target="#b11">[12]</ref>. In addition, the speed of direct methods can be increased by using efficient compositional image alignment schemes <ref type="bibr" target="#b12">[13]</ref>.</p><p>The authors are with GRASP Lab, University of Pennsylvania, Philadelphia, PA 19104, USA, {tynguyen, chenste, sshreyas, cjtaylor, kumar}@seas.upenn.edu. * : The authors have equal contributions <ref type="figure">Fig. 1</ref>: Above: Synthetic data; Below: Real data; Homography estimation results from the unsupervised neural network. Red represents the ground truth correspondences, and yellow represents the estimated correspondences. These images depict an example of large levels of displacement and illumination shifts in which feature-based, direct and/or supervised learning methods fail.</p><p>The second approach are feature-based methods. These methods first extract keypoints in each image using local invariant features (e.g. Scale Invariant Feature Transform (SIFT) <ref type="bibr" target="#b13">[14]</ref>). They then establish a correspondence between the two sets of keypoints using feature matching, and use RANSAC <ref type="bibr" target="#b14">[15]</ref> to find the best homography estimate. While these methods have better performance than direct methods, they can be inaccurate when they fail to detect sufficient keypoints, or produce incorrect keypoint correspondences due to illumination and large viewpoint differences between the images <ref type="bibr" target="#b15">[16]</ref>. In addition, these methods are significantly faster than direct methods but can still be slow due to the computation of the features, leading to the development of other feature types such as Oriented FAST and Rotated BRIEF (ORB) <ref type="bibr" target="#b16">[17]</ref> which are more computationally efficient than SIFT, but have worse performance.</p><p>Inspired by the success of data-driven Deep Convolutional Neural Networks (CNN) in computer vision, there has been an emergence of CNN approaches to estimating optical flow <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, dense matching <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, depth estimation <ref type="bibr" target="#b22">[23]</ref>, and homography estimation <ref type="bibr" target="#b23">[24]</ref>. Most of these works, including the most relevant work on homography estimation, treat the estimation problem as a supervised learning task. These supervised approaches use ground truth labels, and as a result are limited to synthetic datasets where the ground truth can be generated for free, or require costly labeling of real-world data sets.</p><p>Our work develops an unsupervised, end-to-end, deep learning algorithm to estimate homographies. It improves upon these prior traditional and supervised learning methods by minimizing a pixel-wise intensity error metric that does not need ground truth data. Unlike the hand-crafted featurebased approaches, or the supervised approach that needs costly labels, our model is adaptive and can easily learn good features specific to different data sets. Furthermore, our framework has fast inference times since it is highly parallel. These adaptive and speed properties make our unsupervised networks especially suitable for real world robotic tasks, such as stitching UAV images. We demonstrate that our unsupervised homography estimation algorithm has comparable or better accuracy, and better inference speed, than feature-based, direct, and supervised deep learning methods on synthetic and real-world UAV data sets. In addition, we demonstrate that it can handle large displacements (? 65% image overlap) with large illumination variation. <ref type="figure">Fig. 1</ref> illustrates qualitative results on these data sets, where our unsupervised method is able to estimate the homography whereas the other approaches cannot.</p><p>Our unsupervised algorithm is a hybrid approach that combines the strengths of deep learning with the strengths of both traditional direct methods and feature-based methods. It is similar to feature-based methods because it also relies on features to compute the homography estimates, but it differs in that it learns the features rather than defining them. It is also similar to the direct methods because the error signal used to drive the network training is a pixelwise error. However, rather than performing an online optimization process, it transfers the computation offline and "caches" the results through these learned features. Similar unsupervised deep learning approaches have been successful in computer vision tasks such as monocular depth and camera motion estimation <ref type="bibr" target="#b24">[25]</ref>, indicating that our framework can be scaled to tackle general nonlinear motions such as those encountered in optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION</head><p>We assume that images are obtained by a perspective pinhole camera and present points by homogeneous coordinates, so that a point (u, v) T is represented as (u, v, 1) T and a point (x, y, z) T is equivalent to the point (x/z, y/z, 1) T . Suppose that x = (u, v, 1) T and x = (u , v , 1) T are two points. A planar projective transformation or homography that maps x ? x is a linear transformation represented by a non-singular 3 ? 3 matrix H such that:</p><formula xml:id="formula_0">? ? u v 1 ? ? = ? ? h 11 h 12 h 13 h 21 h 22 h 23 h 31 h 32 h 33 ? ? ? ? u v 1 ? ? Or x = Hx<label>(1)</label></formula><p>Since H can be multiplied by an arbitrary non-zero scale factor without altering the projective transformation, only the ratio of the matrix elements is significant, leaving H eight independent ratios corresponding to eight degrees of freedom. This mapping equation can also represented by two equations:</p><formula xml:id="formula_1">u = h 11 u + h 12 v + h 13 h 31 u + h 32 v + h 33 ; v = h 21 u + h 22 v + h 23 h 31 u + h 32 v + h 33 (2)</formula><p>The problem of finding the homography induced by two images I A and I B is to find a homography H AB such that Eqn. (1) holds for all points in the overlapping of the two images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SUPERVISED DEEP HOMOGRAPHY MODEL</head><p>The deep learning approach most similar to our work is the Deep Image Homography Estimation <ref type="bibr" target="#b23">[24]</ref>. In this work, DeTone et al. use supervised learning to train a deep neural network on a synthetic data set. They use the 4point homography parameterization H 4pt <ref type="bibr" target="#b25">[26]</ref> rather than the conventional 3 ? 3 parameterization H. Suppose that</p><formula xml:id="formula_2">u A k = (u A k , v A k , 1) T and u B k = (u B k , v B k , 1) T for k = 1, 2, 3, 4 are 4 fixed points in image I A and I B respectively, such that u k 2 = Hu k 1 . Let ?u k = u B k ? u A k , ?v k = v B k ? v A k .</formula><p>Then H 4pt is the 4 ? 2 matrix of points (?u k , ?v k ). Both parameterizations are equivalent since there is a one-to-one correspondence between them.</p><p>In a deep learning framework though, this parameterization is more suitable than the 3 ? 3 parameterization H because H mixes the rotation, translation, scale, and shear components of the homography transformation. The rotation and shear components tend to have a much smaller magnitude than the translation component, and as a result although an error in their values can greatly impact H, it will have a small effect on the L2 loss function of the elements of H, which is detrimental for training the neural network. In addition, the high variance in the magnitude of the elements of the 3 ? 3 homography matrix makes it difficult to enforce H to be non-singular. The 4-point parameterization does not suffer from these problems.</p><p>The network architecture is based on VGGNet <ref type="bibr" target="#b26">[27]</ref>, and is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>(a). The network input is a batch of image patch pairs. The patch pairs are generated by taking a fullsized image, cropping a square patch P A at a random position p, perturbing the four corners of by a random value within [??, ?] to generate a homography H AB , applying (H AB ) ?1 to the full-sized image, and then cropping a square patch P B of the same size and at the same location as the patch P A from the warped image. These image patches are used to avoid strange border effects near the edges during the synthetic data generation process, and to standardize the network input size. The applied homography H AB is saved in the 4 point parameterization format, H * 4pt . The network outputs a 4 point parameterization estimateH 4pt .</p><p>The error signal used for gradient backpropagation is the Euclidean L2 norm, denoted as L H , of the estimated 4-point homographyH 4pt versus the ground truth H * 4pt : </p><formula xml:id="formula_3">L H = 1 2 ||H 4pt ? H * 4pt || 2 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. UNSUPERVISED DEEP HOMOGRAPHY MODEL</head><p>While the supervised deep learning method has promising results, it is limited in real world applications since it requires ground truth labels. Drawing inspiration from traditional direct methods for homography estimation, we can define an analogous loss function. Given an image pair I A (x) and I B (x) with discrete pixel locations represented by homogeneous coordinates {x i = (x i , y i , 1) T }, we want our network to output H 4pt that minimizes the average L1 pixel-wise photometric loss</p><formula xml:id="formula_4">L PW = 1 |x i | ? x i |I A (H (x i )) ? I B (x i )|<label>(4)</label></formula><p>whereH 4pt defines the homography transformation H (x i ).</p><p>We chose the L1 error versus the L2 error because previous work has observed that it is more suitable for image alignment problems <ref type="bibr" target="#b27">[28]</ref>, and empirically we found the network to be easier to train with the L1 error. This loss function is unsupervised since there is no ground truth label. Similar to the supervised case, we choose the 4-point parameterization which is more suitable than the 3 ? 3 parameterization. In order to compare our unsupervised deep learning algorithm with the supervised algorithm, we use the same VGGNet architecture to output theH 4pt . <ref type="figure" target="#fig_0">Fig. 2</ref>(c) depicts our unsupervised learning model. The regression module represents the VGGNet architecture and is shared by both the supervised and unsupervised methods. Although we do not investigate other possible architectures, different regression models such as SqueezeNet <ref type="bibr" target="#b28">[29]</ref> may yield better performance due to advantages in size and computation require-ments. The second half of <ref type="figure" target="#fig_0">Fig. 2(c)</ref> represents the main contribution of this work, which consists of the differentiable layers that allow the network to be successfully trained with the loss function (4).</p><p>Using the pixel-wise photometric loss function yields additional training challenges. First, every operation, including the warping operation H (x i ), must remain differentiable to allow the network to be trained via backpropagation. Second, since the error signal depends on differences in image intensity values rather than the differences in the homography parameters, training the deep network is not necessarily as easy or stable. Another implication of using a pixel-wise photometric loss function is the implied assumption that lighting and contrast between the input images remains consistent. In traditional direct methods such as ECC, this appearance variation problem is addressed by modifying the loss function or preprocessing the images. In our unsupervised algorithm, we standardize our images by the mean and variance of the intensities of all pixels in our training dataset, perform data augmentation by injecting random illumination shifts, and use the standard L1 photometric loss. We found that even without modifying the loss function, our deep neural network is still able to learn to be invariant to illumination changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Inputs</head><p>The input to our model consists of three parts. The first part is a 2-channel image of size 128 ? 128 ? 2 which is the stack of P A and P B -two patches cropped from the two images I A and I B . The second part is the four corners in I A , denoted as C A 4pt . Image I A is also part of the input as it is necessary for warping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tensor Direct Linear Transform</head><p>We develop a Tensor Direct Linear Transform (Tensor DLT) layer to compute a differentiable mapping from the 4point parameterizationH 4pt toH, the 3 ? 3 parameterization of homography. This layer essentially applies the DLT algorithm <ref type="bibr" target="#b29">[30]</ref> to tensors, while remaining differentiable to allow backpropagation during training. As shown in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>, the input to this layer are the corresponding corners in the image pairs C A 4pt andC B 4pt , and the output is the estimate of the 3?3 homography parameterizationH.</p><p>The DLT algorithm is used to solve for the homography matrix H given a set of four point correspondences <ref type="bibr" target="#b29">[30]</ref>. Let H be the homography induced by a set of four 2D to 2D correspondences, x i ? x i . According to the definition of a homography given in Eqn. (1), x i ? Hx i . This relation can also be expressed as x i ? Hx i = 0.</p><p>Let h jT be the j-th row of H, then:</p><formula xml:id="formula_5">Hx i = ? ? h 1T x i h 2T x i h 3T x i ? ? = ? ? x T i h 1 x T i h 2 x T i h 3 ? ?<label>(5)</label></formula><p>where h j is the column vector representation of h jT . Let x i = (u i , v i , 1) T , then:</p><formula xml:id="formula_6">x i ? Hx i = ? ? v i x T i h 3 ? x T i h 2 x T i h 1 ? u i x T i h 3 u i x T i h 2 ? v i x T i h 1 ? ? = 0<label>(6)</label></formula><p>This equation can be rewritten as:</p><formula xml:id="formula_7">? ? 0 T 3?1 ?x T i v i x T i x T i 0 T 3?1 ?u i x T i ?v i x T i u i x T i 0 T 3?1 ? ? ? ? h 1 h 2 h 3 ? ? = 0.<label>(7)</label></formula><p>which has the form A</p><p>i h = 0 for each i = 1, 2, 3, 4 correspondence pair, where A (3) i is a 3 ? 9 matrix, and h is a vector with 9 elements consisting of the entries of H. Since the last row in A (3) i is dependent on the other rows, we are left with two linear equations A i h = 0 where A i is the first 2 rows of A</p><p>i . Given a set of 4 correspondences, we can create a system of equations to solve for h and thus H. For each i, we can stack A i to form Ah = 0. Solving for h results in finding a vector in the null space of A. One popular approach is singular value decomposition (SVD) <ref type="bibr" target="#b30">[31]</ref>, which is a differentiable operation. However, taking the gradients in SVD has high time complexity and has practical implementation issues <ref type="bibr" target="#b31">[32]</ref>. An alternative solution to this problem is to make the assumption that the last element of h 3 , which is H 33 is equal to 1 <ref type="bibr" target="#b32">[33]</ref>.</p><p>With this assumption and the fact that x i = (u i , v i , 1), we can rewrite Eqn. <ref type="bibr" target="#b6">(7)</ref> in the form? i? =b i for each i = 1, 2, 3, 4 correspondence points where? i is the 2 ? 8 matrix representing the first 8 columns of A i ,</p><formula xml:id="formula_10">A i = 0 0 0 ?u i ?v i ?1 v i u i v i v i u i v i 1 0 0 0 ?u i u i ?u i v i ,b</formula><p>i is a vector with 2 elements representing the last column of A i subtracted from both sides of the equation,</p><formula xml:id="formula_11">b i = [?v i , u i ] T ,</formula><p>and? is a vector consisting of the first 8 elements of h (with H 33 omitted).</p><p>By stacking these equations, we get:</p><formula xml:id="formula_12">A? =b,<label>(8)</label></formula><p>Eqn. (8) has a desirable form because?, and thus H, can be solved for using? + , the pseudo-inverse of?. This operation is simple and differentiable with respect to the coordinates of x i and x i . In addition, the gradients are easier to calculate than for SVD.</p><p>This approach may still fail if the correspondence points are collinear: if three of the correspondence points are on the same line, then solving for H is undetermined. We alleviate this problem by first making the initial guess of H 4pt to be zero, implying thatC B 4pt ? C A 4pt . We then set a small learning rate such that after each training iteration,C B 4pt does not move too far away from C A 4pt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Transformation Layer</head><p>The next layer applies the 3 ? 3 homography estimat? H output by the Tensor DLT to the pixel coordinates x i of image I A in order to get warped coordinates H (x i ). These warped coordinates are necessary in computing the photometric loss function in Eqn. (4) that will train our neural network. In addition to warping the coordinates, this layer must also be differentiable so that the error gradients can flow through via backpropagation. We thus extend the Spatial Transformer Layer introduced in <ref type="bibr" target="#b33">[34]</ref> by applying it to homography transformations.</p><p>This layer performs an inverse warping in order to avoid holes in the warped image. This process consists of 3 steps: (1) Normalized inverse computationH inv of the homography estimate; (2) Parameterized Sampling Grid Generator (PSGG); and (3) Differentiable Sampling (DS).</p><p>The first step, computing a normalized inverse, involves normalizing the height and width coordinates of images I A and I B into a range such that ?1 ? u i , v i ? 1 and ?1 ? u i , v i ? 1. Thus given a 3 ? 3 homography estimateH, the invers? H inv used for warping is computed as follows: </p><formula xml:id="formula_13">H inv = M ?1H?1 M where M = ? ? W /2 0 W /2 0 H /2 H /2 0 0 1 ? ? with W</formula><formula xml:id="formula_14">v i 1 ? ? = H inv (G i ) =H inv ? ? u i v i 1 ? ?<label>(9)</label></formula><p>Based on the sampling points H inv (G i ) computed from PSGG, the last step (DS) produces a sampled warped im-</p><formula xml:id="formula_15">age V of size H ? W with C channels, where V (x i ) = I A (H (x i )).</formula><p>The sampling kernel k(?) is applied to the grid H inv (G i ) and the resulting image V is defined as</p><formula xml:id="formula_16">V C i = H ? n W ? m I c nm k(u i ? m; ? u )k(v i ? n; ? v ), ?i ? [1...H W ] , ?c ? [1..C]<label>(10)</label></formula><p>where H,W are the height and width of the input image I A , ? u and ? v are the parameters of k(?) defining the image interpolation. I c nm is the value at location (n, m) in channel c of the input image, and V c i is the value of the output pixel at location (u i , v i ) in channel c. Here, we use bilinear interpolation such that the Eqn. (10) becomes</p><formula xml:id="formula_17">V C i = H ? n W ? m I c nm max(0, 1 ? |u i ? m|) max(0, 1 ? |v i ? n|)<label>(11)</label></formula><p>To allow backpropagation of the loss function, gradients with respect to I and G for bilinear interpolation are defined as</p><formula xml:id="formula_18">?V c i ? I c nm = H ? n W ? m max(0, 1 ? |u i ? m|) max(0, 1 ? |v i ? n|) (12) ?V c i ? u i = H ? n W ? m I c nm max(0, 1 ? |v i ? n|) ? ? ? 0 if |m ? u i | ? 1 1 if m ? u i ?1 if m &lt; u i (13) ?V c i ? v i = H ? n W ? m I c nm max(0, 1 ? |u i ? m|) ? ? ? 0 if |n ? v i | ? 1 1 if n ? v i ?1 if n &lt; v i<label>(14)</label></formula><p>This allows backpropagation of the loss gradients using the chain rule because ? u i ? h jk</p><formula xml:id="formula_19">and ? v i ? h jk</formula><p>can be easily derived from Eqn. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION RESULTS</head><p>The intended use case for our algorithm is in estimating homographies for aerial multi-robot systems applications such as image mosaicing and collision avoidance. As a result, we demonstrate our unsupervised algorithm's accuracy, inference speed, and robustness to illumination variation relative to SIFT, ORB, ECC and the supervised deep learning method. We evaluate these methods on a synthetic dataset similar to the dataset used in <ref type="bibr" target="#b23">[24]</ref>, and on a real-world aerial image dataset. Since ORB's performance is inferior to that of SIFT, we only report ORB's performance in <ref type="figure">Fig. 4</ref> and omit it in the remaining figures.</p><p>Both the supervised and unsupervised approaches use the VGGNet architecture to generate homography estimates H 4pt . The deep learning approaches are implemented in Tensorflow <ref type="bibr" target="#b34">[35]</ref> using stochastic gradient descent with a batch size of 128, and an Adam Optimizer <ref type="bibr" target="#b35">[36]</ref> with, ? 1 = 0.9, ? 2 = 0.999 and ? = 10 ?8 . We empirically chose the initial learning rate for the supervised algorithm and unsupervised algorithm to be 0.0005 and 0.0001 respectively.</p><p>The ECC direct method is a standard Python OpenCV implementation while the feature-based approaches are Python OpenCV implementations of SIFT RANSAC and ORB RANSAC. We found that in our synthetic dataset, using all detected features gives better performance, while in our aerial dataset, choosing the 50 best features is superior. These feature pairs are then used to calculate the homography using RANSAC with a threshold of 5 pixels. For the ECC method, we use identity matrix as the initialization and set 1000 as the maximum number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Synthetic Data Results</head><p>This section analyzes the performance profile of the Unsupervised, Supervised, SIFT, and ECC methods on our synthetic dataset. We want to test how well our approach performs under illumination variation and large image displacement.</p><p>To account for illumination variation, we globally standardize our images based on the mean and variance of pixel intensities of all images in our training dataset. We additionally inject random color, brightness and gamma shifts during the training. We do not utilize any further preprocessing and use the L1 photometric loss function. To highlight the effect of displacement amount on each method, we break down the accuracy performance in terms of: 85% image overlap (small displacement), 75% image overlap (moderate displacement), and 65% image overlap (large displacement). We follow the synthetic data generation process on the MS-COCO dataset used in <ref type="bibr" target="#b23">[24]</ref>. The amount of image overlap is controlled by the point perturbation parameter ?. The evaluation metric is the 4pt-Homography RMSE from Eqn. (3) comparing the estimated homography to the ground truth homography.</p><p>We train the deep networks from scratch for 300, 000 iterations over ? 30 hours, using two GPUs. This long training procedure only needs to be performed once, as the resulting model can be used as an initial pre-trained model for other data sets. We observed that the supervised model started overfitting after 150, 000 iterations so stopped training early. SIFT, ORB and ECC estimated homographies using the full images, while the deep learning methods are only given access to the small patches (? 21% pixels). This disadvantages our methods, and would result in better performance for the traditional methods, at the expense of slower running times. <ref type="figure">Fig. 3</ref> displays the results of each method broken down by overlap and performance percentile. We break down the results by performance percentile to illustrate the various performance profiles of each method. Specifically, SIFT tends to do very well 60% of the time, but in the worst 40% of the time it performs very poorly, sometimes completely failing to detect enough features to estimate a homography. On the other hand, the deep learning methods tends to have much more consistent performance, which can be more desireable in practical applications such as using homographies for collision avoidance for aerial multi-robot systems. Both the <ref type="figure">Fig. 3</ref>: Synthetic 4pt-Homography RMSE (lower is better). Unsupervised has comparable performance with the supervised method and performs better than the other approaches especially when the displacement is large. <ref type="figure">Fig. 4</ref>: Speed Versus Performance Tradeoff. Lower left is better. Suffixes GPU and CPU reflect the computational resource. All the feature-based methods are run on the CPU. The unsupervised network run on the GPU dominates all the other methods by having both the highest throughput and best performance.</p><p>learning methods and the feature-based methods outperform the direct method (ECC).</p><p>Interestingly, whereas direct method ECC has problems with illumination variation and large displacement, our unsupervised method is able to handle these scenarios even though it uses photometric loss functions. One potential hypothesis is that our method can be viewed as a hybrid between direct methods and feature based methods. The large receptive field of neural networks may allow it to handle large image displacement better than a direct method. In addition, whereas image gradients are used to update homography parameters in direct methods, with neural networks, these gradients are used to update network parameters which correspond to improving learned features. Finally, direct methods are an online optimization process that use gradients from a single pair of images, whereas training a deep network is an offline optimization process that averages gradients across multiple images. Injecting noise into this training process can further improve robustness to different appearance variations. Understanding the relationship between the neural network and photometric loss functions is an important direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Aerial Dataset Results</head><p>This section analyzes the performance profile of each method on a representative dataset of aerial imagery captured by a UAV. In addition to accuracy performance, an equally important consideration for real world application is inference speed. As a result, we also discuss the performance to speed tradeoffs of each method.</p><p>Our aerial dataset contains 350 image pairs resized to 240 ? 320, captured by a DJI Phantom 3 Pro platform in Yardley, Pensylvania, USA in 2017. We divided it into 300 train and 50 test samples. We did not label the train set, but for evaluation purposes, we manually labeled the ground truth by picking 4 pairs of correspondences for each test sample. We also randomly inject illumination noise in both the training and testing sets. The evaluation metrics are the same for the synthetic data. To reduce training time, we finetune the neural networks on the aerial image data. Our unsupervised algorithm can directly use the aerial dataset image pairs. However, since we do not have ground truth homography labels, we have to perform a similar synthetic data generation process as in the synthetic dataset in order to finetune the supervised neural network. We fine tune both models over 150, 000 iterations for roughly 15 hours with data augmentation. <ref type="figure" target="#fig_3">Fig. 6</ref> displays the performance profile for the Unsupervised, Supervised, SIFT, and ECC methods. <ref type="figure">Fig 4 displays</ref> the speed and performance tradeoff for these methods, and additionally the featured based method ORB. The featurebased methods are tested on a 16-core Intel Xeon CPU, and the deep learning methods are tested on the same CPU and an NVIDIA Titan X GPU. The closer to the lower left hand corner, the better the performance and faster the runtime.</p><p>Both Figs. 6 and 4 demonstrate that our unsupervised algorithm has the best performance of all methods. In addition, <ref type="figure">Fig. 4</ref> also shows that our unsupervised method on the GPU has both the best performance and the fastest inference times. SIFT has the second best performance after our unsupervised algorithm, but has a much slower runtime (approximately 200 times slower). ORB has a faster runtime than SIFT, but at the expense of poorer performance. The ECC direct method approach has the worst performance and runtime of all the methods. A qualitative example where both SIFT and ECC fail to deliver a good result while our method succeeds is illustrated in <ref type="figure" target="#fig_2">Fig. 5</ref>.  One of the most interesting results is that while the supervised and unsupervised approaches performed comparably on the synthetic data, the supervised approach had drastically poorer performance on the aerial image dataset. This shift is due to the fact that ground truth labels are not available for our aerial dataset. The generalization gap from synthetic (train) to real (test) data is an important problem in machine learning. The best practical approach is to additionally finetune the model on the new distribution of data. In a robotic field experiment, this can be achieved by flying the UAV to collect a few sample images and fine-tuning on those images. However, this fine-tuning is only possible with our unsu-pervised algorithm. Our aerial dataset results highlight the fact that even though synthetic data can be generated from real images, a pair of synthetic images is still very different from a pair of real images. These results demonstrate that the independence of our unsupervised algorithm from expensive ground truth labels has large practical implications for realworld performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We have introduced an unsupervised algorithm that trains a deep neural network to estimate planar homographies. Our approach outperforms the corresponding supervised network on both synthetic and real-world datasets, demonstrating the superiority of unsupervised learning in image warping problems. Our approach achieves faster inference speed, while maintaining comparable or better accuracy than featurebased and direct methods. We demonstrate that the unsupervised approach is able to handle large displacements and large illumination variations that are typically challenging for direct approaches that use the same photometric loss function. The speed and adaptive nature of our algorithm makes it especially useful in aerial multi-robot applications that can exploit parallel computation.</p><p>In this work, we do not investigate robustness against occlusion, leaving it as future work. However, as suggested in <ref type="bibr" target="#b23">[24]</ref>, we could potentially address this issue by using data augmentation techniques such as artificially inserting random occluding shapes into the training images. Another direction for future work is investigating different improvements to achieve sub-pixel accuracy in the top 30% performance percentile.</p><p>Finally, our approach is easily scalable to more general warping motions. Our findings provide additional evidence for applying deep learning methods, specifically unsupervised learning, to various robotic perception problems such as stereo depth estimation, or visual odometry. Our insights on estimating homographies with unsupervised deep neural network approaches provide an initial step in a structured progression of applying these methods to larger problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of homography estimation methods; (a) Benchmark supervised deep learning approach; (b) Feature-based methods; and (c) Our unsupervised method. DLT: direct linear transform; PSGG: parameterized sampling grid generator; DS: differentiable sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and H are the width and height of the I B . The second step (PSGG) creates a grid G = {G i } of the same size as the second image I B . Each grid element G i = (u i , v i ) corresponds to pixels of the second image I B . Applying the inverse homographyH inv to these grid coordinates provides a grid of pixels in the first image I A . ? ? u i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Unsupervised (Success, RMSE = 15.6) (b) Unsupervised (Success, RMSE = 4.50) (c) SIFT (Fail, RMSE = 105.2) (d) SIFT (Success, RMSE = 6.06) (e) ECC(Success, RMSE = 66.4) (f) ECC(Success, RMSE = 48.10) Qualitative visualization of estimation methods on aerial dataset. Left: hard case, right: moderate case. ECC performs better than SIFT in the case of small displacement, but performs worse than SIFT in case of large displacement. Unsupervised network outperforms both SIFT and ECC approaches. Supervised network is omitted due to limited space and its poor performance on this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>4pt-homography RMSE on aerial images (lower is better). Unsupervised outperforms other approaches significantly.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGEMENTS</head><p>We gratefully acknowledge the support of ARL grants W911NF-08-2-0004 and W911NF-10-2-0016, ARO grant W911NF-13-1-0350, N00014-14-1-0510, N00014-09-1-1051, N00014-11-1-0725, N00014-15-1-2115 and N00014-09-1-103, DARPA grants HR001151626/HR0011516850 USDA grant 2015-67021-23857 NSF grants IIS-1138847, IIS-1426840 CNS-1446592 CNS-1521617 and IIS-1328805, Qualcomm Research, United Technologies, and TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA. We would also like to thank Aerial Applications for the UAV data set.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognising panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1218</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Monocular slam for real-time applications on mobile platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Neo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d reconstruction based on homography mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ARPA96</title>
		<meeting>ARPA96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1007" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Easy tour: a new image-based virtual tour system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM SIGGRAPH international conference on Virtual Reality continuum and its applications in industry</title>
		<meeting>the 2004 ACM SIGGRAPH international conference on Virtual Reality continuum and its applications in industry</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="467" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selfcalibration for metric 3d reconstruction using homography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MVA</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="86" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image mosaicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Capel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Mosaicing and Superresolution</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="47" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image alignment and stitching: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="104" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 7th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="255" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parametric image alignment using enhanced correlation coefficient maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Psarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1858" to="1865" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Heask: Robust homography estimation based on appearance similarity and keypoint correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="368" to="387" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fourier lucas-kanade algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Navarathna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1383" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rationalizing efficient compositional image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="354" to="372" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An improved ransac homography algorithm for feature based image mosaic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th WSEAS International Conference on Signal Processing</title>
		<meeting>the 7th WSEAS International Conference on Signal Processing</meeting>
		<imprint>
			<publisher>WSEAS</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01925</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06852</idno>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepmatching: Hierarchical deformable dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="323" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to detect and match keypoints with deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep image homography estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07813</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Parameterizing homographies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-RI-TR-06-11</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is l2 a good loss function for neural networks for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1511</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">521540518</biblScope>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Singular value decomposition and least squares solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische mathematik</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="403" to="420" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating the jacobian of the singular value decomposition: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Papadopoulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Lourakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="554" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Tensorflow: Largescale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
