<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extract Free Dense Labels from CLIP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Change Loy</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Extract Free Dense Labels from CLIP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive Language-Image Pre-training (CLIP) has made a remarkable breakthrough in open-vocabulary zero-shot image recognition. Many recent studies leverage the pre-trained CLIP models for image-level classification and manipulation. In this paper, we wish examine the intrinsic potential of CLIP for pixel-level dense prediction, specifically in semantic segmentation. To this end, with minimal modification, we show that MaskCLIP yields compelling segmentation results on open concepts across various datasets in the absence of annotations and fine-tuning. By adding pseudo labeling and self-training, MaskCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods by large margins, e.g., mIoUs of unseen classes on PASCAL VOC/PASCAL Context/COCO Stuff are improved from 35.6/20.7/30.3 to 86.1/66.7/54.7. We also test the robustness of MaskCLIP under input corruption and evaluate its capability in discriminating fine-grained objects and novel concepts. Our finding suggests that MaskCLIP can serve as a new reliable source of supervision for dense prediction tasks to achieve annotation-free segmentation. Source code is available here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale visual-language pre-training models such as CLIP [45] capture expressive visual and language features. Various downstream vision tasks, e.g., text-driven image manipulation [42], image captioning [25], view synthesis <ref type="bibr" target="#b29">[30]</ref>, and object detection <ref type="bibr" target="#b18">[19]</ref>, have attempted to exploit such features for improved generality and robustness. For instance, conducting zero-shot image classification based on raw CLIP features leads to a competitive approach that matches the performance of fully-supervised counterparts <ref type="bibr" target="#b44">[45]</ref>.</p><p>In this paper, we take a step further to explore the applicability of CLIP features for pixel-level dense prediction tasks such as semantic segmentation. This investigation is meaningful in that previous studies mainly leverage CLIP features as a global image representation. In contrast, our exploration wishes to ascertain the extent of CLIP features in encapsulating object-level and local semantics for dense prediction. Different from the conventional pre-training task of image classification on iconic images, CLIP learns from images of complex scenes and their descriptions in natural language, which (1) encourages it ? Bo Dai completed this work when he was with S-Lab, NTU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>to embed local image semantics in its features, (2) enables it to learn concepts in open vocabulary, and (3) captures rich contextual information, such as the co-occurrence/relation of certain objects and priors of the spatial locations. We believe all these merits contribute significantly to its potential for dense prediction tasks.</p><p>In this paper, we summarize both our success and failure experience on leveraging CLIP features for dense prediction. We find it essential to not break the visual-language association in the original CLIP feature space. In our earlier exploration, we experienced failures with the attempt to fine-tune the image encoder of CLIP for the segmentation task, e.g., initializing DeepLab <ref type="bibr" target="#b4">[5]</ref> with the weights of CLIP's image encoder and fine-tune the backbone on segmentation. In addition, we found it is of utmost importance to avoid any unnecessary attempts to manipulate the text embeddings of CLIP. Such an approach would fail in segmenting unseen classes. In our successful model, named MaskCLIP, we show that one can simply extract dense patch-level features from the CLIP's image encoder, i.e., the value features of the last attention layer, without breaking the visual-language association. Classification weights for dense prediction, which are essentially 1?1 convolutions, can be directly obtained from the text embeddings of CLIP's text encoder without any deliberate mapping. In our empirical study, MaskCLIP yields reasonable predictions in both quantitative performance measured by mIoU metric and qualitative results. Besides, MaskCLIP can be based on all variants of CLIP, including ResNets and ViTs. And we provide side-by-side comparisons between the two popular backbone networks. We also propose two mask refinement techniques for MaskCLIP to further improve its performance, namely key smoothing and prompt denoising, both require no training. Specifically, key smoothing computes the similarity between the key features (of the last attention layer) of different patches, which are used to smooth the predictions. Prompt denoising removes prompts with classes that unlikely exist in the image, thus with fewer distractors, predictions become more accurate.</p><p>However, it is hard to further improve the segmentation capacity of MaskCLIP as its architecture is restricted to be the image encoder of CLIP. To relax MaskCLIP from the architectural constraint and to incorporate more advanced architectures such as PSPNet <ref type="bibr" target="#b54">[55]</ref> and DeepLab <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, we notice that instead of deploying MaskCLIP at the inference time, we can deploy it at the training time, where it serves as a generalizable and robust annotator that provides high-quality pseudo labels. Together with a standard self-training strategy, the resulting model, termed MaskCLIP+, achieves a strikingly remarkable performance.</p><p>Apart from annotation-free and open-vocabulary segmentation, MaskCLIP+ can also be applied to the transductive zero-shot semantic segmentation task, where MaskCLIP only generates pseudo labels for the unseen classes. On the three standard segmentation benchmarks, namely PASCAL VOC <ref type="bibr" target="#b14">[15]</ref>, PASCAL Context <ref type="bibr" target="#b37">[38]</ref>, and COCO Stuff <ref type="bibr" target="#b1">[2]</ref>, MaskCLIP+ improves the state-of-the-art results in terms of mIoU of unseen classes, by 50.5%, 46%, and 24.4%, respectively (35.6 ? 86.1, 20.7 ? 66.7, and 30.3 ? 54.7). Thanks to the the generality and robustness of CLIP features, MaskCLIP+ can be readily applied to various extended settings of semantic segmentation, including the segmentation of finegrained classes (e.g., attribute-conditioned classes like white car and red bus) or novel concepts (such as Batman and Joker as shown in <ref type="figure" target="#fig_0">Figure 1</ref>), as well as the segmentation of moderately corrupted inputs. We show more interesting results in the experiment section.</p><p>Semantic segmentation is notorious for its high dependency on labeled training data. Many methods have been explored to get around such stringent requirement, e.g., through using weak labels like image tags, bounding boxes, and scribbles. Our study, for the first time, shows that features learned via large-scale visual-language pre-training can be readily used to facilitate open vocabulary dense prediction. The proposed model, MaskCLIP, shows promising potential in providing rich and meaningful dense pseudo labels for training existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transferable Representation Learning. Pre-training is widely used for dense prediction tasks. Yosinski et al . <ref type="bibr" target="#b51">[52]</ref> show that ImageNet <ref type="bibr" target="#b10">[11]</ref> pre-training greatly speeds up the convergence of the downstream object detection task. Later, extensive research is conducted on making the pre-training a human-laborfree process. In particular, self-supervised representation learning constructs pretext tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39]</ref> or relies on contrastive learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>, clustering <ref type="bibr" target="#b2">[3]</ref>, and bootstrapping <ref type="bibr" target="#b17">[18]</ref> to obtain supervision signals. Another line of work seeks to learn visual representation from natural language. Some studies <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b52">53]</ref> propose to learn from image-caption pairs. Recently, CLIP <ref type="bibr" target="#b44">[45]</ref> and ALIGN <ref type="bibr" target="#b30">[31]</ref> perform contrastive learning on very large-scale web-curated image-text pairs and show promising pre-trained representations with impressive zero-shot transferability. The success of CLIP inspires a new way of studies that transfer the pre-trained CLIP model to various downstream tasks such as text-driven im-age manipulation <ref type="bibr" target="#b41">[42]</ref>, image captioning <ref type="bibr" target="#b24">[25]</ref>, view synthesis <ref type="bibr" target="#b29">[30]</ref>, and object detection <ref type="bibr" target="#b18">[19]</ref>. Different from these methods that typically apply CLIP right off the shelf for image encoding, we explore ways to adapt CLIP for pixel-level dense prediction. A concurrent work, DenseCLIP <ref type="bibr" target="#b45">[46]</ref>, aims to better fine-tune the CLIP pre-trained weights on target semantic segmentation datasets without keeping the zero-shot transferability, which are different from our setting. To examine the intrinsic potential of CLIP for dense prediction tasks, we refrain from any fine-tuning and major architectural modification. Zero-Shot Visual Recognition. Zero-shot learning aims at classifying instances of those categories that are not seen during training. Common clues to infer unseen categories include shared attributes and visual-semantic mapping. As the latter does not require extra annotations, the paradigm is well-suited for zero-shot dense prediction tasks. Zhao et al . <ref type="bibr" target="#b53">[54]</ref> project image pixel features and word concepts into a joint space. Kato et al . <ref type="bibr" target="#b31">[32]</ref> fuse semantic features into visual features as guidance. ZS3Net <ref type="bibr" target="#b0">[1]</ref> proposes to generate fake pixel-level features from semantic features for the unseen. SPNet <ref type="bibr" target="#b49">[50]</ref> learns a projection from visual space to semantic space. Other studies like <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>, and <ref type="bibr" target="#b19">[20]</ref>, improve the generative ZS3Net in terms of uncertainty, structural consistency, and context, respectively, while STRICT <ref type="bibr" target="#b40">[41]</ref> boosts the SPNet through self-training. Depending on whether the unlabeled pixels are observed during training, the setting can be split into inductive (not observed) and transductive. We show that the proposed MaskCLIP not only achieves new SOTA on the zero-shot segmentation setting but can also deal with more difficult settings where all the categories are unseen during training. Self-Training. Self-training leverages the model trained on labeled data to generate pseudo labels for the unlabeled, which then are used to iteratively improve the previous model. Self-training has firstly become popular in the semisupervised classification task <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref> and is also recently applied in the semi-supervised/zero-shot semantic segmentation settings <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b7">8]</ref>. Our MaskCLIP+ adopts the same philosophy where the pseudo labels are obtained from both frozen MaskCLIP and MaskCLIP+ itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our study serves as an early attempt that explores the applicability of CLIP features for pixel-level dense prediction tasks. We start with a brief introduction of CLIP and a na?ves solution as the preliminary, followed by presenting the proposed MaskCLIP in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary on CLIP</head><p>CLIP <ref type="bibr" target="#b44">[45]</ref> is a visual-language pre-training method that learns both visual and language representations from large-scale raw web-curated image-text pairs. It consists of an image encoder V(?) and a text encoder T (?), both jointly trained to respectively map the input image and text into a unified representation space.</p><p>CLIP adopts contrastive learning as its training objective, where ground-truth image-text pairs are regarded as positive samples, and mismatched image-text pairs are constructed as negative ones. In practice, the text encoder is implemented as a Transformer <ref type="bibr" target="#b48">[49]</ref>. As for the image encoder, CLIP provides two alternative implementations, namely a Transformer and a ResNet <ref type="bibr" target="#b22">[23]</ref> with global attention pooling layer. Our method can be based on both encoder architectures.</p><p>We believe CLIP has inherently embedded local image semantics in its features as it learns to associate image content with natural language descriptions, the latter of which contain complex and dense semantic guidance across multiple granularities. For example, to correctly identify the image corresponds to the description the man at bat readies to swing at the patch while the umpire looks on <ref type="bibr" target="#b8">[9]</ref>, CLIP must divide image semantics into local segments and properly align image semantics with singular mentioned concepts like man, bat, swing, patch, man at bat, man at patch, and man readies to swing, instead of handling the image as a whole. Such uniqueness is absent from training with solely image labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conventional Fine-Tuning Hinders Zero-Shot Ability</head><p>The current de facto pipeline of training a segmentation network is (1) initializing the backbone network with the ImageNet <ref type="bibr" target="#b10">[11]</ref> pre-trained weights, (2) adding segmentation-specific network modules with randomly initialized weights, and (3) jointly fine-tuning the backbone and newly added modules.</p><p>It is natural to follow these standard steps to adapt CLIP for segmentation. Here, we start our exploration by applying this pipeline on DeepLab <ref type="bibr" target="#b4">[5]</ref> with two CLIP-specific modifications. Specifically, we first replace the ImageNet pretrained weights with weights of the image encoder of CLIP. Second, we adopt a mapper M that maps text embeddings of CLIP to the weights of DeepLab classifier (the last 1 ? 1 convolutional layer). The modified model can be formulated as follows:</p><formula xml:id="formula_0">DeepLab(x) = C ? (H(V * l (x))),<label>(1)</label></formula><formula xml:id="formula_1">? = M(t),<label>(2)</label></formula><p>where V * l (?) denotes the DeepLab backbone, which is a ResNet dilated by a factor of l. H(?) denotes the randomly initialized ASPP module <ref type="bibr" target="#b4">[5]</ref>, and C ? (?) is the DeepLab classifier, whose weights, denoted as ?, are determined by the text embedding of CLIP via the mapper M. Ideally, by updating the classifier weights with the corresponding text embedding, the adapted DeepLab is able to segment different classes without re-training.</p><p>To evaluate the segmentation performance of this modified DeepLab on both seen and unseen classes, we train it on a subset of classes in the dataset, considering the remaining classes as unseen ones. We have tried a series of mapper architectures. Although they perform well on seen classes, in all these cases the modified DeepLab fails to segment unseen classes with satisfying performance. Compared to the conventional fine-tuning method, the key to the success of MaskCLIP is keeping the pretrained weights frozen and making minimal adaptation to preserve the visuallanguage association. Besides, to compensate for the weakness of using the CLIP image encoder for segmentation, which is designed for classification, MaskCLIP+ uses the outputs of MaskCLIP as pseudo labels and trains a more advanced segmentation network such as DeepLabv2 <ref type="bibr" target="#b4">[5]</ref> We hypothesize that this is mainly because the original visual-language association of CLIP features has been broken: (1) the backbone is slightly different from the image encoder in terms of network architecture; (2) weights initialized from the image encoder have been updated during fine-tuning; (3) an extra mapper, which is trained only on data of seen classes, is introduced therefore leading to insufficient generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MaskCLIP</head><p>Failing the fine-tuning attempt, we turn to a solution that avoids introducing additional parameters and modifying the feature space of CLIP. To this end, we carefully revisit the image encoder of CLIP, especially its unique global attention pooling layer. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), different from conventional global averaged pooling, the image encoder of CLIP adopts a Transformer-style multihead attention layer where globally average-pooled feature works as the query, and feature at each spatial location generates a key-value pair. Consequently, the output of this layer is a spatial weighted-sum of the incoming feature map followed by a linear layer F(?):</p><formula xml:id="formula_2">AttnPool(q, k, v) = F( i softmax(q k T i C )v i ) = i softmax(q k T i C )F(v i ),<label>(3)</label></formula><formula xml:id="formula_3">q = Emb q (x), k i = Emb k (x i ), v i = Emb v (x i ),<label>(4)</label></formula><p>where C is a constant scaling factor and Emb(?) denotes a linear embedding layer 3 . x i represents the input feature at spatial location i andx is the average of all x i . The outputs of the Transformer layer serve as a comprehensive representation of the whole image. We believe that this is possible because F(v i ) computed at each spatial location already captures a rich response of local semantics that correspond well with tokens in the text embeddings of CLIP. Based on such a hypothesis, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), we directly modify the image encoder of CLIP in our new attempt: (1) removing the query and key embedding layers; (2) reformulating the value-embedding layer and the last linear layer into two respective 1 ? 1 convolutional layers. Moreover, we keep the text encoder unchanged and it takes prompts with target classes as the input.</p><p>The resulting text embedding of each class is used as the classifier. We name the resulting model as MaskCLIP since it yields pixel-level mask predictions instead of a global image-level prediction. We then evaluate MaskCLIP on various standard segmentation benchmarks as well as web-crawled images. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, MaskCLIP can output reasonable results without any fine-tuning nor annotations. More qualitative results and quantitative results with respect to the mIoU metric are included in the experiment section.</p><p>One might argue that, since the global attention pooling is a self-attention layer, even without modification, it can also generate dense features. However, since queryq is the only query trained during the CLIP pre-training, this na?ves solution fails. We treat this solution as the baseline and compare its results with ours in the experiments. Moreover, the Transformer layer in ViT is very similar to the global attention pooling. In fact, the only two differences are: (1) the global query is generated by a special [CLS] token instead of the average among all spatial locations; (2) Transformer layer has a residual connection. Therefore, by replacingq with q [cls] and adding input x to the output, MaskCLIP can work with the ViT backbone.</p><p>Despite the simplicity of MaskCLIP in comparison to existing segmentation approaches, the proposed method enjoys multiple unique merits inherited from CLIP. First, MaskCLIP can be used as a free segmentation annotator to provide rich and novel supervision signals for segmentation methods working with limited labels. Second, since the visual-language association of CLIP is retained in MaskCLIP, it naturally possesses the ability to segment open vocabulary classes, as well as fine-grained classes described by free-form phrases, such as white car and red bus. Third, since the CLIP is trained on raw web-curated images, CLIP demonstrates great robustness to natural distribution shift <ref type="bibr" target="#b44">[45]</ref> and input corruptions <ref type="bibr" target="#b46">[47]</ref>. We verify that MaskCLIP preserves such robustness to some extent. Key Smoothing and Prompt Denoising. To further improve the performance of MaskCLIP, we propose two refinement strategies, namely key smoothing and prompt denoising. Recall that, in Eq. 3, besidesq, key features k i also get trained during CLIP pre-training. However, in the original MaskCLIP, k i is simply discard. Hence, here we seek to utilize this information to refine the final output. Key features can be viewed as the descriptor of the corresponding patch, therefore patches with similar key features should yield similar predictions. With this hypothesis, we propose to smooth the predictions with:</p><formula xml:id="formula_4">pred i = j cos( k i ?k i ? 2 , k j ?k j ? 2 )pred i ,<label>(5)</label></formula><p>where k i and pred i are key features and class confidence predictions at spatial location i, while ??? 2 and cos(?) denote L2 normalization and cosine similarity. We name this strategy as key smoothing.</p><p>In addition, we also observe that when dealing with many target classes, since only a small proportion of the classes appear in a single image, the rest classes are in fact distractors and undermine the performance. Therefore, we propose prompt denoising, which removes the prompt with target class if its class confidence at all spatial locations is all less than a threshold t = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MaskCLIP+</head><p>While MaskCLIP does not require any training, its network architecture is rigid because it adopts the image encoder of CLIP. To relax it from this constraint and benefit from more advanced architectures tailored for segmentation, such as DeepLab <ref type="bibr" target="#b4">[5]</ref> and PSPNet <ref type="bibr" target="#b54">[55]</ref>, we propose MaskCLIP+. Instead of directly applying MaskCLIP for test-time prediction, MaskCLIP+ regard its predictions as training-time pseudo ground-truth labels. Together with an adopted self-training strategy, MaskCLIP+ is thus free from the restriction on its backbone architecture. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), we take DeepLabv2 <ref type="bibr" target="#b4">[5]</ref> as the backbone of MaskCLIP+ to ensure a fair comparison with previous segmentation methods. MaskCLIP-Guided Learning. In MaskCLIP+, we leverage the predictions of MaskCLIP to guide the training of another target network comprising an architecture tailored to segmentation task. In parallel to the target network, we feed the same pre-processed image input to the MaskCLIP and use the predictions of MaskCLIP as pseudo ground-truth labels to train the target network. In addition, we replace the classifier of the target network with that of MaskCLIP, to preserve the network's ability for open vocabulary prediction.</p><p>MaskCLIP-guided learning is also applicable in the transductive zero-shot segmentation setting. Specifically, while pixels of both seen and unseen classes are observed, only annotations of seen classes are available. In this case, we only use MaskCLIP to generate pseudo labels for the unlabeled pixels. Compared to SOTA methods, MaskCLIP+ obtains remarkably better results across three standard benchmarks, namely PASCAL VOC 2012 <ref type="bibr" target="#b14">[15]</ref>, PASCAL Context <ref type="bibr" target="#b37">[38]</ref>, and COCO Stuff <ref type="bibr" target="#b1">[2]</ref>, where the results of MaskCLIP+ are even on par with that of fully-supervised baselines.</p><p>We note that some related attempts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51]</ref>, targeting object detection, perform knowledge distillation between the image-level visual features of CLIP and the features of a target model. Different from such feature-level guidance, we adopt pseudo labels in our case. This is because our target network, with a segmentation-tailored architecture, is structurally distinct from the image encoder of CLIP. Therefore, distillation by feature matching may be a sub-optimal strategy. In fact, as reported by <ref type="bibr" target="#b18">[19]</ref>, under zero-shot setting, such feature-level guidance indeed results in conflicts between the performance of seen and unseen classes. On the contrary, by adopting pseudo labels in MaskCLIP+, we do not observe any performance drop on seen classes. Self-Training. It is expected that after certain training iterations, the target network guided by MaskCLIP will outperform MaskCLIP, rendering the latter suboptimal for further guidance as it gradually becomes an inferior model over time. Empirically, we also find that MaskCLIP-guided learning reaches an upper bound at around 1/10 of the standard training schedule. To further improve the performance, we swap out MaskCLIP and let the target model generate pseudo labels for itself. This is commonly referred to as self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We conduct experiments on three standard segmentation benchmarks, namely PASCAL VOC 2012 <ref type="bibr" target="#b14">[15]</ref>, PASCAL Context <ref type="bibr" target="#b37">[38]</ref>, and COCO Stuff <ref type="bibr" target="#b1">[2]</ref>. PASCAL VOC 2012 contains 1,426 training images with 20 object classes plus a background class. Following common practice, we augment it with the Semantic Boundaries Dataset <ref type="bibr" target="#b20">[21]</ref>. PASCAL Context labels PASCAL VOC 2010 (4,998/5,105 train/validation images) with segmentation annotations of 520 object/stuff classes, from which the most common 59 classes are treated as foreground while the rest are regarded as background. COCO Stuff extends the COCO dataset, which contains segmentation annotations of 80 object classes on 164K images, with additional 91 stuff classes. Text Embedding. We follow the same process to construct text embeddings as Gu et al . <ref type="bibr" target="#b18">[19]</ref>. Specifically, we feed prompt engineered texts into the text encoder of CLIP with 85 prompt templates, such as there is a {class name} in the scene, and average the resulting 85 text embeddings of the same class. Implementation Details. We implement our method on the MMSegmentation 4 codebase and inherit its training configurations. Input resolutions are set as 512x512. When using ViT, the pre-trained positional embeddings adopt bicubic interpolation. MaskCLIP requires no training and we train MaskCLIP+ on 4 Tesla V100 GPUs with a batch size of 16. For annotation-free segmentation, we perform MaskCLIP-guided learning for 4k/8k iterations on PASCAL Context/COCO Stuff with DeepLabv2-ResNet101 as the backbone segmentor. Self-training is not used in this setting. For zero-shot segmentation, we choose the lightest training schedule provided by MMSegmentation, which is 20k/40k/80k for PASCAL VOC/PASCAL Context/COCO Stuff. The first 1/10 training iterations adopt MaskCLIP-guided learning and the rest adopts self-training. For fair comparisons, we choose DeepLabv2 as the target model for PASCAL VOC  Robustness Under Corruption. CLIP is trained on web-curated images, whose quality and distribution are more diverse than well-pre-processed datasets. Radford et al . <ref type="bibr" target="#b44">[45]</ref> and Ravula et al . <ref type="bibr" target="#b46">[47]</ref> demonstrate the robustness of CLIP on natural distribution shift and artificial corruption respectively. While these explorations are done for image classification, we benchmark its robustness for dense prediction tasks. Specifically, we impose various corruptions used in ImageNet-C <ref type="bibr" target="#b23">[24]</ref> with different severity levels on images in PASCAL Context and evaluate on MaskCLIP. In <ref type="table" target="#tab_0">Table 1b</ref>, MaskCLIP models based on CLIP-ViT-B/16 are much more robust than CLIP-ResNet-50. In particular, CLIP-ViT-B/16 rarely suffers from degradation across a wide range of corruptions with level 1 severity and is cable of generating reasonable labels even under the most severe corruptions (level 5 6 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-Shot Segmentation</head><p>Apart from annotation-free segmentation, MaskCLIP+ can also be applied to the zero-shot segmentation task with minor effort. Specifically, in the zero-shot setting, pixels of certain classes do not have annotations, to which MaskCLIP can assign reliable pseudo labels. Zero-shot Setups. Traditionally, zero-shot segmentation methods train on a subset of classes, named seen classes, with ground-truth annotations, and during inference, both seen and unseen classes are evaluated. Depending on whether the unlabeled pixels are observed during training, the setting can be split into inductive (not observed) and transductive (observed). Our method conforms to the transductive setting. The selection of seen classes varies among previous works and we follow the most common setups, where for PASCAL VOC, the background class is ignored and potted plant, sheep, sofa, train, tv monitor are chosen as the 5 unseen classes; for PASCAL Context, the background is not ignored and cow, motorbike, sofa, cat, boat, fence, bird, tv monitor, keyboard, aeroplane are unseen; and for COCO Stuff, frisbee, skateboard, cardboard, carrot, scissors, suitcase, giraffe, cow, road, wall concrete, tree, grass, river, clouds, playing field are unseen. We report the mean Intersection over Union (mIoU) of seen, unseen, and all classes as well as the harmonic mean (hIoU) of seen and unseen mIoUs as evaluation metrics.</p><p>We compare MaskCLIP+ with SOTA methods including SPNet <ref type="bibr" target="#b49">[50]</ref>, ZS3Net <ref type="bibr" target="#b0">[1]</ref>, CaGNet <ref type="bibr" target="#b19">[20]</ref>, and STRICT <ref type="bibr" target="#b40">[41]</ref>. ZS3Net and CaGNet are generative approaches, while SPNet is non-generative and more simple but requires postpossessing step of calibration (SPNet-C). STRICT improves SPNet by a selftraining strategy and is free of calibration. Compare with these methods, our MaskCLIP+ does not rely on any particular network architecture nor postpossessing. Note that similar to CLIP, all methods, except for ZS3Net, do not exclude unseen classes during pre-training. Besides, MaskCLIP+ also follows the rule that pixel-level annotations of unseen classes are prohibited. Thus, the comparison is fair.</p><p>Despite being simple, MaskCLIP+ achieves a strikingly good result. As shown in <ref type="table" target="#tab_1">Table 2</ref>, it surpasses all methods on all datasets with large margins. On  <ref type="table" target="#tab_1">Table 2</ref> for more specific numbers. Ablation Studies of MaskCLIP+. We perform ablation studies on the PASCAL VOC zero-shot segmentation setting. As shown in <ref type="table" target="#tab_2">Table 3</ref>, we first examine the two proposed strategies in MaskCLIP+. Compared to the adapted DeepLabv2, whose classifier is replaced with the MaskCLIP classifier, MaskCLIPguided learning improves the unseen mIoU from 3.7 to 72.8 and the result is further improved by self-training to 86.1. However, there is a slight degradation on seen classes when using self-training (from 89.5 to 88.8) partially due to model drifting. Overall, MaskCLIP+ performs better than MaskCLIP on unseen classes and surpasses the baseline DeepLabv2 on seen classes in the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents our exploration of applying CLIP in semantic segmentation, as an early attempt that studies the applicability of pre-trained visual-language models in pixel-level dense prediction tasks. While the conventional fine-tuning paradigm fails to benefit from CLIP, we find the image encoder of CLIP already possesses the ability to directly work as a segmentation model. The resulting model, termed MaskCLIP, can be readily deployed on various semantic segmentation settings without re-training. On top of the success of MaskCLIP, we further propose MaskCLIP+ that leverages MaskCLIP to provide trainingtime pseudo labels for unlabeled pixels, which thus can be applied to more segmentation-tailored architectures beyond just the image encoder of CLIP. On standard transductive zero-shot segmentation benchmarks, MaskCLIP+ significantly improves previous SOTA results. More importantly, MaskCLIP+ can be readily employed for segmenting more challenging unseen classes, such as celebrities and animation characters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Qualitative Results on Annotation-Free Segmentation</head><p>In <ref type="figure" target="#fig_3">Figure 5</ref>, we show more qualitative results of MaskCLIP on the PASCAL Context dataset in the annotation-free setting. The results are consistent with our analysis in the main submission, where prompt denoising (PD) removes the unconfident distraction classes, key smoothing (KS) aggressively smooths the noisy predictions, and MaskCLIP+ yields the best results through pseudo-labeltraining. We find the predictions of KS are often dominated by a few classes and we show a failure case in the <ref type="figure" target="#fig_3">Figure 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Robustness Results on Annotation-Free Segmentation</head><p>In our main submission, we test the robustness of MaskCLIP under artificial corruptions. We use corrupting operations provided by the official code of ImageNet-C <ref type="bibr" target="#b23">[24]</ref>. In particular, the severity levels are controlled by a series of coefficients of corruption operators. Limited by space, in the main submission, we only include results of level 1 and level 5. Here, we extend the table to all levels. As shown in <ref type="table" target="#tab_4">Table 4</ref>, CLIP-ViT-B/16 consistently outperforms CLIP-ResNet-50 by large margins and shows decent robustness. We also supplement a baseline for the robustness test to compare with Table 1b in our main submission. In particular, we train an FCN segmentation model with the ViT-B/16 backbone (initialized with ImageNet-21K pre-trained weights) on PASCAL Context in a fully supervised manner for 40K iterations, then test the model on corrupted inputs. <ref type="table" target="#tab_5">Table 5</ref> shows that MaskCLIP performs particularly well on Gaussian/shot/impulse noises.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Quantitative Results on Zero-Shot Segmentation</head><p>In <ref type="table" target="#tab_6">Table 6</ref>, we report the mIoUs on seen classes of various methods. As mentioned in our main submission, across three standard datasets, using pseudo labels as the guidance, instead of distillation by feature matching, does not affect MaskCLIP+'s performance on seen classes. Apart from Intersection over Union (IoU), some zero-shot segmentation methods also report pixel accuracy (pAcc) and mean accuracy (mAcc) as evaluation metrics. For comprehensive comparisons, we provide performance with the mentioned metrics in <ref type="table" target="#tab_7">Table 7</ref>. In terms of the overall and unseen pAcc/mAcc, MaskCLIP+ still surpasses the previous SOTA methods by large margins and reaches near the fully-supervised baselines. However, its pAcc/mAcc of seen classes on PASCAL VOC and COCO-Stuff fall behind SPNet and CaGNet+ST by a bit. Different from mIoU, pAcc and mAcc punish only false negatives but not false positives (mIoU punishes both). Previous methods are much more confident on seen classes than unseen classes, therefore yields more predictions on seen classes, which consequently avoids false negatives on seen classes. In fact, SPNet biases towards seen classes so much that without calibration (reduce the confidence of seen classes by scaling factors), its performance on unseen classes is almost zero. MaskCLIP+, on the contrary, is more balanced between seen and unseen classes.   <ref type="figure">Fig. 7</ref>: Open-vocabulary segmentation with a larger target text set </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Vocabulary Used in Open-Vocabulary Segmentation</head><p>In <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_2">Figure 4</ref> of our main submission, all images share the same background classes, i.e., building, ground, grass, tree, sky. For foreground classes, different images have a different set of targets, which are shown right below each image in <ref type="figure" target="#fig_2">Figure 4</ref>. In <ref type="figure">Figure 7</ref>, we supplement an example with a larger vocabulary, with Batman, Joker, James Gordon, The Penguin, Robin, Alfred, Catwoman, Harley Quinn as the foreground and all classes except person in the Cityscapes as the background. We observe that Batman's jaw is segmented as James Gordon and part of Joker's suit is classified into The Penguin. Since certain local features are shared among multiple characters, it reveals that sometimes MaskCLIP cannot see broadly enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Input Resolution and Multi-Scale Ensemble</head><p>There is a trade-off in terms of the input resolution of MaskCLIP. Using the same input resolution as CLIP (224x224) assures the resolution/positional encoding matching but at the cost of yielding smaller output. We empirically find there exists a sweet spot at 336x336. We also find that multi-scale ensembles mitigate the resolution problem. (See <ref type="table" target="#tab_8">Table 8</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Pseudo Code of MaskCLIP+</head><p>The complete training process of MaskCLIP+ is illustrated in Algorithm 1. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Here we show the original image in (a), the segmentation result of MaskCLIP+ in (b), and the confidence maps of MaskCLIP and MaskCLIP+ for Batman in (c) and (d) respectively. Through the adaptation of CLIP, MaskCLIP can be directly used for segmentation of fine-grained and novel concepts (e.g., Batman and Joker ) without any training operations and annotations. Combined with pseudo labeling and self-training, MaskCLIP+ further improves the segmentation result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of MaskCLIP/MaskCLIP+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative results on Web images. Here we show the segmentation results of MaskCLIP and MaskCLIP+ on various unseen classes, including fine-grained classes such as cars in different colors/imagery properties, celebrities, and animation characters. All results are obtained without any annotation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>More qualitative results on PASCAL Context. Here all results are obtained without any annotation. PD and KS refer to prompt denoising and key smoothing respectively. Row 2, Col 4 shows a failure case of KS, where all the pixels in the image are labeled as the horse. Note that, PASCAL Context does not contain bear or teddy bear classes and MaskCLIP predicts the teddy bear pixels as bedclothes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Annotation-free segmentation (mIoU). (a) We evaluate the performance of MaskCLIP(+) on two standard datasets. For Pascal Context, we ignore the evaluation on the background class. The target model of MaskCLIP+ is Deeplabv2-ResNet101. KS and PD denote key smoothing and prompt denoising respectively. And they are not used in MaskCLIP+. (b) We test the robustness of MaskCLIP on Pascal Context under various types of corruption The baseline in the table refers to directly using dense features from the CLIP's image encoder without any modification. As shown in the table, MaskCLIP outperforms the baseline by huge margins, indicating it is essential to avoid computing attention of the last attention layer and instead value features should MaskCLIP inherits the open-vocabulary ability from CLIP and does not require annotations. Therefore, we can deploy it on several interesting setups where the target classes are (1) more fine-grained, such as red car, yellow car ; (2) of certain imagery properties, e.g., blurry; (3) novel concepts like Batman, Joker. To this end, we collect images from Flickr then directly evaluate these images on MaskCLIP and train MaskCLIP+ with only MaskCLIP-guided learning. Note that, for the background, we enumerate a set of classes that might appear in the background and regard them as a whole as the background class. Results inFigure 4are impressive given the open-vocabulary targets and being annotation-free. Besides, results from MaskCLIP+ are less noisy and more accurate than MaskCLIP, which is complementary to the quantitative results.</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell>Method</cell><cell>CLIP</cell><cell>PASCAL Context</cell><cell>COCO Stuff</cell><cell>Corruption</cell><cell>level 1 level 5 r50 vit16 r50 vit16</cell></row><row><cell>Baseline</cell><cell>r50 vit16</cell><cell>8.3 9.0</cell><cell>4.6 4.3</cell><cell cols="2">None Gaussian Noise 13.7 19.6 2.1 6.8 18.5 21.7 18.5 21.7</cell></row><row><cell></cell><cell>r50</cell><cell>18.5</cell><cell>10.2</cell><cell>Shot Noise</cell><cell>14.0 19.6 2.4 7.5</cell></row><row><cell></cell><cell>+KS</cell><cell>21.0</cell><cell>12.4</cell><cell cols="2">Impulse Noise 9.9 17.3 2.1 7.2</cell></row><row><cell>MaskCLIP</cell><cell cols="2">+PD +KS+PD 21.8 19.0</cell><cell>10.8 12.8</cell><cell cols="2">Speckle Noise 15.1 20.0 5.6 11.4 Gaussian Blur 17.4 21.6 4.3 14.1</cell></row><row><cell></cell><cell>vit16</cell><cell>21.7</cell><cell>12.5</cell><cell cols="2">Defocus Blur 15.7 20.8 6.6 15.5</cell></row><row><cell></cell><cell cols="2">+KS +PD +KS+PD 25.5 23.9 23.1</cell><cell>13.8 13.2 14.6</cell><cell>Spatter JPEG</cell><cell>17.1 20.5 7.8 12.2 15.7 20.8 7.6 14.5</cell></row><row><cell>MaskCLIP+</cell><cell>r50 vit16</cell><cell>23.9 31.1</cell><cell>13.6 18.0</cell><cell></cell><cell></cell></row></table><note>and COCO Stuff and DeepLabv3+ for PASCAL Context. All use the ResNet- 101 backbone initialized with the ImageNet pre-trained weights. Finally, we use the publicly available CLIP-ResNet-50 and CLIP-ViT-B/16 models 5 .4.1 Annotation-Free Segmentation In this challenging setting, no annotation is provided during training. We first evaluate the mIoU performance on two standard datasets, PASCAL Context and COCO-Stuff. Then we collect images from Flickr to show interesting quali- tative results on novel concepts, such as Batman and Joker. Finally, we test the robustness of MaskCLIP under various image corruptions. Performance on Standard Datasets. In Table 1a, we show mean Inter- section over Union (mIoU) results on PASCAL Context and COCO-Stuff.Fig. 3: Qualitative results on PASCAL Context. Here all results are ob- tained without any annotation. PD and KS refer to prompt denoising and key smoothing respectively. With PD, we can see some distraction classes are re- moved. KS is more aggressive. Its outputs are much less noisy but are dominated by a small number of classes. Finally, MaskCLIP+ yields the best results be directly used. The results also show that key smoothing and prompt denois- ing are effective and are orthogonal to each other. Therefore, we empirically conclude that for each spatial location, the query features should be discarded and key/value features can be re-organized into final predictions. Furthermore, with the predictions of MaskCLIP as pseudo labels, MaskCLIP+ significantly improves the performance, e.g., on PASCAL Context, without any human an- notation, MaskCLIP+(ViT-B/16) obtains 31.1 mIoU. One may notice that ViT almost consistently surpasses ResNet. Apart from ViT-B/16 has more FLOPs than ResNet-50, another possible reason is that ViT only downsamples the input by 16 times whereas ResNet downsamples 32 times, which particularly matters for dense prediction tasks. Besides quantitative results, in Figure 3, we also vi- sualize the outputs of each MaskCLIP variant. Open-Vocabulary Segmentation on Web-Crawled Images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot segmentation performances. ST stands for selftraining. mIoU(U) denotes mIoU of the unseen classes. SPNet-C is the SPNet with calibration. On PASCAL Context, all methods use DeepLabv3+-ResNet101 as the backbone segmentation model and the rest two datasets use DeepLabv2-ResNet101. For MaskCLIP+, CLIP-ResNet-50 is used to generate pseudo labels</figDesc><table><row><cell>Method</cell><cell cols="9">PASCAL-VOC mIoU(U) mIoU hIoU mIoU(U) mIoU hIoU mIoU(U) mIoU hIoU COCO-Stuff PASCAL-Context</cell></row><row><cell>Inductive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPNet</cell><cell>0.0</cell><cell cols="2">56.9 0.0</cell><cell>0.7</cell><cell cols="2">31.6 1.4</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>SPNet-C</cell><cell>15.6</cell><cell cols="2">63.2 26.1</cell><cell>8.7</cell><cell cols="2">32.8 14.0</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ZS3Net</cell><cell>17.7</cell><cell cols="2">61.6 28.7</cell><cell>9.5</cell><cell cols="2">33.3 15.0</cell><cell>12.7</cell><cell cols="2">19.4 15.8</cell></row><row><cell>CaGNet</cell><cell>26.6</cell><cell cols="2">65.5 39.7</cell><cell>12.2</cell><cell cols="2">33.5 18.2</cell><cell>18.5</cell><cell cols="2">23.2 21.2</cell></row><row><cell>Transductive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPNet+ST</cell><cell>25.8</cell><cell cols="2">64.8 38.8</cell><cell>26.9</cell><cell cols="2">34.0 30.3</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ZS3Net+ST</cell><cell>21.2</cell><cell cols="2">63.0 33.3</cell><cell>10.6</cell><cell cols="2">33.7 16.2</cell><cell>20.7</cell><cell cols="2">26.0 23.4</cell></row><row><cell cols="2">CaGNet+ST 30.3</cell><cell cols="2">65.8 43.7</cell><cell>13.4</cell><cell cols="2">33.7 19.5</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>STRICT</cell><cell>35.6</cell><cell cols="2">70.9 49.8</cell><cell>30.3</cell><cell cols="2">34.9 32.6</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="4">MaskCLIP+ 86.1 88.1 87.4</cell><cell cols="3">54.7 39.6 45.0</cell><cell cols="3">66.7 48.1 53.3</cell></row><row><cell></cell><cell cols="9">+50.5 +17.2 +37.6 +24.4 +4.7 +12.4 +46.0 +22.1 +29.9</cell></row><row><cell>Fully Sup.</cell><cell>?</cell><cell>88.2</cell><cell>?</cell><cell>?</cell><cell>39.9</cell><cell>?</cell><cell>?</cell><cell>48.2</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablations of MaskCLIP+. Experiments are performed on the PAS-CAL VOC dataset under the zero-shot setting</figDesc><table><row><cell>Method</cell><cell cols="3">mIoU(S) mIoU(U) mIoU hIoU</cell></row><row><cell>Adapted DeepLabv2</cell><cell>83.4</cell><cell>3.7</cell><cell>63.5 7.0</cell></row><row><cell cols="2">+ MaskCLIP-Guided 89.5</cell><cell>72.8</cell><cell>85.3 80.3</cell></row><row><cell>+ Self-Training</cell><cell>88.8</cell><cell>86.1</cell><cell>88.1 87.4</cell></row><row><cell cols="4">PASCAL VOC, PASCAL Context, and COCO Stuff, in terms of unseen mIoUs,</cell></row><row><cell cols="4">MaskCLIP+ improves the previous SOTA by 50.5, 24.4, and 46.0 respectively</cell></row><row><cell cols="4">(on a scale of 100). Note that the overall mIoU of MaskCLIP+ is on par with that</cell></row><row><cell cols="2">of fully supervised baselines. Please refer to</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(Row 2, Col 4), where one class dominates the whole image. Moreover, the behavior of MaskCLIP and MaskCLIP+ in Row 3 is interesting. Since the PASCAL Context dataset does not contain bear or teddy bear classes, MaskCLIP classifies the teddy bear pixels into bedclothes, which is the most related class. Meanwhile, through pseudo-label-training, after observing the true bedclothes pixels, MaskCLIP+ decides to treat the teddy bear as part of the chair that it sits on.In our main submission, we show qualitative results of fine-grained classes (red car, yellow car ), objects with certain imagery properties (blurry car ), and novel Bill Gates). Since MaskCLIP preserves the open-vocabulary ability, we can evaluate it on many interesting setups. InFigure 6we test whether MaskCLIP can segment out different car brands and sports. Similar to our main submission, the evaluation images are crawled from Flickr and all results are obtained without any annotation. MaskCLIP and MaskCLIP+ again demonstrate powerful open-vocabulary ability on subtle concepts. Note that, in the basketball and football examples, MaskCLIP not only correctly distinguishes athletes playing different sports, but also separates audience and players.</figDesc><table><row><cell>Input</cell><cell>MaskCLIP</cell><cell>MaskCLIP+</cell><cell>Input</cell><cell>MaskCLIP</cell><cell>MaskCLIP+</cell></row><row><cell></cell><cell></cell><cell cols="2">Bugatti, Cadillac, Porsche, Lamborghini</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">baseball player, basketball player, soccer player, football player</cell><cell></cell></row><row><cell cols="6">Fig. 6: More qualitative results on Web images. MaskCLIP and</cell></row><row><cell cols="6">MaskCLIP+ can yield reasonable segmentation results of different car brands</cell></row><row><cell cols="3">and sports without any annotation</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">concepts (Batman,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>More robustness results. Here we evaluate MaskCLIP on PASCAL Context in the annotation-free setting under ImageNet-C corruptions across all severity levels. Results are reported in the mIoU metric</figDesc><table><row><cell>Corruption</cell><cell>level 1 r50 vit16 r50 vit16 r50 vit16 r50 vit16 r50 vit16 level 2 level 3 level 4 level 5</cell></row><row><cell>None</cell><cell>18.5 21.7 18.5 21.7 18.5 21.7 18.5 21.7 18.5 21.7</cell></row><row><cell cols="2">Gaussian Noise 13.7 19.6 11.2 17.7 7.9 14.8 4.7 11.1 2.1 6.8</cell></row><row><cell>Shot Noise</cell><cell>14.0 19.6 11.0 17.6 7.8 14.8 4.0 10.4 2.4 7.5</cell></row><row><cell cols="2">Impulse Noise 9.9 17.3 8.1 15.9 6.7 14.4 4.1 10.9 2.1 7.2</cell></row><row><cell cols="2">Speckle Noise 15.1 20.0 13.6 19.0 9.6 16.0 7.6 14.0 5.6 11.4</cell></row><row><cell cols="2">Gaussian Blur 17.4 21.6 14.4 20.4 11.1 18.9 8.1 17.3 4.3 14.1</cell></row><row><cell>Defocus Blur</cell><cell>15.7 20.8 14.0 20.1 10.9 18.6 8.5 17.1 6.6 15.5</cell></row><row><cell>Spatter</cell><cell>17.1 20.5 13.0 17.9 10.9 16.4 10.1 14.5 7.8 12.2</cell></row><row><cell>JPEG</cell><cell>15.7 20.8 14.3 20.1 13.3 19.5 10.3 17.4 7.6 14.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Baselines for Robustness Test. Evaluation on PASCAL Context under level 5 corruptions with the ViT-B/16 backbone. N.: Noise, B.: Blur</figDesc><table><row><cell cols="9">None Gauss N. Shot Impulse Speckle Gauss B. Defocus Spatter JPEG</cell></row><row><cell>MaskCLIP 21.7</cell><cell>6.8</cell><cell>7.5</cell><cell>7.2</cell><cell>11.4</cell><cell>14.1</cell><cell>15.5</cell><cell>12.2</cell><cell>14.5</cell></row><row><cell>Fully Sup. 54.5</cell><cell>5.1</cell><cell>6.7</cell><cell>4.8</cell><cell>22.7</cell><cell>37.1</cell><cell>40.1</cell><cell cols="2">31.5 39.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Zero-shot segmentation performances on seen classes (mIoU)</figDesc><table><row><cell>Method</cell><cell cols="3">PASCAL-VOC COCO-Stuff PASCAL-Context</cell></row><row><cell>SPNet</cell><cell>75.8</cell><cell>34.6</cell><cell>?</cell></row><row><cell>SPNet-C</cell><cell>78.0</cell><cell>35.2</cell><cell>?</cell></row><row><cell>ZS3Net</cell><cell>77.3</cell><cell>34.7</cell><cell>20.8</cell></row><row><cell>CaGNet</cell><cell>78.4</cell><cell>35.5</cell><cell>24.8</cell></row><row><cell>SPNet</cell><cell>77.8</cell><cell>34.6</cell><cell>?</cell></row><row><cell>ZS3Net</cell><cell>78.0</cell><cell>34.9</cell><cell>27.0</cell></row><row><cell>CaGNet</cell><cell>78.6</cell><cell>35.6</cell><cell>?</cell></row><row><cell>STRICT</cell><cell>82.7</cell><cell>35.3</cell><cell>?</cell></row><row><cell>MaskCLIP+</cell><cell>88.8</cell><cell>38.2</cell><cell>44.4</cell></row><row><cell></cell><cell>(+6.1)</cell><cell>(+2.9)</cell><cell>(+17.4)</cell></row><row><cell>Fully Sup.</cell><cell>88.6</cell><cell>38.1</cell><cell>44.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Zero-shot segmentation performances (pAcc &amp; mAcc) Context pAcc (S) pAcc (U) pAcc pAcc (S) pAcc (U) pAcc pAcc (S) pAcc (U) pAcc Context mAcc (S) mAcc (U) mAcc mAcc (S) mAcc (U) mAcc mAcc (S) mAcc (U) mAcc</figDesc><table><row><cell cols="9">Method PASCAL-SPNet PASCAL-VOC COCO-Stuff 94.8 0.0 76.9 65.6 1.7 51.3 ? ?</cell><cell>?</cell></row><row><cell>SPNet-C</cell><cell>88.8</cell><cell>29.6</cell><cell>77.6</cell><cell>61.8</cell><cell>24.5</cell><cell>53.4</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ZS3Net</cell><cell>93.0</cell><cell>21.5</cell><cell>79.4</cell><cell>64.3</cell><cell>22.8</cell><cell>54.7</cell><cell>53.5</cell><cell>58.6</cell><cell>52.8</cell></row><row><cell>CaGNet</cell><cell>89.5</cell><cell>43.0</cell><cell>80.7</cell><cell>65.6</cell><cell>25.5</cell><cell>56.6</cell><cell>55.2</cell><cell>66.8</cell><cell>56.6</cell></row><row><cell>ZS3Net</cell><cell>91.9</cell><cell>34.1</cell><cell>81.0</cell><cell>65.8</cell><cell>24.9</cell><cell>56.3</cell><cell>46.8</cell><cell>70.2</cell><cell>49.5</cell></row><row><cell>CaGNet</cell><cell>87.0</cell><cell>58.6</cell><cell>81.6</cell><cell>65.9</cell><cell>26.7</cell><cell>56.8</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">MaskCLIP+ 94.6</cell><cell>91.4</cell><cell>94.0</cell><cell>64.2</cell><cell>79.4</cell><cell>67.6</cell><cell>73.9</cell><cell>82.3</cell><cell>74.8</cell></row><row><cell></cell><cell cols="9">(-0.2) (+32.8) (+12.4) (-1.7) (+52.7) (+10.8) (+18.7) (+12.1) (+18.2)</cell></row><row><cell>Fully Sup.</cell><cell>?</cell><cell>?</cell><cell>94.0</cell><cell>?</cell><cell>?</cell><cell>68.1</cell><cell>?</cell><cell>?</cell><cell>74.8</cell></row><row><cell cols="9">Method PASCAL-SPNet PASCAL-VOC COCO-Stuff 94.6 0.0 70.9 50.3 0.0 45.9 ? ?</cell><cell>?</cell></row><row><cell>SPNet-C</cell><cell>87.9</cell><cell>23.9</cell><cell>71.9</cell><cell>46.3</cell><cell>16.1</cell><cell>43.6</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ZS3Net</cell><cell>87.7</cell><cell>15.8</cell><cell>73.5</cell><cell>50.4</cell><cell>27.0</cell><cell>48.4</cell><cell>23.8</cell><cell>43.2</cell><cell>27.0</cell></row><row><cell>CaGNet</cell><cell>88.7</cell><cell>39.4</cell><cell>76.4</cell><cell>50.7</cell><cell>27.0</cell><cell>48.5</cell><cell>35.7</cell><cell>49.8</cell><cell>36.8</cell></row><row><cell>ZS3Net</cell><cell>85.7</cell><cell>26.4</cell><cell>73.8</cell><cell>50.4</cell><cell>27.2</cell><cell>48.6</cell><cell>32.3</cell><cell>57.1</cell><cell>36.4</cell></row><row><cell>CaGNet</cell><cell>83.9</cell><cell>50.7</cell><cell>75.6</cell><cell>50.6</cell><cell>27.3</cell><cell>48.5</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">MaskCLIP+ 93.7</cell><cell>92.6</cell><cell>93.4</cell><cell>50.8</cell><cell cols="2">72.4 52.7</cell><cell>55.4</cell><cell>80.0</cell><cell>59.5</cell></row><row><cell></cell><cell cols="9">(-0.9) (+41.9) (+17.0) (+0.1) (+45.1) (+4.1) (+19.7) (+22.9) (+22.7)</cell></row><row><cell>Fully Sup.</cell><cell>?</cell><cell>?</cell><cell>93.4</cell><cell>?</cell><cell>?</cell><cell>53.0</cell><cell>?</cell><cell>?</cell><cell>59.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Input resolutions and multi-scale ensemble. Here, we evaluate MaskCLIP on the PASCAL Context dataset</figDesc><table><row><cell cols="4">Input Res. 224 336 520 [224, 520] [224, 336, 520]</cell></row><row><cell>mIoU</cell><cell>22.72 23.02 21.68</cell><cell>25.16</cell><cell>26.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Algorithm 1: MaskCLIP+ pseudo code P ? MaskCLIP model; T ? text embeddings of target classes; V1 ? target model initialized w/ IN pre-trained; V1 ? load T to classifier weights of V1; D ? images for training; Ng ? MaskCLIP-guided learning iterations; Ns ? self-training iterations; for i = 1, 2, . . . , Ng d? y ? model prediction Vi(Di); y ? pseudo labels from MaskCLIP P (Di); L ? cross entropy loss LCE(?, y); Vi+1 ? SGD model update; end for j = Ng + 1, Ng + 2, . . . , Ng + Ns d? y ? model prediction Vj(Dj); y ? self-generated pseudo labels Vj(Dj); L ? cross entropy loss LCE(?, y); Vj+1 ? SGD model update; end</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Here we have simplified the formula by ignoring the channel-wise splitting and concatenation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/open-mmlab/mmsegmentation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/openai/CLIP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The severity level is controlled by certain coefficients, such as kernel size, specified in ImageNet-C<ref type="bibr" target="#b23">[24]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This study is supported under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). This study is also supported by Singapore MOE AcRF Tier 2 (MOE-T2EP20120-0001) and Shanghai AI Laboratory.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Zero-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-supervised learning of visual features through embedding images into text topic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusi?ol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Beyond instance-level image retrieval: Leveraging captions to learn a global visual representation for semantic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Context-aware feature generation for zero-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Clipscore: A referencefree evaluation metric for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Uncertainty-aware learning for zero-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic image segmentation with self-correcting networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Label propagation for deep semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Putting nerf on a diet: Semantically consistent few-shot view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Zero-shot semantic segmentation via variational mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICMLW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Consistent structural relation learning for zero-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised segmentation based on error-correcting supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Palm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A closer look at self-training for zero-label semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pastore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cermelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Meta pseudo labels</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning visual representations using images with captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Denseclip: Language-guided dense prediction with context-aware prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Inverse problems leveraging pre-trained contrastive representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning visual representations with caption annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Semantic projection network for zero-and few-label semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Zsd-yolo: Zero-shot yolo detection using vision-language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In: NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multimodal contrastive training for visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Faieta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Open vocabulary scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
