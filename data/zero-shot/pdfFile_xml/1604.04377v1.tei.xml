<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DARI: Distance metric And Representation Integration for Person Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
							<email>liya@gzhu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
							<email>ericwangqing@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DARI: Distance metric And Representation Integration for Person Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The past decade has witnessed the rapid development of feature representation learning and distance metric learning, whereas the two steps are often discussed separately. To explore their interaction, this work proposes an end-to-end learning framework called DARI, i.e. Distance metric And Representation Integration, and validates the effectiveness of DARI in the challenging task of person verification. Given the training images annotated with the labels, we first produce a large number of triplet units, and each one contains three images, i.e. one person and the matched/mismatch references. For each triplet unit, the distance disparity between the matched pair and the mismatched pair tends to be maximized. We solve this objective by building a deep architecture of convolutional neural networks. In particular, the Mahalanobis distance matrix is naturally factorized as one top fully-connected layer that is seamlessly integrated with other bottom layers representing the image feature. The image feature and the distance metric can be thus simultaneously optimized via the one-shot backward propagation. On several public datasets, DARI shows very promising performance on re-identifying individuals cross cameras against various challenges, and outperforms other state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Distance/similarity measure between images plays a fundamental role in many computer vision applications, e.g., person verification , matching heterogeneous data <ref type="bibr" target="#b19">(Zhai et al. 2012)</ref>, and multimedia retrieval <ref type="bibr" target="#b2">(Chechik et al. 2012)</ref>. Conventional Mahalanobis distance metric learning models, which aim to seek a linear transformation by pulling the distances between similar pairs while pushing the distances between dissimilar pairs, are theoretically appealing in producing discriminative similarity or distance measure from the given training data <ref type="bibr" target="#b15">(Weinberger, Blitzer, and Saul 2005a)</ref>. However, these approaches usually are performed in the original data space or the hand-engineered feature space (i.e. representation), and thus are limited in capturing variety of image appearance and handling complicated nonlinear manifold.</p><p>In this paper, we investigate the possible interaction between feature learning and distance metric learning, and address the very challenging task of person verification (i.e. matching individuals across cameras). <ref type="figure" target="#fig_0">Figure 1</ref> shows some examples of this task, where the pedestrians across cameras distinctly vary in appearance and structure caused by pose, lighting and view angle changes. The proposed framework is built based on the convolutional neural network (CNN) <ref type="bibr">(Le-Cun et al. 1989)</ref>, which jointly optimizes the human representation as well as the proper distance metric to robustly match individuals against various real challenges. We call this framework DARI (i.e. Distance metric And Representation Integration).</p><p>We aim at preserving similarity of the same person while discriminating the different individuals, and thus define DARI in the form of maximizing relative distance. Specifically, we organize the training images into triplet units, and each unit contains one person image and the matched/mismatch references. For all triplet units, the objective of DARI is to maximize the distance disparity between the matched pairs and the mismatched pairs. In particular, DARI seeks optimal solutions for both feature representation and distance metric, while the existing approaches of person verification <ref type="bibr" target="#b17">(Xu et al. 2013;</ref> focuses on only either of the two components. We adopt the deep CNN architecture to extract the discriminative features from the input images, where the convolutional layers, maxpooling operators, and one full connection layer are stacked up. And the Mahalanobis distance matrix is applied with the generated feature as the matching metric. Due to the positive semi-definite requirement for the Mahalanobis metric, directly optimizing the metric matrix is computational intensive. We propose to decompose the Mahalanobis matrix inspired by <ref type="bibr" target="#b11">(Mignon and Jurie 2012)</ref>, and further factorize the matrix into a fully-connected layer on the top of our deep architecture. In this way, the distance metric is seamlessly integrated with the image feature represented by the other layers of neural networks. The joint optimization can be then efficiently achieved via the standard backward propagation. Therefore, by means of the nonlinearity learning of deep neural networks, DARI is capable of representing the complicated transformation to identify the people in the wild.</p><p>To scale up our approach to the large amount of training data, we implement the training in a batch-process fashion. In each round of training, we randomly select a relatively small number (say 60 ? 70) of images, and use them to organize the triplet units. By taking the triplets as the inputs, we update the model parameters by the stochastic gradient descent (SGD) algorithm <ref type="bibr" target="#b9">(LeCun et al. 1998)</ref>. Another arising issue is that the triplet organization cubically enlarges the number (say 4800) of training samples, as one image can be included into more than one triplet. To overcome it, we calculate the gradients on the images instead of the produced triplets, and thus reduce the computation cost by making it only depends on the number of the selected images.</p><p>The key contribution of this paper is a novel end-to-end framework that naturally fuses the concept of feature learning and metric learning via the deep neural networks. To the best of our knowledge, such an approach is original to the community. On several challenging benchmarks for person verification (e.g., CUHK03 <ref type="bibr" target="#b10">(Li et al. 2014)</ref>,CUHK01 <ref type="bibr" target="#b10">(Li, Zhao, and Wang 2012)</ref> and iLIDS (Zheng, Gong, and Xiang 2013)), our DARI framework demonstrates superior performances over other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>A number of approaches, e.g., local metric learning and kernelized metric learning, have been suggested to learn multiple or nonlinear metrics from training data with complicated nonlinear manifold structure. In local metric learning, local metrics can be learned independently for each region or by considering the data manifold structure <ref type="bibr" target="#b12">(Noh, Zhang, and Lee 2010;</ref><ref type="bibr" target="#b15">Wang, Kalousis, and Woznica 2012;</ref><ref type="bibr" target="#b16">Weinberger, Blitzer, and Saul 2005b)</ref>. In kernelized metric learning, a kernel function is exploited to implicitly embed instances into the reproducing kernel Hilbert space (RKHS), and a Mahalanobis distance metric is then learned in the RKHS space <ref type="bibr" target="#b14">(Wang et al. 2011)</ref>. Actually, kernelized method with Gaussian RBF kernel can also be treated as local learning approach. As pointed out in (Bengio 2009), local learning are also shallow models, and generally are insufficient in coping with highly varying appearance and deformations of images. Another efficient local distance metric learning <ref type="bibr" target="#b18">(Yang et al. 2006</ref>) was also proposed for classification and retrieval. To handle heterogeneous data, <ref type="bibr" target="#b17">(Xiong et al. 2012</ref>) propose a method using a random forest-based classifier to strengthen the distance function with implicit pairwise position dependence.</p><p>On the other hand, deep convolutional models have been intensively studied and achieved extremely well performance. Compared with the multiple layer perceptron, CNN contains much less parameters to be learned, and can be efficiently trained using stochastic gradient descent. With the increasing of large scale training data and computational resources, deeper CNN and novel regularization methods had been developed, and deep CNN has gained great success in many visual recognition tasks, e.g., image classification <ref type="bibr" target="#b8">(Krizhevsky, Sutskever, and Hinton 2012a)</ref>, object detection (Szegedy, Toshev, and Erhan 2013), and scene labeling (Pinheiro and Collobert 2014).</p><p>Despite the success of deep learning in variety of vision tasks, little studies were conducted on metric learning with deep architecture. Chopra et al. <ref type="bibr" target="#b3">(Chopra, Hadsell, and LeCun 2005)</ref> suggested a energy-based model (EBM) for discriminative similarity metric learning for image pairs. Stacked restricted Boltzmann machines (RBMs) had also been exploited to learn nonlinear transformation for data visualization and supervised embedding <ref type="bibr" target="#b11">(Min et al. 2010</ref>). <ref type="bibr" target="#b1">Cai et al. (Cai et al. 2010</ref>) proposed a deep nonlinear metric learning method by combining logistic regression and independent subspace analysis. Hu et al. <ref type="bibr" target="#b7">(Hu, Lu, and Tan 2014)</ref> adopted the forward multi-layer neural network to learn deep metric for hand-crafted features. Compared with these approaches, the proposed DARI model considers the prominence of CNN in capturing salient and incorporates the Mahalanobis distance with the generated image features into one optimization target for distance metric and representation integration.</p><p>One approach close to ours was proposed by Wang et al. <ref type="bibr" target="#b10">(Wang et al. 2014)</ref>, which addresses the triplet-based similarity learning for image retrieval. However, our work have significant differences with that work. First, we derive our formulation from a novel angle, i.e. integrating feature learning and distance metric learning. Second, our learning method has advantage in the triplet generation and the batchbased gradient descent learning. Specifically, given m training triplets containing n distinct images (n &lt;&lt; m), their algorithm optimizes with 3?m forward and backward propagations, while only n rounds is required for our approach because we derive to calculate the gradient over the images. Last, our deep architecture is specifically designed (only two conv layers are used) and we train our model from scratch, while they utilized the Alex's model <ref type="bibr" target="#b9">(Krizhevsky, Sutskever, and Hinton 2012b)</ref> that is pre-trained on the ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework Primal Formulation</head><p>Given a fixed feature representation, metric learning is to learn a distance function by satisfying the constraint according to the label information of samples. Here we define our formulation via relative distance comparison based on the triplet-based constraint. As is discussed in <ref type="bibr" target="#b4">(Ding et al. 2015)</ref>, the triplet models allows the images of one identity lying on a manifold while maximizing the margin between within-class pairs from between-class pairs, and tends to result in better tradeoff between adaptability and discriminability.</p><p>More precisely, the relative distance constraint is defined with a set of triplet units T = {&lt; I i , I j , I k &gt;}, in which &lt; I i , I j &gt; is a pair of matched images (images of the same individual) and &lt; I i , I k &gt; contains two mismatched images from the labeled image set I = {I l , y l } with y l denoting the label. Let M denote the metric matrix and F W (I i ) denote the feature representations of the ith image learned by the feature network with the network parameters W. Then Mahalanobis distance between I i and I j using the CNN features can be written as follows:</p><formula xml:id="formula_0">d 2 (I i , I j ) = ?F W (I i , I j ) T M?F W (I i , I j ) (1) where ?F W (I i , I j ) = F W (I i ) ? F W (I j ) denotes the fea- ture difference between the image I i and I j . For each train- ing triplet &lt; I i , I j , I k &gt; in T , the desired distance should satisfy: d 2 (I i , I j ) &lt; d 2 (I i , I k ). Let ?d 2 (I i , I j , I k ) denote d 2 (I i , I k ) ? d 2 (I i , I j )</formula><p>, we turn this relative constraints into the minimization of the following hinge-loss like objective function where ? tr(M) acts as a regularization term as in <ref type="bibr" target="#b14">(Shen et al. 2012)</ref>.</p><formula xml:id="formula_1">H(W, M) = ?&lt;I i ,I j ,I k &gt;?T (1 ? ?d 2 (Ii, Ij, I k )) + + ?tr(M)</formula><p>In the following, we use to denote ?&lt;Ii,Ij ,I k &gt;?T for notation simplicity. By the definition of ?d 2 , we get the following objective functions: <ref type="figure">Figure 2</ref>: Illustration of our learning formulation. It tends to maximize the distance disparity between the matched pair and mismatched pair for each triplet, in terms of optimization.</p><formula xml:id="formula_2">H(W, M) = (1 ? (?F W (Ii, I k ) T M?F W (Ii, I k ) ? ?F W (Ii, Ij) T M?F W (Ii, Ij)))+ + ? tr(M) s.t. M 0 (2) I j I i I k I j I i I k</formula><p>An intuitive solution to Eqn.</p><p>(2) is to iteratively optimize W and M with either of them fixed. It is, however, computationally expensive, as the PSD projection is necessarily imposed once M is updated to ensure M to be positive semi-definite. In this work, to overcome this issue we propose an unified solution by incorporating M into the deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Architecture</head><p>We factorize the metric matrix M into L T L as M satisfies the positive semi-definite constraint. The distance measure in Eqn.</p><p>(1) can be then transformed as,</p><formula xml:id="formula_3">d 2 (Ii, Ij) = ||L?F W (Ii, Ij)|| 2 (3)</formula><p>The objective function H(W, M) in Eqn.</p><p>(2) can be then reformulated with tr(M) = ||L|| 2 H , as</p><formula xml:id="formula_4">H(W, L) = (1 ? (||L?F W (Ii, I k )|| 2 ? ||L?F W (Ii, Ij)|| 2 ))+ + ?||L|| 2 H<label>(4)</label></formula><p>Thus, we can take M as a linear transform on the output of the CNN-based feature representation. In literature, (Weinberger, Blitzer, and Saul 2005a) (Mignon and Jurie 2012) also proposed to decompose the Mahalanobis distance matrix for simplifying the distance metric learning. They attempt to pursue a low-dimensional projection in Euclidean space which embeds the distance metric while ensuring the PSD constraint. However, their solution are complicated requiring additional hypothesis. In this work, we implement a fully connected layer to represent L, which is stacked over the layers representing image features, making the distance metric tightly combined with the deep neural network. Specifically, we treat L as the neuron weight of the layer, and the network can represent LF W (I i ) by taking F W (I i ) as the input. Then, the feature learning and the distance metric can be thus jointly optimized in an end-to-end way.</p><p>In the following, we denote W + = (W, L) for notation simplicity. As the regularization term on L will be automatically implemented by the built-in weight decay mechanisms in CNN networks, we neglect this part in the objective function.</p><formula xml:id="formula_5">H(W + ) = (1 ? (||?F W + (Ii, I k )|| 2 ? ||?F W + (Ii, Ij)|| 2 ))+<label>(5)</label></formula><p>Integrating the metric learning and feature learning into one CNN network yields several advantages. First, this leads to a good property of efficient matching. In particular, for each sample stored in a database, we can precomputed its feature representation and the corresponding decomposed Mahalanobis distance matrix. Then the similarity matching in the testing stage can be very fast. Second, it integrates feature learning and metric learning by building an end-to-end deep architecture of neural networks.</p><p>As discussed above, our model defined in Eqn. (5) jointly handles similarity function learning and feature learning. This integration is achieved by building a deep architecture of convolutional neural networks, which is illustrated in <ref type="figure">Figure 2</ref>. Our deep architecture is composed of two subnetworks: feature learning sub-network and metric learning sub-network. The feature learning sub-network contains two convolution-RELU-pooling layers and one fully-connected layer. Both the pooling layers are max-pooling operations with the size of 3 ? 3 and the stride size is set as 3 pixels. The first convolutional layer includes 32 kernels of size 5 ? 5 ? 3 with a stride of 2 pixels. The second convolutional layer contains 32 filters of size 5 ? 5 ? 32 and the filter stride step is set as 1 pixel. A fully-connected layer is followed and it outputs a vector of 400 dimensions. We further normalize the output vector of this fully-connected layer before it is fed to the metric learning sub-network by y i = xi ? ?x 2 i , where</p><p>x i , y i denote the value of the ith neuron before and after normalization respectively. Accordingly, the back propagation process accounts for the normalization operation using the chain rule during calculation of the partial derivatives. The metric learning sub-network includes only one fullyconnected layer. The neural layer outputs LF W (I i ). In this way, the distance metric is tightly integrated with the feature representations, and they can be jointly optimized during the model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Algorithm</head><p>Given a labeled dataset with M classes (persons) and each class has N images, then the number of all possible meaningful triplets is N * (N ? 1) * (M ? 1) * N * M . Even for a dataset of moderate size, it is intractable to load all these triplets into the limited memory for the model training. To overcome this issue, we apply batch learning to optimize the parameters, in which the network parameters are updated by the gradient derived only from a small part of all the triplets in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Process</head><p>In the batch learning process, we need to generate a subset of triplets from all the possible triplets in each iteration. The simplest method is to generate triplets randomly. However, this method makes the number of distinct images be approximately three times the number of the generated triplets because each triplet contains three images, and the likelihood of two triplets sharing the same image is very low. This triplet generation method is very inefficient because there are only a few distance constraints placed on the selected images in each iteration. Instead, to capitalize on the strength of relative distance comparison model, a more reasonable triplet generation method would be one that satisfies the two following conditions: 1. In each iteration, large number of triplets are generated from small number of images to ensure the selected images can be loaded to the memory while rich distance constraints are posed on these images; 2. When increased numbers of iterations are executed, all the possible relative distance constraints between any two classes should be considered in the training process.</p><p>These two principles lead to our proposed triplet generation scheme as follows. In each iteration, we select a fixed number of classes (persons), and construct the triplets only using these selected classes. More precisely, for each image in each class, we randomly construct a certain number of triplets with the matched reference coming from the same class and the mismatched references coming from the remaining selected classes. The complete mini-batch learning process is presented in Algorithm 1.</p><p>Algorithm 1 Learning DARI with batch-process Input:</p><p>Training images {I i }; Output:</p><p>Network Parameters W 1: while t &lt; T do 2:</p><formula xml:id="formula_6">t ? t + 1; 3:</formula><p>Randomly select a set of classes (persons) from the training set;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Construct a set of triplets from the selected classes;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Calculate the gradient ?W for the generated triplets using Algorithm 2; 6:</p><formula xml:id="formula_7">W t = W t?1 ? ? t ?W 7: end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Optimization</head><p>Under the mini-batch training framework, a key step is to calculate the gradient for the triplet set in each iteration. A straight method is to calculate the gradient for each triplet according to the loss function, and sum these gradients to get the overall gradient. But with this approach three separate memory units and a network propagation would be needed for every triplet. This is inefficient as there will be duplicated network propagation for the same image, recalling that for each batch we generate triplets from a known subset of images. We now show that there exists an optimized algorithm in which the computational load mainly depends on the number of distinct images rather than the number of the triplets.</p><p>It would be difficult to write the objective function in Eqn. (5) directly as the sum of image-based loss items because it takes the following form (for notation simplicity, we use W to denote W + in the rest of the paper):</p><formula xml:id="formula_8">H(W) = loss(F W (Ii), F W (Ij), F W (I k ))</formula><p>Fortunately, because the loss function for a specific triplet is defined by the outputs of the images in this triplet, the total loss can also be considered as follows, where {I i } represents the set of all the distinct images in the triplets and m denote the size of the distinct images in the triplets.</p><formula xml:id="formula_9">H(W) = H(F W (I 1 ), F W (I 2 ), ..., F W (I i ), ..., F W (I m ))</formula><p>By the derivative rule, we have the following equations, where W l represents the network parameters, X l i represents the feature maps of the image I i at the l th layer and ?H ?W l (I i ) denote the partial derivative derived from image I i .</p><formula xml:id="formula_10">?H ?W l = ? m i=1 ?H ?X l i ?X l i ?W l = ? m i=1 ?H ?W l (I i ) (6) ?H ?X l i = ?H ?X l+1 i ?X l+1 i ?X l i<label>(7)</label></formula><p>Eqn. 6 shows that the overall gradient is the sum of the image-based terms (image-based gradient). Eqn. 7 shows that the partial derivative with respect to the feature maps of each image can be calculated recursively. With Eqn. 6</p><p>and Eqn. 7, the gradients with respect to the network parameters can be obtained by summing the image based gradients using the network back propagation algorithm. The central premise is that we have computed the partial derivative of the output layer's activation for every image, which can be easily obtained from Eqn. 5. Algorithm 2 gives the detailed process. This optimized algorithm has two obvious merits: 1. We can conveniently use exiting deep learning implementations such as Caffe 1 to train our model.</p><p>2. The number of network propagation executions can be reduced to the number of distinct images in the triplets, a crucial advantage for large scale datasets.</p><p>Algorithm 2 Calculating gradients for optimization Input:</p><p>Training triplets T = {&lt; I i , I j , I k &gt;}; Output:</p><p>The gradient of network parameters: ?W = ?H ?W if I i =I i then 10: Sum the partial derivative: ?W+= ?H ?W (I i ); 21: end for Evaluations Datasets and Implementation details. We conduct our experiments using three challenging human verification datasets, i.e. CUHK03 <ref type="bibr" target="#b10">(Li et al. 2014</ref>), CUHK01 <ref type="bibr" target="#b10">(Li, Zhao, and Wang 2012)</ref> and iLIDS <ref type="bibr" target="#b20">(Zheng, Gong, and Xiang 2013)</ref> . All the images are resized to 250 ? 100 for the experiment. The weights of the filters and the full connection parameters are initialized from two zero-mean Gaussian distributions with standard deviation 0.01 and 0.001 respectively. The bias terms were set with the constant 0. During the training, we select 60 persons to construct 4800 triplets in each iteration. Before feeding to the network, the images are mirrored with 0.5 probability and cropped to the size 230 ? 80 at the center with a small random perturbation to augment the training data. We implement our learning algorithm based on the Caffe framework, where we revise the data layer and loss layer to generate the triplets and apply our loss function. We execute the code on a PC with GTX780 GPU and quad-core CPU. And stop the training process when there are less than 10 triplets whose distance constraints are violated, i.e. the distance between the matched pair is greater than the distance between the mismatched pair.</p><formula xml:id="formula_11">partialSum+ = 2(F W (I k ) ? F W (I j )) 11: else if I i =I j then 12: partialSum? = 2(F W (I i ) ? F W (I j ) 13: else if I i =I k then 14: partialSum+ = 2(F W (I i ) ? F W (I k )</formula><p>Evaluation Protocol. We adopt the widely used cumulative match curve (CMC) approach <ref type="bibr" target="#b5">(Gray, Brennan, and Tao 2007)</ref> for quantitative evaluation. We follow the standard setting to randomly partition each dataset into training set and test set without overlap. In each testing, the test set is further divided into a gallery set and a probe set without overlap for 10 times. A rank n rate is obtained for each time, and we use the average rate as the final result.</p><p>Component analysis. In order to demonstrate how the joint optimization of distance metric with feature representation contributes to performance, we implement a simplified model for comparison by discarding the distance metric learning(i.e. the last neural layer). In this implementation, we only optimize CNN-based feature representation by the back-propagation method.</p><p>Experiments on CUHK03 Dataset. This benchmark <ref type="bibr" target="#b10">(Li et al. 2014</ref>) is the largest one up to date, which contains 14096 images of 1467 pedestrians collected from 5 different pairs of camera views, making it an ideal place for deep learning. Each person is observed by two disjoint camera views and has an average of 4.8 images in each view. We follow the standard setting of using CUHK03 to randomly partition this dataset for 10 times without overlap, and a training set (including 1367 persons) and a test set (including 100 persons) are obtained. In each testing, the testing set is further randomly divided into a gallery set of 100 images (i.e. one image per person) and a probe set (including images of individuals from different camera views in contrast to the gallery set) without overlap for 10 times.</p><p>We introduce several types of state-of-the-arts in this experiment. First, we adopt Local Distance Metric Learning (LDM) (Guillaumin, Verbeek, and Schmid 2009), the learning-to-rank method (RANK) (McFee and Lanckriet 2010) for comparison, which learn distance metrics based on a fixed feature representation. Following their implementation, the handcrafted features of dense color histograms and dense SIFT uniformly sampled from patches are adopted. Two methods especially designed for person re-identification are introduced in this experiment: KISSME <ref type="bibr">(Kostinger et al. 2012), eSDC (Zhao, Ouyang, and</ref><ref type="bibr" target="#b20">Wang 2013)</ref>. Moreover, we compare with a recently proposed deep learning method, DRSCH , FPNN <ref type="bibr" target="#b10">(Li et al. 2014</ref>) and IDLA <ref type="bibr" target="#b0">(Ahmed, Jones, and Marks 2015)</ref>. DRSCH  learns hashing code with regularized similarity for image retrieval and person reidentification. FPNN <ref type="bibr" target="#b10">(Li et al. 2014</ref>) learns pairs of filters to extract person representation and IDLA <ref type="bibr" target="#b0">(Ahmed, Jones, and Marks 2015)</ref> is also recently proposed deep learning method for person re-identification.</p><p>The results are shown in <ref type="figure">Fig. 3 (a)</ref>. It is encouraging to observe that our approach achieves a new state-of-the-art on CUHK03. Note that without the joint optimization of  <ref type="figure">Figure 3</ref>: Quantitative results on the three datasets: (a) CUHK03, (b) CUHK01 and (c) iLIDS dataset. Our DARI framework leads superior performances over existing state-of-the-arts overall. Note that "Ours-nj" represents a simplified version of our model, i.e. discarding the joint optimization of distance metric and CNN-based feature.</p><p>distance metric and representation, the performance (i.e., " Ours-nj") degenerates from 55.4% to 45.4%. Experiments on CUHK01 Dataset. CUHK01 contains 971 individuals, each of which has two samples captured by two disjoint camera views. We partition this dataset into a training set and a testing set exactly following <ref type="bibr" target="#b10">(Li et al. 2014</ref>) <ref type="bibr" target="#b0">(Ahmed, Jones, and Marks 2015)</ref>: 100 persons are used for testing and the remaining 871 persons for training. Each person has two images for each view and we randomly select one into the gallery set. Single-shot is adopted in the evaluation of CMC curve.</p><p>In addition to comparing with the methods adopted in the experiment on CUHK03, we introduce a recently proposed method which also addresses the interaction of representation learning and metric Learning (denoted as L + X) <ref type="bibr" target="#b11">(Liao et al. 2015)</ref>. <ref type="figure">Fig. 3 (b)</ref> shows the comparison of our DARI framework with other approaches. DARI achieves a new state of the art, with a rank-1 recognition rate of 65.95%. The gain of the joint optimization of distance metric and CNN-based feature is also clear on this dataset, 65.95% over 57.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-dataset Evaluation</head><p>The iLIDS dataset (Zheng, Gong, and Xiang 2013) was constructed from video images captured in a busy airport arrival hall. It has 119 pedestrians, totaling 479 images. We conduct a challenging task, i.e. cross-dataset task using this dataset, which accords with the real-world surveillance applications. Specifically, we randomly split this dataset into a gallery set and a probe set: the gallery contains only one image of each pedestrian and the remaining images form the probe set. Our model is trained on CUHK03 and tested on this iLIDS dataset without fine-tuning the parameters.</p><p>We compare our DARI with several existing methods such as Xing's <ref type="bibr" target="#b17">(Xing et al. 2002), and</ref><ref type="bibr">MCC (Globerson and</ref><ref type="bibr" target="#b4">Roweis 2005)</ref>. They all use an ensemble of color histograms and texture histograms as the feature representation. Note that the results reported by these competing methods are generated by a different setting: both of the training and the testing data are from this dataset. <ref type="figure">Fig. 3 (c)</ref> shows the quantitative results. Our superior performance over other approaches demonstrate the good generalization power of DARI. On the other hand, without in-corporating Mahalanobis distance matrix, the performance (i.e. "Ours-nj" in ) clearly degenerates from 42.8% to 36.9%, which highlights the significance of the joint optimization of feature representation and distance metric. In the following, we further evaluate our approach under different implementation setting on iLIDS.</p><p>Data Augmentation Strategy. We crop the center of the images with random perturbation to augment the training data. This mechanism can effectively alleviate the overfitting problems. Without this augmentation scheme, the top 1 performance drop by about 30 percent relatively.</p><p>Triplet Generation Scheme. We compared two generation strategy. In the first strategy, we select 60 persons for each iteration and only construct 60 triplets for these persons. In the second strategy, we select the same number of persons while constructing 4800 triplets for these persons. As expected by our analysis, the learning process of the first strategy is much slower than the second strategy and when the learning process of the second strategy converges in 7000 iterations, the performance of the first strategy only achieves about 70 percent of the second strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Typical examples of person verification across cameras. Each column corresponds to one individual, and the large variations exist between the two examples due to the light, pose and view point changes.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://caffe.berkeleyvision.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have presented a novel deep learning framework incorporating Mahalanobis distance matrix with convolutional neural networks. In future work, we will extend our approach for larger scale heterogeneous data, thereby exploring new applications.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marks ; Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
	<note>Learning deep architectures for ai</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep nonlinear metric learning with independent subspace analysis for face verification</title>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadsell</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
	<note>Metric learning by collapsing classes</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PETS. Citeseer</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="498" to="505" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relaxed Pairwise Learned Metric for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hirzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="780" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><forename type="middle">;</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinton ; Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>Pereira, F.</editor>
		<editor>Burges, C.</editor>
		<editor>Bottou, L.</editor>
		<editor>and Weinberger, K.</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinton ; Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
	<note>Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
	<note>ACCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative local metric learning for nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Lee ; Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<editor>Lafferty, J.</editor>
		<editor>Williams, C.</editor>
		<editor>Shawe-Taylor, J.</editor>
		<editor>Zemel, R.</editor>
		<editor>and Culotta, A.</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1822" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
	<note>Pinheiro and Collobert</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Positive semidefinite metric learning using boosting-like algorithms</title>
	</analytic>
	<monogr>
		<title level="m">Burges</title>
		<editor>Shawe-Taylor, J.</editor>
		<editor>Zemel, R.</editor>
		<editor>Bartlett, P.</editor>
		<editor>Pereira, F.</editor>
		<editor>and Weinberger, K.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">98888</biblScope>
			<biblScope unit="page" from="1170" to="1178" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalousis</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Woznica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
	<note>Parametric local metric learning for nearest neighbor classification</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blitzer</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human re-identification by matching compositional template with cluster sampling</title>
		<idno>Xu et al. 2013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD</title>
		<meeting>the 18th ACM SIGKDD</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3152" to="3159" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An efficient algorithm for local distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiview metric learning with global consistency and local smoothness</title>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bit-scalable deep hashing with regularized similarity learning for image retrieval. TIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
	<note>BMVC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
