<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Detection with Spiking Neural Networks on Automotive Event Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Cordone</surname></persName>
							<email>loic.cordone@renault.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7248</orgName>
								<orgName type="institution" key="instit1">Renault University C?te d&apos;Azur LEAT</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Miramond</surname></persName>
							<email>benoit.miramond@univ-cotedazur.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">University C?te d&apos;Azur LEAT / CNRS UMR 7248</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Thierion</surname></persName>
							<email>philippe.thierion@renault.com</email>
							<affiliation key="aff2">
								<orgName type="department">Software Factory Renault Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Detection with Spiking Neural Networks on Automotive Event Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-spiking neural networks</term>
					<term>event cameras</term>
					<term>object detection</term>
					<term>SSD</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automotive embedded algorithms have very high constraints in terms of latency, accuracy and power consumption. In this work, we propose to train spiking neural networks (SNNs) directly on data coming from event cameras to design fast and efficient automotive embedded applications. Indeed, SNNs are more biologically realistic neural networks where neurons communicate using discrete and asynchronous spikes, a naturally energy-efficient and hardware friendly operating mode. Event data, which are binary and sparse in space and time, are therefore the ideal input for spiking neural networks. But to date, their performance was insufficient for automotive real-world problems, such as detecting complex objects in an uncontrolled environment. To address this issue, we took advantage of the latest advancements in matter of spike backpropagation -surrogate gradient learning, parametric LIF, SpikingJelly frameworkand of our new voxel cube event encoding to train 4 different SNNs based on popular deep learning networks: SqueezeNet, VGG, MobileNet, and DenseNet. As a result, we managed to increase the size and the complexity of SNNs usually considered in the literature. In this paper, we conducted experiments on two automotive event datasets, establishing new state-of-the-art classification results for spiking neural networks. Based on these results, we combined our SNNs with SSD to propose the first spiking neural networks capable of performing object detection on the complex GEN1 Automotive Detection event dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Event cameras have several interesting features for embedded automotive applications: very low latency, high dynamic range and low power consumption. They are composed of independent photoreceptor pixels that detect a change in brightness. They output binary events, a type of information that contains the position, the precise time and polarity of every brightness change. But since the data they produce is asynchronous and sparse by nature, it is difficult to use classical neural networks that were designed for processing frames.</p><p>In spiking neural networks, the information is transmitted between neurons by discrete binary spikes, making the whole network asynchronous and sparse by nature. They are therefore particularly adapted to the processing of event data. Due to This material is based upon work supported by the French technological research agency (ANRT) through a CIFRE thesis in collaboration with Renault. the nature of their operations, they are also hardware friendly: previous works showed that on specialized hardware, spiking neural networks consume 50% less energy than traditional neural networks while maintaining the same accuracy <ref type="bibr" target="#b0">[1]</ref>, and the recent release of new neuromorphic hardware such as Intel Loihi 2 <ref type="bibr" target="#b1">[2]</ref> could further improve these results.</p><p>Using the latest advancements in matter of spike backpropagation, we train state-of-the-art spiking neural networks to process real-world automotive event data and obtained good performance on classification and object detection tasks.</p><p>The main contributions of this work can be summarized as follows:</p><p>1) We present a novel approach to encode event data called voxel cube that preserves their binarity and temporal information while keeping a low number of timesteps. 2) We propose a new challenging dataset for classification on automotive event data: GEN1 Automotive Classification, generated using the Prophesee object detection dataset of the same name. 3) We train four different spiking neural networks for classification tasks based on popular neural network architectures (SqueezeNet, VGG, MobileNet, DenseNet) and evaluate them on two automotive event datasets, setting new state-of-the-art results for spiking neural networks. 4) We present spiking neural networks for object detection composed of a spiking backbone and SSD bounding box regression heads that achieve qualitative results on the real-world GEN1 Automotive Detection event dataset. To the best of our knowledge, it constitutes the first spiking neural networks capable of doing object detection on real-world event dataset. Our code is available upon request and will be available online in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Learning on event data</head><p>With its microsecond temporal resolution, event data cannot be directly processed by modern deep learning models. Over the years, several preprocessing methods have emerged to convert events into a dense representation that can be used in Deep Neural Networks (DNNs).</p><p>The simplest preprocessing method is to accumulate the events pixel-wise over a given period of time, creating an histogram <ref type="bibr" target="#b2">[3]</ref>. The result can be viewed as an event frame, with no time dimension and two output channels containing the event count for each polarity. Time surfaces <ref type="bibr" target="#b3">[4]</ref>, where the timestamp of the last received event is stored in each pixel, represent an alternative to histograms that better preserve the temporal information inside an event frame. A recent preprocessing method called "event cube" <ref type="bibr" target="#b4">[5]</ref> has been proposed to combine the simplicity of histograms with the temporal information of time surfaces. In event cubes, a given period of time is split in n micro time bins, and each pixel stores not the event count but the event temporal distance from the center of the neighboring micro time bins. The temporal information is thus contained in the channel dimension, of size 2 ? n.</p><p>The simplest way to retain the temporal information is to represent events as a voxel grid <ref type="bibr" target="#b5">[6]</ref>, where each voxel represents a pixel and a time interval. Although it requires more memory and more computation, this is the preferred representation for spiking neural networks as both the data and the network operate on a fixed number of timesteps. The accumulation of events on the time interval is usually a sum, as in <ref type="bibr" target="#b6">[7]</ref>, but a binary accumulation as in <ref type="bibr" target="#b7">[8]</ref> has the advantage of interpreting events as binary spikes, and therefore benefit from the spiking neural networks energy gains on specialized hardware. This paper introduces a novel representation called "voxel cube" that combines binary voxel grids with event cubes to obtain a binary representation with a small number of timesteps that maintains high temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spiking Neural Networks</head><p>Spiking neural networks more closely mimic biological neural networks by incorporating the concept of time into their operating model. The neurons communicate using binary spikes following a spiking neuron model e.g. the Leaky Integrate-and-Fire (LIF) neuron <ref type="bibr" target="#b8">[9]</ref>, that is widely used due to the simplicity of its operations. Neurons are connected by scalar weights, modeling the synapses. As in DNNs, it is possible to learn these weights, but since spikes are discrete and thus non-differentiable, it is not possible to use the popular backpropagation learning algorithm on SNNs. For static data, SNNs can be created by converting to the spike domain a trained DNN, using rate coding for example. However, the results are inevitably inferior, making the direct learning of SNNs more relevant especially for temporal data. To overcome the lack of a backpropagation algorithm on spikes, several learning rules have been proposed over the years.</p><p>Spike-timing-dependent plasticity (STDP) <ref type="bibr" target="#b9">[10]</ref> is a bioplausible unsupervised learning rule where the weight connecting two neurons is modified according to the delay between the firing of the presynaptic and postsynaptic neurons. For a long time, results obtained with STDP were not competitive with supervised learning even on simple tasks, but recent progress <ref type="bibr" target="#b10">[11]</ref> could change the picture. However, STDP still has not proved its effectiveness on real-word complex tasks such as object detection.</p><p>Recently, supervised learning rules based on backpropagation have enabled the training of SNNs with excellent results. Reference <ref type="bibr" target="#b11">[12]</ref> presented SLAYER, an error backpropagation method for SNNs capable of learning both synaptic weights and axonal delays, allowing them to tackle bigger datasets with deeper networks. Another approach introduced in <ref type="bibr" target="#b12">[13]</ref> is to train SNNs using surrogate gradient learning: during the forward pass, an Heaviside step function generates spikes and during the backward pass, the gradient of this nondifferentiable function is approximated using a surrogate gradient, for example the gradient of a sigmoid function. This work also demonstrated that SNNs are equivalent to RNNs, enabling their learning with backpropagation through time in popular deep learning frameworks.</p><p>Using this surrogate gradient learning rule, multiple Pytorch-based frameworks for training SNNs have emerged, such as Nengo <ref type="bibr" target="#b13">[14]</ref> or SpikingJelly <ref type="bibr" target="#b14">[15]</ref>. Thanks to their automatic differentiation and strong GPU acceleration, these new frameworks have led to the training of deeper spiking neural networks achieving state-of-the-art results on classification problems <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>. It also becomes possible to make the neuron parameters learnable. For example, reference <ref type="bibr" target="#b6">[7]</ref> introduced the Parametric LIF (PLIF) neuron model, where the learnable time constants make the network less sensitive to initial values and can speed up learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object Detection on event data</head><p>The task of object detection is to determine where objects are located in a given image (object localization) and which category each object belongs to (object recognition). DNNs have brought significant performance gains over handcrafted methods by learning semantic, high-level and deeper features. For embedded real-time applications, one-step object detection frameworks such as SingleShot MultiBox Detector (SSD) <ref type="bibr" target="#b16">[17]</ref> have emerged. They map image pixels to bounding box coordinates and class probabilities, modeling object detection as a regression or classification task.</p><p>Object detection on event data, which is sparse and contains high temporal information, is still a challenge. The simplest approach is to integrate the events as a single frame, and to train a deep neural network without temporal information. This is used for example in the YOLE network <ref type="bibr" target="#b17">[18]</ref>. With an event preprocessing method that retains temporal information, it is possible to design more complex object detection networks. In <ref type="bibr" target="#b18">[19]</ref> authors propose Matrix-LSTM a network based on voxel grids representations and composed of a matrix of LSTM cells that sequentially processes event features. These features are then fed to a pretrained object detection model. A more ambitious approach is to train an unique neural network directly on voxel grids, using the temporal information to output the bounding boxes and the classes. Such an approach is presented in <ref type="bibr" target="#b19">[20]</ref>: Recurrent Event-camera Detector (RED) is a recurrent neural network composed of feed-forward convolutional layers followed by ConvLSTM. While the convolutional layers extract low-level features from events, the ConvLSTM layers extract high-level spatio-temporal patterns thanks to their memory. The output is then fed to SSD bounding box regression heads.</p><p>This demonstrates that learning to detect objects from events is possible, but it still requires deep neural networks that do not take full advantage of the properties of event-data and that would be difficult to embed in power constrained environment. Hybrid-SNN <ref type="bibr" target="#b20">[21]</ref> proposes a partial solution by presenting an hybrid neural network composed of a SNN backbone for efficient event-based feature extraction, and an ANN head to solve object detection tasks. To the best of our knowledge, the work presented in our paper is the first complete spiking neural network capable of doing object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Voxel cubes</head><p>Most event sensors output events with microsecond temporal resolution, but in order to use them in modern deep learning models, we need to convert them into a dense representation. In our case, we accumulate events over time windows of ?t seconds. The resulting representation is called a voxel grid, where each voxel represent a pixel and a time interval. Indeed, a sample lasting d seconds is now divided in d ?t = T timesteps. Usually, the events are stored in the form of a 4D CT HW tensor, with C the number of channels, T the number of timesteps, H and W the height and width of the data.</p><p>We can then use spiking neural networks directly on event data, they will therefore operate on T timesteps. But in order to keep the high temporal resolution of event data, we need to have a large number of timesteps T , which increases linearly the number of computations of the SNN and thus the inference time and the energy consumed.</p><p>Inspired by event cubes <ref type="bibr" target="#b4">[5]</ref>, we propose a novel event preprocessing called voxel cubes. In voxel cubes, each time window ?t is subdivided in n micro time bins lasting therefore ?t n seconds. Events belonging to a micro time bin will be stored in the channels dimension, providing finer temporal information to the first layer of the network. The number of channels C is therefore equals to 2 ? n, each polarity being stored in n channels. Contrary to event cubes, each event contributes only to the time bin where it falls into, and the accumulation of multiple events in the same micro time bin is binary. This loss of information is justified by the need to keep binary inputs, in order to leverage the energy efficiency of spiking neural networks running on specialized hardware. An illustration of this encoding is proposed in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>By transferring temporal information to the channel dimension, voxel cubes allow us to reduce the number of timesteps T without loosing temporal precision compared to voxel grids, as we show in section V-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spiking Neural Networks models</head><p>Inspired by popular deep learning convolutional neural networks (CNNs), we designed 4 different spiking neural networks using only strided convolutions, max pooling, batch normalization and PLIF neurons. Convolutions and max pooling have been used extensively with SNNs [8] <ref type="bibr" target="#b21">[22]</ref>. Since batch normalization layers can be merged with a preceding or subsequent convolution layer at inference, it is possible to use them to train SNNs as long as they are placed before the PLIF neurons. We explore their importance in section V-B.</p><p>In CNNs, the final layers used for classification need to be adapted to be compatible with spikes. To this end, we propose a spiking classifier simply composed of a layer of batch normalization, a 1 ? 1 convolution outputting num classes channels and PLIF neurons. By using a 1D convolution, this classifier is able to process feature maps of any size without requiring e.g. a layer of average pooling, which would be incompatible with spikes operations. The final predictions are obtained by summing all output spikes first in the spatial dimension, then in the time dimension.</p><p>We propose spiking variants of VGG, SqueezeNet, MobileNet and DenseNet by replacing their ReLU activation functions by PLIF neurons. All spiking neural networks use the spiking classifier described above in lieu of their own classifier.</p><p>1) Spiking VGG: Introduced in <ref type="bibr" target="#b22">[23]</ref>, VGG is a convolutional neural network composed of up to 19 convolutional layers followed by 3 fully-connected layers. Our Spiking VGG replaced the final classifier but kept the same architecture, with the addition of batch normalization before each spiking convolutional layer.</p><p>2) Spiking SqueezeNet: SqueezeNet <ref type="bibr" target="#b23">[24]</ref> is a small CNN that uses Fire modules: squeeze layers (1 ? 1 convolutions) before expand layers composed of a mix of 1 ? 1 and 3 ? 3 convolutions. This results in a low number of parameters, an appealing property for embedded applications. We replaced all convolutional layers by their spiking equivalent to obtain our Spiking SqueezeNets, along the addition of batch normalization.</p><p>3) Spiking MobileNet: MobileNet <ref type="bibr" target="#b24">[25]</ref> is a model designed to be used in mobile applications that use depthwise separable convolutions, requiring less parameters and computations than normal convolutions. For our Spiking MobileNet, we dropped the activation function between the depthwise and pointwise convolutions, and moved all batch normalization layers before the convolutional layers. Removing the activation function makes our network non-spike as the inputs of the pointwise convolution are not spikes anymore. However, it can be shown that a depthwise separable convolution is equivalent to a normal convolution with specific weights. Thus, we used depthwise separable convolutions in the training of our spiking MobileNets as it provided better results (see section V-C), and we return to a full-compatible SNN at inference by replacing them by their equivalent convolutions. Our Spiking MobileNet contains only one of the five identical depthwise separable layers near the end of the network. We have varied the number of filters of the first layer to obtain networks of different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Spiking DenseNet:</head><p>To promote gradient propagation, ResNets use element-wise addition, an operation difficult to operate in the spike domain. Reference <ref type="bibr" target="#b15">[16]</ref> proposed Spiking ResNets with different residual connections, but the one based on an AND accumulation -in our opinion the only one compatible with an implementation on specialized hardware -produces unsatisfactory results. DenseNet <ref type="bibr" target="#b25">[26]</ref> is an architecture that promotes gradient propagation by using channel-wise concatenations, which is an operation preserving the spike representation. We replaced the ReLU activations by PLIF neurons to obtain our Spiking DenseNet. We have varied the depth and growth rate to obtain different versions of Spiking DenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object Detection with Spiking Neural Networks</head><p>The SSD object detection framework <ref type="bibr" target="#b16">[17]</ref> consists of a backbone and multiple predictor heads. The heads take as inputs feature maps generated by the backbone at different scales to predict bounding boxes and their associated classes. To obtain a complete spiking neural network capable of doing object detection, we replaced the CNN backbone by the SNN backbones designed for classification, and we used spiking convolutions instead of normal convolutions in the extra layers. Therefore, the feature maps fed to the SSD heads are spikes, and since the heads consist of only one convolution the whole network is indeed a SNN.</p><p>The spiking neural network operates on T timesteps, therefore the final bounding boxes and classes predicted by the network are obtained by doing a sum over the T timesteps. The output of the network still requires a post-processing to filter predictions, but we assume that this step would be done on conventional hardware outside the spiking neural network scope.</p><p>One-shot object detectors such as SSD struggles with the class imbalance problem due to the overwhelming number of predictions classified as background. Reference <ref type="bibr" target="#b26">[27]</ref> introduced focal loss, a modulation term applied to the crossentropy loss function that tremendously helps the learning of one-shot object detectors. Thus, we trained our spiking neural network with focal loss, as the hard negative mining originally used by SSD did not achieved satisfying performances.</p><p>As in the original SSD architecture, we used three extra blocks of convolutions to generate smaller feature maps after our spiking backbone. Each block consists of a 1 ? 1 spiking convolution to reduce the number of channels, followed by a 3 ? 3 spiking convolution with a stride of 2. Once again, we used batch normalization layers before each convolution.</p><p>The anchors used by our network were generated with a minimum ratio of 0.5 and a maximum ratio of 0.8, accounting for the smaller objects present in the object detection dataset we studied.</p><p>The architecture of our spiking DenseNet + SSD is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Increasingly smaller feature maps generated by the DenseNet backbone are fed to the SSD heads, and the 3 extra blocks further reduce features maps to a final size of 2 ? 1. Similar architectures are used for our spiking MobileNet + SSD and VGG + SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluated our spiking neural networks on two automotive classification datasets: Prophesee NCARS and a new event dataset we called Prophesee GEN1 Automotive Classification, generated from the object detection dataset Prophesee GEN1. We then evaluated our object detection spiking networks on this specific dataset, the Prophesee GEN1 Automotive Detection dataset.</p><p>1) Prophesee NCARS dataset: The Prophesee NCARS dataset <ref type="bibr" target="#b27">[28]</ref> is a classification composed of 24k samples of length 100ms captured with a Prophesee GEN1 event camera mounted behind the windshield of a moving car. The samples represent either a car or background. Samples have variable size as they are cropped from recordings of resolution 304 ? 240 pixels.</p><p>2) Prophesee GEN1 Detection dataset: Composed of 39 hours of recording, the Prophesee GEN1 Automotive Detection dataset <ref type="bibr" target="#b28">[29]</ref> is the largest event-based dataset to date. Recorded with a Prophesee GEN1 sensor mounted on a car dashboard, it contains over 255k manually annotated bounding boxes of two classes: cars and pedestrians.</p><p>3) Prophesee GEN1 Classification dataset: We generated a classification dataset from the Prophesee GEN1 Detection dataset by cropping each bounding box (car or pedestrian) form individual samples. As it is the case for NCARS, each sample represents 100ms of events preceding the annotated bounding box. The main difference between this dataset and NCARS lies in the presence of a pedestrian class. Indeed, we believe that the features learned by a network are more relevant if it is trained to classify two different classes rather than one class vs. background.</p><p>To avoid imbalance in the number of samples for each class, we rebalanced the training set: we undersampled the cars and oversampled the pedestrians by doing horizontal flip data augmentation. The code used to generate this classification dataset will be available online. B. Implementation details 1) Voxel Cubes : For classification, we used samples of 100ms encoded as binary voxel cubes. For object detection, we chose to train our network only on the 100ms preceding annotated bounding boxes. Thus, our SNN make predictions based solely on these 100ms, the potentials being reset after each sample. For both tasks, the samples were encoded as binary voxel cubes of 5 timesteps and 2 micro time bins, as it represented the best compromise between performance and number of operations.</p><p>2) Hyperparameters: All classification models were trained using the AdamW optimizer with a 1e ?4 weight decay and an initial learning rate of 5e ?3 , except for the VGG models that used an initial learning rate of 5e ?4 . The object detection models used an initial learning rate of 1e ?3 . The models were trained with a batch size of 64 over 10 epochs for classification and 200 epochs for object detection, using a cosine annealing learning rate scheduler that gradually decrease the learning rate towards 0. All convolutions were initialized using the Kaiming uniform method, and all batch normalization layers were initialized with a weight of 1 and a bias of 0. The Parametric LIF neurons all had an initial membrane time constant ? of 2, a membrane threshold of 1 and the ATan function as the surrogate function. Norm of the gradient values were clipped to a maximum of 1 to avoid exploding gradients. All presented results represent the best ones among 5 runs.</p><p>All trainings were done with the SpikingJelly framework [15] using 16-bit automatic mixed precision, running on a 48-GB NVidia RTX A6000 and a 104-threads Intel Xeon Gold 6230R.</p><p>3) Performance metrics: For classification, we measure the performance by using an accuracy metric. For object detection, we report COCO mAP <ref type="bibr" target="#b29">[30]</ref>, the mean Average Precision over 10 IoU ([.50:.05:.95]), as it is widely used for evaluating object detection models.</p><p>But assessing the performance of spiking neural networks is not limited to that, as multiple others features are needed to take advantage of their benefits when embedded in specialized hardware. For all our results we report the following metrics:</p><p>? Number of parameters: embedded systems have high constraints in term of memory, therefore it is important to design networks with a low number of parameters. ? ACCs: spiking neural networks do not require multiplicative operations, enabling substantial energy savings on specialized hardware. Thus, we chose to report the number of operations of our SNNs by using the number of accumulations operations (ACCs), to accentuate the potential energy savings. Indeed, all spiking convolutions operations amount to ACCs, and each PLIF neuron only requires 1 ACC per timestep to update their potential. We did not count the ACCs in the batch normalization layers as they can be fused with the convolutional layers. ? Sparsity: finally, we measured the number of spikes emitted after each activation layer to represent the global sparsity of the network compared to an fully dense equivalent DNN. Indeed, processing events with SNNs preserves the data sparsity. On specialized hardware, computations are only performed when there are spikes, therefore an highly sparse network would consume less power than its dense counterpart. The sparsity is obtained by averaging the number of spikes over the whole test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Automotive Object Classification</head><p>For both classification datasets, the samples were resized to 64?64 pixels using nearest-neighbor interpolation to keep the input events binary.</p><p>We present the best accuracies obtained by our SNNs on the Prophesee NCARS dataset compared with other state-ofthe-art models in <ref type="table" target="#tab_0">Table I</ref>. All of our models beat previous results for spiking neural networks and compete with the best neural networks in the literature. The spiking SqueezeNet, our smallest SNN, struggles to reach over 80% test accuracy while spiking DenseNets, MobileNets and VGG are all capable to exceed 90% test accuracy on NCARS.  <ref type="table" target="#tab_0">Table II</ref> provides extensive results of all our spiking neural networks on both automotive classification datasets. Spiking SqueezeNet models, while having a very low number of parameters and number of ACCs per timestep, are not competitive with other architectures for the NCARS and the GEN1 classification datasets. Our spiking VGG models provide the best accuracies for both datasets, while maintaining a relatively low number of ACCs per timestep. But these architectures have an high number of parameters, making it difficult to embed them. Spiking MobileNets reach high accuracies but require high numbers of parameters and ACCs per timestep, penalized by the replacement of their depthwise seperable convolutions by normal convolutions. However, they are the only models for which the accuracy increases as the model gets bigger. Finally, spiking DenseNets reach competitive accuracies while requiring a low number of parameters and a moderate amount of ACCs per timestep. Using the densely connected layers of DenseNets, surrogate gradient method has no trouble to learn across 100+ spiking layers. The accuracy decreases however when the growth factor and the number of layers are both high, but we believe that better results could be achieved on these big networks with longer trainings.</p><p>For SNNs to be truly efficient, they require both low sparsity and a low number of timesteps. All of our spiking neural networks have a sparsity inferior to 40% of both datasets. As these SNNs operate on 5 timesteps, this means that they require at most twice the number of operations of an equivalent dense ANN. The operations would however consume less power on a specialized hardware, as they are simple ACCs. These sparsity results could further be improved by adding a regularization term to the loss, constraining the number of spikes emitted, as it was done in <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Automotive Object Detection</head><p>For object detection models, we used as backbones the best variants: VGG-11, MobileNet-64, DenseNet121-24. We  <ref type="bibr" target="#b19">[20]</ref>. The spiking backbones were pretrained on the NCARS dataset. Our results are presented in <ref type="table" target="#tab_0">Table III</ref>. Our spiking models achieve competitive mAP with a small number of parameters and ACCs per timestep. We reach 0.19 COCO mAP with our DenseNet121-24 + SSD model, with only 8.2M parameters and 2.33G ACCs per timestep. Our spiking models outperform a traditional neural network with over 5 times more parameters. The three models show relatively similar performance, proving that spiking backbones are able to provide meaningful spike feature maps to do object detection on real-world event data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Influence of the number of timesteps and micro time bins</head><p>Spiking neural networks are recurrent neural network operating on a fixed number of timesteps T . Encoding event data in a representation that keep their temporal information can thus benefit SNNs. In section IV-B1, we presented an event data encoding called voxel cubes, that preserve the temporal information of events while minimizing the number of timesteps. Indeed, the number of computations performed  by SNNs increases linearly with the number of timesteps, it is thus important to keep it small. <ref type="figure" target="#fig_2">Fig. 3</ref> plots the NCAR accuracy of our SNNs on selected combinations of number of timesteps and micro time bins. The number of timesteps remains the most important parameter for obtaining good accuracies with SNNs. Indeed, the best results are almost always achieved with 10 timesteps, the maximum value we tested. Results obtained with 1 timestep are significantly worse, even with a high number of micro time bins, proving if necessary that SNNs need to operate on several timesteps to be performant. Increasing the number of micro time bins do not always improve the results, even if it seems to help when the number of timesteps is low. We notice that our spiking SqueezeNet is unable to learn with 1 or 2 timesteps, indicating a strong dependence of this network on the temporality of the data.</p><p>Undoubtedly, these results depend on the samples duration and the data temporality. In our case, for our samples lasting 100ms, the best compromise between number of timesteps, number of micro time bins and accuracy seems to be 5 timesteps and 2 micro time bins, which is the encoding format we used for all models except SqueezeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Influence of Batch Normalization and PLIF neurons</head><p>Batch Normalization layers seem to be vital to the training of complex SNNs: as we can in <ref type="table" target="#tab_0">Table IV</ref>, removing them (while adding a bias term to the convolutions) makes our networks significantly less performant or they do not learn at all. More surprisingly, the placement of the batch normalization layers seems to also play a part in the efficiency of our networks, as placing batch normalization layers before the convolutions produce better results than placing them after. In DNNs, placing batch normalization layers before or after convolutions do not make a significant difference, as both placements provide benefits in training speed and convergence.</p><p>In our case, we believe that batch normalization layers placed before convolutions are effective in SNNs because they transform highly sparse feature maps of spikes into a dense decimal representation. As a result, the weights learned by convolutions are updated through backpropagation whether they have received spikes or not. Without batch normalization, only the weights receiving spikes would have been updated meaningfully, leading to slower convergence.</p><p>We restate that batch normalization can be used when training of SNNs because their parameters can be fused with the parameters of the subsequent convolutions. In light of this, we believe our findings on the placement of batch normalization layers in convolutional SNNs can make a difference in the training of larger and more complex SNNs.</p><p>On the other hand, PLIF neurons introduced in [7] also help during the training of our large spiking neural networks. Replacing them with simple LIF neurons (with a time constant ? of 2) leads to poorer accuracies for all networks. Their presence is not as important as batch normalization layers but the SNNs seem to benefit from learning different time constants for each layer. It also reduces the number of hyperparameters to be tuned, so we can only encourage their use for the training of SNNs on event data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Influence of depthwise separable convolutions</head><p>We used depthwise separable convolutions during the training of our spiking MobileNets for two reasons: the smaller number of parameters made the trainings faster and the larger networks were able attain better accuracies. As we can see in <ref type="table" target="#tab_4">Table V</ref>, the 32 and 64 input channels variants of spiking Mo-bileNets using depthwise separable convolutions reach higher accuracies than their normal convolution counterparts.</p><p>We presume that the higher number of parameters induced by the normal convolutions makes the training of the spiking neural networks with surrogate gradient more difficult. As it is the case for the spiking VGGs, the accuracy actually drop when adding parameters to the normal convolutions MobileNets, while it increases for the MobileNets trained with depthwise seperable convolutions.</p><p>However, the smaller variant seems to benefit from the added parameters as the accuracy is 5% higher with normal convolutions than with depthwise separable convolutions. We therefore recommend to use depthwise separable convolutions in SNNs only as a second resort, when the accuracy achieved with normal convolutions decreases as the networks get larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORKS</head><p>We designed trained four different spiking neural networks models based on SqueezeNet, VGG, MobileNet and DenseNet, setting new state-of-the-art results on two automotive classification event datasets for spiking neural networks. We then used these networks combined to SSD bounding box regression heads to design the first spiking neural networks capable of doing object detection on the real-world event dataset Prophesee GEN1, achieving 0.19mAP with less than 10M parameters. All our SNNs are performant without requiring an high number of timesteps thanks to our voxel cube event encoding.</p><p>These results highlight the rapid progression of spiking neural networks in the last few years. For a long time restricted to small datasets, spiking neural networks now show their strengths when trained directly on temporal data. Future works would include the implementation of these spiking neural networks on a low-power neuromorphic hardware <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which will enable power-efficient embedded applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Voxel cube encoding. For a given number of timesteps, voxel cubes better preserve the temporal information of events by exploiting the channel dimension. Here, the voxel cube illustrated uses 2 micro time bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Our spiking DenseNet + SSD architecture for object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Influence of the number of timesteps and micro time bins on NCARS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH STATE-OF-THE-ART MODELS ON PROPHESEE NCARS</figDesc><table><row><cell>Methods</cell><cell>Representation</cell><cell>Network</cell><cell>NCARS acc</cell></row><row><cell>HATS [28]</cell><cell>TimeSurface</cell><cell>N/A</cell><cell>0.902</cell></row><row><cell>Gabor-SNN [28]</cell><cell>Spike</cell><cell>SNN</cell><cell>0.789</cell></row><row><cell>HybridSNN [21]</cell><cell>VoxelGrid</cell><cell>SNN</cell><cell>0.77</cell></row><row><cell>HybridSNN [21]</cell><cell>VoxelGrid</cell><cell>SNN-CNN</cell><cell>0.906</cell></row><row><cell>YOLE [18]</cell><cell>VoxelGrid</cell><cell>CNN</cell><cell>0.927</cell></row><row><cell>Asynet [31]</cell><cell>VoxelGrid</cell><cell>CNN</cell><cell>0.944</cell></row><row><cell>EvS-S [32]</cell><cell>Graph</cell><cell>GNN</cell><cell>0.931</cell></row><row><cell>SqueezeNet 1.1</cell><cell>VoxelCube</cell><cell>SNN</cell><cell>0.845</cell></row><row><cell>VGG-11</cell><cell>VoxelCube</cell><cell>SNN</cell><cell>0.924</cell></row><row><cell>MobileNet-64</cell><cell>VoxelCube</cell><cell>SNN</cell><cell>0.917</cell></row><row><cell>DenseNet169-16</cell><cell>VoxelCube</cell><cell>SNN</cell><cell>0.904</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>BETWEEN OUR SPIKING MODELS ON AUTOMOTIVE CLASSIFICATION</figDesc><table><row><cell>Models</cell><cell cols="2">#Params ACCs/ts</cell><cell cols="2">NCARS Accuracy ? Sparsity ?</cell><cell cols="2">GEN1 Classification Accuracy ? Sparsity ?</cell></row><row><cell>SqueezeNet 1.0</cell><cell>0.74M</cell><cell>0.05G</cell><cell>0.731</cell><cell>31.26%</cell><cell>0.627</cell><cell>6.65%</cell></row><row><cell>SqueezeNet 1.1</cell><cell>0.72M</cell><cell>0.02G</cell><cell>0.846</cell><cell>25.13%</cell><cell>0.674</cell><cell>6.79%</cell></row><row><cell>VGG-11</cell><cell>9.23M</cell><cell>0.61G</cell><cell>0.924</cell><cell>12.04%</cell><cell>0.969</cell><cell>14.69%</cell></row><row><cell>VGG-13</cell><cell>9.41M</cell><cell>0.92G</cell><cell>0.910</cell><cell>14.53%</cell><cell>0.970</cell><cell>19.03%</cell></row><row><cell>VGG-16</cell><cell>14.72M</cell><cell>1.26G</cell><cell>0.905</cell><cell>14.91%</cell><cell>0.977</cell><cell>18.79%</cell></row><row><cell>MobileNet-16</cell><cell>1.18M</cell><cell>0.27G</cell><cell>0.842</cell><cell>17.57%</cell><cell>0.949</cell><cell>15.15%</cell></row><row><cell>MobileNet-32</cell><cell>7.41M</cell><cell>1.06G</cell><cell>0.902</cell><cell>18.53%</cell><cell>0.955</cell><cell>14.37%</cell></row><row><cell>MobileNet-64</cell><cell>18.81M</cell><cell>4.20G</cell><cell>0.917</cell><cell>17.14%</cell><cell>0.966</cell><cell>30.60%</cell></row><row><cell>DenseNet121-16</cell><cell>1.76M</cell><cell>1.01G</cell><cell>0.889</cell><cell>27.99%</cell><cell>0.970</cell><cell>20.31%</cell></row><row><cell>DenseNet169-16</cell><cell>3.16M</cell><cell>1.19G</cell><cell>0.893</cell><cell>30.12%</cell><cell>0.969</cell><cell>23.12%</cell></row><row><cell>DenseNet121-24</cell><cell>3.93M</cell><cell>2.25G</cell><cell>0.904</cell><cell>33.59%</cell><cell>0.975</cell><cell>27.26%</cell></row><row><cell>DenseNet169-24</cell><cell>7.05M</cell><cell>2.66G</cell><cell>0.879</cell><cell>34.02%</cell><cell>0.962</cell><cell>28.29%</cell></row><row><cell>DenseNet121-32</cell><cell>6.95M</cell><cell>3.98G</cell><cell>0.898</cell><cell>38.32%</cell><cell>0.966</cell><cell>29.46%</cell></row><row><cell>DenseNet169-32</cell><cell>12.48M</cell><cell>4.72G</cell><cell>0.825</cell><cell>37.48%</cell><cell>0.967</cell><cell>40.35%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="5">COMPARISON WITH STATE-OF-THE-ART MODELS ON PROPHESEE GEN1</cell></row><row><cell>Methods</cell><cell cols="4">#Params ACCs/ts Sparsity ? mAP ?</cell></row><row><cell>Asynet [31]</cell><cell>133M</cell><cell>-</cell><cell>-</cell><cell>0.15</cell></row><row><cell>MatrixLSTM [19]</cell><cell>65M</cell><cell>-</cell><cell>-</cell><cell>0.31</cell></row><row><cell>RED [20]</cell><cell>24M</cell><cell>-</cell><cell>-</cell><cell>0.40</cell></row><row><cell>VGG-11+SDD</cell><cell>12.64M</cell><cell>11.07G</cell><cell>22.22%</cell><cell>0.174</cell></row><row><cell>MobileNet-64+SSD</cell><cell>24.26M</cell><cell>4.34G</cell><cell>29.44%</cell><cell>0.147</cell></row><row><cell>DenseNet121-24+SSD</cell><cell>8.2M</cell><cell>2.33G</cell><cell>37.20%</cell><cell>0.189</cell></row><row><cell cols="5">evaluated our results on the Prophesee GEN1 Detection test</cell></row><row><cell cols="5">set after having filtered boxes with diagonal smaller than 30</cell></row><row><cell>pixels as it is done in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV INFLUENCE</head><label>IV</label><figDesc>OF BATCH NORMALIZATION AND PLIF NEURONS WHEN TRAINING SNNS ON NCARS.</figDesc><table><row><cell>Models</cell><cell cols="4">Accuracy ? Normal Post-BN No BN No PLIF</cell></row><row><cell>SqueezeNet 1.1</cell><cell>0.846</cell><cell>0.511</cell><cell>0.500</cell><cell>0.560</cell></row><row><cell>VGG-11</cell><cell>0.924</cell><cell>0.771</cell><cell>0.550</cell><cell>0.889</cell></row><row><cell>MobileNet-64</cell><cell>0.917</cell><cell>0.857</cell><cell>0.654</cell><cell>0.701</cell></row><row><cell>DenseNet121-24</cell><cell>0.904</cell><cell>0.839</cell><cell>0.611</cell><cell>0.836</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V INFLUENCE</head><label>V</label><figDesc>OF DEPTHWISE SEPARABLE CONVOLUTIONS WHEN TRAINING SPIKING MOBILENETS ON NCARS</figDesc><table><row><cell>Models</cell><cell cols="2">Acc (dw sp conv) Acc (normal conv)</cell></row><row><cell>MobileNet-16</cell><cell>0.842</cell><cell>0.906</cell></row><row><cell>MobileNet-32</cell><cell>0.902</cell><cell>0.898</cell></row><row><cell>MobileNet-64</cell><cell>0.917</cell><cell>0.807</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This material is based upon work supported by the French technological research agency (ANRT) through a CIFRE thesis in collaboration with Renault.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Confronting machinelearning with neuroscience for neuromorphic architectures design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abderrahmane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Efficient neuromorphic signal processing with loihi 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03746</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event-based vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HOTS: a hierarchy of event-based time-surfaces for pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1346" to="1359" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Event preprocessing tutorial -event cube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prophesee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simultaneous optical flow and intensity estimation from an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bardow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating learnable membrane time constant to enhance learning of spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from event cameras with sparse spiking convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cordone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spikenet: A simulator for modeling large networks of integrate and fire neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gautrais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Rullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="989" to="996" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spike-timing dependent plasticity and mutual information maximization for a spiking neuron model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toyoizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning event-based spatio-temporal feature descriptors via local synaptic plasticity: a biologically-realistic perspective of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Safa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bourdoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ocket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Catthoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gielen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00791</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SLAYER: spike layer error reassignment in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="51" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nengo: a python tool for building large-scale functional brain models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bekolay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<ptr target="https://github.com/fangwei123456/spikingjelly" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssd: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asynchronous convolutional networks for object detection in neuromorphic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Event-based Vision Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A differentiable recurrent surface for asynchronous event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to detect objects with a 1 megapixel event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Tournemire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hybrid snn-ann: energyefficient classification and object detection for event-based vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kugele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfeil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chicca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Event-Driven Categorization Model for AER Image Sensors Using Multispike Encoding and Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3649" to="3657" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: alexnet-level accuracy with 50x fewer parameters and &lt;0.5mn model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mobilenets: efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girschick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hats: histograms of averaged time surfaces for robust event-based object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A large scale event-based detection dataset for automotive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Tournemire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Migliore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08499</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Eventbased asynchronous sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Messikommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph-based asynchronous event processing for rapid object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Low-activity supervised convolutional spiking neural networks applied to speech commands recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design space exploration of hardware spiking neurons for embedded artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abderrahmane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="366" to="386" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
