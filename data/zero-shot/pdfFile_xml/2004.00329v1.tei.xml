<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Lanzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Alletto</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Panasonic R&amp;D Company of America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a novel approach for bottomup multi-person 3D human pose estimation from monocular RGB images. We propose to use high resolution volumetric heatmaps to model joint locations, devising a simple and effective compression method to drastically reduce the size of this representation. At the core of the proposed method lies our Volumetric Heatmap Autoencoder, a fully-convolutional network tasked with the compression of ground-truth heatmaps into a dense intermediate representation. A second model, the Code Predictor, is then trained to predict these codes, which can be decompressed at test time to re-obtain the original representation. Our experimental evaluation shows that our method performs favorably when compared to state of the art on both multi-person and single-person 3D human pose estimation datasets and, thanks to our novel compression strategy, can process full-HD images at the constant runtime of 8 fps regardless of the number of subjects in the scene. Code and models available at https://github.com/fabbrimatteo/LoCO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human Pose Estimation (HPE) has seen significant progress in recent years, mainly thanks to deep Convolutional Neural Networks (CNNs). Best performing methods on 2D HPE are all leveraging heatmaps to predict body joint locations <ref type="bibr">[3,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b43">43]</ref>. Heatmaps have also been extended for 3D HPE, showing promising results in single person contexts <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b41">41]</ref>.</p><p>Despite their good performance, these methods do not easily generalize to multi-person 3D HPE, mainly because of their high demands for memory and computation. This drawback also limits the resolution of those maps, that have to be kept small, leading to quantization errors. Using larger volumetric heatmaps can address those issues, but at the cost of extra storage, computation and training complexity. In this paper, we propose a simple solution to the aforementioned problems that allows us to directly predict highresolution volumetric heatmaps while keeping storage and computation small. This new solution enables our method to tackle multi-person 3D HPE using heatmaps in a singleshot bottom-up fashion. Moreover, thanks to our highresolution output, we are able to produce fine-grained absolute 3D predictions even in single person contexts. This allows our method to achieve state of the art performance on the most popular single person benchmark <ref type="bibr" target="#b10">[11]</ref>.</p><p>The core of our proposal relies on the creation of an alternative ground-truth representation that preserves the most informative content of the original ground-truth but reduces its memory footprint. Indeed, this new compressed representation is used as the target ground-truth during our network training. We named this solution LoCO, Learning on Compressed Output.</p><p>By leveraging on the analogy between compression and dimensionality reduction on sparse signals <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr">1]</ref>, we empirically follow the intuition that 3D body poses can be represented in an alternative space where data redundancy is exploited towards a compact representation. This is done by minimizing the loss of information while keeping the spatial nature of the representation, a task for which convolutional architectures are particularly suitable. Concurrently w.r.t. our proposal, compression-based approaches have been effectively used for both dataset distillation and input compression <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b46">46]</ref> but, to the best of our knowledge, this is the first time they are applied to ground truth remapping. For this purpose, deep self-supervised networks such as autoencoders represent a natural choice for searching, in a data-driven way, for an intermediate representation.</p><p>Specifically, our HPE pipeline consists of two modules: at first, the pretrained Volumetric Heatmap Autoencoder is used to obtain a smaller/denser representation of the volumetric heatmaps. These "codes" are then used to supervise the Code Predictor, which aims at estimating multiple 3D joint locations from a monocular RGB input.</p><p>To summarize, the novel aspects of our proposal are:</p><p>? We propose a simple and effective method that maps high-resolution volumetric heatmaps to a compact and more tractable representation. This saves memory and computational resources while keeping most of the informative content.</p><p>? This new data representation enables the adoption of volumetric heatmaps to tackle multi-person 3D HPE in a bottom-up fashion, an otherwise intractable problem. Experiments on both real <ref type="bibr" target="#b11">[12]</ref> and simulated environments <ref type="bibr" target="#b7">[8]</ref> (see <ref type="figure" target="#fig_0">Fig. 1</ref>) show promising results even in 100 meters wide scenes with more than 50 people. Our method only requires a single forward pass and can be applied with constant running time regardless of the number of subjects in the scene.</p><p>? We further demonstrate the generalization capabilities of LoCO by applying it to a single person context. Our fine-grained predictions establish a new state of the art on Human3.6m <ref type="bibr" target="#b10">[11]</ref> among bottom-up methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-Person 3D HPE Single person 3D HPE from a monocular camera has become extremely popular in the last few years. Literature can be classified into three different categories: (i) approaches that first estimate 2D joints and then project them to 3D space, (ii) works that jointly estimate 2D and 3D poses, (iii) methods that learn the 3D pose directly from the RGB image. The majority of works on single person 3D HPE first compute 2D poses and leverages them to estimate 3D poses, either using off-the-shelf 2D HPE methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">4]</ref> or by having a dedicated module in the 3D HPE pipeline <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>Joint learning of 2D and 3D pose is also shown to be beneficial <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">30]</ref>, often in conjunction with large-scale datasets that only provide 2D pose groundtruth and exploiting anatomical or structure priors.</p><p>Finally, recent works estimate 3D pose information directly <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref>. Among these, Pavlakos et al. <ref type="bibr" target="#b29">[29]</ref> were the first to propose a fine discretization of the 3D space around the target by learning a coarse-to-fine prediction scheme in an end to end fashion.</p><p>Multi-Person 3D HPE To the best of our knowledge, very few works tackle multi-person 3D HPE from monocular images. We can categorize them into two classes: topdown and bottom-up approaches.</p><p>Top-down methods first identify bounding boxes likely to contain a person using third party detectors and then perform single-person HPE for each person detected. Among them, Rogez et al. <ref type="bibr" target="#b37">[37]</ref> classifies bounding boxes into a set of K-poses. These poses are scored by a classifier and refined using a regressor. The method implicitly reasons using bounding boxes and produces multiple proposals per subject that need to be accumulated and fused. Zanfir et al. <ref type="bibr" target="#b52">[52]</ref> combine a single person model that incorporates feed-forward initialization and semantic feedback, with additional constraints such as ground plane estimation, mutual volume exclusion, and joint inference. Dabral et al. <ref type="bibr" target="#b5">[6]</ref>, instead, propose a two-staged approach that first estimates the 2D keypoints in every Region of Interest and then lifts the estimated keypoints to 3D. Finally, Moon et al. <ref type="bibr" target="#b22">[23]</ref> predict absolute 3D human root localization, and root-relative 3D single-person for each person independently. However, these methods heavily rely on the accuracy of the people detector and do not scale well when facing scenes with dozens of people.</p><p>In contrast to top-down approaches, bottom-up methods produce multi-person joint locations in a single shot, from which the 3D pose can be inferred even under strong occlusions. Mehta et al. <ref type="bibr" target="#b20">[21]</ref>, predict 2D and 3D poses for all subjects in a single forward pass regardless of the number of people in the scene. They exploit occlusion-robust posemaps that store 3D coordinates at each joint 2D pixel location. However, their 3D pose read-out strategy strongly depends on the 2D pose output which makes it limited by the accuracy of the 2D module. Their method also struggles to resolve scenes with multiple overlapping people, due to the missing 3D reasoning in their joint-to-person association process. Zanfir et al. <ref type="bibr" target="#b53">[53]</ref>, on the other hand, utilize a multitask deep neural network where the person grouping problem is formulated as an integer program based on learned body part scores parameterized by both 2D and 3D information. Similarly to the latter, our method directly learns a Multi-Person 3D Pose Representation In a top-down framework, the simplest 3D pose representation can be expressed by a vector of joints. By casting 3D HPE as a coordinate regression task, Rogez et al. <ref type="bibr" target="#b37">[37]</ref> and Zanfir et al. <ref type="bibr" target="#b52">[52]</ref> indeed utilize x, y, z coordinates of the human joints w.r.t. a known root location. On the other hand, bottomup approaches require a representation whose coding does not depend on the number of people (e.g. an image map). Among the most recent methods, Mehta et al. <ref type="bibr" target="#b20">[21]</ref> and Zanfir et al. <ref type="bibr" target="#b53">[53]</ref> both utilize a pose representation composed by joint-specific feature channels storing the 3D coordinate x, y, or z at the joint/limb 2D pixel location. This representation, however, suffers when multiple overlapping people are present in the scene. In contrast to all these approaches, we adopted the volumetric heatmap representation proposed by Pavlakos et al. <ref type="bibr" target="#b29">[29]</ref>, overcoming all the limitations that arise when facing a multi-person context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The following subsections summarize the key elements of LoCO. Section 3.1 gives a preliminary definition of the chosen volumetric heatmap representation and elaborates on its merits. Section 3.2 illustrates our proposed data mapping which addresses the high dimensional nature of the volumetric heatmaps by producing a compact and more tractable representation. Next, in Section 3.3, we describe how our strategy can be easily exploited to effectively tackle the problem of multi-person 3D HPE in a single-shot bottom-up fashion. Finally, Section 3.4 illustrates our simple refining approach that prevents poses from being implausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Volumetric Heatmaps</head><p>By considering a voxelization of the RGB-D volumetric space <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">29]</ref>, we refer as a volumetric heatmap, h, the 3D confidence map with size D ? H ? W , where D represents the depth dimension (appropriately quantized), while H and W represent the height and width of the image plane respectively. Given the body joint j with pseudo-3D coordinates u j = (u 1,j , u 2,j , u 3,j ), where u 1,j ? {1, ..., D} is the quantized distance of joint j from the camera, and u 2,j ? {1, ..., H} and u 3,j ? {1, ..., W } are respectively the row and column indexes of its pixel on the image plane, the value of h j at a generic location u is obtained by centering a fixed variance Gaussian in u j :</p><formula xml:id="formula_0">h j (u) = e ? u?u j 2 ? 2<label>(1)</label></formula><p>In a multi-person context, in the same image we can simultaneously have several joints of the same kind (e.g. "left ankle"), one for each of the K different people in the image. In this case we aggregate those K volumetric heatmaps h j (k) , into a single heatmap h j with a max operation:</p><formula xml:id="formula_1">h j (u) = max k {h j (k) (u)}<label>(2)</label></formula><p>Finally, considering N different types of joint and K block layer in ch. out ch. stride <ref type="table" target="#tab_6">Table 1</ref>: Structure of the encoder part of the Volumetric Heatmap Autoencoder (VHA). The decoder is not shown as it is perfectly mirrored to the encoder. VHAv1:</p><formula xml:id="formula_2">e-c2d Conv2D + ReLU D D/d 1 s 1 Conv2D + ReLU D/d 1 D/d 2 s 2 Conv2D + ReLU D/d 2 D/d 3 s 2 e-c3d Conv3D + ReLU N 4 1 Conv3D + ReLU 4 1 1</formula><formula xml:id="formula_3">(d 1 , d 2 , d 3 ) = (1, 2, 2) and (s 1 , s 2 , s 3 ) = (1, 2, 1); for VHAv2: (d 1 , d 2 , d 3 ) = (2, 4, 4) and (s 1 , s 2 , s 3 ) = (2, 2, 1); VHAv3: (d 1 , d 2 , d 3 ) = (2, 4, 8) and (s 1 , s 2 , s 3 ) = (2, 2, 2)</formula><p>people, we have a set of N volumetric heatmaps (each associated with a joint type), H = {h j , j = 1, ..., N }, resulting from the aggregation of the individual heatmaps of the K people in the scene. Note that, given pseudo-3D coordinates u = (u 1 , u 2 , u 3 ) and the camera intrinsic parameters, i.e. focal length f = (f x , f y ) and principal point (c x , c y ), the corresponding 3D coordinates x = (x, y, z) in the camera reference system can be retrieved by directly applying the equations of the pinhole camera model. The benefit of choosing a volumetric heatmap representation over a direct 3D coordinate regression is that it casts the highly non-linear problem to a more tractable configuration of prediction in a discretized space. In fact, joint predictions do not estimate a unique location but rather a per voxel confidence, which makes it easier for a network to learn the target function <ref type="bibr" target="#b29">[29]</ref>. In the context of 2D HPE, the benefits of predicting confidences for each pixel instead of image coordinates are well known <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b45">45]</ref>. Moreover, in a multi-person environment, directly regressing the joint coordinates is unfeasible when the number of people is not known a priori, making volumetric heatmaps a natural choice for tackling bottom-up multi-person 3D HPE.</p><p>The major disadvantage of this representation is that it is memory and computational demanding, requiring some compromise during implementation that limits its full potential. Some of those compromises consist in utilizing low resolution heatmaps that introduce quantization errors or complex training strategies that involve coarse-to-fine predictions through iterative refining of network output <ref type="bibr" target="#b29">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Volumetric Heatmap Autoencoder</head><p>To overcome the aforementioned limitations without introducing quantization errors or training complexity, we propose to map volumetric heatmaps to a more tractable representation. Inspired by <ref type="bibr" target="#b16">[17]</ref>, we propose a multiple branches Volumetric Heatmap Autoencoder (VHA) that takes a set of N volumetric heatmaps H as input. At first, the volumetric heatmaps {h 1 , ..., h N } are processed independently with a 2D convolutional block (e-c2d) in which the kernel does not move along the D dimension. In order to capture the mutual influence between joints locations, the obtained maps are then stacked along a fourth dimension and processed by a subsequent set of 3D convolutions (e-c3d). The resulting encoded representation, e(H) is finally decoded by its mirrored architecture d (e (H)) =H. The general structure of the model is outlined in <ref type="figure" target="#fig_1">Fig. 2</ref> top.</p><p>The goal of the VHA is therefore to learn a compressed representation of the input volumetric heatmaps that preserve their information content, which results in the preservation of the position of the Gaussian peaks of the various joints in the original maps. For the purpose, we maximize the F1-score, F1 Q H , QH , between the set of ground truth peaks (Q H ) and the set of the decoded maps (QH). We define the set of peaks as follows:</p><formula xml:id="formula_4">Q H = n=1,...,N {u : h n (u) &gt; u ?u ? N?} (3)</formula><p>where N? is the 6-connected neighborhood of?, i.e. the set of coordinates N? = {u : u ?? = 1} at unit distance from?. Since the procedure for extracting the coordinate sets from the volumetric heatmaps is not differentiable, the former objective cannot be directly optimized as a loss component for training the VHA. To address this issue, we propose to use mean squared error (MSE) loss between H andH as training loss.</p><p>Note that our proposed mapping purposely reduces the volumetric heatmap's fourth dimension, making its shape coherent with the output of 2D convolutions and thus exploitable by regular CNN backbones. Additional architecture details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Code Predictor and Body Joints Association</head><p>The input of the Code Predictor is represented by a RGB image, I, while its output, f (I), aims to predict the codes obtained with the VHA, <ref type="figure" target="#fig_1">Fig. 2</ref>. The architecture, <ref type="figure" target="#fig_1">Fig. 2</ref> bottom, is inspired by <ref type="bibr" target="#b49">[49]</ref> thus composed by a pre-trained feature extractor (convolutional part of Inception v3 <ref type="bibr" target="#b42">[42]</ref>), and a fully convolutional block (f -c2d) composed of four convolutions. We trained the Code Predictor by minimizing the MSE loss between f (I) and e (H), where H is the volumetric heatmap associated with the image I.</p><p>At inference time, the pseudo-3D coordinates of the body joints are obtained from the decoded volumetric heatmapH = d(f (I)) through a local maxima search. Eventually, if camera parameters are available, the pinhole camera equations recover the true three-dimensional coordinates of the detected joints. Additional details in the supplementary material. F1 on JTA F1 on Panoptic F1 on Human3.6m model bottleneck size @0vx @1vx @2vx @0vx @1vx @2vx @0vx @1vx @2vx  <ref type="table">Table 2</ref>: VHA bottleneck/code size and performances on the JTA, Panoptic and Human3.6m (protocol P2) test set in terms of F1 score at different thresholds @0, @1, and @2 voxel(s); @t indicates that a predicted joint is considered true positive if the distance from the corresponding ground truth joint is less than t</p><formula xml:id="formula_5">VHA (1) D 2 ? H 2 ? W<label>2</label></formula><p>As in almost all recent 2D HPE bottom-up approaches <ref type="bibr">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref> (i.e. methods which does not require a people detection step) detected joints have to be linked together to obtain people skeletal representations. In a single person context, joint association is trivial. On the other hand, in a multi-person environment, linking joints is significantly more challenging. For the purpose, we rely on a simple distance-based heuristic where, starting from detected heads (i.e. the joint with the highest confidence), we connect the remaining (N ? 1) joints by selecting the closest ones in terms of 3D Euclidean distance. Associations are further refined by rejecting those that violates anatomical constraints (e.g. length of a limb greater than a certain threshold). Despite its simplicity, this approach is particularity effective when 3D coordinates of body joints are available, especially in surveillance scenarios where proxemics dynamics often regulate the spatial relationships between different individuals. Additional details are reported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pose Refiner</head><p>The predicted 3D poses are subsequently refined by a MLP network trained to account for miss-detections and location errors. The objective of the Pose Refiner is indeed to make sure that the detected poses are complete (i.e. all the N joints are always present). To better understand how the Pose Refiner works, we define the concept of 3D poses and root-relative poses. Given a person k, its 3D pose is the set</p><formula xml:id="formula_6">p (k) = x (k)</formula><p>n , n = 1, ..., N of the 3D coordinates of its N joints. The corresponding root-relative pose is then given by:</p><formula xml:id="formula_7">p rr (k) = x (k) n ? x (k) 1 l n , n = 2, ..., N<label>(4)</label></formula><p>where x 1 are the 3D coordinates of the root joint ("headtop" in our experiments) and l n is a normalization constant computed on the training set as the maximum length of the vector that points from the root joint to any other joint of the same person.</p><p>The Pose Refiner is hence trained with MSE loss taking as input the root-relative version of the 3D poses with randomly removed joints, and an additional Gaussian noise applied to the coordinates. Given the 3D position of the root joint and the refined poses, it is straightforward to re-obtain the corresponding 3D poses by using Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>A series of experiments have been conducted on two multiperson datasets, namely JTA <ref type="bibr" target="#b7">[8]</ref> and CMU Panoptic <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b12">13]</ref>, as well as one well established single-person benchmark: Human3.6m <ref type="bibr" target="#b10">[11]</ref>.</p><p>JTA is a large synthetic dataset for multi-person HPE and tracking in urban scenarios. It is composed of 512 Full HD videos, 30s long, each containing an average of 20 people per frame. Due to its recent publication date, this dataset does not have a public leaderboard and it is not mentioned in other comparable HPE works. Despite this limitation, we believe it is crucial to test LoCO on JTA because it is much more complex and challenging than older benchmarks. CMU Panoptic is another large dataset containing both single-person and multi-person sequences for a total of 65 sequences (5.5 hours of video). It is less challenging than JTA as the number of people per frame is much more limited, but it is currently the largest real-world multi-person dataset with 3D annotations.</p><p>To further demonstrate the generalization capabilities of LoCO, we also provide a direct comparison with other HPE approaches on the single person task. Without any modification to the multi-person pipeline, we achieve state of the art results on the popular Human3.6m dataset.</p><p>For each dataset we also show the upper bound obtained by using the GT volumetric heatmaps in order to highlight the strengths of this data representation. In all the following tables, we will indicate with LoCO (n) our complete HPE pipeline, composed of the Code Predictor, the decoder of VHA (n) and the subsequent post-processing. LoCO (n) + is the same system with the addition of the Pose Refiner.</p><p>For all the experiments in the paper we utilized Adam optimizer with learning rate 10 ?4 . We employed batch size 1 when training the VHA and batch size <ref type="bibr" target="#b7">8</ref>   <ref type="table">Table 3</ref>: Comparison of our LoCO approach with other strong baselines and competitors on the JTA test set. In PR (precision), RE (recall) and F1, @t indicates that a predicted joint is considered "true positive" if the distance from the corresponding ground truth joint is less than t. Last two rows contain the upper bounds obtained using the ground truth location maps and volumetric heatmaps respectively Code Predictor. We employed Inception v3 <ref type="bibr" target="#b42">[42]</ref> as backbone for the Code Predictor, which is followed by 3 convolutions with ReLU activation having kernel size 4 and with 1024, 512 and 256 channels respectively. A last 1 ? 1 convolution is performed to match the compressed volumetric heatmap's number of channels. Additional training details in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Compression Levels</head><p>In order to understand how different code sizes in the VHA affects the performance of our Code Predictor network, multiple VHA versions have been tested. Specifically, we designed three VHA versions with decreasing bottleneck sizes. Each version has been trained on JTA first and then finetuned on CMU Panoptic and Human3.6m. VHA's architecture details are depicted in Tab. 1 for every version. As shown in Tab. 2, as the bottleneck size decreases, there is a corresponding decrease in the F1-score. Intuitively, the more we compress, the less information is being preserved. VHA <ref type="bibr">(1)</ref> is only considered when using JTA, as VHA <ref type="bibr">(2)</ref> and VHA <ref type="bibr">(3)</ref> already obtain an almost lossless compression on Panoptic and Human3.6m, due to their smaller number of people in the scene.</p><p>All the experiments has been conducted considering a 14 joints volumetric heatmap representation of shape 14 ? D ? H ?W , where H and W are height and width downsampled by a factor of 8, while D has been fixed to 316 bins. Note that the real-world depth grid covered by our representation is a uniform discretization in [0, 100]m for JTA, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">HPE Experiments on JTA Dataset</head><p>On the JTA dataset we compared LoCO against the Location Maps based approaches of <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Currently the Location Maps representation is the most relevant alternative to volumetric heatmaps to approach the 3D HPE task in a bottom-up fashion and therefore represents our main competitor.</p><p>A Location Maps is a per-joint feature channel that stores the 3D coordinate x, y, or z at the joint 2D pixel location. For each joint there are three location-maps and the 2D heatmap. The 2D heatmap encodes the pixel location of the joint as a confidence map in the image plane. The 3D position of a joint can then be obtained from its Location Map at the 2D pixel location of the joint. For a fair comparison, we utilized the same network (Inception v3 + f -c2d) to directly predict the Location Maps. The very low F1 score demonstrate that Location Maps are not suitable for images with multiple overlapping people, not being able to effectively handle the challenging situations peculiar of crowded surveillance scenarios (see Tab. 3).</p><p>Additionally, we report a comparison with a strong topdown baseline that uses YOLOv3 <ref type="bibr" target="#b33">[33]</ref> for the people detection part and <ref type="bibr" target="#b18">[19]</ref> as the single-person pose estimator. <ref type="bibr" target="#b18">[19]</ref>, like almost all single person methods, provides root-relative joint coordinates and not the absolute 3D position. We thus performed the 3D alignment according to <ref type="bibr" target="#b37">[37]</ref> by minimizing the distance between 2D pose and re projected 3D pose. We outperform this top-down pipeline by a large margin in terms of F1-score, while being significantly faster; LoCO is able to process Full HD images with more than 50 people at 8 FPS on a Tesla V100 GPU, while the top-down baseline runs at an average of 0.5 FPS (16 times slower). The re- call gap is mostly due to the fact that the detection phase in top-down approaches usually miss overlapped or partially occluded people on the crowded JTA scenes.</p><p>Finally, we compared against an end-to-end model trained to directly predict the volumetric heatmaps without compression ("Uncompr. Volumetric Heatmaps" in Tab. 3). Specifically, we stacked the Code Predictor and the VHA <ref type="bibr">(2)</ref> 's decoder and trained it in an end-to-end fashion. Our technique outperforms this version at every compression rate. In fact, the sparseness of the target makes it difficult to effectively exploit the redundancy of body poses in the ground truth annotation leading to a more complex training phase.</p><p>We point out that LoCO (2) + obtains by far the best result in terms of F1-score compared to all evaluated approaches and baselines, thus demonstrating the effectiveness of our method. Moreover, the best result has been obtained using the VHA (2) 's mapping, which seemingly exhibits the best compromise between information preserved and density of representation. It is also very interesting to note that the upper bound for Volumetric Heatmaps is much higher than that of Location Maps (last two rows of Tab. 3), highlighting the superiority of volumetric heatmaps in crowded scenarios. It is finally worth noticing that LoCO (1) + and LoCO (3) + obtain very close results, indicating that an ex-tremely lossy compression can lead to a poor solution as much as utilizing a too sparse and oversized representation.</p><p>Following the protocol in <ref type="bibr" target="#b7">[8]</ref>, we trained all our models (and those with Location Maps) on the 256 sequences of the JTA training set and tested our complete pipeline only on every 10 th frame of the 128 test sequences. Qualitative results are presented in <ref type="figure" target="#fig_3">Fig. 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">HPE Experiments on Panoptic Dataset</head><p>Here we propose a comparison between LoCO and three strong multi-person approaches <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b32">32]</ref> on CMU Panoptic following the test protocol defined in <ref type="bibr" target="#b52">[52]</ref>. The results, shown in the Tab. 4, are divided by action type and are expressed in terms of Mean Per Joint Position Error (MPJPE). MPJPE is calculated by firstly associating predicted and ground truth poses, by means of a simple Hungarian algorithm. In the Tab. 4 we also report the F1-score: the solely MPJPE metric is not meaningful as it does not take into account missing detections or false positive predictions.</p><p>The obtained results show the advantages of using volumetric heatmaps for 3D HPE, as LoCO <ref type="bibr">(2)</ref> + achieves the best result in terms of average MPJPE on the Panoptic test set. For the sake of fairness, we also tested on the no longer maintained "mafia" sequence. However, the older version   . "(a)" indicates the addition of rigid alignment to the test protocol; N is the number of joints considered by the method. Last row: results with ground truth volumetric heatmaps of the dataset utilizes a different convention for the joint positions. This, in fact, is reflected by the worst performance in that sequence only. Once again, the best trade-off is obtained using VHA <ref type="bibr">(2)</ref> , due to VHA (3) 's mapping partial loss of information. The GT upper bound in Tab. 4 further demonstrate the potential of our representation. Qualitative results are presented in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">HPE Experiments on Human3.6m Dataset</head><p>In analogy with previous experiments, we tested LoCO on Human3.6m. Unlike most existing approaches, we apply our multi-person method as it is, without exploiting the knowledge of the single-person nature of the dataset, as we want to demonstrate its effectiveness even in this simpler context. Results, with and without rigid alignement, are reported in terms of MPJPE following the P1 and P2 protocols. In the P1 protocol, six subjects (S1, S5, S6, S7, S8 and S9) are used for training and every 64 th frame of subject S11/camera 2 is used for testing. For the P2 protocol, all the frames from subjects S9 and S11 are used for testing and only S1, S5, S6, S7 and S8 are used for training. Tab. 5 shows a comparison with recent state-of-the-art multi-person methods, showing that our method is well reports better numerical performance, they leverage additional data for training and evaluate on a more redundant set of joints containing pelvis, torso and neck. It is worth noticing that LoCO (3) + performs substantially better than LoCO (2) +, demonstrating that a smaller representation is preferred when the same amount of information is preserved (99.7 and 100.0 F1@0vx respectively on VHA <ref type="bibr">(3)</ref> and VHA <ref type="bibr">(2)</ref> ). Qualitative results are presented in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>In conclusion, we presented a single-shot bottom-up approach for multi-person 3D HPE suitable for both crowded surveillance scenarios and for simpler, even single person, contexts without any changes. Our LoCO approach allows us to exploit volumetric heatmaps as a ground truth representation for the 3D HPE task. Instead, without compression, this would lead to a sparse and extremely high dimensional output space with consequences on both the network size and the stability of the training procedure. In comparison with top-down approaches, we removed the dependency on the people detector stage, hence gaining both in terms of robustness and assuring a constant processing time at the increasing of people in the scene. The experiments show state-of-the-art performance on all the considered datasets. We also believe that this new simple compression strategy can foster future research by enabling the full potential of the volumetric heatmap representation in contexts where it was previously intractable. Pose Refiner The structure of the Pose Refiner is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. It is a simple network composed by three fully connected layers with ReLU activation followed by a skip connection. Input and output are normalized root-relative representations of a single 3D pose, with values in range [0, 1]. During training, Gaussian noise (mean: 0m, variance: 0.08m) is applied to the input pose while some joints are randomly removed with probability 0.1. The removed joints are coded with a default value of (?1, ?1, ?1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Volumetric Heatmap Spaces</head><p>In our experiments, we defined our Volumetric Heatmap representation according to two different pseudo-3D spaces, depending on which dataset we used:</p><p>? The first space is defined as S 1 = D ?H ?W , where H and W are the height and width, downsampled by a actor of 8, of the image plane and D is the maximum distance from the camera in meters, quantized with 316 bins. We adopted S 1 for JTA. ? The second space is defined as S 2 = Z ? H ? W , where H and W are defined as in S 1 , and Z is the maximum z axis value of the real 3D space in the standard coordinate system centered to the camera. Z is expressed in meters and quantized with 316 bins. We adopted S 2 for Panoptic and Human3.6m.</p><p>Although the difference between these two spaces is minimal, we adopted S 1 for JTA because this dataset already provide a maximum camera distance, which is 100 meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detection Experiments</head><p>To show how our LoCO approach can be effectively adopted also for the detection task in crowded scenarios under heavy occlusions, we have tested our system in terms of 2D people detection comparing it with YOLOv3 [2] on the JTA test set. Using LoCO, we predict 3D poses and project them on 2D bounding boxes using the camera intrinsic parameters.</p><p>In terms of precision, recall, and F1 (with the bounding box IoU threshold at 0.5), using our LoCO (2) + trained on JTA, we get 81.94, 69.73, and 75.39 respectively; with out of the box YOLOv3, instead, we obtain 99.12, 30.81 and 44.50. Although our model is less precise than YOLOv3 (around -20%), it surpasses it by a large margin (around +40%) in terms of recall, resulting in an F1-score that is clearly in our favor (almost +30%). The scenes in JTA, in fact, are extremely crowded and present a very high percentage of occlusion with multiple overlapping people. It is not easy for a detector to handle situations of this type, while a part-based bottom-up method is much less affected by this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Skeleton Grouping Details</head><p>Let's consider K different type of joints and N 0 , . . . , N K?1 number of detections for each joint type. Given N 0 predicted heads, j 0,0 , . . . , j 0,N0?1 ? R 3 , and N k , k ? [1, K ? 1] predicted joints of another type, j k,0 , . . . , j k,N k ?1 ? R 3 , we define K ? 1 cost matrices, D 1 , . . . , D k?1 , as follows: D k : {0, . . . , N 0 ? 1} ? {0, . . . , N k ? 1} ? R where each element d a,b is defined as</p><formula xml:id="formula_8">d a,b = j 0,a ? j k,b if j 0,a ? j k,b ? 1.5 ? ? k +? otherwise<label>(1)</label></formula><p>The threshold ? k in (1) is the maximum distance between a head and a joint of type k (belonging to the same person) on the entire training set. For each k = 1, . . . , K ? 1, jointhead associations are made with the Hungarian algorithm using D k as cost matrix. The same joint-grouping procedure is applied on both multi-person datasets. By removing the anatomical constraints, results on Panoptic show an MPJPE degradation of about 9 millimeters while on JTA, no degradation in terms of metric has been observed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional Qualitative Results</head><p>We report the results of our Pose Refiner on JTA. When the input pose fed to our Pose Refiner has few missing joints and position errors, the reconstruction is consistent with the ground truth pose ( <ref type="figure" target="#fig_1">Fig. 2 -first two rows)</ref>. Conversely, when the input pose is more degraded, the reconstruction is still plausible, but not always coherent with the ground truth ( <ref type="figure" target="#fig_1">Fig. 2 -last two rows)</ref>. Additionally, <ref type="figure" target="#fig_3">Fig. 3</ref> depicts some qualitative results of LoCO on JTA, CMU Panoptic and Hu-man3.6m. Our method can be applied to both crowded scenarios and single person contexts, displaying good generalization capabilities in a wide range of contexts. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*Figure 1 :</head><label>1</label><figDesc>Work done while interning at Panasonic R&amp;D Company of America Examples of 3D poses estimated by our LoCO approach. Close-ups show that 3D poses are correctly computed even in very complex and articulated scenarios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematization of the proposed LoCO pipeline. At training time, the Encoder e produces the compressed volumetric heatmaps e(H) which are used as ground truth from the Code Predictor f . At test time, the intermediate representation f (I) computed by the Code Predictor is fed to the Decoder d for the final output. In our case, H = H/8 and W = W/8 mapping from image features to 3D joint locations, with no need of explicit bounding box detections or 2D proxy poses, while simultaneously being robust to heavy occlusions and multiple overlapping people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[0, 7]m for Panoptic and [1.8, 8.1]m for Human3.6m. Thus, every bin has a depth size of approximately 0.32m for JTA and 0.02m for Panoptic and Human3.6m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results of LoCO (2) + on the JTA and Panoptic datasets. We show both the 3D poses (JTA: 2 nd row, Panoptic: 4 th row) and the corresponding 2D versions re-projected on the image plane (JTA: 1 st row, Panoptic: 3 rd row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of LoCO (3) + on the Hu-man3.6m dataset suited even in the single person context, as LoCO (3) + achieves state of the art results among bottom up methods. Note that, although Moon et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Structure of our Pose Refiner: 3 fully connected layers with ReLU activation followed by a skip connection. N is the number of joints. In all our experiments we considered N = 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Qualitative results of our Pose Refiner model on the JTA dataset. 1 st and 2 nd rows: examples where the output is anatomically plausible and consistent with the ground truth; 3 rd and 4 th rows: examples where the output is anatomically plausible, but inconsistent with the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Additional qualitative results of our LoCO approach. 1 st and 2 nd rows: result of LoCO (2) + on the JTA dataset; 3 rd and 4 th rows: result of LoCO (2) + on the CMU Panoptic dataset; 5 th and 6 th rows: result of LoCO (3) + on the Human3.6m dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>23.51 23.08 38.85 39.17 38.49 [33] + [19] 75.88 28.36 39.14 92.85 34.17 47.38 96.33 35.33 49.03 Uncompr. Volumetric Heatmaps 25.37 24.40 24.47 45.40 43.11 43.51 55.55 52.44 53.08</figDesc><table><row><cell></cell><cell>PR</cell><cell>RE</cell><cell>F1</cell><cell>PR</cell><cell>RE</cell><cell>F1</cell><cell>PR</cell><cell>RE</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell>@0.4 m</cell><cell></cell><cell></cell><cell>@0.8 m</cell><cell></cell><cell></cell><cell>@1.2 m</cell></row><row><cell>Location Maps [21, 22]</cell><cell>5.80</cell><cell>5.33</cell><cell>5.42</cell><cell cols="3">24.06 21.65 22.29</cell><cell cols="3">41.43 36.96 38.26</cell></row><row><cell cols="7">Location Maps [21, 22] + ref. 23.28 LoCO (1) 5.82 5.89 5.77 48.10 42.73 44.76 65.63 58.58 61.24</cell><cell cols="3">72.44 64.84 67.70</cell></row><row><cell>LoCO (1) +.</cell><cell cols="3">49.37 43.45 45.73</cell><cell cols="3">66.87 59.02 62.02</cell><cell cols="3">73.54 65.07 68.29</cell></row><row><cell>LoCO (2)</cell><cell cols="3">54.76 46.94 50.13</cell><cell cols="3">70.67 60.48 64.62</cell><cell cols="3">77.00 65.92 70.40</cell></row><row><cell>LoCO (2) +.</cell><cell cols="3">55.37 47.84 50.82</cell><cell cols="3">70.63 60.94 64.76</cell><cell cols="3">76.81 66.31 70.44</cell></row><row><cell>LoCO (3)</cell><cell cols="3">48.18 41.97 44.49</cell><cell cols="3">66.96 58.22 61.77</cell><cell cols="3">74.43 64.71 68.65</cell></row><row><cell>LoCO (3) +.</cell><cell cols="3">49.15 42.84 45.36</cell><cell cols="3">67.16 58.45 61.92</cell><cell cols="3">74.39 64.76 68.57</cell></row><row><cell>GT Location Maps [21, 22]</cell><cell cols="3">76.07 64.83 69.59</cell><cell cols="3">76.07 64.83 69.59</cell><cell cols="3">76.07 64.83 69.59</cell></row><row><cell>GT Volumetric Heatmaps</cell><cell cols="3">99.96 99.96 99.96</cell><cell cols="3">99.99 99.99 99.99</cell><cell cols="3">99.99 99.99 99.99</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">when training the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison on the CMU Panoptic dataset. Results are shown in terms of MPJPE [mm] and F1 detection score.</figDesc><table><row><cell cols="7">Last row: results with ground truth volumetric heatmaps</cell></row><row><cell></cell><cell>method</cell><cell>N</cell><cell>P1</cell><cell>P1 (a)</cell><cell>P2</cell><cell>P2 (a)</cell></row><row><cell>top-down</cell><cell cols="3">Rogez et al. [36] 13 63.2 Dabral et al. [6] 16 -Rogez et al. [37] 13 54.6 Moon et al. [23] 17 35.2</cell><cell>53.4 -45.8 34.0</cell><cell>87.7 -65.4 54.4</cell><cell>71.6 65.2 54.3 53.3</cell></row><row><cell>bottom-up</cell><cell cols="3">Mehta et al. [22] 17 Mehta et al. [21] 17 LoCO (2) + 14 84.0 --LoCO (3) + 14 51.1</cell><cell>--75.4 43.4</cell><cell>80.5 69.9 96.6 61.0</cell><cell>--77.1 49.1</cell></row><row><cell cols="2">GT Vol. Heatmaps</cell><cell cols="2">14 15.6</cell><cell>14.9</cell><cell>15.0</cell><cell>14.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison on the Human3.6m dataset in terms of average MPJPE [mm]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Tab. 1 reports the detailed structures of the various f -c2d blocks utilized in our experiments. ConvTr2D refers to transposed 2D convolutions while Conv2D refers to simple 2D convolutions. For each layer, we provide: number of input channels, number of output channels, kernel size and stride. In all the proposed experiments we utilized InceptionV3[3]  pretrained on Imagenet[1]  as backbone architecture.</figDesc><table><row><cell cols="2">Fabbri 1* Fabio Lanzi 1 Simone Calderara 1 Stefano Alletto 2 Rita Cucchiara 1</cell></row><row><cell>1 University of Modena and Reggio Emilia</cell><cell>2 Panasonic R&amp;D Company of America</cell></row><row><cell>{name.surname}@unimore.it</cell><cell>{name.surname}@us.panasonic.com</cell></row><row><cell>1. Implementation Details</cell><cell></cell></row><row><cell>For the sake of reproducibility, in this section we illustrate the architectures of the Code Predictor and the Pose Refiner modules of our LoCO pipeline.</cell><cell></cell></row><row><cell>Code Predictor Inspired by [4], our method simply adds a few convolutional layers (f -c2d) to the last convolution stage of a backbone network.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Structure of the three f -c2d block variants of the Code Predictor used for our HPE experiments. F represents the number of output channels of the exploited feature extractor. In all our experiments we used Inception v3 with F = 2048</figDesc><table><row><cell>of America</cell></row></table><note>* Work done while interning at Panasonic R&amp;D Company</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by Panasonic Corporation and by the Italian Ministry of Education, Universities and Research under the project COSMOS PRIN 2015 programme 201548C5NT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Significant dimension reduction of 3d brain mri using 3d convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Iyatomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="5162" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d human pose esti-mation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint 3d-multiview prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to detect and track visible and occluded body joints in a virtual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to refine human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">Scott</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d human pose estimation with 2d marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Synthetic occlusion augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) -Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis: neural network models and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fraunholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Selbig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principal Manifolds for Data Visualization and Dimension Reduction</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Integral human pose regression. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Marquez</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards image understanding from deep compression without decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rbert</forename><surname>Torfason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eirkur Gstsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A folded neural network autoencoder for dimensionality reduction. Procedia Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prokhorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Dataset distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
