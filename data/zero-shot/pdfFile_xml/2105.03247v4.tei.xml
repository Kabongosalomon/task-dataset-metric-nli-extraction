<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOTR: End-to-End Multiple-Object Tracking with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Bin</roleName><forename type="first">Fangao</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>1?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MOTR: End-to-End Multiple-Object Tracking with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multiple-Object Tracking</term>
					<term>Transformer</term>
					<term>End-to-End</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal modeling of objects is a key challenge in multipleobject tracking (MOT). Existing methods track by associating detections through motion-based and appearance-based similarity heuristics. The post-processing nature of association prevents end-to-end exploitation of temporal variations in video sequence. In this paper, we propose MOTR, which extends DETR [6] and introduces "track query" to model the tracked instances in the entire video. Track query is transferred and updated frame-by-frame to perform iterative prediction over time. We propose tracklet-aware label assignment to train track queries and newborn object queries. We further propose temporal aggregation network and collective average loss to enhance temporal relation modeling. Experimental results on DanceTrack show that MOTR significantly outperforms state-of-the-art method, ByteTrack [42] by 6.5% on HOTA metric. On MOT17, MOTR outperforms our concurrent works, TrackFormer <ref type="bibr" target="#b17">[18]</ref> and TransTrack <ref type="bibr" target="#b28">[29]</ref>, on association performance. MOTR can serve as a stronger baseline for future research on temporal modeling and Transformer-based trackers. Code is available at https://github.com/megvii-research/MOTR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>object queries, served as a decoupled representation of objects, are fed into the Transformer decoder and interacted with the image feature to update their representation. Bipartite matching is further adopted to achieve one-to-one assignment between the object queries and ground-truths, eliminating post-processes, like NMS. Different from object detection, MOT can be regarded as a sequence prediction problem. The way to perform sequence prediction in the end-to-end DETR system is an open question. Iterative prediction is popular in machine translation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. The output context is represented by a hidden state, and sentence features iteratively interact with the hidden state in the decoder to predict the translated words. Inspired by these advances in machine translation, we intuitively regard MOT as a problem of set of sequence prediction since MOT requires a set of object sequences. Each sequence corresponds to an object trajectory. Technically, we extend object query in DETR to track query for predicting object sequences. Track queries are served as the hidden states of object tracks. The representations of track queries are updated in the Transformer decoder and used to predict the object trajectory iteratively, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Specifically, track queries are updated through self-attention and cross-attention by frame features. The updated track queries are further used to predict the bounding boxes. The track of one object can be obtained from all predictions of one track query in different frames.</p><p>To achieve the goal above, we need to solve two problems: 1) track one object by one track query; 2) deal with newborn and terminated objects. To solve the first problem, we introduce tracklet-aware label assignment (TALA). It means that predictions of one track query are supervised by bounding box sequences with the same identity. To solve the second problem, we maintain a track query set of variable lengths. Queries of newborn objects are merged into this set while queries of terminated objects are removed. We name this process the entrance and exit mechanism. In this way, MOTR does not require explicit track associations during inference. Moreover, the iterative update of track queries enables temporal modeling regarding both appearance and motion.</p><p>To enhance the temporal modeling, we further propose collective average loss (CAL) and temporal aggregation network (TAN). With the CAL, MOTR takes video clips as input during training. The parameters of MOTR are updated based on the overall loss calculated for the whole video clip. TAN introduces a shortcut for track query to aggregate the historical information from its previous states via the key-query mechanism in Transformer.</p><p>MOTR is a simple online tracker. It is easy to develop based on DETR with minor modifications on label assignment. It is a truly end-to-end MOT framework, requiring no post-processes, such as the track NMS or IoU matching employed in our concurrent works, TransTrack <ref type="bibr" target="#b28">[29]</ref>, and TrackFormer <ref type="bibr" target="#b17">[18]</ref>. Experimental results on MOT17 and DanceTrack datasets show that MOTR achieves promising performance. On DanceTrack <ref type="bibr" target="#b27">[28]</ref>, MOTR outperforms the state-of-the-art ByteTrack <ref type="bibr" target="#b41">[42]</ref> by 6.5% on HOTA metric and 8.1% on AssA.</p><p>To summarize, our contributions are listed as below:</p><p>-We present a fully end-to-end MOT framework, named MOTR. MOTR can implicitly learn the appearance and position variances in a joint manner. -We formulate MOT as a problem of set of sequence prediction. We generate track query from previous hidden states for iterative update and prediction. -We propose tracklet-aware label assignment for one-to-one assignment between track queries and objects. An entrance and exit mechanism is introduced to deal with newborn and terminated tracks. -We further propose CAL and TAN to enhance the temporal modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformer-based Architectures. Transformer <ref type="bibr" target="#b30">[31]</ref> was first introduced to aggregate information from the entire input sequence for machine translation. It mainly involves self-attention and cross-attention mechanisms. Since that, it was gradually introduced to many fields, such as speech processing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref> and computer vision <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref>. Recently, DETR <ref type="bibr" target="#b5">[6]</ref> combined convolutional neural network (CNN), Transformer and bipartite matching to perform end-to-end object detection. To achieve the fast convergence, Deformable DETR <ref type="bibr" target="#b44">[45]</ref> introduced deformable attention module into Transformer encoder and Transformer decoder. ViT <ref type="bibr" target="#b8">[9]</ref> built a pure Transformer architecture for image classification. Further, Swin Transformer <ref type="bibr" target="#b15">[16]</ref> proposed shifted windowing scheme to perform self-attention within local windows, bringing greater efficiency. VisTR <ref type="bibr" target="#b35">[36]</ref> employed a direct end-to-end parallel sequence prediction framework to perform video instance segmentation. Multiple-Object Tracking. Dominant MOT methods mainly followed the tracking-by-detection paradigm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref>. These approaches usually first employ object detectors to localize objects in each frame and then perform track association between adjacent frames to generate the tracking results. SORT <ref type="bibr" target="#b2">[3]</ref> conducted track association by combining Kalman Filter <ref type="bibr" target="#b37">[38]</ref> and Hungarian algorithm <ref type="bibr" target="#b10">[11]</ref>. DeepSORT <ref type="bibr" target="#b38">[39]</ref> and Tracktor <ref type="bibr" target="#b1">[2]</ref> introduced an extra cosine distance and compute the appearance similarity for track association. Track-RCNN <ref type="bibr" target="#b25">[26]</ref>, JDE <ref type="bibr" target="#b36">[37]</ref> and FairMOT <ref type="bibr" target="#b42">[43]</ref> further added a Re-ID branch on top of object detector in a joint training framework, incorporating object detection and Re-ID feature learning. TransMOT <ref type="bibr" target="#b7">[8]</ref> builds a spatial-temporal graph transformer for association. Our concurrent works, TransTrack <ref type="bibr" target="#b28">[29]</ref> and TrackFormer <ref type="bibr" target="#b17">[18]</ref> also develop Transformer-based frameworks for MOT. For direct comparison with them, please refer to Sec. 3.7. Iterative Sequence Prediction. Predicting sequence via sequence-to-sequence (seq2seq) with encoder-decoder architecture is popular in machine translation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> and text recognition <ref type="bibr" target="#b24">[25]</ref>. In seq2seq framework, the encoder network encodes the input into intermediate representation. Then, a hidden state with task-specific context information is introduced and iteratively interacted with the intermediate representation to generate the target sequence through the decoder network. The iterative decode process contains several iterations. In each iteration, hidden state decodes one element of target sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query in Object Detection</head><p>DETR <ref type="bibr" target="#b5">[6]</ref> introduced a fixed-length set of object queries to detect objects. Object queries are fed into the Transformer decoder and interacted with image features, extracted from Transformer encoder to update their representation. Bipartite matching is further adopted to achieve one-to-one assignment between the updated object queries and ground-truths. Here, we simply write the object query as "detect query" to specify the query used for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detect Query and Track Query</head><p>When adapting DETR from object detection to MOT, two main problems arise: 1) how to track one object by one track query; 2) how to handle newborn and terminated objects. We extend detect queries to track queries in this paper. Track query set is updated dynamically, and the length is variable. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the track query set is initialized to be empty, and the detect queries in DETR are used to detect newborn objects (object 3 at T 2 ). Hidden states of detected objects produces track queries for the next frame; track queries assigned to terminated objects are removed from the track query set (object 2 at T 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tracklet-Aware Label Assignment</head><p>In DETR, one detect (object) query may be assigned to any object in the image since the label assignment is determined by performing bipartite matching between all detect queries and ground-truths. While in MOTR, detect queries are only used to detect newborn objects while track queries predict all tracked objects. Here, we introduce the tracklet-aware label assignment (TALA) to solve this problem.</p><p>Generally, TALA consists of two strategies. For detect queries, we modify the assignment strategy in DETR as newborn-only, where bipartite matching is Track query set is updated dynamically, and the length is variable. Track query set is initialized to be empty, and the detect queries are used to detect newborn objects. Hidden states of all detected objects are concatenated to produce track queries for the next frame. Track queries assigned to terminated objects are removed from the track query set.</p><p>conducted between the detect queries and the ground-truths of newborn objects.</p><p>For track queries, we design an target-consistent assignment strategy. Track queries follow the same assignment of previous frames and are therefore excluded from the aforementioned bipartite matching. Formally, we denote the predictions of track queries as Y tr and predictions of detect queries as Y det . Y new is the ground-truths of newborn objects. The label assignment results for track queries and detect queries can be written as ? tr and ? det . For frame i, label assignment for detect queries is obtained from bipartite matching among detect queries and newborn objects, i.e.,</p><formula xml:id="formula_0">? i det = arg min ? i det ??i L( Y i det | ? i det , Y i new ),<label>(1)</label></formula><p>where L is the pair-wise matching cost defined in DETR and ? i is the space of all bipartite matches among detect queries and newborn objects. For track query assignment, we merge the assignments for newborn objects and tracked objects from the last frame, i.e., for i &gt; 1:</p><formula xml:id="formula_1">? i tr = ? i?1 tr ? ? i?1 det .<label>(2)</label></formula><p>For the first frame (i = 1), track query assignment ? 1 tr is an empty set ? since there are no tracked objects for the first frame. For successive frames (i &gt; 1), the track query assignment ? i tr is the concatenation of previous track query assignment ? i?1 tr and newborn object assignment ? i?1 det . In practice, the TALA strategy is simple and effective thanks to the powerful attention mechanism in Transformer. For each frame, detect queries and track queries are concatenated and fed into the Transformer decoder to update their representation. Detect queries will only detect newborn objects since query interaction by self-attention in the Transformer decoder will suppress detect queries that detect tracked objects. This mechanism is similar to duplicate removal in DETR that duplicate boxes are suppressed with low scores.</p><formula xml:id="formula_2">! " # ? ? Video Stream Enc Dec ! " " # # " Predictions Enc Dec C Enc Dec Detect Query Track Query QIM ? ? C ? ? $% # QIM $% &amp; ' ' ' Fig. 3:</formula><p>The overall architecture of MOTR. "Enc" represents a convolutional neural network backbone and the Transformer encoder that extracts image features for each frame. The concatenation of detect queries q d and track queries q tr is fed into the Deformable DETR decoder (Dec) to produce the hidden states. The hidden states are used to generate the prediction Y of newborn and tracked objects. The query interaction module (QIM) takes the hidden states as input and produces track queries for the next frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MOTR Architecture</head><p>The overall architecture of MOTR is shown in <ref type="figure">Fig. 3</ref>. Video sequences are fed into the convolutional neural network (CNN) (e.g. ResNet-50 <ref type="bibr" target="#b9">[10]</ref>) and Deformable DETR <ref type="bibr" target="#b44">[45]</ref> encoder to extract frame features. For the first frame, there are no track query and we only feed the fixedlength learnable detect queries (q d in <ref type="figure">Fig. 3</ref>) into the Deformable DETR <ref type="bibr" target="#b44">[45]</ref> decoder. For successive frames, we feed the concatenation of track queries from the previous frame and the learnable detect queries into the decoder. These queries interact with image feature in the decoder to generate the hidden state for bounding box prediction. The hidden state is also fed into the query interaction module (QIM) to generate the track queries for the next frame.</p><p>During training phase, the label assignment for each frame is described in Sec. 3.3. All predictions of the video clip are collected into a prediction bank { Y 1 , Y 2 , . . . , Y N }, and we use the proposed collective average loss (CAL) described in Sec. 3.6 for supervision. During inference time, the video stream can be processed online and generate the prediction for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Query Interaction Module</head><p>In this section, we describe query interaction module (QIM). QIM includes object entrance and exit mechanism and temporal aggregation network (TAN). Object Entrance and Exit. As mentioned above, some objects in video sequences may appear or disappear at intermediate frames. Here, we introduce the way we deal with the newborn and terminated objects in our method. For any frame, track queries are concatenated with the detect queries and input to the Transformer decoder, producing the hidden state (see the left side of <ref type="figure" target="#fig_2">Fig. 4</ref>).</p><p>During training, hidden states of terminated objects are removed if the matched objects disappeared in ground-truths or the intersection-over-union (IoU) between predicted bounding box and target is below a threshold of 0.5. It means that the corresponding hidden states will be filtered if these objects disappear at current frame while the rest hidden states are reserved. For newborn objects, the corresponding hidden states are kept based on the assignment of newborn object ? i det defined in Eq. 1. For inference, we use the predicted classification scores to determine appearance of newborn objects and disappearance of tracked objects, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. For object queries, predictions whose classification scores are higher than the entrance threshold ? en are kept while other hidden states are removed. For track queries, predictions whose classification scores are lower than the exit threshold ? ex for consecutive M frames are removed while other hidden states are kept. Temporal Aggregation Network. Here, we introduce the temporal aggregation network (TAN) in QIM to enhance temporal relation modeling and provide contextual priors for tracked objects.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the input of TAN is the filtered hidden state for tracked objects (object "1"). We also collect the track query q i tr from the last frame for temporal aggregation. TAN is a modified Transformer decoder layer. The track query from the last frame and the filtered hidden state are summed to be the key and query components of the multi-head self-attention (MHA). The hidden state alone is the value component of MHA. After MHA, we apply a feed-forward network (FFN) and the results are concatenated with the hidden state for newborn objects (object "3") to produce the track query set q i+1 tr for the next frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Collective Average Loss</head><p>Training samples are important for temporal modeling of track since MOTR learns temporal variances from data rather than hand-crafted heuristics like Kalman Filtering. Common training strategies, like training within two frames, fail to generate training samples of long-range object motion. Different from them, MOTR takes video clips as input. In this way, training samples of longrange object motion can be generated for temporal learning.</p><p>Instead of calculating the loss frame-by-frame, our collective average loss (CAL) collects the multiple predictions</p><formula xml:id="formula_3">Y = { Y i } N i=1 .</formula><p>Then the loss within the whole video sequence is calculated by ground-truths Y = {Y i } N i=1 and the match-</p><formula xml:id="formula_4">ing results ? = {? i } N i=1</formula><p>. CAL is the overall loss of the whole video sequence, normalized by the number of objects:</p><formula xml:id="formula_5">L o ( Y | ? , Y ) = N n=1 (L( Y i tr | ? i tr , Y i tr ) + L( Y i det | ? i det , Y i det )) N n=1 (V i )<label>(3)</label></formula><p>where V i = V i tr +V i det denotes the total number of ground-truths objects at frame i. V i tr and V i det are the numbers of tracked objects and newborn objects at frame i, respectively. L is the loss of single frame, which is similar to the detection loss in DETR. The single-frame loss L can be formulated as:</p><formula xml:id="formula_6">L( Y i | ?i , Y i ) = ? cls L cls + ? l1 L l1 + ? giou L giou<label>(4)</label></formula><p>where L cls is the focal loss <ref type="bibr" target="#b13">[14]</ref>. L l1 denotes the L1 loss and L giou is the generalized IoU loss <ref type="bibr" target="#b20">[21]</ref>. ? cls , ? l1 and ? giou are the corresponding weight coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Discussion</head><p>Based on DETR, our concurrent works, TransTrack <ref type="bibr" target="#b28">[29]</ref> and TrackFormer <ref type="bibr" target="#b17">[18]</ref> also develop the Transformer-based frameworks for MOT. However, our method shows large differences compared to them:</p><p>TransTrack models a full track as a combination of several independent short tracklets. Similar to the track-by-detection paradigm, TransTrack decouples MOT as two sub-tasks: 1) detect object pairs as short tracklets within two adjacent frames; 2) associate short tracklets as full tracks by IoU-matching. While for MOTR, we model a full track in an end-to-end manner through the iterative update of track query, requiring no IoU-matching.  TrackFormer shares the idea of track query with us. However, TrackFormer still learns within two adjacent frames. As discussed in Sec. 3.6, learning within short-range will result in relatively weak temporal learning. Therefore, Track-Former employs heuristics, such as Track NMS and Re-ID features, to filter out duplicate tracks. Different from TrackFormer, MOTR learns stronger temporal motion with CAL and TAN, removing the need of those heuristics. For direct comparison with TransTrack and TrackFormer, please refer to the <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Here, we clarify that we started this work independently long before Track-Former and TransTrack appear on arXiv. Adding that they are not formally published, we treat them as concurrent and independent works instead of previous works on which our work is built upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>Datasets. For comprehensive evaluation, we conducted experiments on three datasets: DanceTrack <ref type="bibr" target="#b27">[28]</ref>, MOT17 <ref type="bibr" target="#b18">[19]</ref>, and BDD100k <ref type="bibr" target="#b40">[41]</ref>. MOT17 <ref type="bibr" target="#b18">[19]</ref> contains 7 training sequences and 7 test sequences. DanceTrack <ref type="bibr" target="#b27">[28]</ref> is a recent multiobject tracking dataset featuring uniform appearance and diverse motion. It contains more videos for training and evaluation thus providing a better choice to verify the tracking performance. BDD100k <ref type="bibr" target="#b40">[41]</ref> is an autonomous driving dataset with an MOT track featuring multiple object classes. For more details, please refer to the statistics of datasets, shown in <ref type="table" target="#tab_1">Table 2</ref>. Evaluation Metrics. We follow the standard evaluation protocols to evaluate our method. The common metrics include Higher Order Metric for Evaluating Multi-object Tracking <ref type="bibr" target="#b16">[17]</ref> (HOTA, AssA, DetA), Multiple-Object Tracking Accuracy (MOTA), Identity Switches (IDS) and Identity F1 Score (IDF1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Following the settings in CenterTrack <ref type="bibr" target="#b43">[44]</ref>, MOTR adopts several data augmentation methods, such as random flip and random crop. The shorter side of the input image is resized to 800 and the maximum size is restricted to 1536. The inference speed on Tesla V100 at this resolution is about 7.5 FPS. We sample keyframes with random intervals to solve the problem of variable frame rates. Besides, we erase the tracked queries with the probability p drop to generate more samples for newborn objects and insert track queries of false positives with the probability p insert to simulate the terminated objects. All the experiments are conducted on PyTorch with 8 NVIDIA Tesla V100 GPUs. We also provide a memory-optimized version that can be trained on NVIDIA 2080 Ti GPUs.</p><p>We built MOTR upon Deformable-DETR <ref type="bibr" target="#b44">[45]</ref> with ResNet50 <ref type="bibr" target="#b9">[10]</ref> for fast convergence. The batch size is set to 1 and each batch contains a video clip of 5 frames. We train our model with the AdamW optimizer with the initial learning rate of 2.0?10 ?4 . For all datasets, we initialize MOTR with the official Deformable DETR <ref type="bibr" target="#b44">[45]</ref> weights pre-trained on the COCO <ref type="bibr" target="#b14">[15]</ref> dataset. On MOT17, we train MOTR for 200 epochs and the learning rate decays by a factor of 10 at the 100 th epoch. For state-of-the-art comparison, we train on the joint dataset (MOT17 training set and CrowdHuman <ref type="bibr" target="#b22">[23]</ref> val set). For ?5k static images in CrowdHuman val set, we apply random shift as in <ref type="bibr" target="#b43">[44]</ref> to generate video clips with pseudo tracks. The initial length of video clip is 2 and we gradually increase it to 3,4,5 at the 50 th ,90 th ,150 th epochs, respectively. The progressive increment of video clip length improves the training efficiency and stability. For the ablation study, we train MOTR on the MOT17 training set without using the CrowdHuman dataset and validate on the 2DMOT15 training set. On DanceTrack, we train for 20 epochs on the train set and learning rate decays at the 10 th epoch. We gradually increase the clip length from 2 to 3,4,5 at the 5 th ,9 th ,15 th epochs. On BDD100k, we train for 20 epochs on the train set and learning rate decays at the 16 th epoch. We gradually increase the clip length from 2 to 3 and 4 at the 6 th and 12 th epochs. <ref type="table" target="#tab_2">Table 3</ref> compares our approach with state-of-the-art methods on MOT17 test set. We mainly compare MOTR with our concurrent works based on Transformer: TrackFormer <ref type="bibr" target="#b17">[18]</ref> and TransTrack <ref type="bibr" target="#b28">[29]</ref>. Our method gets higher IDF1 scores, surpassing TransTrack and TrackFormer by 4.5%. The performance of MOTR on the HOTA metric is much higher than TransTrack by 3.1%. For the MOTA metric, our method achieves much better performance than TrackFormer (71.9% vs. 65.0%). Interestingly, we find that the performance of TransTrack is better than our MOTR on MOTA. We suppose the decoupling of detection and tracking branches in TransTrack indeed improves the object detection performance. While in MOTR, detect and track queries are learned through a shared Transformer decoder. Detect queries are suppressed on detecting tracked objects, limiting the detection performance on newborn objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">State-of-the-art Comparison on MOT17</head><p>If we compare the performance with other state-of-the-art methods, like Byte-Track <ref type="bibr" target="#b41">[42]</ref>, it shows that MOTR is frustratingly inferior to them on the MOT17 dataset. Usually, state-of-the-art performance on the MOT17 dataset is dominated by trackers with good detection performance to cope with various appearance distributions. Also, different trackers tend to employ different detectors for object detection. It is pretty difficult for us to fairly verify the motion performance of various trackers. Therefore, we argue that the MOT17 dataset alone is not enough to fully evaluate the tracking performance of MOTR. We further evaluate the tracking performance on DanceTrack <ref type="bibr" target="#b27">[28]</ref> dataset with uniform appearance and diverse motion, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">State-of-the-art Comparison on DanceTrack</head><p>Recently, DanceTrack <ref type="bibr" target="#b27">[28]</ref>, a dataset with uniform appearance and diverse motion, is introduced (see Tab. 2). It contains much more videos for evaluation and provides a better choice to verify the tracking performance. We further conduct the experiments on the DanceTrack dataset and perform the performance comparison with state-of-the-art methods in Tab. 4. It shows that MOTR achieves much better performance on DanceTrack dataset. Our method gets a much higher HOTA score, surpassing ByteTrack by 6.5%. For the AssA metric, our method also achieves much better performance than ByteTrack (40.2% vs. 32.1%). While for the DetA metric, MOTR is inferior to some state-of-the-art methods. It means that MOTR performs well on temporal motion learning while the detection performance is not that good. The large improvements on HOTA are mainly from the temporal aggregation network and collective average loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Generalization on Multi-Class Scene</head><p>Re-ID based methods, like FairMOT <ref type="bibr" target="#b42">[43]</ref>, tend to regard each tracked object (e.g., person) as a class and associate the detection results by the feature similarity. However, the association will be difficult when the number of tracked objects is very large. Different from them, each object is denoted as one track query in MOTR and the track query set is of dynamic length. MOTR can easily deal with  the multi-class prediction problem, by simply modifying the class number of the classification branch. To verify the performance of MOTR on multi-class scenes, we further conduct the experiments on the BDD100k dataset (see Tab. 5). Results on bdd100k validation set show that MOTR performs well on multi-class scenes and achieves promising performance with fewer ID switches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>MOTR Components.  Erasing and Inserting Track Query. In MOT datasets, there are few training samples for two cases: entrance objects and exit objects in video sequences. Therefore, we adopt track query erasing and inserting to simulate these two cases with probability p drop and p insert , respectively. <ref type="table" target="#tab_6">Table 6c</ref> reports the performance using different value of p drop during training. MOTR achieves the best performance when p drop is set to 0.1. Similar to the entrance objects, track queries transferred from the previous frame, whose predictions are false positives, are inserted into the current frame to simulate the case of object exit. In <ref type="table" target="#tab_6">Table 6d</ref>, we explore the impact on tracking performance of different p insert . When progressively increasing p insert from 0.1 to 0.7, our MOTR achieves the highest score on MOTA when p insert is set to 0.3 while the IDF1 score is decreasing. Object Entrance and Exit Threshold. <ref type="table" target="#tab_6">Table 6e</ref> investigates the impact of different combination of object entrance threshold ? en and exit threshold ? ex in QIM. As we vary the object entrance threshold ? en , we can see that the performance is not that sensitive to ? en (within 0.5% on MOTA) and using an entrance threshold of 0.8 produces relatively better performance. We also further conduct experiments by varying the object exit threshold ? ex . It is shown that using a threshold of 0.5 results in slightly better performance than that of 0.6. In our practice, ? en with 0.6 shows better performance on the MOT17 test set.</p><p>Sampling Interval. In <ref type="table" target="#tab_6">Table 6f</ref>, we evaluate the effect of random sampling interval on tracking performance during training. When the sampling interval increases from 2 to 10, the IDS decreases significantly from 209 to 155. During training, the network is easy to fall into a local optimal solution when the frames are sampled in a small interval. Appropriate increment on sampling interval can simulate real scenes. When the random sampling interval is greater than 10, the tracking framework fails to capture such long-range dynamics, leading to relatively worse tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>MOTR, an online tracker, achieves end-to-end multiple-object tracking. It implicitly learns the appearance and position variances in a joint manner thanks to the DETR architecture as well as the tracklet-aware label assignment. However, it also has several shortcomings. First, the performance of detecting newborn objects is far from satisfactory (the result on the MOTA metric is not good enough). As we analyzed above, detect queries are suppressed on detecting tracked objects, which may go against the nature of object query and limits the detection performance on newborn objects. Second, the query passing in MOTR is performed frame-by-frame, limiting the efficiency of model learning during training. In our practice, the parallel decoding in VisTR <ref type="bibr" target="#b35">[36]</ref> fails to deal with the complex scenarios in MOT. Solving these two problems above will be an important research topic for Transformer-based MOT frameworks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a) DETR achieves end-to-end detection by interacting object queries with image features and performs one-to-one assignment between the updated queries and objects. (b) MOTR performs set of sequence prediction by updating the track queries. Each track query represents a track. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Update process of detect (object) queries and track queries under some typical MOT cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>The structure of query interaction module (QIM). The inputs of QIM are the hidden state produced by Transformer decoder and the corresponding prediction scores. In the inference stage, we keep newborn objects and drop exited objects based on the confidence scores. Temporal aggregation network (TAN) enhances long-range temporal modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>The effect of CAL on solving (a) duplicated boxes and (b) ID switch problems. Top and bottom rows are the tracking results without and with CAL, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with other MOT methods based on Transformer.</figDesc><table><row><cell>Method</cell><cell>IoU match NMS ReID</cell></row><row><cell>TransTrack [29]</cell><cell></cell></row><row><cell>TrackFormer [18]</cell><cell></cell></row><row><cell>MOTR (ours)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of chosen datasets for evaluation.</figDesc><table><row><cell>Datasets</cell><cell cols="3">Class Frame Video ID</cell></row><row><cell cols="2">DanceTrack [28] 1</cell><cell cols="2">106k 100 990</cell></row><row><cell>MOT17 [19]</cell><cell>1</cell><cell>11k</cell><cell>14 1342</cell></row><row><cell>BDD100K [41]</cell><cell>8</cell><cell cols="2">318k 1400 131k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison between MOTR and existing methods on the MOT17 dataset under the private detection protocols. The number is marked in bold if it is the best among the Transformer-based methods.</figDesc><table><row><cell>Methods</cell><cell>HOTA?</cell><cell>AssA?</cell><cell>DetA?</cell><cell>IDF1?</cell><cell>MOTA?</cell><cell>IDS?</cell></row><row><cell>CNN-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tracktor++[2]</cell><cell>44.8</cell><cell>45.1</cell><cell>44.9</cell><cell>52.3</cell><cell>53.5</cell><cell>2072</cell></row><row><cell>CenterTrack[44]</cell><cell>52.2</cell><cell>51.0</cell><cell>53.8</cell><cell>64.7</cell><cell>67.8</cell><cell>3039</cell></row><row><cell>TraDeS [40]</cell><cell>52.7</cell><cell>50.8</cell><cell>55.2</cell><cell>63.9</cell><cell>69.1</cell><cell>3555</cell></row><row><cell>QDTrack [20]</cell><cell>53.9</cell><cell>52.7</cell><cell>55.6</cell><cell>66.3</cell><cell>68.7</cell><cell>3378</cell></row><row><cell>GSDT [35]</cell><cell>55.5</cell><cell>54.8</cell><cell>56.4</cell><cell>68.7</cell><cell>66.2</cell><cell>3318</cell></row><row><cell>FairMOT[43]</cell><cell>59.3</cell><cell>58.0</cell><cell>60.9</cell><cell>72.3</cell><cell>73.7</cell><cell>3303</cell></row><row><cell>CorrTracker [32]</cell><cell>60.7</cell><cell>58.9</cell><cell>62.9</cell><cell>73.6</cell><cell>76.5</cell><cell>3369</cell></row><row><cell>GRTU [33]</cell><cell>62.0</cell><cell>62.1</cell><cell>62.1</cell><cell>75.0</cell><cell>74.9</cell><cell>1812</cell></row><row><cell>MAATrack [27]</cell><cell>62.0</cell><cell>60.2</cell><cell>64.2</cell><cell>75.9</cell><cell>79.4</cell><cell>1452</cell></row><row><cell>ByteTrack [42]</cell><cell>63.1</cell><cell>62.0</cell><cell>64.5</cell><cell>77.3</cell><cell>80.3</cell><cell>2196</cell></row><row><cell>Transformer-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TrackFormer [18]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>63.9</cell><cell>65.0</cell><cell>3528</cell></row><row><cell>TransTrack[29]</cell><cell>54.1</cell><cell>47.9</cell><cell>61.6</cell><cell>63.9</cell><cell>74.5</cell><cell>3663</cell></row><row><cell>MOTR (ours)</cell><cell>57.8</cell><cell>55.7</cell><cell>60.3</cell><cell>68.6</cell><cell>73.4</cell><cell>2439</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison between MOTR and existing methods on the DanceTrack<ref type="bibr" target="#b27">[28]</ref> dataset. Results for existing methods are from DanceTrack<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell>Methods</cell><cell>HOTA</cell><cell>AssA</cell><cell>DetA</cell><cell>MOTA</cell><cell>IDF1</cell></row><row><cell>CenterTrack [44]</cell><cell>41.8</cell><cell>22.6</cell><cell>78.1</cell><cell>86.8</cell><cell>35.7</cell></row><row><cell>FairMOT [43]</cell><cell>39.7</cell><cell>23.8</cell><cell>66.7</cell><cell>82.2</cell><cell>40.8</cell></row><row><cell>QDTrack [20]</cell><cell>45.7</cell><cell>29.2</cell><cell>72.1</cell><cell>83.0</cell><cell>44.8</cell></row><row><cell>TransTrack [29]</cell><cell>45.5</cell><cell>27.5</cell><cell>75.9</cell><cell>88.4</cell><cell>45.2</cell></row><row><cell>TraDes [40]</cell><cell>43.3</cell><cell>25.4</cell><cell>74.5</cell><cell>86.2</cell><cell>41.2</cell></row><row><cell>ByteTrack [42]</cell><cell>47.7</cell><cell>32.1</cell><cell>71.0</cell><cell>89.6</cell><cell>53.9</cell></row><row><cell>MOTR (ours)</cell><cell>54.2</cell><cell>40.2</cell><cell>73.5</cell><cell>79.7</cell><cell>51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison between MOTR and existing methods on the BDD100k[41] validation set.</figDesc><table><row><cell>Methods</cell><cell>mMOTA</cell><cell>mIDF1</cell><cell>IDSw</cell></row><row><cell>Yu et al. [41]</cell><cell>25.9</cell><cell>44.5</cell><cell>8315</cell></row><row><cell>DeepBlueAI [1]</cell><cell>26.9</cell><cell>/</cell><cell>13366</cell></row><row><cell>MOTR (ours)</cell><cell>32.0</cell><cell>43.5</cell><cell>3493</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table 6ashows the impact of integrating different components. Integrating our components into the baseline can gradually improve overall performance. Using only object query of as original leads to numerous IDs since most objects are treated as entrance objects. With track query introduced, the baseline is able to handle tracking association and improve IDF1 from 1.2 to 49.8. Further, adding TAN to the baseline improves MOTA by 7.8% and IDF1 by 13.6%. When using CAL during training, there are extra 8.3% and 7.1% improvements in MOTA and IDF1, respectively. It demonstrates that TAN combined with CAL can enhance the learning of temporal motion. Collective Average Loss. Here, we explored the impact of video sequence length on the tracking performance in CAL. As shown inTable 6b, when the length of the video clip gradually increases from 2 to 5, MOTA and IDF1 metrics are improved by 8.3% and 7.1%, respectively. Thus, multi-frame CAL can greatly boost the tracking performance. We explained that multiple frames CAL can help the network to handle some hard cases such as occlusion scenes. We observed that duplicated boxes, ID switches, and object missing in occluded scenes are significantly reduced. To verify it, we provide some visualizations inFig. 5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies on our proposed MOTR. All experiments use the single-level C5 feature in ResNet50. The exploration of different combinations of ?ex and ?en in QIM network.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) The impact of increasing video clip</cell></row><row><cell cols="7">(a) The effect of our contributions. TrackQ:</cell><cell cols="3">length in Collective Average Loss dur-</cell></row><row><cell cols="7">track query. TAN: temporal aggregation net-</cell><cell cols="3">ing training on tracking performance.</cell></row><row><cell cols="5">work. CAL: collective average loss.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">TrackQ TAN CAL MOTA? IDF1? IDS?</cell><cell cols="3">Length MOTA? IDF1? IDS?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell cols="3">1.2 33198</cell><cell>2</cell><cell cols="2">44.9</cell><cell>63.4</cell><cell>257</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>37.1</cell><cell>49.8</cell><cell>562</cell><cell></cell><cell>3</cell><cell cols="2">51.6</cell><cell>59.4</cell><cell>424</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>44.9</cell><cell>63.4</cell><cell>257</cell><cell></cell><cell>4</cell><cell cols="2">50.6</cell><cell>64.0</cell><cell>314</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>47.5</cell><cell>56.1</cell><cell>417</cell><cell></cell><cell>5</cell><cell cols="2">53.2</cell><cell>70.5</cell><cell>155</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>53.2</cell><cell cols="2">70.5 155</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">(c) Analysis on random track query eras-</cell><cell cols="4">(d) Effect of random false positive inserting</cell></row><row><cell cols="5">ing probability p drop during training.</cell><cell></cell><cell cols="4">probability pinsert during training.</cell></row><row><cell>pdrop</cell><cell cols="2">MOTA?</cell><cell>IDF1?</cell><cell>IDS?</cell><cell></cell><cell></cell><cell>pinsert</cell><cell cols="2">MOTA?</cell><cell>IDF1?</cell><cell>IDS?</cell></row><row><cell>5e-2</cell><cell></cell><cell>49.0</cell><cell>60.4</cell><cell>411</cell><cell></cell><cell></cell><cell>0.1</cell><cell>51.2</cell><cell>71.7</cell><cell>148</cell></row><row><cell>0.1</cell><cell></cell><cell>53.2</cell><cell>70.5</cell><cell>155</cell><cell></cell><cell></cell><cell>0.3</cell><cell>53.2</cell><cell>70.5</cell><cell>155</cell></row><row><cell>0.3</cell><cell></cell><cell>51.1</cell><cell>69.0</cell><cell>180</cell><cell></cell><cell></cell><cell>0.5</cell><cell>52.1</cell><cell>62.0</cell><cell>345</cell></row><row><cell>0.5</cell><cell></cell><cell>48.5</cell><cell>62.0</cell><cell>302</cell><cell></cell><cell></cell><cell>0.7</cell><cell>50.7</cell><cell>57.7</cell><cell>444</cell></row><row><cell>(e) ? ex</cell><cell cols="6">0.6 0.6 0.6 0.5 0.6 0.7</cell><cell cols="3">(f) The effect of random sampling in-terval on tracking performance. Intervals MOTA? IDF1? IDS?</cell></row><row><cell>? en</cell><cell cols="6">0.7 0.8 0.9 0.8 0.8 0.8</cell><cell>3</cell><cell></cell><cell>53.2</cell><cell>64.8 218</cell></row><row><cell cols="7">MOTA? 52.7 53.2 53.1 53.5 53.2 52.8</cell><cell>5</cell><cell></cell><cell>50.8</cell><cell>62.8 324</cell></row><row><cell cols="7">IDF1? 69.8 70.5 70.1 70.5 70.5 68.3</cell><cell cols="2">10</cell><cell>53.2</cell><cell>70.5 155</cell></row><row><cell>IDS?</cell><cell cols="6">181 155 142 153 155 181</cell><cell cols="2">12</cell><cell>53.1</cell><cell>69</cell><cell>158</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://competitions.codalab.org/competitions/24910" />
		<title level="m">CodaLab Competition -CVPR 2020 BDD100K Multiple Object Tracking Challenge</title>
		<imprint>
			<date type="published" when="2022-07" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Online; accessed 19</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High-speed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AVSS</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sign language transformers: Joint end-to-end sign language recognition and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end multispeaker speech recognition with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00194</idno>
		<title level="m">Transmot: Spatial-temporal graph transformer for multiple object tracking</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning by tracking: Siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hota: A higher order metric for evaluating multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<title level="m">Trackformer: Multiobject tracking with transformers</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Quasi-dense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep network flow for multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICRA</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07786</idno>
		<title level="m">Multi-object tracking with siamese track-rcnn</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modelling ambiguous assignments for multi-person tracking in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dancetrack: Multi-object tracking in uniform appearance and diverse motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14690</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurlPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurlPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple object tracking with correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A general recurrent tracking framework without real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint object detection and multi-object tracking with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An introduction to the kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Track to detect and segment: An online multi-object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06864</idno>
		<title level="m">Bytetrack: Multi-object tracking by associating every detection box</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>In: ICLR (2020) 1, 3, 6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
