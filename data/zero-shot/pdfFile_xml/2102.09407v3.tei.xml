<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Rational Activations to Boost Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Delfosse</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schramowski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Mundt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Molina</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
						</author>
						<title level="a" type="main">Adaptive Rational Activations to Boost Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. Specifically, neural plasticity should be critical in the context of constantly changing reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are particularly suitable for adaptable activation functions in deep neural networks. Inspired by residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version. The proposed joint rational activation allows for desirable degrees of flexibility, yet regularises plasticity to an extent that avoids overfitting by leveraging a mutual set of activation function parameters across layers. We demonstrate that equipping popular algorithms with (joint) rational activations leads to consistent improvements on Atari games, notably making DQN competitive to DDQN and Rainbow. 1 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Networks' efficiency in approximating any function has made them the most used approximation function for many machine learning tasks. This is no different in reinforcement learning (RL), where the introduction of the DQN algorithm <ref type="bibr" target="#b30">(Mnih et al., 2015)</ref> has sparked the development of various neural solutions. In concurrence with neuroscientific explanations of brainpower residing in com- Each row corresponds to a state where a new, more challenging part of the environment (right column, e.g. increasing enemy speed and complexity) has been uncovered and is additionally trained on.</p><p>binations stemming from trillions of connections <ref type="bibr" target="#b11">(Garlick, 2002)</ref>, present advances have emphasised the role of the neural architecture <ref type="bibr" target="#b24">(Liu et al., 2018;</ref><ref type="bibr" target="#b41">Xie et al., 2019)</ref>. As such, RL improvements have first been mainly obtained through a focus on enhancing algorithms <ref type="bibr" target="#b31">(Mnih et al., 2016;</ref><ref type="bibr" target="#b18">Haarnoja et al., 2018;</ref><ref type="bibr" target="#b1">Banerjee et al., 2021)</ref> and only recently by searching for well-performing architectural patterns <ref type="bibr" target="#b28">(Miao et al., 2021)</ref>.</p><p>However, research has also progressively shown that individual neurons shoulder more complexity than initially expected, with the latest results demonstrating that dendritic compartments can compute complex functions (e.g. XOR) <ref type="bibr" target="#b13">(Gidon et al., 2020)</ref>, previously categorised as unsolvable by single-neuron systems. This finding seems to have renewed interest in activation functions <ref type="bibr" target="#b12">(Georgescu et al., 2020;</ref><ref type="bibr">Misra, 2020)</ref>. In fact, many functions have been adopted across different domains <ref type="bibr" target="#b34">(Redmon et al., 2016;</ref><ref type="bibr">Brown et al., 2020;</ref><ref type="bibr">Schulman et al., 2017)</ref>. To reduce the bias introduced by a fixed activation function and achieve higher expressive power, one can further learn which activation function is performant for a particular task <ref type="bibr" target="#b44">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b24">Liu et al., 2018)</ref>, learn to combine arbitrary families of activation functions <ref type="bibr" target="#b26">(Manessi &amp; Rozza, 2018)</ref>, or find coefficients for polynomial activations as weights to be optimised <ref type="bibr">(Goyal et al., 2019)</ref>.</p><p>Whereas these prior approaches have all contributed to their respective investigated scenarios, there exists a finer approach that elegantly encapsulates the challenges brought on by reinforcement learning problems. Specifically, we can learn rational activation functions <ref type="bibr">(Molina et al., 2020)</ref>. Not only can rational activation functions converge to any continuous function, but they have further been proven to be better approximants than polynomials in terms of convergence <ref type="bibr" target="#b38">(Telgarsky, 2017)</ref>. Even more crucially, their ability to adapt while learning equips a model with high neural plasticity, i.e. capability to adjust to the environment and its transformations <ref type="bibr" target="#b11">(Garlick, 2002)</ref>. We argue that adapting to environmental changes is essential, making rational activation functions particularly suitable for dynamic RL environments. To provide a visual intuition, we showcase an exemplary evolution of two rational activation functions together with their respective changing input distributions in the dynamic "Time Pilot" environment in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In this work, we propose the use of plasticity via rational activations in deep RL agents, as a central element to satisfy the requirements originating from diverse and dynamic environments. Apart from demonstrating the suitability of adaptive activation functions, we further identify and address potential caveats to overcome remaining practical barriers. Our specific contributions are:</p><p>(i) We motivate why neural plasticity is a key aspect for Deep RL agents and that rational activations are adequate as adaptable activation functions. For this purpose, we not only highlight that rational activation functions adapt their parameters over time, but further prove that they can dynamically embed residual connections, which we refer to as residual plasticity.</p><p>(ii) As the introduction of additional representational capacity could hinder generalisation, we then propose a joint rational variant. By making use of weight-sharing across rational activations in different layers, we thus include regularisation, necessary for RL <ref type="bibr" target="#b10">(Farebrother et al., 2018;</ref><ref type="bibr">Roy et al., 2020;</ref><ref type="bibr">Yarats et al., 2021)</ref>.</p><p>(iii) We empirically demonstrate that the inclusion of rational activations brings significant improvements to DQN and Rainbow algorithms on Atari games and that our joint variant further increases performance.</p><p>(iv) Finally, we investigate the overestimation phenomenon of predicting too large return values, which has previously been argued to originate from an unsuitable representational capacity of the learning architecture <ref type="bibr" target="#b39">(van Hasselt et al., 2016)</ref>. As a result of our introduced (rational) neural and residual plasticity, such overestimation can practically be reduced.</p><p>We proceed as follows. We start off by arguing in favour of plasticity for deep reinforcement learning. Then we show how rational activation functions provide a particularly suitable candidate to provide plasticity in neural networks and present our empirical evaluation. Before concluding, we touch upon related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Plasticity via Rational Activation Functions for Deep RL</head><p>Let us start by arguing why deep reinforcement learning agents require extensive plasticity and show that parametric rational activation functions provide appropriate means to add such additional plasticity to the network.</p><p>As motivated in the introduction, RL is subject to inherent distribution shifts. During the training process, agents progressively uncover new states (input drift) and, as the policy improves, the cumulative reward signal is modified (output drift). More precisely, for input drifts, we can distinguish environments according to how much they change through learning. For simplicity, we categorise according to three intuitive categories: stationary, dynamic and progressive environments. To illustrate, consider the example of Atari 2600 games. Kangaroo, Space Invaders and Tennis can be characterised as stationary since the game's input distribution does not change significantly. Asterix, Enduro and Q*bert are dynamic environments, as different inputs are provided to the agents early in the game; hence no policy improvement is required to uncover (input) distribution shifts. On the contrary, Jamesbond, Seaquest, and Time Pilot are progressive environments. The agent needs to master early stages before being provided with additional states, i.e. exposed to a significant shift in the input distribution.</p><p>How do we efficiently improve RL agents' ability to adapt to environments and their changes? To deal with these distribution shifts, our agents require high neural plasticity and thus benefit from adaptive architectures. To elaborate further in our work, let us consider the popular DQN algorithm <ref type="bibr" target="#b30">(Mnih et al., 2015)</ref>, that employs a ?-parameterised neural network to approximate the Q-value function of a state S t and action a. This network is updated following the Q-learning equation: Q (S t , a; ?) ? R t+1 + ? max a Q (S t+1 , a; ?). In <ref type="figure">Figure 2</ref>. Neural plasticity is essential for reinforcement learning. Human normalised mean scores for rigid DQN agents, agents with non-rational, rational, tempered, and regularised plasticity are shown with standard deviation across 5 random seeded experimental repetitions. Larger scores are better. Tempered plasticity, allowing initial adaptation to the environments, but not their transformations in experimental repetitions, performs better on stationary environments. Regularised plasticity performs well across all environment types. addition to network connectivity playing an important role, we now highlight the importance of individual neurons by modifying the network architecture of the algorithm via the use of learnable activation functions, to show that they are a presently underestimated component. To emphasise the utility of the upcoming proposed rational and joint rational activation functions, we will interleave early results into this section. The latter serves the primary purpose to not only motivate the suitability of the rational parameterisation to provide plasticity, but also discern the individual benefits of (joint-) rational activations, in the spirit of ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Rational Neural Plasticity</head><p>A rational function of order (m, n) is a universal approximator, defined by:</p><formula xml:id="formula_0">R(x) = P(x) Q(x) = m j=0 a j x j 1 + n k=1 b k x k ,<label>(1)</label></formula><p>where a j and b k are learnable parameters (corresponding to m+n+1 parameters in total). In <ref type="figure">Fig. 2</ref>, we show that our proposition to use such a rational parametrisation substantially enhances RL agents. More precisely, by comparing agents with rigid networks (a fixed Leaky ReLU baseline) to agents with rational plasticity (i.e. with a rational activation function at each layer), we see that using rational functions boosts the agents to superhuman performances on 7 out of 9 games. The acquired extra neural plasticity seems to play a significant role in these Atari environments, especially in progressive ones.</p><p>In order to discern the benefits of general plasticity through any adaptive activation function, over the proposed use of the rational parametrisation, <ref type="figure">Fig. 2</ref> additionally includes PELU <ref type="bibr" target="#b39">(Trottier et al., 2017)</ref>. PELU is a parametrised ELU that uses 3 parameters to control its slope, saturation and exponential decay, and has been shown to outperform other learnable alternatives on classification tasks <ref type="bibr" target="#b14">(Godfrey, 2019)</ref>.</p><p>However, in contrast to the rational parameterisation, it seems to fall behind and only boosts the agents to superhuman performance on 3 out of 9 games (contrary to 7). This implies that the type of parameterisation provided by the rational activation choice is particularly suitable.</p><p>To highlight the desirability of rational activations even further, we additionally distinguish between the plasticity of agents towards their specific environment and the plasticity allowing them to adapt while the environment itself is changing. To this end, we also show agents with rational activations that are tempered in <ref type="figure">Fig. 2</ref>. They have been extracted from agents with rational plasticity, that adapted to their specific environment. The plasticity of the rational activation functions is then tempered ("stopped") in repeated application to emphasise the necessity to continuously adapt during training. Whereas providing agents with such tempered, tailored to the task, activations already boosts performances, rational plasticity at all times seems essential, particularly in the dynamic and progressive environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Rational Residual Plasticity</head><p>The prior paragraphs have showcased the advantage of agents equipped with rational activation functions, rooted in their ability to update their parameters over time. However, we argue that the observed boost in performance is not only due to parameters adapting to distributional drifts. In addition to neural plasticity, rational activations can embed one of the most popular techniques to stabilise training of deep neural networks; namely, they can dynamically make use of a residual connection. We refer to this as residual plasticity.</p><p>Rationals are closed under residual connection and provide residual plasticity. We here show that using a residual connection together with a rational function is equivalent to using a rational with strictly higher degree in the numerator.</p><p>Theorem 2.1. Let R be a rational activation function of order (m, n). If m &gt; n, then R embeds a residual connection.</p><p>Proof. Let us consider a rational function R = P/Q of order (m, n), with coefficients A [m] = (a j ) m j=0 ? R m+1 of P and</p><formula xml:id="formula_1">B [n] = (b i ) n i=0 ? R n+1 of Q (with b 0 = 1)</formula><p>. We denote by ? (resp. ) the Hadamard product (resp. division). Let X ? R n1?????nx be a tensor corresponding to the input of the rational function of an arbitrary layer in a given neural network. We derive</p><formula xml:id="formula_2">X ?k = k i=1 X. Furthermore, we use GV [k] (X) = [1, X, X ?2 , ..., X ?k ] ? R (n1?????nx)?k+1</formula><p>to denote the tensor containing the powers up to k of the tensor X. Note that GV <ref type="bibr">[k]</ref> can be understood as a generalised Vandermonde tensor, similar as introduced in <ref type="bibr" target="#b42">(Xu et al., 2016)</ref>.</p><formula xml:id="formula_3">For V [k] = (v i ) k i=0 ? R k+1 , let GV [k] .V [k] = k</formula><p>i=0 v i X ?i be the weighted sum over the tensor elements of the last dimension. Now, we apply the rational activation function R with residual connection to X:</p><formula xml:id="formula_4">y(X) = R(X) + X = GV [m] (X).A [m] GV [n] (X).B [n] + X = (GV [m] (X).A [m] + X ? GV [n] (X).B [n] ) GV [n] (X).B [n] = (GV [m] (X).A [m] + GV [n+1] (X).B [n+1] 0 ) GV [n] (X).B [n] = GV [max(m,n+1)] (X).C [max(m,n+1)] GV [n] (X).B [n] = R(X), where B [n+1] 0 = (b 0,i ) n+1 i=0 ? R n+2 (with b 0,0 = 0 and b 0,i = b i?1 for i ? {1, ..., n + 1}), C [max(m,n+1)] = (c j ) max(m,n+1) j=0</formula><p>with c j = a j + b j?1 , a j = 0 for all j / ? {0, ..., m} and b j = 0 for all j / ? {0, ..., n}. R is a rational function of order (m , n ), with m &gt; n .</p><p>In other words, rational activation functions of order m &gt; n can embed residual connections if necessary. Using the same degrees for numerator and denominator certifies asymptotic stability, but our derived configuration allows rationals to implicitly use residual connections. Importantly, note that these potential residual connections are not rigid, as these functions can progressively learn a j = 0 for all j &gt; n, i.e. we have residual plasticity.</p><p>Rationals to potentially replace residual blocks. Recall that residual neural networks (ResNets) were initially introduced following the intuition that it is easier to optimise the residual mapping than to optimise the original, unreferenced mapping <ref type="bibr" target="#b20">(He et al., 2016)</ref>. Formally, residual blocks of ResNets propagate an input X through two paths: a transforming block of layers that preserves the dimensionality (F ) and a residual connection (identity). In very deep ResNets, it has been observed that feature re-combinations does not occur inside the residual blocks but that transitions to new levels of representations occur during dimensionality changes <ref type="bibr" target="#b40">(Veit et al., 2016;</ref><ref type="bibr" target="#b17">Greff et al., 2017)</ref>.</p><p>To investigate this hypothesis, Veit et al. have conducted lesioning experiments, where a residual block is removed from the network, and surrounding ones are fine-tuned to recover. Whereas we emphasize that we do not claim that the residual in rationals can replace entire convolutional blocks or that they are generally equivalent, we hypothesize that under the conditions investigated by Veit et al. of very deep networks, residual blocks could learn complex activation function-like behaviours. To test this conjecture, we repeat the lesioning experiments, but also test replacing the lesioned block with a rational function that satisfies the residual connection condition derived above. Results are provided in appendix (cf. A.1) and show that such rational functions can efficiently replace some residual blocks of very deep ResNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Natural Rational Regularisation</head><p>We have motivated and shown that the combination of neural and residual plasticity form the central pillars for why rational activation functions are desirable in deep RL. In particular for dynamic and progressive environments, rational plasticity has been observed to provide a substantial boost over alternatives. However, if we circle back to this figure and take a more careful look at the stationary environments, we can observe that our previously investigated tempered rational plasticity (for emphasis, initially allowed to tailor to the task but later "stopped" in experimental repetition) can also have an upper edge over full plasticity. The extra rational plasticity at all times might reduce the generalisation capabilities of its agents, particularly on non-diverse stationary environments. In fact, prior works have highlighted the necessity for regularisation methods in RL <ref type="bibr" target="#b10">(Farebrother et al., 2018;</ref><ref type="bibr">Roy et al., 2020;</ref><ref type="bibr">Yarats et al., 2021)</ref>.</p><p>We thus propose a naturally regularised rational activation version. For this regularised variant, we again draw inspiration from residual blocks. In particular, Greff et al. have indicated that sharing the weights can improve learning performances, as shown in Highway <ref type="bibr" target="#b25">(Lu &amp; Renals, 2016)</ref> and Residual Networks <ref type="bibr">(Liao &amp; Poggio, 2016)</ref>. In the spirit of these findings, we propose the regularised joint rationals, where the key idea is to constraint the input to propagate through different layers but always be activated by the same learnable rational activation function. Rational functions thus share a mutual set of parameters across the network. As observable in <ref type="figure">Fig. 2</ref>, this regularised form of plasticity increases the agents' scores in the stationary environments and does not deteriorate performances in the progressive ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Empirical Evidence for Plasticity</head><p>Our intention here is to investigate the benefits of neural plasticity through rational networks for deep reinforcement learning. That is, we investigated the following questions:</p><p>(Q1) Do neural networks equipped with rational plasticity outperform rigid baselines?</p><p>(Q2) Can neural plasticity make up for more heavy algorithmic RL advancements?</p><p>(Q3) Can plasticity address the overestimation problem?</p><p>(Q4) As plasticity can be obtained via adding parameters, how many more parameters would rigid networks need to measure up to rational ones?</p><p>To this end, we compare 3 our rational networks using the original DQN algorithm <ref type="bibr" target="#b30">(Mnih et al., 2015)</ref> on 15 different games of the Atari 2600 domain <ref type="bibr">(Brockman et al., 2017)</ref> and compare these architectures to ones equipped with the Leaky ReLU baseline, the learnable PELU, as well as SiLU (SiLU(x) = x ? sigmoid(x)) and its derivative dSiLU. <ref type="bibr">Elfwing et al.</ref> showed that SiLU or a combination of SiLU (on convolutional layers) and its derivative (on fully connected layers) perform better than ReLU in DQN agents on several games (2018). SiLU and dSiLU are -to our knowledge-the only activation functions specifically designed for RL applications. We then compare increased neural plasticity provided by (joint-)rational networks to algorithm improvements, namely the Double DQN (DDQN) method <ref type="bibr" target="#b39">(van Hasselt et al., 2016)</ref>, that tackles DQN's overestimation problem, as well as Rainbow <ref type="bibr" target="#b22">(Hessel et al., 2018)</ref>. Rainbow incorporates multiple algorithm improvements brought to DQN-Double Q-learning, prioritised experience replay, duelling network architecture, multi-step target, distributional learning and stochastic networks-and is widely used also as a baseline <ref type="bibr">(Lin et al., 2020;</ref><ref type="bibr">Hafner et al., 2021)</ref>. We further explain how neural plasticity can help readdress overestimation. Finally, we evaluate the number of additional weights needed by rigid networks to approximate rational ones.</p><p>In practice, we used safe rational activation functions <ref type="bibr">(Molina et al., 2020)</ref>, i.e. we used the absolute value of the sum in the denominator to avoid poles. This stabilises training and makes the function continuous without creating instabilities. Rationals are shared across layers (adding only 10 parameters per layer) or through the whole network for the regularised version. We base our experiments on the original neural networks used by the DQN, DDQN and SiLU authors and the same hyper-parameters (cf. Appendix A.6.2) across all the Atari agents. For a fair comparison, we report performances using the normalised (cf. Eq. 4 in Appendix) mean and standard deviation of the scores obtained by fully trained agents over five seeded reruns for every (D)DQN agent. However, since often only the best performing RL agent (among the reruns) is reported in the literature, we also provide tables of such scores (cf. Appendix A.2). For the Rainbow algorithm, we unfortunately can only report the results of single runs. A single run took more than 40 days on an NVIDIA Tesla V100 GPU; Rainbow is known to be computationally quite demanding (Obando-Ceron &amp; Castro, 2021).</p><p>(Q1) DQN with neural plasticity is better than rigid baselines. To start off, we compared RL agents with additional plasticity (from PELU and rationals) to rigid DQN baselines: Leaky ReLU, as well as agents equipped with SiLU and SiLU+dSiLU activation functions.</p><p>The results summarised in Tab. 1 confirm what our earlier figure had shown, but on a larger scale. As one can see, DQN agents with plasticity clearly outperform their rigid activation counterparts. Furthermore, RL agents with functions of the SiLU family outperform Leaky ReLU ones on less than half of the tested games. More importantly, they do not perform better than non-rigid rational networks. DQN with regularised plasticity even obtains higher mean scores than the non-regularised versions 9 out of 15 times. Actually, DQN with non-rational plasticity (PELU) obtained super-human performances on 6 out of 15 games, in contrast to the 11 out of 15 times of agents with rational plasticity.</p><p>This clearly shows that plasticity, and above all rational plasticity pays off for (deep) RL agents, providing an affirmative answer to Q1.</p><p>(Q2) DQN with neural plasticity can beat more complex RL approaches such as Rainbow. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the learning curves of Rainbow and DQN, both with Leaky ReLU baselines, as well as with full and regularised plasticity. While Rainbow is computationally much heavier (? 8 times slower than DQN in our experiments, with higher memory needs), its rigid form never outperforms the much simpler and more efficient DQN with neural plasticity, and its rational versions dominate in only 1 out of 8 games (Enduro). Rainbow even lost to vanilla DQN on 3 games.</p><p>Therefore, DQN agents with rational plasticity are competitive alternatives to the complicated and expensive Rainbow method, answering question (Q2) affirmatively.</p><p>(Q3) Neural plasticity directly tackles the overestimation problem. Revisiting <ref type="figure" target="#fig_2">Fig. 3</ref>  We have compared the original DDQN approach (i.e. equipped with Leaky ReLU), which is rigid, to vanilla DQN with neural plasticity on Atari games. As one can see in Tab. 1, DQN with rational plasticity even outperforms the more complex DDQN algorithm on every considered Atari game. This reinforces the affirmative answer to (Q1) from earlier on.</p><p>More importantly, we have computed the relative overestimation values of the (D)DQN, both with and without neural plasticity, following:</p><formula xml:id="formula_5">overestimation = Q-value ? R R ,<label>(2)</label></formula><p>where the return R corresponds to R = ? t=0 ? t r t , with the observed reward r t and the discount factor ?.</p><p>The results are summarised in <ref type="figure" target="#fig_3">Fig. 4</ref>. As one can see, plasticity helps to reduce overestimation drastically. The game for which DDQN substantially reduces the overestimation are Jamesbond, Kangaroo, Tennis, Time Pilot and Seaquest. For these games, DDQN obtains the best performances among all rigid variants only on Jamesbond (cf. Tab. 1). Moreover, <ref type="figure" target="#fig_2">Fig. 3</ref> reveals that the performance drops of corresponding DDQN rigid agents are only delayed and not prevented. The performance drops thus happen after the 200th epoch, after which the training of RL agents is usually stopped.</p><p>Whereas we agree that overestimation might play a role in the performance drops on progressive environments (cf. <ref type="figure" target="#fig_2">Fig. 3</ref>: Jamesbond, TimePilot and Seaquest), it cannot fully explain the phenomena. Instead, RL agents with higher neural plasticity can handle these games much better while only having a few more parameters. Hence, we advocate that neural plasticity better deals with distribution shifts due to the dynamics of games with progressive environments.</p><p>Perhaps surprisingly, (regularised) rational plasticity not only works well on challenging progressive environments but also on much simpler ones such as Enduro, Pong and Q*bert, where more flexibility is likely to hurt. Luckily, overestimation due to high flexibility does not happen here, as one can see in <ref type="figure" target="#fig_3">Fig. 4</ref>. Moreover, the activation functions learned for these games have a simpler profile than those learned on more complicated games like Kangaroo and Time Pilot (cf. Appendix A.5). The rational activation functions thus seem to adapt to the environment's complexity and the policy they need to model. This clearly provides an affirmative answer to (Q3).</p><p>(Q4) Adding parameters through rationals efficiently augments plasticity. Compared to rigid alternatives, the joint-rational networks embed 10 more parameters in total and always outperform (cf. Tab. 1) PELU ones (that have 12 additional parameters). Our proposed method to add plasticity via rational activation functions thus efficiently augments the capacity of the network. However, ReLU layers can theoretically approximate rational functions <ref type="bibr" target="#b38">(Telgarsky, 2017)</ref>. Augmenting the number of layers (or neurons per layer) is thus, theoretically, a costly alternative to augment the plasticity. How many parameters are practically needed in rigid networks to obtain similar performances? Searching for bigger equivalent architectures for RL agents is tedious, as RL training curves possess considerably more variance and noise than SL ones <ref type="bibr" target="#b28">(Miao et al., 2021)</ref>, but this question is not restricted to reinforcement learning. We thus answer it by highlighting the generality of our insights, demonstrated by further investigation on a classification scenario, in addition to the RL experiments of Tab 1. We report our findings in Tab. 2, where networks with rational activations are shown to not only outperform Leaky ReLU ones at the same amount of parameters, but also to outperform deeper and more heavily parametrised neural networks (indicated by the pairs of colours). For example, a rational activated VGG4 not only performs better than a rigid Leaky ReLU VGG4 at 1.37M parameters, but even performs similarly to the 4.71M parameters VGG6 that uses Leaky ReLU.</p><p>All experimental results together clearly show that neural plasticity through rational networks considerably benefits deep reinforcement learning in highly efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Next to the related work discussed throughout the paper, our work on plasticity is also related to research lines on neural architecture search and activation functions in deep reinforcement learning.</p><p>Neural Architectures for Deep Reinforcement Learning. Cobbe et al. showed that the architecture of IMPALA <ref type="bibr" target="#b9">(Espeholt et al., 2018)</ref>, notably containing residual blocks, improved the performances over the original Nature-CNN network used in <ref type="bibr">DQN (2019)</ref>. Motivated by these findings, <ref type="bibr" target="#b28">(Miao et al., 2021)</ref> recently applied neural architecture search to RL tasks and demonstrated that the optimal architecture and its complexity highly depend on the environment. Their search provides different architectures for different environments, with varying activation functions across layers and potential residual connections. Continuously modifying the complexity of the neural network based on the noisy reward signal in a complex architectural space is extremely resource demanding, particularly for large scale problems. Many RL specific problems, such as noisy rewards <ref type="bibr" target="#b21">(Henderson et al., 2018)</ref>, input interdependency <ref type="bibr" target="#b30">(Mnih et al., 2015)</ref>, policy instabilities <ref type="bibr" target="#b18">(Haarnoja et al., 2018)</ref>, sparse rewards, difficult credit assignment <ref type="bibr">(Mesnard et al., 2021)</ref>, complicate an automated architecture search.</p><p>The choice of activation functions. Many different activation functions have been adopted across different domains (e.g. Leaky ReLU in YOLO <ref type="bibr" target="#b34">(Redmon et al., 2016)</ref>, Tanh in PPO <ref type="bibr">(Schulman et al., 2017)</ref>, GELU in GPT-3 <ref type="bibr">(Brown et al., 2020)</ref>), indicating that the relationship between the choice of activation functions and performance of neural net-works is highly dependent on the task, architecture, hyperparameters and the dataset (or environment). As shown in this paper, parametric functions provide plasticity. <ref type="bibr">Molina et al.</ref> showed that rationals outperform other learnable activation function types on a set of SL tasks (2020). Telgarsky showed that rationals are locally better approximants than polynomials (2017). Finally, Boull? et al. later showed that composing low-degree rational functions require fewer parameters to approximate a ReLU Network (2020). While this is indeed more efficient in terms of parameters, compositions of rationals functions need to be computed sequentially, which is slower when using gradient descent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>We have shown the benefits of rational activation functions for deep RL as a consequence of both their neural and residual plasticity. In our derivation for closure under residuals, we have deduced that the degree of the polynomial in the numerator needs to be greater than that of the denominator. Correspondingly, we have based our empirical investigations on the degrees (5, 4). Interesting future work would be to further automatically select suitable degrees that fulfil this condition. As additional future avenues, one should also explore neural plasticity in more advanced deep RL approaches, including short term memory <ref type="bibr" target="#b23">(Kapturowski et al., 2019)</ref>, finer exploration strategy <ref type="bibr">(Badia et al., 2020)</ref>, or include neural plasticity into architecture search, without having to perform a combinatorial search across possible activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have highlighted the central role of neural plasticity in deep reinforcement learning algorithms, and have motivated the use of rational activation functions, as a lightweight way of boosting RL agents performances. We derived a condition, under which rational functions embed residual connections. Then the naturally regularised joint-rational activation function was developed inspired by weight sharing in residual networks.</p><p>The simple DQN algorithm equipped with these (regularised) rational forms of plasticity becomes a competitive alternative to more complicated and costly algorithms, such as Double DQN and Rainbow. Fortunately, the complexity of these rational functions also seem to automatically adapt to the one of the environment used for training. Their use could be a substitute for more expensive architectural searches. We thus hope that they will be adopted in future deep reinforcement learning algorithms, as they can provide agents with the necessary neural plasticity required by stationary, dynamic and progressive reinforcement learning environments.</p><p>Liao, Q. and Poggio, T. A. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. CoRR, abs/1604.03640, 2016.</p><p>Lin, Z., Wu, Y., Peri, S. V., Sun, W., Singh, G., Deng, F., Jiang, J., and Ahn, S. SPACE: unsupervised objectoriented scene representation via spatial attention and decomposition. In 8th International Conference on Learning Representations (ICLR), 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>As mentioned in the main body, the appendix contains additional materials and supporting information for the following aspects: results on replacing residual blocks with rational activation functions (A.1), every final and maximal scores obtained by the reinforcement learning agents used in our experiments (A.2), the evolutions of these scores (A.3), the different environment types with illustrations of their changes (A.4), graphs of the learned rational activation functions (A.5) and technical details for reproducibility (A.6).</p><p>A.1. Residual block learn of deep ResNet learn activation function-like behavior.</p><p>We present in this section lesioning experiments, where a residual block is lesioned from a pretrained Residual Network, and the surrounding blocks are fine-tuned (with a learning rate of 0.001) for 15 epochs. These lesioning experiments were first conducted by <ref type="bibr" target="#b40">Veit et al. (2016)</ref>. We also perform rational lesioning, where we replace a block by an (identity initialised) 4 rational activation function (instead of removing the block), and train the activation function along with the surrounding blocks. The used rational functions have the same order as in every other experiment ((m, n) = (5, 4)), that satisfies the rational residual property derive in the paper). We report recovery percentages, computed following:</p><formula xml:id="formula_6">recovery = 100 ? finetuned ? surgered original ? surgered .<label>(3)</label></formula><p>We also provide the amount of dropped parameters of each lesioning. As the goal is to show that flexible rational functions can achieve similar modelling capacities to the residual blocks, we did not apply regularisation methods and mainly focused on training accuracies. We can clearly observe that rational activation functions lead to performance improvements that even surpass the original model, or are able to maintain performances when the amount of dropped parameters rises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Complete raw scores table for Deep Reinforcement Learning</head><p>Through this work, we showed the performance superiority of reinforcement learning agents that embed additional plasticity provided by learnable rational activation functions. We used human normalised scores (cf. Eq. 4) for readability. For completeness, we provide in this section the final raw scores of every trained agent. As many papers provide the maximum obtained score among every epoch and every agent, even if we consider it to be an inaccurate and noisy indicator of the performances, for which random actions can still be taken (because of -greedy strategy also being used in evaluation). A fairer indicator to compare methods is the mean score. We thus also provide final mean scores (of agents retrained among 5 seeded reruns) with standard deviation. We start off by providing the human scores used for normalisation (provided by <ref type="bibr">van Hasselt et al., in</ref>     <ref type="figure">Figure 6</ref>. Images extracted from DQN agents with full plasticity playing the set of 15 Atari 2600 games used in this paper. Stationary environments (e.g. Pong, Video Pinball) do not evolve during training, dynamic ones provide different input/output distributions that are early accessible in the game (e.g Q*bert, Enduro) and progressive ones (e.g. Jamesbond, Time Pilot) require the agent to improve for the it to evolve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Learned rational activation functions</head><p>We have explained in the main text how rational functions of agents used on different games can exhibit different complexities. This section provides the learned parametric rational functions learned by DQN agents with full plasticity (left) and by those with regularised plasticity (right) after convergence for every different tested game of the gym Atari 2600 environment. Kernel Density Estimations (with Gaussian kernels) of input distributions indicates where the functions are most activated. Rational functions from agents trained on simpler games (e.g. Enduro, Pong, Q*bert) have simpler profiles (i.e. fewer distinct extremas). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Technical details to reproduce the experiments</head><p>We here provide details on our experiments for reproducibility.</p><p>A.6.1. SUPERVISED LEARNING EXPERIMENTS For the lesioning experiment, we used an available 5 pretrained Residual Network (original). We then remove the corresponding block (and potentially replace it with an identity initialised rational activation function) (surgered). We finetune the new models, allowing for optimisation of the previous and next layers (and potentially the rational function) for 15 epochs with SGD (learning rate of 0.001).</p><p>For the classification experiments conducted on CIFAR10 and CIFAR100, we let every network learn for 60 epochs. We use SGD as the optimisation algorithm, with a learning rate of 0.02 and 128 as batch size. The VGG networks contain successive VGG blocks that all consist of n convolutional layers, i input channels and o output channels, stride 3 and padding 1, followed by an activation function, and 1 Max Pooling layer. For each used architecture, the (n, i, o) parameters of the successive blocks are:</p><p>? VGG4: (1, 3, 64) ? ? (1, 64, 128) ? ? <ref type="bibr">(2,</ref><ref type="bibr">128,</ref><ref type="bibr">256)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Neural plasticity due to trainable activation functions allows Deep RL agents to adapt to environments of increasing complexity. Rational activations (left columns), with shared parameters in the last two layers, evolve together with their input distributions (shaded blue) when learning with DQN on Time Pilot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure bestviewed in colour. A full description of the environments and their types is provided in Appendix A.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Networks with rational (Rat.) and regularized (Reg.) rational plasticity compared to rigid baselines (DQN, DDQN and Rainbow) over five random seeded runs on eight Atari 2600 games. The resulting mean scores (lines) and standard deviation (transparent area) during training are shown. As one can see, DDQN does not resolve performance drops but only delays them (e.g. particularly pronounced on Seaquest). A figure including the evolution of every agent on all Atari 2600 games is provided in Appendix A.3. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Plasticity naturally reduces overestimation. Relative overestimation values (log scale, smaller values are better, see Eq. 2) of both rigid DQN and DDQN, as well as DQN with rational and regularised rational plasticity. Each trained agent is evaluated on 100 completed games (5 random seeds per game per agent, i.e. 20 completed games for each seed). Agents with rational plasticity lower overestimation values as much or further than rigid DDQN ones, which has specifically been introduced to this end.Figure bestviewed in colour. drops. To mitigate this problem, van Hasselt et al. introduced a second network to separate action selection from action evaluation, resulting in Double DQN (DDQN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Smoothed (cf.  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Profiles (dark blue) and input distributions (light blue) of rational functions (left) and joint-rational ones (right) of DQN agents on the different tested games. (Joint-)rational functions from agents of simpler games have simpler profiles (i.e. fewer distinct extrema).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Neural plasticity leads to vast performance improvements. Normalised mean scores and standard deviations (in percentage, cf. Appendix A.6 for the equation) of rigid baselines (i.e. DQN and DDQN with Leaky ReLU, DQN with SiLU and SiLU + dSiLU), as well as DQN with plasticity: using PELU, rational (full) and joint-rational (regularised), are reported over five experimental random seeded repetitions (larger mean values are better). The best results are highlighted in bold and runner-ups denoted with ? markers. The last rows summarise the number of times best mean scores were obtained by each agent and the number of super-human performances.</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell>DQN</cell><cell></cell><cell>DDQN</cell><cell></cell><cell cols="2">DQN with Plasticity</cell></row><row><cell>Activation</cell><cell>LReLU</cell><cell>SiLU</cell><cell>d+SiLU</cell><cell>LReLU</cell><cell>PELU</cell><cell>rational</cell><cell>joint-rational</cell></row><row><cell>Asterix</cell><cell>1.85?1.2</cell><cell>0.52?0.6</cell><cell>2.14?1.4</cell><cell>48.9?17.7</cell><cell>25.8?3.7</cell><cell>242?23.5</cell><cell>168?32.6?</cell></row><row><cell>Battlezone</cell><cell>11.4?7.0</cell><cell cols="2">21.2?15.0 11.3?6.7</cell><cell>68.2?34.8</cell><cell cols="2">46.6?19.5 70.1?2.1?</cell><cell>77.4?8.7</cell></row><row><cell>Breakout</cell><cell>558?166</cell><cell cols="2">93.9?57.6 11.7?14.0</cell><cell>286?122</cell><cell>788?79.2</cell><cell cols="2">1134?130? 1210?36.0</cell></row><row><cell>Enduro</cell><cell>16.3?21.3</cell><cell cols="2">37.0?17.7 0.37?0.5</cell><cell>47.7?18.1</cell><cell cols="2">24.5?42.6 141?15.0</cell><cell>129?14.7?</cell></row><row><cell>Jamesbond</cell><cell>8.62?6.4</cell><cell>6.08?3.7</cell><cell>5.28?4.4</cell><cell>10.7?11.1</cell><cell cols="2">74.2?51.5 308?48.5?</cell><cell>312?59.5</cell></row><row><cell>Kangaroo</cell><cell>11.8?12.5</cell><cell cols="2">128?95.6? 13.9?18.5</cell><cell>17.2?14.5</cell><cell cols="2">57.7?14.6 107?43.1</cell><cell>193?86.8</cell></row><row><cell>Pong</cell><cell>101?5.5</cell><cell cols="2">96.1?12.0 104?3.3</cell><cell>91.3?30.8</cell><cell cols="3">106.4?2.2 107.0?2.4? 107.3?2.7</cell></row><row><cell>Qbert</cell><cell>55.4?17.1</cell><cell cols="2">14.2?17.0 2.74?0.2</cell><cell>74.0?21.7</cell><cell>101?6.6</cell><cell>120?2.8</cell><cell>117?4.9?</cell></row><row><cell>Seaquest</cell><cell>0.57?0.4</cell><cell>3.67?4.1</cell><cell>0.18?0.2</cell><cell>2.17?0.9</cell><cell>9.21?2.5</cell><cell>16.3?0.5?</cell><cell>18.4?3.3</cell></row><row><cell>Skiing</cell><cell cols="5">-90.7?37.9 -111?-0.7 -85.5?43.4 -86.9?46.6 -111?-.7</cell><cell>-59.5?60.7</cell><cell>-60.2?56.1?</cell></row><row><cell>Spaceinvaders</cell><cell>33.9?4.3</cell><cell cols="2">33.1?11.9 32.4?12.4</cell><cell>31.0?1.0</cell><cell cols="2">50.1?3.3? 42.3?3.1</cell><cell>95.1?17.7</cell></row><row><cell>Tennis</cell><cell>8.94?17.3</cell><cell cols="2">26.3?53.3 78.5?64.3</cell><cell>32.1?51.6</cell><cell>106?53.3</cell><cell cols="2">257.8?2.8? 258.3?5.2</cell></row><row><cell>Timepilot</cell><cell>14.9?14.3</cell><cell cols="2">19.3?31.0 18.3?38.1</cell><cell>6.61?7.5</cell><cell>124?26.1</cell><cell>341?105</cell><cell>253?11.0?</cell></row><row><cell>Tutankham</cell><cell>0.03?2.8</cell><cell cols="2">58.2?48.6 2.89?4.0</cell><cell>24.4?-0.4</cell><cell cols="2">91.6?29.3 130?10.7?</cell><cell>134?29.3</cell></row><row><cell>Videopinball</cell><cell>440?123</cell><cell cols="3">55.8?61.9 -4.03?32.5 626?241</cell><cell>299?168</cell><cell cols="2">1616?1026 906?539?</cell></row><row><cell># Wins</cell><cell>0/15</cell><cell>0/15</cell><cell>0/15</cell><cell>0/15</cell><cell>0/15</cell><cell>6/15</cell><cell>9/15</cell></row><row><cell cols="2"># Super-Human 3/15</cell><cell>1/15</cell><cell>1/15</cell><cell>2/15</cell><cell>6/15</cell><cell>11/15</cell><cell>11/15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Shallow Rational Networks perform as deeper Leaky ReLU ones. VGG networks with different numbers of layers are evaluated on CIFAR10 and CIFAR100. Rational VGG4 has similar performances as VGG6 network, with 3.5 times less parameters, and Rational VGG6 outperforms VGG8, with two times less parameters. Shaded colour pairs included for emphasis.</figDesc><table><row><cell cols="2">Architecture</cell><cell>VGG4</cell><cell>VGG6</cell><cell>VGG8</cell></row><row><cell cols="2">Activation</cell><cell cols="3">LReLU Rat. LReLU Rat. LReLU Rat.</cell></row><row><cell>C10</cell><cell cols="4">Train 83.0?.3 87.1?.6 86.9?.1 89.2?.2 86.7?.2 89.2?.1 Test 80.0?1. 84.3?.5 83.2?.6 85.4?.6 83.4?1. 85.8?.9</cell></row><row><cell>C100</cell><cell cols="4">Train 64.7?.8 70.2?.0 70.6?.6 75.4?.1 71.8?.3 74.3?.3 Test 56.6?.8 58.9?.6 59.0?.5 58.4?.3 57.1?1. 59.1?.9</cell></row><row><cell cols="2"># Net params</cell><cell>1.37M</cell><cell>4.71M</cell><cell>9.27M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Rational functions improve lesioning. The recovery percentages for finetuned networks after lesioning<ref type="bibr" target="#b40">(Veit et al., 2016)</ref> of a ResNet layer's (L) block (B) are shown. Residual blocks were lesioned, i.e. replaced with the identity (Base) or a rational from a pretrained ResNet101 (44M parameters). Then, the surrounding blocks (and implanted rational activation function) are retrained for 15 epochs. Larger percentages are better, best results are in bold.</figDesc><table><row><cell cols="2">Recovery (%) Lesioning</cell><cell cols="2">L2B3 L3B19 L3B22 L4B2</cell></row><row><cell>Training</cell><cell cols="2">Original (Veit et al., 2016) 100.9 90.5 100 Rational (ours) 101.1 104 120</cell><cell>58.9 91.1</cell></row><row><cell>Testing</cell><cell cols="3">Original (Veit et al., 2016) 93.1 97.1 81.6 81.7 Rational (ours) 90.5 97.6 91.5 85.3</cell></row><row><cell></cell><cell>% dropped params</cell><cell cols="2">0.63 2.51 2.51 10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 Table 6 .</head><label>56</label><figDesc>), then provide final mean and maximum obtained raw scores of every agent. Final mean and maximum obtained scores obtained by rigid Rainbow agents (i.e. using Leaky ReLU), as well as Rainbow with full (i.e. using rational activation functions) and regularised (i.e. using joint-rational ones) plasticity (only 1 run because of computational cost, larger values are better).A.3. Evolution of the scores on every gameThe main part present some graphs that compares performance evolutions of the Rainbow and DQN agents with plasticity, as well as Rigid DQN, DDQN and Rainbow agents. We provide hereafter the evolution of the scores of every tested DQN and the DDQN agents on the complete game set. DQN agents with higher plasticity are always the best-performing ones. Experiments on several games (e.g. Jamesbond, Seaquest) show that using DDQN does not prevent the agent's performance drop but only delays it.</figDesc><table><row><cell>Human scores used for normalisation:</cell></row></table><note>Asterix: 7536, Battlezone: 33030, Breakout: 27.9, Enduro: 740.2, Jamesbond: 368.5, Kangaroo: 2739, Pong: 15.5, Q*bert: 12085, Seaquest: 40425.8, Skiing: ?3686.6, Space Invaders: 1464.9, Tennis: ?6.7, Time Pilot: 5650, Tutankham: 138.3, Video Pinball: 15641.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Eq. 5) evolutions of the scores on every tested game for DQN agents with full (i.e. using rational activation functions) and regularised (i.e. using joint-rational ones) plasticity, and original DQN agents using Leaky ReLU, SiLU and SiLU+dSiLU, as well as for DDQN agents with Leaky ReLU.A.4. Environments types: stationary, dynamics and progressiveThe used environments have been separated in 3 categories, describing their potential changes through agents learning. This categorisation is here illustrated with frames of the tested games. As one can see: Breakout, Kangaroo, Pong, Skiing, Space Invaders, Tennis, Tutankham and VideoPinball can be categorised as stationary environment, as changes are minimal for the agents in these games. Asterix, BattleZone, Q*bert and Enduro present environment changes, that are early reached by the playing agents, and are thus dynamic environments. Finally, Jamesbond, Seaquest and Time Pilot correspond to progressive environments, as the agents needs to master early changes to access new parts of these environments.</figDesc><table><row><cell>Asterix</cell><cell>Battle Zone</cell><cell>Breakout</cell><cell cols="2">Enduro Jamesbond Kangaroo</cell><cell>Pong</cell><cell>Q*bert</cell></row><row><cell>Seaquest</cell><cell>Skiing</cell><cell>Space Inv.</cell><cell>Tennis</cell><cell cols="2">Time Pilot Tutankham Video Pinb.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">AI and Machine Learning Group, CS Department, TU Darmstadt, Germany 2 Centre for Cognitive Science, TU Darmstadt, Germany. Correspondence to: Quentin Delfosse &lt;quentin.delfosse@cs.tu-darmstadt.de&gt;. 1 Rational library provided at https://github.com/ ml-research/rational_activations/ 2 RL experiments provided at https://github.com/ ml-research/rational_rl/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">30.000 GPU hours, carried out on a DGX-2 Machine with Nvidia Tesla V100 with 32GB.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">all weights are initially set to 0 but a1 (and b0), both set to 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The only difference from the evaluation of <ref type="bibr" target="#b30">(Mnih et al., 2015)</ref> and of <ref type="bibr" target="#b39">(van Hasselt et al., 2016)</ref> evaluation is the use of the Adam optimiser instead of RMSProp, for every evaluated agent.</p><p>Normalisation techniques. To compute human normalised scores, we used the following equation: score normalised = 100 ? score agent ? score random score human ? score random ,</p><p>For readability, the curves plotted in the <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 6</ref> are smoothed following:</p><p>with ? = 0.9.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Never give up: Learning directed exploration strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vitvitskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improved soft actor-critic: Mixing prioritized off-policy samples with on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Noman</surname></persName>
		</author>
		<idno>abs/2109.11767</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rational neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boull?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakatsukasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Townsend</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gym</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantifying generalization in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simplifying reinforcement learning research. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>D&amp;apos;eramo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tateo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mushroomrl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalization and regularization in DQN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Farebrother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno>abs/1810.00123</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the nature of the general factor of intelligence: the role of individual differences in neural plasticity as an explanatory mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Non-linear neurons with human-like apical dendrite activations. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ristea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dendritic action potentials and computation in human layer 2/3 cortical neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Zolnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fidzinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bolduan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papoutsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holtkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Larkum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An evaluation of parametric activation functions for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Godfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning activation functions: A new paradigm of understanding neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lall</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Highway and residual networks learn unrolled iterative estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mastering atari with discrete world models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning that matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Artificial Intelligence AAAI</title>
		<meeting>the Thirty-Second Conference on Artificial Intelligence AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirty-Second Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent experience replay in distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Small-footprint deep neural networks with highway connections for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning combinations of activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Counterfactual credit assignment in modelfree reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">RL-DARTS: differentiable architecture search for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faust</surname></persName>
		</author>
		<idno>abs/2106.02229</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A self regularized non-monotonic activation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Conference 2020</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pad? activation units: End-to-end learning of flexible activation functions in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schramowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting rainbow: Promoting more insightful and inclusive deep reinforcement learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Obando-Ceron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Promoting coordination through policy regularization in multi-agent deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020 (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural networks and rational functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Parametric exponential linear unit for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trottier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gigu?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of the Thirtieth Conference on Artificial Intelligence (AAAI)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Generalized vandermonde tensors. Frontiers of Mathematics in China</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image augmentation is all you need: Regularizing deep reinforcement learning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Vgg6</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>1, 3, 64) ? ? (1, 64, 128) ? ? (2, 128, 256) ? ? (2, 256, 512</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Only activation functions differ between the Leaky ReLU and the Rational versions</title>
		<imprint/>
	</monogr>
	<note>The output of these blocks is then passed on to a classifier (linear layer)</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">REINFORCEMENT LEARNING EXPERIMENTS To ease the reproducibility of our the reinforcement learning experiments, we used the Mushroom RL library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>D&amp;apos;eramo</surname></persName>
		</author>
		<idno>A.6.2</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>We used states consisting of 4 consecutive grey-scaled images. downsampled to 84 ? 84</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The input to the network is thus a 84x84x4 tensor containing a rescaled, and gray-scaled, version of the last four frames. The first convolution layer convolves the input with 32 filters of size 8 (stride 4), the second layer has 64 layers of size 4 (stride 2), the final convolution layer has 64 filters of size 3 (stride 1)</title>
		<imprint/>
	</monogr>
	<note>Network Architecture. This is followed by a fully-connected hidden layer of 512 units</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">All these layers are separated by the corresponding activation functions (either Leaky ReLU, SiLU, SiLU for convolution layers and dSiLU for linear ones, PELU, rational functions and joint-rational ones</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>initialised to approximate Leaky ReLU</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The target network is updated every 10K steps, with a replay buffer memory of initial size 500K, and maximum size 500K, except for Pong, for which all these values are divided by 10. The discount factor ? is set to 0.99 and the learning rate is 0.00025. We do not select the best policy among seeds between epochs. We use the simple -greedy exploration policy</title>
	</analytic>
	<monogr>
		<title level="m">Hyper-parameters. We evaluate the agents every 250K steps, for 125K steps</title>
		<imprint/>
	</monogr>
	<note>with the decreasing linearly from 1 to 0.1 over 1M steps, and an of 0.05 is used for testing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
