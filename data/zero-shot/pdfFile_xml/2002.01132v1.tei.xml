<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D ResNet with Ranking Loss Function for Abnormal Activity Detection in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikha</forename><surname>Dubey</surname></persName>
							<email>shikhad.bhu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Gwangju Institute of Science and Technology</orgName>
								<address>
									<postCode>61005</postCode>
									<settlement>Gwangju</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Boragule</surname></persName>
							<email>abhijeet@gist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical Engineering and Computer Science Gwangju Institute of Science and Technology</orgName>
								<address>
									<postCode>61005</postCode>
									<settlement>Gwangju</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
							<email>mgjeon@gist.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical Engineering and Computer Science Gwangju Institute of Science and Technology</orgName>
								<address>
									<postCode>61005</postCode>
									<settlement>Gwangju</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D ResNet with Ranking Loss Function for Abnormal Activity Detection in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abnormal activity detection is one of the most challenging tasks in the field of computer vision. This study is motivated by the recent state-of-art work of abnormal activity detection, which utilizes both abnormal and normal videos in learning abnormalities with the help of multiple instance learning by providing the data with video-level information. In the absence of temporal-annotations, such a model is prone to give a false alarm while detecting the abnormalities. For this reason, in this paper, we focus on the task of minimizing the false alarm rate while performing an abnormal activity detection task. The mitigation of these false alarms and recent advancement of 3D deep neural network in video action recognition task collectively give us motivation to exploit the 3D ResNet in our proposed method, which helps to extract spatial-temporal features from the videos. Afterwards, using these features and deep multiple instance learning along with the proposed ranking loss, our model learns to predict the abnormality score at the video segment level. Therefore, our proposed method 3D deep Multiple Instance Learning with ResNet (MILR) along with the new proposed ranking loss function achieves the best performance on the UCF-Crime benchmark dataset, as compared to other state-of-art methods. The effectiveness of our proposed method is demonstrated on the UCF-Crime dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>To accomplish human security in a more vigorous way and to obviate many crimes in society, many surveillance cameras have been installed in numerous places such as, shopping complexes, highways, roads, banks, etc. Due to the increasing number of surveillance cameras, plenty of human operators are needed to monitor for any kind of abnormal activities in these videos. Moreover, the demands are laborious and time consuming. Since, for a human being, it is tedious to look at the surveillance videos all the time and additionally, watching multiple videos simultaneously can lead to miss detect the abnormal activity. Therefore, to overcome from all these troubles and to make human work more efficient, an automated abnormal activity detection system is essential, which can generate some signal for any abnormal activity happening in the videos automatically. Therefore, accomplishing abnormal activity detection task with a low false alarm rate and with high accuracy is a challenging and critical task in the computer vision field.</p><p>Several algorithms <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b13">[12]</ref> etc. have been proposed to detect abnormal activities in videos. Recently, W. Sultani et al. <ref type="bibr" target="#b3">[3]</ref>, has proposed a new approach to solve all the shortcomings of the traditional approaches. In their method, to detect the abnormal activity, they have trained their model with the help of multiple instance learning (MIL) <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref> by utilizing the weakly labeled training videos. Our method is motivated by their work. In our study, our main goal is to mitigate the false alarm rate in the abnormal activity detection task. In order to achieve our goal and to extract robust spatial-temporal features from the videos, we have proposed a new 3D deep Multiple Instance Learning with ResNet (MILR) neural network model, which consists of the 3D ResNet <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref> and the deep MIL <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b14">[13]</ref> method, along with the new proposed ranking loss. Where recent advancement of 3D deep neural network in the video action recognition task motivates us to use 3D ResNet <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref> in the abnormal activity detection task. Extracted features from 3D ResNet and video-level labeling are used to train the model in a weakly-supervised manner with the help of deep MIL <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b14">[13]</ref> along with the new proposed ranking loss.</p><p>The rest of this paper is organized as follows: Section II, gives a brief overview of the existing algorithms. Section III, gives the details of our proposed abnormal activity detection method. Detailed implementation and experimentation along with the evaluation of our method are given in Section IV. Lastly, Section V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A brief review of some recent algorithms is presented in this section. Algorithms such as traffic monitoring systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>, <ref type="bibr" target="#b11">[11]</ref>, abandoned object detector <ref type="bibr" target="#b18">[17]</ref> and violence detection in crowds <ref type="bibr" target="#b10">[10]</ref>, etc., are proposed to detect abnormalities in certain specific tasks. Therefore, generalization of these algorithms for detection of other abnormal activities is difficult. Moreover, listing all possible normal and abnormal activities in real-world scenarios is difficult. Therefore, the abnormal activity detector should be less dependent on the prior knowledge of the activity. In order to fulfill such requirements, sparse coding based approaches <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b9">[9]</ref> have been proposed, where the extracted features from the normal videos are used to construct the dictionary to represent those normal videos. Moreover, the network is trained only for the normal videos. At the testing time, the high reconstruction error while generating the dictionary indicates the presence of the abnormal activity. Since it is difficult to list all kinds of normal videos, these methods are prone to generate false alarms.</p><p>The success of the deep learning methods in the image processing tasks <ref type="bibr" target="#b16">[15]</ref> and action recognition <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b19">[18]</ref> task, motivated the researchers to apply these methods in the case of the abnormal activity recognition. Consequently, a deep auto-encoder based approach [5]- <ref type="bibr" target="#b7">[7]</ref> has been proposed to learn the features of the normal activities automatically, but generalization of these methods for real-world scenarios is difficult.</p><p>Moreover, recently a new state-of-art method has been introduced <ref type="bibr" target="#b3">[3]</ref>, in which the detection of the abnormal activity is done with the help of C3D <ref type="bibr" target="#b20">[19]</ref> and MIL <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref>. This method has trained the model with the help of both normal and abnormal videos. Since MIL is basically semi-supervised learning, in which we provide the dataset with the video-level information, such a model is prone to indicate false alarms while detecting the abnormalities. Therefore, our proposed method is motivated by their method. In our method, the features have been extracted from the 3D ResNet <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref> and the network is trained with the help of a new ranking loss function, which reduces the false alarm rate in an abnormal action detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED ABNORMAL ACTIVITY DETECTION ALGORITHM</head><p>The proposed abnormal activity detection algorithm is shown in the <ref type="figure" target="#fig_0">Fig. 1</ref>. It is a combination of mainly three steps. First, each video from the training set are divided into groups of positive instances and negative instances by utilizing their video-level labeling. Then, the 3D ResNet-34 <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>, which is trained on the Kinetics dataset <ref type="bibr" target="#b21">[20]</ref> for the action recognition task, extracts spatial-temporal features from those instances. Next, the extracted features are passed through the deep neural network to predict the abnormality score for each instance of the video. Finally, the network is trained in a weakly-supervised way with the help of the deep MIL <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref> along with the proposed new ranking loss function. All the steps are explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction through the 3D ResNet</head><p>Before extracting features from videos for further processing, first each video from the training set are divided into the equal number of non-overlapping video segments AND Further each segments are grouped into the groups of positive instances and negative instances. In the proposed algorithm, we have taken advantage of the video-level knowledge. The segments from abnormal videos have been grouped into positive instances, and the segments of the normal videos have been grouped into the negative instances. Where, C a and C n are the representation for positive group and negative group respectively <ref type="bibr" target="#b3">[3]</ref>.</p><p>Subsequently, to extract the spatial-temporal features, each instance are passed through the pre-trained 3D ResNet-34 <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref> network, which is pre-trained on the Kinetics dataset <ref type="bibr" target="#b21">[20]</ref>. From the study of S. Tiago et al. <ref type="bibr" target="#b13">[12]</ref>, pre-trained CNNs are good feature extractor in the abnormal activity detection task. However, in our algorithm, we have used the 3D CNN, which is a better spatial-temporal extractor in the video processing task. Moreover, from the study of K. Hara et al. <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref>, we are confident using 3D ResNet, since it outperforms any other 3D CNNs. From <ref type="figure" target="#fig_2">Figures 2 and 3</ref>, using 3D ResNet-34 in the abnormal activity detection task results in better performance than any other algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Multiple Instance Learning and Proposed Ranking Loss Function</head><p>In this paper, the network is trained in a weakly-supervised manner with the help of the deep multiple instance learning (MIL) <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref>. In the absence of frame-level or temporallevel labels, we have used deep MIL as similar to the work of W. Sultani et al. <ref type="bibr" target="#b3">[3]</ref>. By providing video-level labels, MIL can train the network with the help of ranking loss function as follows.</p><p>As discussed in the Section III-A, we have grouped of all instances. Where the group of positive instances is represented as C a , since we have at least one instance with the abnormal activity in it, and the group of negative instances is represented as C n . The number of total instances in both groups is equal to n. Since the accurate labels for the instances of the positive group are unknown, we can use the following optimization function <ref type="bibr" target="#b15">[14]</ref> for the binary classification task as below <ref type="bibr" target="#b15">[14]</ref>:</p><formula xml:id="formula_0">min w 1 m m i=1 a max 0, 1 ? YC i max k?C i (w.? (x k )) ? b + 1 2 w 2 (1)</formula><p>where a represents the hinge loss function, Y Ci represents the group-label, ? (x) is a feature representation of the instance, b represents the bias term, m is the total number of the training examples and w represents model weights.</p><p>In this study, the abnormal detection problem is treated as a regression problem <ref type="bibr" target="#b3">[3]</ref>, since we would like to assign a higher abnormality score to all the abnormal instances than the normal instances. Therefore, ranking loss could be one of the solutions to train our model. If we have instance-level labeling, the ranking objective function is mentioned as below:</p><formula xml:id="formula_1">S (I a ) &gt; S (I n )<label>(2)</label></formula><p>where, I a and I n represent the instances of the abnormal video and normal video respectively, functions S (I a ) and S (I n )</p><p>give the predicted abnormal scores for the corresponding instances of the abnormal and the normal video, respectively. The score function S(x) ranges in between 0 and 1. Proposed Ranking Loss Function: In the absence of the instance-level labeling, we cannot use the above equation 2. Therefore, we propose a new ranking loss function for training our model using MIL. Before proposing the new loss function, we need to consider the possible false alarm cases in our task.</p><p>1) False Alarm Cases: There can be two possible cases of false alarms from our model.</p><p>The first case (case 1) is when our model predicts normal activity as an abnormal activity, which is the case of false positive. The second case (case 2) is when our model predicts abnormal activity as a normal activity, which is the case of false negative.</p><p>2) Proposed Loss Function: As our task is to mitigate all kinds of false alarm rates as described in III-B1, in order to reach our goal, following ranking conditions has been proposed:</p><formula xml:id="formula_2">max i?Ca S I i a &gt; max i?Cn S I i n (3) max i?Ca S I i a &gt; min i?Ca S I i a<label>(4)</label></formula><p>Equations 3 and 4 are to avoid case 1 of the false alarm, when the model predicts a normal instance as an abnormal instance. Equation 3 compares the maximum ranked instances from each group <ref type="bibr" target="#b3">[3]</ref>, where the maximum ranked instance from the positive group is most likely to be the true positive and the maximum ranked instance from the negative group can be the case of false positive. Equation 4 compares the maximum ranked instance and minimum ranked instance from the positive group, where the maximum ranked instance from the positive group is most likely to be the true positive and the minimum ranked instance from the positive group can be the case of false positive.</p><p>As mentioned below, arrange the instances of positive group in the descending order of the abnormality score:</p><formula xml:id="formula_3">[M 1 , M 2 , M 3 , ..., M n ] = order desc i?Ca S I i a<label>(5)</label></formula><p>where n is the total number of the instances in each group.</p><formula xml:id="formula_4">M 2 (S (I a )) &gt; max i?Cn S I i n (6) M 3 (S (I a )) &gt; max i?Cn S I i n<label>(7)</label></formula><p>Equations 6 and 7 are used to avoid both cases 1 and 2 of the false alarms. Equations 6 and 7 compare the second and third ranked instances of the positive group with the maximum ranked instances of the negative group, respectively. We have proposed these ranking equations, since the videos of the training dataset are large in size, and there can be multiple instances of the abnormal activity in the video. Therefore, we need to maximize abnormality scores of the other instances of the positive group to avoid the case 2 of the false alarm; where the model predicts the abnormal instance as a normal instance. Moreover, we need to minimize the abnormality score of all the instances of the negative group.</p><p>We don't require any instance-level labeling from equation 3 to equation 7. Therefore, in terms of the predicted abnormality scores we would like to make all the negative instances far apart from all the positive instances. Moreover, in order to satisfy all the ranking conditions from equation 3 to equation 7, the new proposed ranking loss function is given as below in the hinge loss formulation:</p><formula xml:id="formula_5">l (C a , C n ) = l 1 (C a , C n ) + l 2 (C a , C a ) + l 3 (C a , C n ) + l 4 (C a , C n ) (8)</formula><p>where, l 1 (C a , C n ) , l 2 (C a , C a ) , l 3 (C a , C n ) and l 4 (C a , C n ) are defined as below: l 4 (C a , C n ) = max 0, 1 ? M 3 (S (I a )) + max i?Cn S I i n (12) As in the study of W. Sultani et al. <ref type="bibr" target="#b3">[3]</ref>, to maintain the temporal smoothness of the abnormality score and to maintain the sparsity of the abnormal scores, two constraints have been proposed as below:</p><formula xml:id="formula_6">temporal constraint = ? 1 n?1 i S I i a ? S I i+1 a 2<label>(13)</label></formula><formula xml:id="formula_7">sparsity constraint = ? 2 n i S I i n<label>(14)</label></formula><p>Now, the proposed ranking loss can be written as:</p><formula xml:id="formula_8">l (C a , C n ) = l 1 (C a , C n ) + l 2 (C a , C a ) + l 3 (C a , C n ) + l 4 (C a , C n ) + temporal constraint + sparsity constraint (15)</formula><p>Therefore, the final loss function to train our proposed model is given as below:</p><formula xml:id="formula_9">L (W) = l (C a , C n ) + ? 3 W 2<label>(16)</label></formula><p>where, W are the model weights and ? 1 , ? 2 and ? 3 are the hyper-parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION AND EXPERIMENTATION</head><p>A. Implementation 1) Dataset: There are several standard datasets <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref> are available for the abnormal activity detection task. In UMN dataset <ref type="bibr" target="#b18">[17]</ref>, only one class of abnormality is provided to detect. Subway Exit and Entrance datasets <ref type="bibr" target="#b22">[21]</ref> have only two classes of abnormality to detect. UCSD Ped1 and Ped2 datasets <ref type="bibr" target="#b23">[22]</ref> and Avenue dataset <ref type="bibr" target="#b4">[4]</ref>, consist of some simple abnormal activities, which are less realistic and cannot be generalized for the real-world scenarios. Therefore, in this study, we have used the recently introduced UCF-Crime dataset <ref type="bibr" target="#b3">[3]</ref>. This is the largest and challenging dataset in the abnormal activity detection task. Since this dataset consist of total 1900 long untrimmed real-world videos with 950 normal videos and 950 abnormal videos. Moreover, it has 13 classes of real-world abnormal activities.</p><p>Training set and Testing set: The whole dataset is divided into two sets, training set and testing set <ref type="bibr" target="#b3">[3]</ref>. Both the sets consist videos of all 13 classes of abnormal activities. Training set consists of 810 abnormal videos and 800 normal videos. Where, testing set consists of 140 abnormal videos and 150 normal videos.</p><p>2) Details of the Implementation: In our implementation, before the feature extraction task, each training and testing videos have been divided into 32 non-overlapping video segments. Then each video frame is resized into 112 * 112 pixel size and their frame rate per second (fps) is fixed to 30. Then in the training phase, these segments are divided into the group of positive instances and the group of negative instances. Then, 3D ResNet-34 <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b17">[16]</ref> extracts spatial-temporal features for every 16 frames of the video.As shown in the <ref type="figure" target="#fig_0">Fig. 1</ref>, these extracted 512D features for each instance are passed through fully connected (FC) neural network of 3-layers. Where, the first FC layer consists of 128 units with the ReLU activation function and second FC layer consists of 32 units and the last FC layer has 1 unit with the sigmoid activation function. Similar to the W. Sultani et al. <ref type="bibr" target="#b3">[3]</ref> study, 60% dropout has been used in between the 3-FC layers. Our proposed network is trained using Adagrad optimizer with a 0.001 initial learning rate. For the best performance of our model, the values of all hyper-parameters are set as <ref type="bibr" target="#b3">[3]</ref>, ? 1 = ? 2 = 8 * 10 ?5 and ? 3 = 0.01. Moreover, the batch of 60 randomly chosen videos (30 abnormal and 30 normal) has been used while training the model. Then the network has been trained for 25, 000 epochs with the help of the deep MIL along with the new proposed ranking loss as given in equation 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimentation</head><p>1) Testing Phase: In the testing phase, each testing video is divided into 32 non-overlapping video segments and each video frame is resized into 112 * 112 pixel size and their frame rate per second (fps) is fixed to 30. Then all the segments are passed through our proposed deep neural network, which predicts the abnormality score for each video segment of the video.</p><p>2) Evaluation Methods: Similar to the previous abnormal activity detection algorithms <ref type="bibr" target="#b3">[3]</ref>- <ref type="bibr" target="#b5">[5]</ref> etc., to evaluate the effectiveness of our proposed method, we have also used two quantitative evaluation methods as receiver operating characteristic curve (ROC-Curve) and area under the curve (AUC).</p><p>3) Comparison with the other State-of-Art Methods: In this paper, to check the effectiveness of our proposed method, we have compared it with the other three state-of-art methods. The first one is C. Lu et al. <ref type="bibr" target="#b4">[4]</ref> algorithm, which uses dictionary learning based approach. The second one is M. Hasan et al. <ref type="bibr" target="#b5">[5]</ref> algorithm, which uses deep auto-encoder based approach and the third one is the recently introduced approach by W. Sultani et al. <ref type="bibr" target="#b3">[3]</ref>, which uses C3D <ref type="bibr" target="#b20">[19]</ref> network to extract features and to predict the abnormality. Our method is motivated by W. Sultani et al. <ref type="bibr" target="#b3">[3]</ref> algorithm. Additionally, SVM binary classifier is used as a baseline algorithm in the comparison.</p><p>As shown in <ref type="figure" target="#fig_2">Figures 2, 3</ref> and in <ref type="table" target="#tab_0">Table I</ref>, ROC-Curve and AUC methods show the quantitative comparisons of our algorithm with other state-of-art methods. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the comparison of all the ROC-Curves and it shows that our proposed method outperforms in all other methods. Similarly from the <ref type="table" target="#tab_0">Table I and</ref>  <ref type="figure" target="#fig_3">Fig. 3</ref>, we can say that our proposed  method gives the highest AUC score in all. Additionally, the effectiveness of our proposed ranking loss function is also shown. From these comparisons, it is shown that our proposed algorithm along with the proposed ranking loss shows the best performance in other state-of-art methods. In <ref type="figure" target="#fig_4">Fig. 4</ref>, the qualitative analysis of our proposed algorithm is shown for 6 testing videos, where (a)-(c) show the detection of the abnormal activities correctly with the high abnormality scores for all abnormal video segments and low abnormality scores for all normal video segments, (d) shows the detection   <ref type="table" target="#tab_0">Table II</ref> shows the comparison of false alarm rate at the threshold of 50% of different algorithms. These false alarm rates have been calculated on the normal videos of the UCF-Crime <ref type="bibr" target="#b16">[15]</ref> testing dataset. Since, normal activities take place more often in real-world scenarios, therefore the detector should give less false alarm rate for normal videos. As shown in <ref type="table" target="#tab_0">Table II</ref>, our proposed method gives the lowest false alarm rate on normal videos as compare to other algorithms. Additionally, the effectiveness of using new proposed ranking loss function is also shown in the <ref type="table" target="#tab_0">Table II</ref>. This comparison shows that using both abnormal and normal videos in training the network improves the abnormal activity detection task and we can generalize our method for other realworld related abnormal activity detection. False alarm rate in abnormal videos: Since, our method gives the highest AUC in all, therefore in <ref type="table" target="#tab_0">Table III</ref>, we have just shown the comparison of false alarm rate at the threshold of 50% on the basis of variation of our proposed method. Abnormal activities happen rarely in the real-world scenarios, but still detecting the abnormal activity accurately is an important task. Therefore the detector should give less false alarm rate for abnormal videos. As shown in <ref type="table" target="#tab_0">Table III</ref>, our proposed method along with the new ranking loss function gives the lowest false alarm rate on abnormal videos. The table shows the effectiveness of using the new loss function.</p><p>Therefore, both the tables II and III, indicate that we have reduced both cases of false alarm with the help of our proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this study, we have proposed a deep neural network to detect abnormal activities in the videos. Here, we have used both abnormal and normal videos to train our network. Our proposed model is trained with the help of the deep MIL along with the new proposed ranking loss function. We have validated our proposed algorithm on the UCF-Crime dataset with the help of the ROC-curve and AUC evaluation methods. All the experimental results show that our proposed algorithm gives a considerable improvement in decreasing false alarm rates and gives better accuracy in the abnormal activity detection task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The model of the proposed abnormal activity detection algorithm. Each normal and abnormal videos is divided into groups of positive instances and negative instances. Then the proposed neural network predicts the abnormality score for each instance with the help of MIL along with the new ranking loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>l 1 (l 2 (l 3 (</head><label>123</label><figDesc>C a , C n ) = max 0, 1 ? max i?Ca C a , C a ) = max 0, 1 ? max i?Ca C a , C n ) = max 0, 1 ? M 2 (S (I a )) + max i?Cn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of ROC-Curves: blue, dotted-black, cyan, green, black, and red color curves represent binary classifier, M.Hasan et al. [5], C. Lu et al. [4], W. Sultani et al. [3], the proposed method (3D ResNet+constr.+loss) and the proposed method with new ranking loss (3D ResNet+constr.+new rank. loss), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Proposed Method (3D ResNet + constr. + loss) Proposed Method (3D ResNet + constr. + new rank. loss)AUCAlgorithms et al.<ref type="bibr" target="#b5">[5]</ref> et al.<ref type="bibr" target="#b4">[4]</ref> et al.<ref type="bibr" target="#b3">[3]</ref> AUC Comparison of all the methods on the UCF-Crime Dataset, X-axis describes the algorithm names and Y-axis describes AUC for each algorithm. Our proposed method outperforms in all.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative Analysis of our proposed algorithm on the testing videos of UCF-Crime dataset. The colored windows show the temporal-region of abnormal activities from ground truth. Where (a), (b), (c) and (e) are results of the abnormal activities named as Explosion, Shooting, Arson and Burglary respectively. (d), (f) are results for the normal videos. Where, (e) and (f) show failure cases of our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF ALL THE METHODS ON THE UCF-CRIME DATASET.</figDesc><table><row><cell>Algorithms</cell><cell>AUC</cell></row><row><cell>Binary Classifier</cell><cell>50.0</cell></row><row><cell>M. Hasan et al. [5]</cell><cell>50.6</cell></row><row><cell>C. Lu et al. [4]</cell><cell>65.51</cell></row><row><cell>W. Sultani et al. [3]</cell><cell>75.41</cell></row><row><cell>Proposed Method (3D ResNet + constr. + loss)</cell><cell>75.62</cell></row><row><cell>Proposed Method (3D ResNet + constr. + new rank. loss)</cell><cell>76.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF FALSE ALARM RATE ON NORMAL VIDEOS OF UCF-CRIME TESTING DATASET.</figDesc><table><row><cell></cell><cell>False</cell></row><row><cell>Algorithms</cell><cell>Alarm Rate</cell></row><row><cell></cell><cell>(Normal Videos)</cell></row><row><cell>M. Hasan et al. [5]</cell><cell>27.2</cell></row><row><cell>C. Lu et al. [4]</cell><cell>3.1</cell></row><row><cell>W. Sultani et al. [3]</cell><cell>1.9</cell></row><row><cell>Proposed Method(3D ResNet+constr.+loss)</cell><cell>0.83</cell></row><row><cell>Proposed Method(3D ResNet+constr.+new rank.loss)</cell><cell>0.80</cell></row><row><cell>TABLE III</cell><cell></cell></row><row><cell cols="2">COMPARISON OF FALSE ALARM RATE ON ABNORMAL VIDEOS OF</cell></row><row><cell>UCF-CRIME TESTING DATASET.</cell><cell></cell></row><row><cell></cell><cell>False</cell></row><row><cell>Algorithms</cell><cell>Alarm Rate</cell></row><row><cell></cell><cell>Abnormal Videos</cell></row><row><cell>Proposed Method(3D ResNet+constr.+loss)</cell><cell>0.72</cell></row><row><cell>Proposed Method(3D ResNet+constr.+new rank.loss)</cell><cell>0.67</cell></row><row><cell cols="2">of the normal activity accurately with the lowest abnormality</cell></row><row><cell cols="2">scores almost 0 for all normal video segments. Additionally,</cell></row><row><cell cols="2">(e) and (f) show failure case of our proposed method. This</cell></row><row><cell cols="2">shows our methods get fail in detecting some abnormal</cell></row><row><cell cols="2">activities correctly. From (e) and (f), we can say that it gets</cell></row><row><cell>fail when there is a lot of light variation.</cell><cell></cell></row></table><note>4) Analysis of the Proposed Algorithm: False alarm rate in normal videos:</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Traffic monitoring and accident detection at intersections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamijo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakauchi</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
	<note>in Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">JSAI International Conference on Intelligent Transportation Systems (Cat. No.99TH8383)</title>
		<imprint>
			<date type="published" when="1999-10" />
			<biblScope unit="page" from="703" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection for traffic surveillance based on background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="129" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using hybrid spatio-temporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="2276" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks -ISNN 2017</title>
		<editor>Cong, A. Leung, and Q. Wei</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning regularity in skeleton trajectories for anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Angry crowds: Detecting violent events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AAD: adaptive anomaly detection through traffic surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Bajestani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S H R</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M D</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khodadadeh</surname></persName>
		</author>
		<idno>abs/1808.10044</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="http://arxiv.org/abs/1808.10044" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are pre-trained cnns good feature extractors for anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Nazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ponti</surname></persName>
		</author>
		<idno>abs/1811.08495</idno>
		<ptr target="http://arxiv.org/abs/1811.08495" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiple instance learning : Algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="561" to="568" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real time, online detection of abandoned objects in public areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Atev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Caramelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2006 IEEE International Conference on Robotics and Automation</title>
		<meeting>2006 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2006-05" />
			<biblScope unit="page" from="3775" to="3780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition by learning spatio-temporal features with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="17" to="913" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<ptr target="http://arxiv.org/abs/1705.06950" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
