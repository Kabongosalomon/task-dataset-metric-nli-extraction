<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruida</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>3 Google</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Lou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>3 Google</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
							<email>fabianmanhardt@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>3 Google</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
							<email>tombari@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While 6D object pose estimation has recently made a huge leap forward, most methods can still only handle a single or a handful of different objects, which limits their applications. To circumvent this problem, category-level object pose estimation has recently been revamped, which aims at predicting the 6D pose as well as the 3D metric size for previously unseen instances from a given set of object classes. This is, however, a much more challenging task due to severe intra-class shape variations. To address this issue, we propose GPV-Pose, a novel framework for robust categorylevel pose estimation, harnessing geometric insights to enhance the learning of category-level pose-sensitive features. First, we introduce a decoupled confidence-driven rotation representation, which allows geometry-aware recovery of the associated rotation matrix. Second, we propose a novel geometry-guided point-wise voting paradigm for robust retrieval of the 3D object bounding box. Finally, leveraging these different output streams, we can enforce several geometric consistency terms, further increasing performance, especially for non-symmetric categories. GPV-Pose produces superior results to state-of-the-art competitors on common public benchmarks, whilst almost achieving realtime inference speed at 20 FPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Being able to reliably estimate the 6D object pose, i.e. its 3D rotation and translation, is a fundamental problem in computer vision, as it enables a wide range of applications within AR&amp;VR <ref type="bibr" target="#b37">[37]</ref>, robotics <ref type="bibr" target="#b4">[5]</ref> and 3D understanding <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b54">54]</ref>. Hence, a lot of research has recently been devoted to the domain of pose estimation, producing methods that can reliably estimate the pose at real-time, even under <ref type="bibr">Figure 1</ref>. GPV-Pose consists of three individual branches for pose estimation, symmetry-aware reconstruction and point-wise bounding box voting. To enhance the learning of pose-sensitive features, we make use of two different types of geometric relations, i.e. Point Cloud -Pose (PP) and Point Cloud -Bounding Box -Pose (PBP). In the PP stream, we explicitly analyze how to directly retrieve pose from the point cloud, while in the PBP stream, we first introduce a geometry-guided point-wise bounding box voting approach and then establish geometric correspondences between pose and the bounding box. severe occlusion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref>. Nevertheless, the majority of these methods can only deal with a few objects, in fact, sometimes even just a single instance at a time <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. In addition, a high-quality CAD model is usually required during training and/or inference <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">53]</ref>, which clearly limits the use of such methods in real applications. To deal with this issue, category-level pose estimation attempts to go beyond the instance-level scenario and estimate the pose together with the object's scale for previously unseen objects from known classes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">47]</ref>. The category-level task is inherently more challenging due to the lack of respective CAD models as well as the large intra-class variations among different objects.</p><p>Despite category-level pose estimation being a wellestablished field <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b35">35]</ref>, it has very recently started to enjoy again increasing popularity, thanks to a new line of works based on deep learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b47">47]</ref>. Interestingly, most of these works resort to a learned or manually-designed canonical object space to recover the pose <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">47]</ref> and, in parts, additionally leverage point-cloud based shape priors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">40]</ref> or pose consistency terms <ref type="bibr" target="#b21">[22]</ref> to better deal with intra-class shape variations. Nonetheless, despite these methods achieving remarkable improvements on the benchmarks, their performance is still far from satisfactory due to their insufficient ability to extract pose-sensitive features, as they do not explicitly harness the geometric relationships between pose and point cloud.</p><p>In this paper, we propose GPV-Pose, a novel categorylevel pose estimation framework that leverages geometric constraints to enhance the learning of intra-class object shape characteristics. GPV-Pose stacks three individual branches on top of a 3D graph convolution (3DGC) based encoder <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> for direct pose regression, symmetryaware reconstruction and point-wise bounding box voting. In particular, we introduce a novel decoupled confidencedriven rotation representation, in which the rotation matrix is decomposed as the plane normals of the object bounding box. We demonstrate that our estimated confidence enables closed-form recovery of the 3D rotation and is capable of capturing geometric characteristics of each class. Further, besides enhancing the feature quality by means of symmetry-aware reconstruction, we additionally propose a novel Geometry-guided Point-wise Voting paradigm (GPV), enabling the robust recovery of the object's 3D bounding box. As constituted in <ref type="figure">Fig. 1</ref>, leveraging these three individual estimates, two streams of geometric relationships, i.e. Point Cloud -Pose (PP) and Point Cloud -Bounding Box -Pose (PBP), can be exploited to serve as geometric consistency terms, further improving the learning of category-level shape characteristics and in turn enhancing the overall performance.</p><p>In summary, our main contributions are as follows:</p><p>? We propose a novel geometry-guided category-level pose estimation framework GPV-Pose, which consists of three respective branches for direct pose regression, symmetry-aware reconstruction, and point-wise bounding box voting, giving superior results on common public benchmarks at a high framerate of 20 FPS. ? To enhance the learning of intra-class shape characteristics, we introduce confidence-aware predictions of rotation and the object bounding box, and two parallel streams of geometric constraints, PP and PBP, are naturally converted into geometric consistency terms. ? We propose a novel point-wise object bounding box voting mechanism that aggregates direction, distance and confidence predictions of all points with the confidence-weighted least square algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Instance-level 6D Pose Estimation. Instance-level pose estimation describes the task of estimating 6D pose for known object instances. For monocular methods, there are three different lines of works. Whereas a few methods directly estimate the pose <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b51">51]</ref>, other approaches either learn a latent embedding for latter pose retrieval <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39]</ref> or predict 2D-3D correspondence and use a variant of the PnP paradigm to obtain the final pose <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b53">53]</ref>. Interestingly, a handful methods combine direct regression with correspondence-driven methods, in particular, they first estimate 2D-3D correspondences and then make use of another network to learn the PnP step <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">46]</ref>. A similar division can be also made for RGB-D methods. While a few methods again learn a latent embedding for retrieval purposes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b50">50]</ref>, most methods aim at directly estimating the final 6D object pose <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">45]</ref>. The main difference lies thereby in the way how the two different modalities are fused together.</p><p>Despite great progress in the recent years, instance-level methods can typically only deal with a single or a handful of objects and require an object CAD model for training and testing, limiting its use in practical applications such as autonomous driving.</p><p>Category-level Pose Estimation. Category-level approaches aim at predicting the pose of previously unseen objects <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b47">47]</ref>. Exemplary, Sahin et al. <ref type="bibr" target="#b34">[34]</ref> introduce "Intrinsic Structure Adaptor" to adapt the distribution shifts arising from shape discrepancies. Wang et al. <ref type="bibr" target="#b47">[47]</ref> derive a Normalized Object Coordinate Space (NOCS) and recover pose using the Umeyama's algorithm <ref type="bibr" target="#b41">[41]</ref>, while CASS <ref type="bibr" target="#b0">[1]</ref> introduces a learned canonical shape space. 6D-PACK <ref type="bibr" target="#b43">[43]</ref> computes the pose by means of tracking. To alleviate the influence of intra-class shape variations, a few methods incorporate point-based object shape priors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">40]</ref>. Du-alPoseNet <ref type="bibr" target="#b21">[22]</ref> instead utilizes a dual network for explicit and implicit pose prediction, and introduces a pose refinement strategy by means of pose consistency within both branches. While FS-Net <ref type="bibr" target="#b3">[4]</ref> proposes to represent rotation using decoupled vectors, DO-Net <ref type="bibr" target="#b20">[21]</ref> makes use of symmetry for pose optimization. Additionally, SGPA <ref type="bibr" target="#b1">[2]</ref> adopt visual transformers <ref type="bibr" target="#b42">[42]</ref> for pose estimation. Interestingly, a handful of methods have recently started to investigate pose estimation for articulated objects <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">49]</ref>. Noteworthy, none of the aforementioned works harness different geometric cues to strengthen the model's prediction capabilities, which in turns mitigates the overall performance.</p><p>On the contrary, in this paper we propose a novel geometry-guided category-level pose estimation frame-work, leveraging two different streams of geometric information based on the sampled point cloud and the object bounding box, which we refer to as PP and PBP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GPV-Pose</head><p>Given a RGB-D image, we first employ an off-the-shelf object detector (e.g. Mask-RCNN <ref type="bibr" target="#b6">[7]</ref>) to segment the object of interest from the depth map. We then sample 1028 points from the backprojected 3D point cloud and feed them as input to our proposed GPV-Pose network. Since 3DGC <ref type="bibr" target="#b22">[23]</ref> is insensitive to shift and scale of the given point cloud, we employ 3DGC as the backbone to extract global and perpoint features. We further attach three parallel branches for direct confidence-aware pose prediction, symmetry-aware point cloud reconstruction and point-wise bounding box voting. Leveraging these different outputs, we can naturally enforce several geometric constraints, leading to superior performance. A schematic overview is provided in <ref type="figure">Fig 2.</ref> In the following subsections, we will explain in detail each branch as well as our geometric constraints. Note that throughout this paper we use {R, t, s} to refer to {Rotation, T ranslation, Size}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Confidence-aware Pose Regression</head><p>Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">57]</ref> have shown that predicting the rotation in the form of bounding box plane normals, i.e. equivalent to considering two columns of the rotation matrix, can benefit learning as it resolves discontinuities in SO(3). Hence following these works, as shown in <ref type="figure">Fig. 2</ref> (e), we also predict rotation as two plane normals of the 3D bounding box. However, since certain normals are naturally easier to recover, we additionally estimate a confidence value for each normal in an effort to increase robustness when recovering the final 3D rotation. As an example, imagine predicting the rotation for a laptop, as one plane normal is typically perpendicular to the keyboard surface, it is thus an easier target to compute than the remaining one. Therefore, this normal should receive higher confidence to, in turn, stabilize rotation recovery. Overall, we want to enforce that a higher confidence is accompanied by a more accurate rotation normal prediction. Thus, we define,</p><formula xml:id="formula_0">L Basic rc = ? 1 i?{x,y} c i ? exp(?k 1 |r i ? r gt i | 2 ) 1 ,<label>(1)</label></formula><p>where k 1 is a constant, r gt i is the corresponding ground truth plane normal and ??? 1 denotes the L 1 -loss.</p><p>As shown in <ref type="figure">Fig. 4</ref> (b), given predicted plane normals r y , r x , and their confidence c y , c x , we minimize the following cost function to calibrate the plane normals to be perpendicular normals r y ? and r x ? ,</p><formula xml:id="formula_1">? * 1 , ? * 2 = arg min c y ? 2 1 + c x ? 2 2 s.t. ? 1 + ? 2 + ?/2 = ?,<label>(2)</label></formula><p>where ? denotes the angle between r x and r y , From Eq. 2 we then obtain</p><formula xml:id="formula_2">? * 1 = cx cx+cy (? ? ? 2 ) ? * 2 = cy cx+cy (? ? ? 2 ).<label>(3)</label></formula><p>The calibrated plane normals r ? y and r ? x can be calculated from (? * 1 , ? * 2 ) using the Rodrigues Rotation Formula. Eventually, the rotation matrix is obtained as</p><formula xml:id="formula_3">R ? = [r ? x , r ? y , r ? x ? r ? y ].</formula><p>As for the translation t, provided the predicted residual translation t * and the mean of the input point cloud M P , we compute t = t * + M P . Similarly, given the estimated residual size s * and the pre-computed category mean size C m , we obtain the scale s = s * + C m . Please refer to the supplementary material for all derivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Symmetry Analysis</head><p>We utilize two types of symmetries to extract more effective per-point features: Reflection Symmetry and Rotational Symmetry following <ref type="bibr" target="#b20">[21]</ref>. For reflection symmetry categories (mug, laptop), we predict the corresponding point cloud P ? of the input point cloud P w.r.t. the reflection plane, while for rotational symmetry categories (can, bowl, bottle), we predict P ? that is symmetrical to P w.r.t the symmetry-axis, otherwise we directly reconstruct P with P ? = P . The symmetry-aware reconstruction is supervised by the following loss term</p><formula xml:id="formula_4">L Basic (sym) = ? 2 ?P ? ? ?(P, R, t, R gt , t gt )? 1 ,<label>(4)</label></formula><p>where ?( * ) depends on the symmetry type. Due to space limit, please refer to the supplementary material and <ref type="bibr" target="#b20">[21]</ref> for detailed derivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Point-wise Voting for Bounding Box</head><p>Another major contribution of GPV-Pose resides in our novel confidence-weighted point-wise voting strategy for robust bounding box prediction. To this end, for each observed point p j , we predict its direction n j i , distance d j i and confidence c j i towards each of the six bounding box faces with i ? B and B = {y?, x?, z?}, as shown in <ref type="figure">Fig. 4 (a)</ref>. We then compute the bounding box of the observed object using weighted averaging of all spatial cues. As depicted in <ref type="figure">Fig. 4 (a)</ref>, consider the top bounding box face y+, we obtain the corresponding point p ? j on the top face y+ for p j as</p><formula xml:id="formula_5">p ? j = p j + n j y+ d j y+ .<label>(5)</label></formula><p>The plane parameters, described as the normal N y+ and origin-to-plane distance D y+ for y+, can be estimated using weighted least squares, where the weight of point p j amounts to its confidence c j . All other faces in B follow the same calculations.</p><p>Since we can easily obtain the ground truth vote for each predicted value n j i , d j i , we directly supervise it by means of  <ref type="figure">Figure 2</ref>. Schematic overview of our architecture for GPV-Pose. We first employ an off-the-shelf object detector (e.g. Mask-RCNN <ref type="bibr" target="#b6">[7]</ref>) to segment the object of interest from the depth map, and sample 1028 points from the backprojected depth map as input to GPV-Pose (a-d). We then extract global and per-point features with the help of 3DGC <ref type="bibr" target="#b22">[23]</ref>, processed with three respective branches. The first branch (yellow) outputs confidence-aware rotation {c1, ry, c2, rx}, residual translation t * and size s * parameters, from which we recover the pose {R, t, s} in closed form. Moreover, while the second branch (green) reconstructs the input in a symmetry-aware manner (f), the last branch (blue) conducts point-wise bounding box voting. For each point in the point cloud (g), we predict its direction, distance and confidence towards each bounding box face, moving the points to the respective bounding box faces (h). By employing confidenceweighted least squares we can recover the plane parameters (i). Both branches are then further leveraged to serve as additional geometric guidance during optimization. Note that, at inference, we only need the pose estimation part, with benefits in efficiency.</p><p>the L 1 loss. As for confidence c j i , similar to Eq. 1, we define the following loss term</p><formula xml:id="formula_6">L Basic pc = ? 3 pj ?P i?B c j i ? exp( |d j i n j i ? f i j (p j )r gt i | ?k 2 ) 1 ,<label>(6)</label></formula><p>where k 2 is a constant hyper-parameter and f i j (p j ) denotes the ground truth distance of p j to the face i in B. Considering now again the top face y+, we accordingly obtain</p><formula xml:id="formula_7">f y+ j (p j ) = s gt [y+] 2 ? R T gt (p j ? t gt ),<label>(7)</label></formula><p>with s [y+] being the size along y+ direction, and R gt and t gt referring to the ground truth rotation and translation. Noteworthy, the contributions of our confidenceweighted bounding box voting are two-fold. First, it enforces geometric consistency terms within the PBP stream. Secondly, it also enhances the learning of intra-category geometric features as experimentally proven in the evaluation section. In general, the closer a point is to a bounding box plane, the more confidence it should have to accurately vote for the bounding box plane, as it is less affected by intraclass variations. As a consequence, these geometric guidance enforces the learning of global (green) and per-point (blue) features in <ref type="figure">Fig. 2</ref> to capture more specific categorylevel traits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>Ideal distribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>Fitted distribution Outliers Bin sampling </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Point Cloud -Pose Geometric Consistency</head><p>While most category-level methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22]</ref> predict the pose from uniformly sampled point clouds, they do not consider the geometric relation between pose and the point cloud explicitly. Instead they resort to individual loss terms for each pose parameter to learn such geometric relations implicitly <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22]</ref>. On another note, it is not difficult to deduce that, when transforming the point cloud to the canonical view via the inverse of {R, t} and removing all outliers, the size {s} can be simply computed as the distance between the outermost points along each axis. In this paper, we turn the above transformation into supervision terms to explicitly enforce these geometric relations and, thus, enhance performance.</p><p>To this end, we first transform the point cloud from the camera view to a predefined canonical view. Similar to the point matching loss used in instance-level pose prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">46]</ref>, we then minimize</p><formula xml:id="formula_8">L P C (R,t) = p?P (R T (p ? t) ? p c ) 1 ,<label>(8)</label></formula><p>with P being the sampled point cloud and p c being the corresponding ground truth of p under the canonical view. Next, estimating the correct metric scale can be generally considered as the task of finding the 3D bounding box which encapsulates all object points whilst having minimal volume. Unfortunately, this is only true as long as we do not observe any outliers. Since this is rarely the case when dealing with real data, we instead relax this constraint and try to enforce having as many points as possible in the bounding box of minimal volume. Given sampled points with a few outliers, we first conduct bin sampling to ensure that the points follow an approximate uniform distribution. To this end, we segment the points into 64 bins. Further, whenever one bin overflows we uniformly sub-sample a fixed number of points. The final obtained point set is denoted as P s . This process is also demonstrated in <ref type="figure" target="#fig_0">Fig. 3 (b)</ref>. Then given an estimate of {s}, its probability to be the expected size is calculated as follows, </p><formula xml:id="formula_9">f (s) =min(0, k p |P s | f p (s) ? f d (s))<label>(9)</label></formula><p>where {k s , k n } are constant hyper-parameters. Further, k p is a normalization parameter and ?(p, s) = 1 if p ? s, otherwise ?(p, s) = ?1. From this, our geometric loss term for the scale {s} can be derived as</p><formula xml:id="formula_11">L P C (s) = i?B ? ?(1.0 ? f (s i ))? 1 ,<label>(12)</label></formula><p>where B ? = {x+, y+, z+}. Note that this term is only meaningful when the corresponding bounding box face in B is visible from the given viewpoint, hence, we turn off L P C (s) for occluded faces.</p><p>To summarize, our geometry-aware loss terms between the estimated pose and the corresponding object point cloud are defined as</p><formula xml:id="formula_12">L P C (R,t,s) = ? 4 L P C (R,t) + ? 5 L P C (s) ,<label>(13)</label></formula><p>where ? * denote the balancing weights of the two terms.  <ref type="figure">Figure 4</ref>. Confidence-aware predictions of the bounding box (a) and rotation (b). In (a), for each point, we predict its direction n, distance d and confidence c to each of the six faces, e.g. for face y+, we predict the ny+, dy+, cy+ of each point. In (b), we calibrate the predicted rotation normals rx, ry to be perpendicular normals r x ? , r y ? by minimizing Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Bounding Box -Pose Geometric Consistency</head><p>From Sec. 3.3, we take {t, s} as the center and {length, width, height} of the bounding box from the point-wise voting. Further, as aforementioned, we decompose the rotation {R} into its 3 columns R = [r x , r y , r z ], yet, only predict r x and r y together with the associated confidence, since they already fully describe the 3D rotation, i.e. r z = r x ? r y . We have also shown that r x and r y correspond to the plane normals of the bounding box. Thus given the estimated six bounding box faces {N i , D i }, i ? B, we can define the following Bounding box -Pose consistencies</p><formula xml:id="formula_13">? ? ? ? ? L BB (t) = i?{x,y,z} |N T i+ t ? D i+ | ? |N T i? t ? D i? |) 1 L BB (s) = i?|B| s i ? |N T i t ? D i | 1 L BB (R) = ?r y ? N y+ ? 1 + ?r x ? N x+ ? 1 (14)</formula><p>The overall geometric consistency between pose and the estimated bounding box is then computed as the weighted sum of these individual terms</p><formula xml:id="formula_14">L BB (R,t,s) = ? 6 L BB (R) + ? 7 L BB (t) + ? 8 L BB (s) .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Overall Training Objective</head><p>In summary, GPV-Pose employs the following loss function</p><formula xml:id="formula_15">L = ? Basic L Basic + ? BB L BB (R,t,s) + ? P C L P C (R,t,s) ,<label>(16)</label></formula><p>where L Basic contains all loss terms for fully supervising the learning of pose, rotation confidence, symmetrical reconstruction and point-wise bounding box voting. Further, L BB (R,t,s) , L P C (R,t,s) denote our geometric consistency terms as described in Sec. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We evaluate GPV-Pose on the synthetic CAM-ERA25 and real REAL275 <ref type="bibr" target="#b47">[47]</ref> benchmarks for categorylevel pose estimation and the real LineMod <ref type="bibr" target="#b10">[11]</ref> dataset for instance-level pose estimation. CAMERA25 contains 300K synthetic RGB-D images with rendered objects on top of virtual backgrounds, among which 25K images are withhold for testing. The objects cover 6 categories, i.e. bottle, bowl, camera, can, laptop and mug. REAL275 is a more challenging real-world dataset with 13 different scenes. Thereby, 7 scenes with 4.3k images are provided for training, while the remaining 6 scenes with 2.7k images are employed for testing. REAL275 possesses the same categories as CAMERA25. LineMod <ref type="bibr" target="#b10">[11]</ref> is a widely-used instance-level 6D pose estimation benchmark, consisting of individual sequences with 13 objects undergoing mild occlusion. We follow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">46]</ref> and employ ?15% of the RGB-D images for training and the utilize the rest for testing. Implementation Details.</p><p>We train our method purely on real data and follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">40]</ref> to generate instance segmentation masks with an off-the-shelf object detector, i.e. Mask-RCNN <ref type="bibr" target="#b6">[7]</ref>. We uniformly sample 1028 points from the back-projected object depth map and feed them as input to GPV-Pose. We employ several strategies for data augmentation including random scaling, random uniform noise, random rotational and translational perturbations, and bounding box based adjustment similar to FS-Net <ref type="bibr" target="#b3">[4]</ref>. The parameters for all loss terms are kept unchanged during experimentation unless specified, with {? 1 , ..., ? 8 , ? Basic , ? BB , ? P C } = {1/8.0, 1/8.0, 1/8.0, 1.0, 1.0, 1.0, 1.0, 1.0, 8.0, 1.0, 1.0}. We further set {k 1 , k 2 } = {13.7, 1/303.5}, {k s , k n } = {10, 0.5} and keep k p fixed at 1.0, assuming an approximate point number of about 300 after bin sampling. We employ the Ranger optimizer <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b56">56]</ref> and run all our experiments on a single TITAN X GPU with batch size of 24 and base learning rate of 1e-4. The learning rate is annealed at 72% of the training phase using a cosine schedule. The total training epoch is set to 70 for Ours(M), in which we train a separate model for each category and 150 for Ours in which we train a single model for all categories.</p><p>Evaluation Metrics. We follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">47]</ref> and report the mean precision of 3D intersection over union (IoU) at thresholds of 25%, 50%, 75% to jointly evaluate rotation, translation and size. To directly compare errors in rotation and translation, we also adopt the 5 ? 2cm, 5 ? 5cm, 10 ? 5cm, 10 ? 10cm metrics. A pose is thereby considered correct if the translation and rotation errors are both below the given thresholds. For instance-level pose estimation task on LineMOD, we report the commonly employed ADD(-S) metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-Art Methods</head><p>Overall Performance on REAL275. In Tab. 1, we compare GPV-Pose with state-of-the-art competitors on REAL275 <ref type="bibr" target="#b47">[47]</ref>. In line with our work, also DO-Net <ref type="bibr" target="#b20">[21]</ref> and FS-Net <ref type="bibr" target="#b3">[4]</ref> both use only depth observations for pose estimation, whereas all other methods rely on RGB-D data during pose inference. Notice that we provide three variants of our method. Ours(FS-Net) inherits the loss terms of FS-Net but uses our pose estimation network architecture and serves as the baseline for our method. Ours(M) trains a separate model for each category, while Ours trains a single model for all categories. From Tab. 1, it can be deduced that GPV-Pose achieves state-of-the-art performance w.r.t 5 out of total 7 metrics. Specifically, for 3D 50 , GPV-Pose outperforms the previous best method NOCS with 83.0 vs. 80.5. Furthermore, for the 5 ? 5cm metric, we surpass SGPA <ref type="bibr" target="#b1">[2]</ref> with 42.9 compared to 39.6, which increases by 3.3. Interestingly, the best performance for 5 ? 5cm is achieved by Ours(M) with 44.2. In terms of 5 ? 2cm, our method is infe-  <ref type="table">Table 3</ref>. Performance on LineMod. We compare our method with competitors on the instance-level 6D pose estimation task on LineMod. C.L. refers to category-level method.</p><p>rior to SGPA <ref type="bibr" target="#b1">[2]</ref>. The main reason may be that SGPA employs a point cloud based shape prior, while we only rely on the mean size of each category as prior. As for our baseline method Ours(FS-Net), we outperform it by a large margin w.r.t all metrics, clearly proving the effectiveness of our proposed contributions. In <ref type="figure">Fig. 7</ref>, we additionally present a detailed per-category comparison of our method with Du-alPoseNet <ref type="bibr" target="#b21">[22]</ref>. As one can easily deduce, we outperform DualPoseNet by a large margin, especially when viewing the 3D rotation results for non-symmetrical objects such as camera. Moreover, <ref type="figure" target="#fig_3">Fig. 5</ref> presents a qualitative comparison with DualPoseNet on REAL275. GPV-Pose is capable of predicting accurate rotation and translation estimations, even when the objects are only partially detected, as shown in <ref type="figure" target="#fig_3">Fig. 5 (a)</ref> camera, while DualPoseNet tends to fail on such challenging cases. In addition, to get a better grasp of our methodology, in <ref type="figure">Fig. 6</ref>, we demonstrate the bounding box voting results. It can be noted that the computed bounding box faces are accurate and the estimated confidence is reasonable (blue: low -yellow: high). Due to space limitations, we present our results on CAMERA25 in the supplementary material. In general, similar to REAL275, we again achieve comparable or superior performance compared to the state-of-the-art. As speed is a vital factor for many applications, in the last column of Tab. 1, we show the framerate of each method. Regardless of the object detection time, our method can infer at a real-time speed &gt;50 FPS, and is thus very suitable for real applications with time requirements. When using Yolo-V3 [33] + ATSA <ref type="bibr" target="#b55">[55]</ref> to extract object instances, the whole pipeline runs at about 20 FPS, faster than most other methods.</p><p>Performance on Instance-level 6D Pose Estimation. We also apply GPV-Pose to the instance-level 6D pose estimation scenario and compare with state-of-the-art instancelevel and category-level methods. As shown in Tab. 3, GPV-Pose achieves a comparable performance under the ADD(-S) metric (98.2% for GPV-Pose compared to 98.2% for Du-alPoseNet and 99.4% for PVN3D <ref type="bibr" target="#b9">[10]</ref>), whilst running almost in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>Effect of Geometric Terms. In Tab. 2 (B) and (C), we evaluate the performance w.r.t. different configurations of network architectures and geometric loss terms. First, we gradually add each loss term (from</p><formula xml:id="formula_16">B 1 to B 4 ) L BB (R) , L BB (t) , L BB (s)</formula><p>, capturing the geometric relations between pose and the voted bounding box. As can be observed, when adding these terms, the performance consistently improves from 60.4 to 63.2 referring to 3D 75 , and 25.3 to 29.9 w.r.t 5 ? 2cm, which demonstrates the efficacy of the geometric consistencies. Second, from C 1 to C 2 we evaluate the effectiveness of our loss terms L P C (R,t) and L P C (s) , which are derived from the geometric relationship between pose and point cloud. As before, the enforced consistency terms help again improve the estimator's performance (i.e. we achieve an increase from 60.4 to 61.7 for 3D 75 ).  <ref type="figure">Figure 6</ref>. Demonstration of the point-wise bounding box voting results w.r.t three planes {x+, y+, z+}. GPV-Pose predicts point-wise directions, distances and confidences, moving the input points to the bounding box planes (the red points). Plane parameters are then calculated via confidence-weighted least squares. The color of each point (blue: low, yellow: right) reflects its confidence to support the target plane. We normalize the confidence for better visualization. In general, the closer a point is to the target plane, the higher its voting confidence for the plane.</p><p>Effect of Confidence-Aware Rotation Prediction. When incorporating confidence into the rotation prediction, from A 1 to A 2 in Tab. 2, the overall performance shows a significant leap forward from 52.0 to 56.9 referring to 3D 75 and 19.9 to 22.7 for 5 ? 2cm. As aforementioned, for several categories one rotation vector is much easier to predict than the other and thus enables higher accuracy. Exemplary, for laptop, the average confidence for the r y normal lies around 0.88, whereas the average confidence for r x only amounts to 0.45. Since r y is typically perpendicular to the keyboard surface, it is a much easier target to estimate. Therefore, exerting higher weight on r y benefits the performance. <ref type="figure">Figure 7</ref>. Per-category camparison between our method and Du-alPoseNet. We demonstrate average precision vs. different error thresholds on REAL275.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DualPoseNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPV-Pose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel category-level pose estimation network, which we dub GPV-Pose. GVP-Pose jointly predicts the confidence-driven pose, symmetryaware reconstruction and point-wise bounding box voting. Two parallel streams of geometric consistencies, Point Cloud -Pose and Point Cloud -Bounding Box -Pose, are derived and converted into supervision terms to improve the pose accuracy. GPV-Pose achieves superior performance on public datasets at a fast inference speed of 20 FPS, enabling real-time applications. In the future, we plan to incorporate GPV-Pose into holistic 3D understanding via first predicting the pose of each object and, secondly, describing the object interactions by means of a corresponding 3D scene graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of how we predict {s} based on the point cloud only. Given all points projected along the y positive axis, we aim to approximate the ideal distribution of size {s} with combined exponential functions in Eq. 9, as shown in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, s)exp(?k s |s ? p| 2 )),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of our method (green line) and DualPoseNet (blue line). Image (a)-(d) demonstrate 2D segmentation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>C C Pose Estimation C C Symmetry-Aware Reconstruction Bounding Box Voting</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fused</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(d)</cell><cell>3DGC</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell>256</cell><cell cols="2">512</cell><cell cols="2">Global Feature</cell><cell></cell><cell>Feature</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>128</cell><cell>3</cell><cell>(f)</cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Point Cloud</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) (b)</cell><cell></cell><cell>1024 1024 1024</cell><cell>256 256 256</cell><cell>256 256 256</cell><cell>6 4 4</cell><cell>*   *</cell><cell cols="2">Mean Size</cell><cell>Pose</cell><cell>(e)</cell><cell>3DGC Augmented Global Feature</cell><cell>MLP (g)</cell><cell></cell><cell>Pose</cell><cell>Pooling C Concatenation 30 128 256 Per-Point Direction Distance Confidence (h) 512 (i)</cell></row><row><cell>????? Bbox??? ??? ?????</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Mean Size</cell><cell></cell><cell></cell><cell></cell><cell cols="4">?bbox??? view ??canonical</cell></row><row><cell></cell><cell cols="2">256</cell><cell></cell><cell>6 6</cell><cell></cell><cell>3DGC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">???????? ?DO-Net???</cell><cell>???????????? ???? bounding box?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>??bbox???????</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>.4 and Sec. 3.5, respectively. Method P.E. Setting 3D 25 3D 50 3D 75 5 ? 2cm 5 ? 5cm 10 ? 5cm 10 ? 10cm Speed(FPS) Comparison with state-of-the-art methods on REAL275 dataset. Overall best results are in bold and the second best results are underlined. P.E. Setting lists the input data type for pose estimation. Since FS-Net uses different detection results under 3DIoU, we reimplement it as Ours(FS-Net). Here Ours(FS-Net) inherits all loss terms of FS-Net but uses our pose decoder for fair comparison.</figDesc><table><row><cell>NOCS [47]</cell><cell>RGB-D</cell><cell>84.9</cell><cell>80.5</cell><cell>30.1</cell><cell>7.2</cell><cell>10.0</cell><cell>25.2</cell><cell>26.7</cell><cell>5</cell></row><row><cell>CASS [1]</cell><cell>RGB-D</cell><cell>84.2</cell><cell>77.7</cell><cell>-</cell><cell>-</cell><cell>23.5</cell><cell>58.0</cell><cell>58.3</cell><cell>-</cell></row><row><cell>SPD [40]</cell><cell>RGB-D</cell><cell>83.4</cell><cell>77.3</cell><cell>53.2</cell><cell>19.3</cell><cell>21.4</cell><cell>54.1</cell><cell>-</cell><cell>4</cell></row><row><cell>CR-Net [48]</cell><cell>RGB-D</cell><cell>-</cell><cell>79.3</cell><cell>55.9</cell><cell>27.8</cell><cell>34.3</cell><cell>60.8</cell><cell>-</cell><cell>-</cell></row><row><cell>SGPA [2]</cell><cell>RGB-D</cell><cell>-</cell><cell>80.1</cell><cell>61.9</cell><cell>35.9</cell><cell>39.6</cell><cell>70.7</cell><cell>-</cell><cell>-</cell></row><row><cell>DualPoseNet [22]</cell><cell>RGB-D</cell><cell>-</cell><cell>79.8</cell><cell>62.2</cell><cell>29.3</cell><cell>35.9</cell><cell>66.8</cell><cell>-</cell><cell>2</cell></row><row><cell>DO-Net [21]</cell><cell>D</cell><cell>-</cell><cell>80.4</cell><cell>63.7</cell><cell>24.1</cell><cell>34.8</cell><cell>67.4</cell><cell>-</cell><cell>10</cell></row><row><cell>FS-Net [4]</cell><cell>D</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28.2</cell><cell>60.8</cell><cell>64.6</cell><cell>20</cell></row><row><cell>Ours(FS-Net)</cell><cell>D</cell><cell>84.0</cell><cell>81.1</cell><cell>52.0</cell><cell>19.9</cell><cell>33.9</cell><cell>69.1</cell><cell>71.0</cell><cell>20</cell></row><row><cell>Ours(M)</cell><cell>D</cell><cell>84.1</cell><cell>82.0</cell><cell>63.2</cell><cell>30.6</cell><cell>44.2</cell><cell>72.1</cell><cell>73.8</cell><cell>20</cell></row><row><cell>Ours</cell><cell>D</cell><cell>84.2</cell><cell>83.0</cell><cell>64.4</cell><cell>32.0</cell><cell>42.9</cell><cell>73.3</cell><cell>74.6</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>NetworkLoss Terms3D 75 5 ? 2cm 5 ? 5cm 10 ? 5cm Conf. B.Box Symm. L Basic Ablation studies on different configurations of network architectures and loss terms on REAL275 datasets. Conf. Rot. refers to confidence-aware rotation representation. Without this term, we follow FS-Net to recover rotation matrix with SVD. B.Box Voting refers to point-wise bounding box voting. Symm. Recon. is symmetry-aware reconstruction. Without this term, we directly reconstruct the input point cloud and L Basic sym is replaced by L Basic recon = ?7?(P ? , P ). Overall best results are in bold.</figDesc><table><row><cell>L BB</cell><cell>L P C</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11973" to="11982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sgpa: Structure-guided prior adaptation for category-level 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">G2l-net: Global to local network for real-time 6d pose estimation with embedding vector features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4233" to="4242" />
		</imprint>
	</monogr>
	<note>Hyung Jin Chang, Jinming Duan, and Ales Leonardis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1581" to="1590" />
		</imprint>
	</monogr>
	<note>Shen Linlin, and Ales Leonardis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-supervised 6d object pose estimation for robot manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinke</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bretl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3665" to="3671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">So-pose: Exploiting selfocclusion for direct 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="12396" to="12405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ffb6d: A full flow bidirectional fusion network for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3003" to="3013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gradient Response Maps for Real-Time Detection of Textureless Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Epos: Estimating 6d pose of objects with symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11703" to="11712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single-stage 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cosypose: Consistent multi-view multi-object 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified framework for multi-view multi-class object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (eccv)</title>
		<meeting>the european conference on computer vision (eccv)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Category-level articulated object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3706" to="3715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DeepIM: Deep iterative matching for 6d pose estimation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Donet: Learning categorylevel 6d object pose and size estimation from depth observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chilam</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14193</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06526</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deformable part models revisited: A performance evaluation for object category pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>L?pez-Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1052" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explaining the ambiguity of object detection and 6d pose from visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">Martin</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6841" to="6850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cps++: Improving class-level 6d pose and shape estimation from monocular images with selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05848v3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinyu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose estimation for category specific multiview object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Ozuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="778" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Category-level 6d object pose recovery in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d generic object categorization, localization and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="431" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep multi-state object pose estimation for augmented reality assembly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rambach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nareg</forename><surname>Minaskan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lesur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Pagani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="222" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-path learning for object pose estimation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Durner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Kai O Arras, and Rudolph Triebel</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13916" to="13925" />
		</imprint>
	</monogr>
	<note>En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shape prior deformation for categorical 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marcelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">6-pack: Category-level 6d pose tracker with anchor-based keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10059" to="10066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DenseFusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Category-level 6d object pose estimation via cascaded relation and recurrent reconstruction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03437</idno>
		<title level="m">Captra: Category-level pose tracking for rigid and articulated objects from point clouds</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PoseCNN: A convolutional neural network for 6D object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gradient centralization: A new optimization technique for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="635" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dpod: Dense 6d pose object detector in rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Holistic 3d scene understanding from a single image with implicit representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8833" to="8842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Asymmetric two-stream architecture for accurate rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><forename type="middle">Xiao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
