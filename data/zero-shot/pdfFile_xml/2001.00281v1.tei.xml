<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ZeroQ: A Novel Zero Shot Quantization Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
							<email>keutzer@berkeley.educaiyaohui@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ZeroQ: A Novel Zero Shot Quantization Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Quantization is a promising approach for reducing the inference time and memory footprint of neural networks. However, most existing quantization methods require access to the original training dataset for retraining during quantization. This is often not possible for applications with sensitive or proprietary data, e.g., due to privacy and security concerns. Existing zero-shot quantization methods use different heuristics to address this, but they result in poor performance, especially when quantizing to ultra-low precision. Here, we propose ZEROQ, a novel zero-shot quantization framework to address this. ZEROQ enables mixed-precision quantization without any access to the training or validation data. This is achieved by optimizing for a Distilled Dataset, which is engineered to match the statistics of batch normalization across different layers of the network. ZEROQ supports both uniform and mixed-precision quantization. For the latter, we introduce a novel Pareto frontier based method to automatically determine the mixed-precision bit setting for all layers, with no manual search involved. We extensively test our proposed method on a diverse set of models, including ResNet18/50/152, MobileNetV2, ShuffleNet, SqueezeNext, and InceptionV3 on ImageNet, as well as RetinaNet-ResNet50 on the Microsoft COCO dataset. In particular, we show that ZEROQ can achieve 1.71% higher accuracy on MobileNetV2, as compared to the recently proposed DFQ [32] method. Importantly, ZEROQ has a very low computational overhead, and it can finish the entire quantization process in less than 30s (0.5% of one epoch training time of ResNet50 on ImageNet). We have open-sourced the ZEROQ framework 1 . * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Despite the great success of deep Neural Network (NN) models in various domains, the deployment of modern NN models at the edge has been challenging due to their prohibitive memory footprint, inference time, and/or energy consumption. With the current hardware support for low-precision computations, quantization has become a popular procedure to address these challenges. By quantizing the floating point values of weights and/or activations in a NN to integers, the model size can be shrunk significantly, without any modification to the architecture. This also allows one to use reducedprecision Arithmetic Logic Units (ALUs) which are faster and more power-efficient, as compared to floating point ALUs. More importantly, quantization reduces memory traffic volume, which is a significant source of energy consumption <ref type="bibr" target="#b13">[15]</ref>.</p><p>However, quantizing a model from single precision to lowprecision often results in significant accuracy degradation. One way to alleviate this is to perform the so-called quantizationaware fine-tuning <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b44">46]</ref> to reduce the performance gap between the original model and the quantized model. Basically, this is a retraining procedure that is performed for a few epochs to adjust the NN parameters to reduce accuracy drop. However, quantization-aware finetuning can be computationally expensive and time-consuming. For example, in online learning situations, where a model needs to be constantly updated on new data and deployed every few hours, there may not be enough time for the finetuning procedure to finish. More importantly, in many realworld scenarios, the training dataset is sensitive or proprietary, meaning that it is not possible to access the dataset that was used to train the model. Good examples are medical data, biometric data, or user data used in recommendation systems.</p><p>To address this, recent work has proposed post-training quantization <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b42">44]</ref>, which directly quantizes NN models without fine-tuning. However, as mentioned above, these methods result in non-trivial performance degradation, especially for low-precision quantization. Furthermore, previous post-training quantization methods usually require limited (unlabeled) data to assist the post-training quantization. However, for cases such as MLaaS (e.g., Amazon AWS and Google Cloud), it may not be possible to access any of the training data from users. An example application case is health care information which cannot be uploaded to the cloud due to various privacy issues and/or regulatory constraints. Another shortcoming is that often post-quantization methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b42">44]</ref> only focus on standard NNs such as ResNet <ref type="bibr" target="#b11">[13]</ref> and Incep-tionV3 <ref type="bibr" target="#b36">[38]</ref> for image classification, and they do not consider more demanding tasks such as object detection.</p><p>In this work, we propose ZEROQ, a novel zero-shot quantization scheme to overcome the issues mentioned above. In particular, ZEROQ allows quantization of NN models, without any access to any training/validation data. It uses a novel approach to automatically compute a mixed-precision configuration without any expensive search. In particular, our contributions are as follows. <ref type="bibr">?</ref> We propose an optimization formulation to generate Distilled Data, i.e., synthetic data engineered to match the statistics of batch normalization layers. This reconstruction has a small computational overhead. For example, it only takes 3s (0.05% of one epoch training time) to generate 32 images for ResNet50 on ImageNet on an 8-V100 system. <ref type="bibr">?</ref> We use the above reconstruction framework to perform sensitivity analysis between the quantized and the original model. We show that the Distilled Data matches the sensitivity of the original training data (see <ref type="figure">Figure 1</ref> and  <ref type="formula">(4)</ref> is defined as the KL-divergence between the output of these two models. For simplicity, we omit the residual connections here, although the same analysis is applied to the residual connections in ZEROQ.</p><p>for details). We then use the Distilled Data, instead of original/real data, to perform post-training quantization. The entire sensitivity computation here only costs 12s (0.2% of one epoch training time) in total for ResNet50. Importantly, we never use any training/validation data for the entire process. ? Our framework supports both uniform and mixed-precision quantization. For the latter, we propose a novel automatic precision selection method based on a Pareto frontier optimization (see <ref type="figure" target="#fig_1">Figure 4</ref> for illustration). This is achieved by computing the quantization sensitivity based on the Distilled Data with small computational overhead. For example, we are able to determine automatically the mixed-precision setting in under 14s for ResNet50. We extensively test our proposed ZEROQ framework on a wide range of NNs for image classification and object detection tasks, achieving state-of-the-art quantization results in all tests. In particular, we present quantization results for both standard models (e.g., ResNet18/50/152 and InceptionV3) and efficient/compact models (e.g., MobileNetV2, ShuffleNet, and SqueezeNext) for image classification task. Importantly, we also test ZEROQ for object detection on Microsoft COCO dataset <ref type="bibr" target="#b26">[28]</ref> with RetinaNet <ref type="bibr" target="#b25">[27]</ref>. Among other things, we show that ZEROQ achieves 1.71% higher accuracy on Mo-bileNetV2 as compared to the recently proposed DFQ <ref type="bibr" target="#b30">[32]</ref> method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Here we provide a brief (and by no means extensive) review of the related work in literature. There is a wide range of methods besides quantization which have been proposed to address the prohibitive memory footprint and inference latency/power of modern NN architectures. These methods are typically orthogonal to quantization, and they include efficient neural architecture design <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b41">43]</ref>, knowledge distillation <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b33">35]</ref>, model pruning <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b27">29]</ref>, and hardware and NN co-design <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b19">21]</ref>. Here we focus on quantization <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b46">48]</ref>, which compresses the model by reducing the bit precision used to represent parameters and/or activations. An important challenge with quantization is that it can lead to significant performance degradation, especially in ultra-low bit precision settings. To address this, existing methods propose quantization-aware fine-tuning to recover lost performance <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b18">20]</ref>. Importantly, this requires access to the full dataset that was used to train the original model. Not only can this be very time-consuming, but often access to training data is not possible.</p><p>To address this, several papers focused on developing post-training quantization methods (also referred to as postquantization), without any fine-tuning/training. In particular, <ref type="bibr" target="#b17">[19]</ref> proposes the OMSE method to optimize the L 2 distance between the quantized tensor and the original tensor. Moreover, <ref type="bibr" target="#b1">[3]</ref> proposed the so-called ACIQ method to analytically compute the clipping range, as well as the per-channel bit allocation for NNs, and it achieves relatively good testing performance. However, they use per-channel quantization for activations, which is difficult for efficient hardware implementation in practice. In addition, <ref type="bibr" target="#b42">[44]</ref> proposes an outlier channel splitting (OCS) method to solve the outlier channel problem. However, these methods require access to limited data to reduce the performance drop <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b42">44]</ref>.</p><p>The recent work of <ref type="bibr" target="#b30">[32]</ref> proposed Data Free Quantization (DFQ). It further pushes post-quantization to zero-shot scenarios, where neither training nor testing data are accessible during quantization. The work of <ref type="bibr" target="#b30">[32]</ref> uses a weight equalization scheme <ref type="bibr" target="#b28">[30]</ref> to remove outliers in both weights and activations, and they achieve similar results with layerwise quantization, as compared to previous post-quantization work with channel-wise quantization <ref type="bibr" target="#b18">[20]</ref>. However, <ref type="bibr" target="#b30">[32]</ref> their performance significantly degrades when NNs are quantized to 6-bit or lower.</p><p>A recent concurrent paper to ours independently proposed to use Batch Normalization statistics to reconstruct input data <ref type="bibr" target="#b10">[12]</ref>. They propose a knowledge-distillation based method to boost the accuracy further, by generating input data that is similar to the original training dataset, using the so-called Inceptionism <ref type="bibr" target="#b29">[31]</ref>. However, it is not clear how the latter approach can be used for tasks such as object detection or image segmentation. Furthermore, this knowledge-distillation process adds to the computational time required for zero-shot quantization. As we will show in our work, it is possible to use batch norm statistics combined with mixed-precision quantization to achieve state-of-the-art accuracy, and importantly this approach is not limited to image classification task. In particular, we will present results on object detection using RetinaNet-ResNet50, besides testing ZEROQ on a wide range of models for image classification (using ResNet18/50/152, MobileNetV2, ShuffleNet, SqueezeNext, and InceptionV3), We show that for all of these cases ZEROQ exceeds state-ofthe-art quantization performance. Importantly, our approach has a very small computational overhead. For example, we can finish ResNet50 quantization in under 30 seconds on an 8 V-100 system (corresponding to 0.5% of one epoch training time of ResNet50 on ImageNet).</p><p>Directly quantizing all NN layers to low precision can lead to significant accuracy degradation. A promising approach to address this is to perform mixed-precision quantization <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b45">47]</ref>, where different bit-precision is used for different layers. The key idea behind mixed-precision quantization is that not all layers of a convolutional network are equally "sensitive" to quantization. A na?ve mixed-precision quantization method can be computationally expensive, as the search space for determining the precision of each layer is exponential in the number of layers. To address this, <ref type="bibr" target="#b37">[39]</ref> uses NAS/RLbased search algorithm to explore the configuration space. However, these searching methods can be expensive and are often sensitive to the hyper-parameters and the initialization of the RL based algorithm. Alternatively, the recent work of <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b35">37]</ref> introduces a Hessian based method, where the bit precision setting is based on the second-order sensitivity of each layer. However, this approach does require access to the original training set, a limitation which we address in ZEROQ.</p><p>III. METHODOLOGY For a typical supervised computer vision task, we seek to minimize the empirical risk loss, i.e.,</p><formula xml:id="formula_0">min ? L(?) = 1 N N i=1 f (M(?; xi), yi),<label>(1)</label></formula><p>where ? ? R n is the learnable parameter, f (?, ?) is the loss function (typically cross-entropy loss), (x i , y i ) is the training input/label pair, M is the NN model with L layers, and N is the total number of training data points. Here, we assume that the input data goes through standard preprocessing normalization of zero mean (? 0 = 0) and unit variance (? 0 = 1). Moreover, we assume that the model has L BN layers denoted as BN 1 , BN 2 , ..., BN L . We denote the activations before the i-th BN layer with z i (in other words z i is the output of the i-th convolutional layer). During inference, z i is normalized by the running mean (? i ) and variance (? 2 i ) of parameters in the i-th BN layer (BN i ), which is pre-computed during the training process. Typically BN layers also include scaling and bias correction, which we denote as ? i and ? i , respectively.</p><p>We assume that before quantization, all the NN parameters and activations are stored in 32-bit precision and that we have no access to the training/validation datasets. To quantize a tensor (either weights or activations), we clip the parameters to a range of [a, b] (a, b ? R), and we uniformly discretize the space to 2 k ? 1 even intervals using asymmetric quantization. That is, the length of each interval will be ? = b?a 2 k ?1 . As a result, the original 32-bit single-precision values are mapped to unsigned integers within the range of [0, 2 k ? 1]. Some work has proposed non-uniform quantization schemes which can capture finer details of weight/activation distribution <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b40">42]</ref>. However, we only use asymmetric uniform quantization, as the non-uniform methods are typically not suitable for efficient hardware execution.</p><p>The ZEROQ framework supports both fixed-precision and mixed-precision quantization. In the latter scheme, different layers of the model could have different bit precisions (different k). The main idea behind mixed-precision quantization is to keep more sensitive layers at higher precision, and more aggressively quantize less sensitive layers, without increasing overall model size. As we will show later, this mixed-precision quantization is key to achieving high accuracy for ultra-low precision settings such as 4-bit quantization. Typical choices for k for each layer are {2, 4, 8} bit. Note that this mixedprecision quantization leads to exponentially large search space, as every layer could have one of these bit precision settings. It is possible to avoid this prohibitive search space if we could measure the sensitivity of the model to the quantization of each layer <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b35">37]</ref>. For the case of posttraining quantization (i.e. without fine-tuning), a good sensitivity metric is to use KullbackLeibler (KL) divergence between the original model and the quantized model, defined as:</p><formula xml:id="formula_1">?i(k) = 1 N N dist j=1 KL(M(?; xj), M(?i(k-bit); xj)).<label>(2)</label></formula><p>where ? i (k) measures how sensitive the i-th layer is when quantized to k-bit, and? i (k-bit) refers to quantized model parameters in the i-th layer with k-bit precision. If ? i (k) is small, the output of the quantized model will not significantly deviate from the output of the full precision model when quantizing the i-th layer to k-bits, and thus the i-th layer is relatively insensitive to k-bit quantization, and vice versa. This process is schematically shown in <ref type="figure">Figure 1</ref> for ResNet18. However, an important problem is that for zero-shot quantization we do not have access to the original training dataset x j in Eq. 2. We address this by "distilling" a synthetic input data to match the statistics of the original training dataset, which we refer to as Distilled Data. We obtain the Distilled Data by solely analyzing the trained model itself, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Distilled Data</head><p>For zero-shot quantization, we do not have access to any of the training/validation data. This poses two challenges. First, we need to know the range of values for activations of each layer so that we can clip the range for quantization (the [a, b] range mentioned above). However, we cannot determine this range without access to the training dataset. This is a problem for both uniform and mixed-precision quantization. Second, another challenge is that for mixed-precision quantization, we need to compute ? i in Eq. 2, but we do not have access to training data x j . A very na?ve method to address these challenges is to create a random input data drawn from a Gaussian distribution with zero mean and unit variance and feed it into the model. However, this approach cannot capture the correct statistics of the activation data corresponding to the original training dataset. This is illustrated in <ref type="figure">Figure 2</ref> (left), where we plot the sensitivity of each layer of ResNet50 on ImageNet measured with the original training dataset (shown in black) and Gaussian based input data (shown in red). As one can see, the Gaussian data clearly does not capture the correct sensitivity of the model. For instance, for the first three layers, the sensitivity order of the red line is actually the opposite of the original training data. To address this problem, we propose a novel method to "distill" input data from the NN model itself, i.e., to generate synthetic data carefully engineered based on the properties of the NN. In particular, we solve a distillation optimization problem, in order to learn an input data distribution that best matches the statistics encoded in the BN layer of the model. In more detail, we solve the following optimization problem:</p><formula xml:id="formula_2">min x r L i=0 ? r i ? ?i 2 2 + ? r i ? ?i 2 2 ,<label>(3)</label></formula><p>where x r is the reconstructed (distilled) input data, and ? r i /? r i are the mean/standard deviation of the Distilled Data's distribution at layer i, and ? i /? i are the corresponding mean/standard deviation parameters stored in the BN layer at layer i. In other words, after solving this optimization problem, we can distill an input data which, when fed into the network, can have a statistical distribution that closely matches the original model. Please see Algorithm 1 for a description. This Distilled Data can then be used to address the two challenges described earlier. First, we can use the Distilled Data's activation range to determine quantization clipping parameters (the [a, b] range mentioned above). Note that some prior work <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b42">44]</ref> address this by using limited (unlabeled) data to determine the activation range. However, this contradicts the assumptions of zero-shot quantization, and may not be applicable for certain applications. Second, we can use the Distilled Data and feed it in Eq. 2 to determine the quantization sensitivity (? i ). The latter is plotted for ResNet50 in <ref type="figure">Figure 2</ref> (left) shown in solid blue color. As one can see, the Distilled Data closely matches the sensitivity of the model as compared to using Gaussian input data (shown in red). We show a visualization of the random Gaussian data as well as the Distilled Data for ResNet50 in <ref type="figure" target="#fig_0">Figure 3</ref>. We can see that the Distilled Data can capture fine-grained local structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pareto Frontier</head><p>As mentioned before, the main challenge for mixedprecision quantization is to determine the exact bit precision configuration for the entire NN. For an L-layer model with m possible precision options, the mixed-precision search space, denoted as S, has an exponential size of m L . For example for ResNet50 with just three bit precision of {2, 4, 8} (i.e., m = 3), the search space contains 7.2 ? 10 23 configurations. However, we can use the sensitivity metric in Eq. 2 to reduce this search space. The main idea is to use higher bit precision for layers that are more sensitive, and lower bit precision for layers that are less sensitive. This gives us a relative ordering on the number of bits. To compute the precise bit precision setting, we propose a Pareto frontier approach similar to the method used in <ref type="bibr" target="#b5">[7]</ref>.</p><p>The Pareto frontier method works as follows. For a target quantized model size of S target , we measure the overall sensitivity of the model for each bit precision configuration that results in the S target model size. We choose the bitprecision setting that corresponds to the minimum overall sensitivity. In more detail, we solve the following optimization problem:</p><formula xml:id="formula_3">min {k i } L i=1 ?sum = L i=1 ?i(ki) s.t. L i=1</formula><p>Pi * ki ? Starget, (4)  <ref type="table" target="#tab_3">Table Ia.</ref> where k i is the quantization precision of the i-th layer, and P i is the parameter size for the i-th layer. Note that here we make the simplifying assumption that the sensitivity of different layers are independent of the choice of bits for other layers (hence ? i only depends on the bit precision for the i-th layer). <ref type="bibr" target="#b0">2</ref> Using a dynamic programming method we can solve the best setting with different S target together, and then we plot the Pareto frontier. An example is shown in <ref type="figure" target="#fig_1">Figure 4</ref> for ResNet50 model, where the x-axis is the model size for each bit precision configuration, and the y-axis is the overall model perturbation/sensitivity. Each blue dot in the figure represents a mixed-precision configuration. In ZEROQ, we choose the bit precision setting that has the smallest perturbation with a specific model size constraint.</p><p>Importantly, note that the computational overhead of computing the Pareto frontier is O(mL). This is because we compute the sensitivity of each layer separately from other layers. That is, we compute sensitivity ? i (i = 1, 2, ..., L) with respect to all m different precision options, which leads to the O(mL) computational complexity. We should note that this Pareto Frontier approach (including the Dynamic Programming optimizer), is not theoretically guaranteed to result in the best possible configuration, out of all possibilities in the exponentially large search space. However, our results show that the final mixed-precision configuration achieves state-ofthe-art accuracy with small performance loss, as compared to the original model in single precision. We abbreviate quantization bits used for weights as "W-bit" (for activations as "A-bit"), top-1 test accuracy as "Top-1." Here, "MP" refers to mixed-precision quantization, "No D" means that none of the data is used to assist quantization, and "No FT" stands for no fine-tuning (retraining). Compared to post-quantization methods OCS <ref type="bibr" target="#b42">[44]</ref>, OMSE <ref type="bibr" target="#b17">[19]</ref>, and DFQ <ref type="bibr" target="#b30">[32]</ref>, ZEROQ achieves better accuracy. ZEROQ ? means using percentile for quantization.</p><p>(a) ResNet50</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>No </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>In this section, we extensively test ZEROQ on a wide range of models and datasets. We first start by discussing the zero-shot quantization of ResNet18/50, MobileNet-V2, and ShuffleNet on ImageNet in Section IV-A. Additional results for quantizing ResNet152, InceptionV3, and SqueezeNext on ImageNet, as well as ResNet20 on Cifar10 are provided in Appendix C. We also present results for object detection using RetinaNet tested on Microsoft COCO dataset in Section IV-B. We emphasize that all of the results achieved by ZEROQ are 100% zero-shot without any need for fine-tuning.</p><p>We also emphasize that we used exactly the same hyperparameters (e.g., the number of iterations to generate Distilled Data) for all experiments, including the results on Microsoft COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ImageNet</head><p>We start by discussing the results on the ImageNet dataset. For each model, after generating Distilled Data based on Eq. 3, we compute the sensitivity of each layer using Eq. 2 for different bit precision. Next, we use Eq. 4 and the Pareto frontier introduced in Section III-B to get the best bit-precision configuration based on the overall sensitivity for a given model size constraint. We denote the quantized results as WwAh where w and h denote the bit precision used for weights and activations of the NN model.</p><p>We present zero-shot quantization results for ResNet50 in <ref type="table" target="#tab_3">Table Ia</ref>. As one can see, for W8A8 (i.e., 8-bit quantization for both weights and activations), ZEROQ results in only 0.05% accuracy degradation. Further quantizing the model to W6A6, ZEROQ achieves 77.43% accuracy, which is 2.63% higher than OCS <ref type="bibr" target="#b42">[44]</ref>, even though our model is slightly smaller (18.27MB as compared to 18.46MB for OCS). <ref type="bibr" target="#b1">3</ref> We show that we can further quantize ResNet50 down to just 12.17MB with mixed precision quantization, and we obtain 75.80% accuracy. Note that this is 0.82% higher than OMSE <ref type="bibr" target="#b17">[19]</ref> with access to training data and 5.74% higher than zero-shot version of OMSE. Importantly, note that OMSE keeps activation bits at 32-bits, while for this comparison our results use 8-bits for the activation (i.e., 4? smaller activation memory footprint than OMSE). For comparison, we include results for PACT <ref type="bibr" target="#b3">[5]</ref>, a standard quantization method that requires access to training data and also requires fine-tuning.</p><p>An important feature of the ZEROQ framework is that it can perform the quantization with very low computational overhead. For example, the end-to-end quantization of ResNet50 takes less than 30 seconds on an 8 Tesla V100 GPUs (one epoch training time on this system takes 100 minutes). In terms of timing breakdown, it takes 3s to generate the Distilled Data, 12s to compute the sensitivity for all layers of ResNet50, and 14s to perform Pareto Frontier optimization.</p><p>We also show ZEROQ results on MobileNetV2 and compare it with both DFQ <ref type="bibr" target="#b30">[32]</ref> and fine-tuning based methods <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b31">33]</ref>, as shown in <ref type="table" target="#tab_3">Table Ib</ref>. For W8A8, ZEROQ has less than 0.12% accuracy drop as compared to baseline, and it achieves 1.71% higher accuracy as compared to DFQ method. Further compressing the model to W6A6 with mixedprecision quantization for weights, ZEROQ can still outperform Integer-Only <ref type="bibr" target="#b16">[18]</ref> by 1.95% accuracy, even though ZEROQ does not use any data or fine-tuning. ZEROQ can achieve 68.83% accuracy even when the weight compression is 8?, which corresponds to using 4-bit quantization for weights on average.</p><p>We also experimented with percentile based clipping to determine the quantization range <ref type="bibr" target="#b23">[25]</ref> (please see Section D for details). The results corresponding to percentile based clipping are denoted as ZeroQ ? and reported in <ref type="table" target="#tab_3">Table I</ref>. We found that using percentile based clipping is helpful for low precision quantization. Other choices for clipping methods have been proposed in the literature. Here we note that our approach is orthogonal to these improvements and that ZEROQ could be combined with these methods.</p><p>We also apply ZEROQ to quantize efficient and highly compact models such as ShuffleNet, whose model size is only 5.94MB. To the best of our knowledge, there exists no prior zero-shot quantization results for this model. ZEROQ achieves a small accuracy drop of 0.13% for W8A8. We can further quantize the model down to an average of 4-bits for weights, which achieves a model size of only 0.73MB, with an accuracy of 58.96%. We also compare with the recent Data-Free Compression (DFC) <ref type="bibr" target="#b10">[12]</ref> method. There are two main differences between ZEROQ and DFC. First, DFC proposes a fine-tuning method to recover accuracy for ultra-low precision cases. This can be time-consuming and as we show it is not necessary. In particular, we show that with mixed-precision quantization one can actually achieve higher accuracy without any need for finetuning. This is shown in <ref type="table" target="#tab_3">Table III</ref> for ResNet18 quantization on ImageNet. In particular, note the results for W4A4, where the DFC method without fine-tuning results in more than 15% accuracy drop with a final accuracy of 55.49%. For this reason, the authors propose a method with post quantization training, which can boost the accuracy to 68.05% using W4A4 for intermediate layers, and 8-bits for the first and last layers. In contrast, ZEROQ achieves a higher accuracy of 69.05% without any need for fine-tuning. Furthermore, the end-to-end zero-shot quantization of ResNet18 takes only 12s on an 8-V100 system (equivalent to 0.4% of the 45 minutes time for one epoch training of ResNet18 on ImageNet). Secondly, DFC method uses Inceptionism <ref type="bibr" target="#b29">[31]</ref> to facilitate the generation of data with random labels, but it is hard to extend this for object detection and image segmentation tasks. We include additional results of quantized ResNet152, InceptionV3, and SqueezeNext on ImageNet, as well as ResNet20 on Cifar10, in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Microsoft COCO</head><p>Object detection is often much more complicated than ImageNet classification. To demonstrate the flexibility of our approach we also test ZEROQ on an object detection task on Microsoft COCO dataset. RetinaNet <ref type="bibr" target="#b25">[27]</ref> is a state-of-the-art single-stage detector, and we use the pretrained model with ResNet50 as the backbone, which can achieve 36.4 mAP. <ref type="bibr" target="#b2">4</ref> One of the main difference of RetinaNet with previous NNs we tested on ImageNet is that some convolutional layers in RetinaNet are not followed by BN layers. This is because of the presence of a feature pyramid network (FPN) <ref type="bibr" target="#b24">[26]</ref>, and it means that the number of BN layers is slightly smaller than that of convolutional layers. However, this is not a limitation and the ZEROQ framework still works well. Specifically, we extract the backbone of RetinaNet and create Distilled Data. Afterwards, we feed the Distilled Data into RetinaNet to measure the sensitivity as well as to determine the activation range for the entire NN. This is followed by optimizing for the Pareto Frontier, discussed earlier.</p><p>The results are presented in <ref type="table" target="#tab_3">Table II</ref>. We can see that for W8A8 ZEROQ has no performance degradation. For W6A6, ZEROQ achieves 35.9 mAP. Further quantizing the model to an average of 4-bits for the weights, ZEROQ achieves 33.7 mAP. Our results are comparable to the recent results of FQN <ref type="bibr" target="#b23">[25]</ref>, even though it is not a zero-shot quantization method (i.e., it uses the full training dataset and requires finetuning). However, it should be mentioned that ZEROQ keeps the activations to be 8-bits, while FQN uses 4-bit activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION STUDY</head><p>Here, we present an ablation study for the two components of ZEROQ: (i) the Distilled Data generated by Eq. 3 to help sensitivity analysis and determine activation clipping range; and (ii) the Pareto frontier method for automatic bit-precision assignment. Below we discuss the ablation study for each part separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Distilled Data</head><p>In this work, all the sensitivity analysis and the activation range are computed on the Distilled Data. Here, we perform an ablation study on the effectiveness of Distilled Data as compared to using just Gaussian data. We use three different types of data sources, (i) Gaussian data with mean "0" and variance "1", (ii) data from training dataset, (iii) our Distilled Data, as the input data to measure the sensitivity and to determine the activation range. We quantize ResNet50 and MobileNetV2 to an average of 4-bit for weights and 8-bit for activations, and we report results in <ref type="table" target="#tab_3">Table IV.</ref> For ResNet50, using training data results in 75.95% testing accuracy. With Gaussian data, the performance degrades to 75.44%. ZEROQ can alleviate the gap between Gaussian data and training data and achieves 75.80%. For more compact/efficient models such as MobileNetV2, the gap between using Gaussian data and using training data increases to 2.33%. ZEROQ can still achieve 68.83%, which is only 0.23% lower than using training data. Additional results for ResNet18, ShuffleNet and SqueezeNext are shown in Table VIII. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensitivity Analysis</head><p>Here, we perform an ablation study to show that the bit precision of the Pareto frontier method works well. To test this, we compare ZEROQ with two cases, one where we choose a bit-configuration that corresponds to maximizing ? sum (which is opposite to the minimization that we do in ZEROQ), and one case where we use random bit precision for different layers. We denote these two methods as Inverse and Random. The results for quantizing weights to an average of 4-bit and activations to 8-bit are shown in <ref type="table">Table V</ref>. We report the best and worst testing accuracy as well as the mean and variance in the results out of 20 tests. It can be seen that ZEROQ results in significantly better testing performance as compared to Inverse and Random. Another noticeable point is that the best configuration (i.e., minimum ? sum ) can outperform 0.18% than the worst case among the top-20 configurations from ZEROQ, which reflects the advantage of the Pareto frontier method. Also, notice the small variance of all configurations generated by ZEROQ. TABLE V: Ablation study for sensitivity analysis on ImageNet (W4A8) with ResNet50. Top-20 configurations are selected based on different sensitivity metric types. We report the best, mean, and worst accuracy among 20 configurations. "ZEROQ" and "Inverse" mean selecting the bit configurations to minimize and maximize the overall sensitivity, respectively, under the average 4-bit weight constraint. "Random" means randomly selecting the bit for each layer and making the total size equivalent to 4-bit weight quantization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We have introduced ZEROQ, a novel post-training quantization method that does not require any access to the training/validation data. Our approach uses a novel method to distill an input data distribution to match the statistics in the batch normalization layers of the model. We show that this Distilled Data is very effective in capturing the sensitivity of different layers of the network. Furthermore, we present a Pareto frontier method to select automatically the bit-precision configuration for mixed-precision settings. An important aspect of ZEROQ is its low computational overhead. For example, the end-to-end zero-shot quantization time of ResNet50 is less than 30 seconds on an 8-V100 GPU system. We extensively test ZEROQ on various datasets and models. This includes various ResNets, InceptionV3, MobileNetV2, ShuffleNet, and SqueezeNext on ImageNet, ResNet20 on Cifar10, and even RetinaNet for object detection on Microsoft COCO dataset. We consistently achieve higher accuracy with the same or smaller model size compared to previous posttraining quantization methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pareto Frontier</head><p>In Section III-B, we presented how we compute the overall sensitivity incurred by performing mixed-precision quantization. In particular, in Eq. 4 we made the simplifying assumption that the sensitivity of each layer to quantization is independent to sensitivity of other layers (we refer to this as independence assumption). This is clearly not the case in practice. One can instead directly compute the sensitivity for each possible bit-precision computation without any approximation but this is not possible as there are m L possible bit-precision configurations. Here we discuss our approach which falls in between these two extremes. Instead of computing the sensitivity of the entire network at once, we break the network into L/a groups, with each group containing a layers. Furthermore, we break the x-axis (model size) of the Pareto frontier plot into b intervals in every steps mentioned below.</p><p>We start with the first a layers of the network. We compute the sensitivity of these layers with the independence assumption. This means we only have to compute m ? a sensitivities. Afterwards, for each interval on the x-axis, we choose topt configurations that have the lowest overall sensitivity when the first a layers are quantized. We then relax the independence assumption for theset configurations and recompute the overall sensitivity, ? sum , without any approximation. This leads to a cost oft of computing Eq. 4. We then select the top t configurations out of these. We conduct this process for all the b intervals for the model size. Therefore, the total cost will bet ? b.</p><p>The next step is to consider the next set of a layers. This is similar to the algorithm for the first step, except that now we need to consider the top t ? b configurations selected for the first a layers. We first make the independence assumption for the second a layers. Then we choose topt ? b configurations out of all t ? b ? m a possible bit configurations for the first 2a layers (this number is obtained by combining the top t ? b configurations of the first a layers and m a possible configurations in the second a layers). Similar to before, we then relax the independence assumption and compute the correct sensitivity, ? sum , without any approximation. We then select the final top t ? b configurations for the first 2a layers based on this.</p><p>This process needs to be performed for all the L/a groups. As a result, the total computational cost becomes (L/a) ?t ? b + m ? L. We find that this approach gives a good trade-off between the two extremes. Our experiments show that the accuracy is not sensitive to the hyperparameters, and we typically sett, t, b, a to be 10, 5, 200, 5, respectively. It should be noted that this approach has a small computational overhead but can automatically lead to bit precision settings with good empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on CIFAR-10</head><p>In this section, we show the results of our ZEROQ on CIFAR-10 dataset with ResNet20. See <ref type="table" target="#tab_3">Table VI</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extra Results on ImageNet</head><p>In this section, we show extra results for our ZEROQ on ImageNet with ResNet152, InceptionV3, and SqueezeNext in <ref type="table" target="#tab_3">Table VII</ref>. We also show more results to illustrate the effect of Distilled Data compared with Gaussian noise in <ref type="table" target="#tab_3">Table VIII</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Clipping</head><p>Quantization maps a single-precision tensor z to a low-precision tensor Q(z). This includes two steps: 1) clipping the original tensor to range [a, b], and then 2) mapping this range to integer range [0, 2 k ? 1]. A simple way is to set [a, b] = [min(z), max(z)] for conventional quantization methods. Recently, more effort has been spent on choosing the "optimal" range of [a, b] <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b23">25]</ref>, which are the so-called clipping methods.</p><p>In all of our experiments above, we use the simplest way, i.e., [a, b] = [min(z), max(z)], to conduct the quantization. The main reason behind this is two-fold: (i) we want to show the efficacy of ZEROQ without the assistance of any other technique; (ii) some of proposed methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b3">5]</ref> need hyper-parameter tuning to get the optimal a and b which can be costly. However, we show that performance of ZEROQ can be further boosted by the weight clipping method, if the slightly higher computational overhead could be afforded. In particular, we use the "percentile" method proposed in <ref type="bibr" target="#b23">[25]</ref>. This method directly clips a single-precision weight tensor to ?-th and (1 ? ?)-th percentiles (we refer the reader to <ref type="bibr" target="#b23">[25]</ref> for more details). As shown in <ref type="table" target="#tab_3">Table I and Table III</ref>, ZEROQ can be further improved by weight clipping.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of Gaussian data (left) and Distilled Data (right). More local structure can be seen in our Distilled Data that is generated according to Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>The Pareto frontier of ResNet50 on ImageNet. Each point shows a mixed-precision bit setting. The x-axis shows the resulting model size for each configuration, and the y-axis shows the resulting sensitivity. In practice, a constraint for model size is set. Then the Pareto frontier method chooses a bit-precision configuration that results in minimal perturbation. We show two examples for 4 and 6-bit mixed precision configuration shown in red and orange. The corresponding results are presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Generation of Distilled Data Input: Model: M with L Batch Normalization layers Output: A batch of distilled data: x r Generate random data from Gaussian: x r Get ? i , ? i from Batch Normalization layers of M, i ? 0, 1, . . . , L // Note that ?0 = 0, ?0 = 1 for j = 1, 2, . . . do Forward propagate M(x r ) and gather intermediate activations Get? i and? i from intermediate activations, i ? 1, . . . , n Compute? 0 and? 0 of x r Compute the loss based on Eq. 3 Backward propagate and update x r</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Quantization results of ResNet50, MobileNetV2, and ShuffleNet on ImageNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Object detection on Microsoft COCO using Reti-naNet. By keeping activations to be 8-bit, our 4-bit weight result is comparable with recently proposed method FQN<ref type="bibr" target="#b23">[25]</ref>, which relies on fine-tuning. (Note that FQN uses 4-bit activations and the baseline used in<ref type="bibr" target="#b23">[25]</ref> is 35.6 mAP).</figDesc><table><row><cell>Method</cell><cell cols="4">No D No FT W-bit A-bit Size (MB) mAP</cell></row><row><cell>Baseline</cell><cell>32</cell><cell>32</cell><cell cols="2">145.10 36.4</cell></row><row><cell>FQN [25]</cell><cell>4</cell><cell>4</cell><cell>18.13</cell><cell>32.5</cell></row><row><cell>ZEROQ</cell><cell>MP</cell><cell>8</cell><cell>18.13</cell><cell>33.7</cell></row><row><cell>ZEROQ</cell><cell>MP</cell><cell>6</cell><cell>24.17</cell><cell>35.9</cell></row><row><cell>ZEROQ</cell><cell>8</cell><cell>8</cell><cell>36.25</cell><cell>36.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Uniform post-quantization on ImageNet with ResNet18. We use percentile clipping for W4A4 and W4A8 settings. ZEROQ ? means using percentile for quantization.</figDesc><table><row><cell>Method</cell><cell cols="6">No D No FT W-bit A-bit Size (MB) Top-1</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>32</cell><cell>32</cell><cell>44.59</cell><cell>71.47</cell></row><row><cell>PACT [5]</cell><cell></cell><cell></cell><cell>4</cell><cell>4</cell><cell>5.57</cell><cell>69.20</cell></row><row><cell>DFC [12]</cell><cell></cell><cell></cell><cell>4</cell><cell>4</cell><cell>5.58</cell><cell>55.49</cell></row><row><cell>DFC [12]</cell><cell></cell><cell></cell><cell>4</cell><cell>4</cell><cell>5.58</cell><cell>68.06</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell>MP</cell><cell>4</cell><cell>5.57</cell><cell>-</cell></row><row><cell>ZEROQ  ?</cell><cell></cell><cell></cell><cell>MP</cell><cell>4</cell><cell>5.57</cell><cell>69.05</cell></row><row><cell>Integer-Only[18]</cell><cell></cell><cell></cell><cell>6</cell><cell>6</cell><cell>8.36</cell><cell>67.30</cell></row><row><cell>DFQ [32]</cell><cell></cell><cell></cell><cell>6</cell><cell>6</cell><cell>8.36</cell><cell>66.30</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell>MP</cell><cell>6</cell><cell>8.35</cell><cell>71.30</cell></row><row><cell>RVQuant [33]</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>11.15</cell><cell>70.01</cell></row><row><cell>DFQ [32]</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>11.15</cell><cell>69.70</cell></row><row><cell>DFC [12]</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>11.15</cell><cell>69.57</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>11.15</cell><cell>71.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study for Distilled Data on ResNet50 and MobileNetv2. We show the performance of ZEROQ with different data to compute the sensitivity and to determine the activation range. All quantized models have the same size as models with 4-bit weights and 8-bit activations.</figDesc><table><row><cell>Method</cell><cell cols="4">W-bit A-bit ResNet50 MobileNetV2</cell></row><row><cell>Baseline</cell><cell>32</cell><cell>32</cell><cell>77.72</cell><cell>73.03</cell></row><row><cell>Gaussian</cell><cell>MP</cell><cell>8</cell><cell>75.44</cell><cell>66.73</cell></row><row><cell cols="2">Training Data MP</cell><cell>8</cell><cell>75.95</cell><cell>69.06</cell></row><row><cell cols="2">Distilled Data MP</cell><cell>8</cell><cell>75.80</cell><cell>68.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>All results show that ZEROQ could exceed previous zero-shot quantization methods. We have open sourced ZEROQ framework [1]. Downsample + . . . . . .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">APPENDIX</cell><cell></cell><cell></cell></row><row><cell></cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell></cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell>I</cell><cell>I/ 2</cell><cell>512 512 conv16/17</cell><cell>I/ 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>128 128</cell><cell>128 128</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>conv6/7</cell><cell>conv8/9</cell><cell></cell><cell></cell></row><row><cell>2 I</cell><cell>I</cell><cell>I</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>64</cell><cell>64 64</cell><cell>64 64</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv1</cell><cell>conv2/3</cell><cell>conv4/5</cell><cell></cell><cell></cell><cell></cell><cell>FC&amp;softmax</cell></row><row><cell>2-bit</cell><cell>2-bit</cell><cell>2-bit</cell><cell>2-bit</cell><cell>2-bit</cell><cell>2-bit</cell><cell>2-bit</cell></row><row><cell>4-bit</cell><cell>4-bit</cell><cell>4-bit</cell><cell>4-bit</cell><cell>4-bit</cell><cell>4-bit</cell><cell>4-bit</cell></row><row><cell>8-bit</cell><cell>8-bit</cell><cell>8-bit</cell><cell>8-bit</cell><cell>8-bit</cell><cell>8-bit</cell><cell>8-bit</cell></row><row><cell></cell><cell cols="4">Fig. 5: Mixed precision illustration of ResNet18 on ImageNet.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>ResNet20 on CIFAR-10 Method No Data No FT W-bit A-bit Size (MB) Top-1</figDesc><table><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>32</cell><cell>32</cell><cell>1.04</cell><cell>94.03</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell>MP</cell><cell>8</cell><cell>0.13</cell><cell>93.16</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell>MP</cell><cell>6</cell><cell>0.20</cell><cell>93.87</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>0.26</cell><cell>93.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Additional results on ImageNet</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) ResNet152</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) InceptionV3</cell></row><row><cell>Method</cell><cell cols="6">No D No FT w-bit a-bit Size (MB) Top-1</cell><cell>Method</cell><cell></cell><cell cols="4">No D No FT W-bit A-bit Size (MB) Top-1</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell cols="2">32 32</cell><cell cols="2">229.62 80.08</cell><cell>Baseline</cell><cell></cell><cell>-</cell><cell>-</cell><cell>32</cell><cell>32</cell><cell>90.92</cell><cell>78.88</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell cols="2">MP 8</cell><cell>28.70</cell><cell>78.00</cell><cell>ZEROQ</cell><cell></cell><cell></cell><cell></cell><cell>MP</cell><cell>8</cell><cell>11.35</cell><cell>77.57</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell cols="2">MP 6</cell><cell>43.05</cell><cell>77.88</cell><cell>OCS[44]</cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell>6</cell><cell>17.22</cell><cell>71.30</cell></row><row><cell>RVQuant [33]</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>57.41</cell><cell>78.35</cell><cell>ZEROQ</cell><cell></cell><cell></cell><cell></cell><cell>MP</cell><cell>6</cell><cell>17.02</cell><cell>78.76</cell></row><row><cell>ZEROQ</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>57.41</cell><cell>78.94</cell><cell cols="2">RVQuant [33]</cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>22.47</cell><cell>74.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ZEROQ</cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>22.47</cell><cell>78.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) SqueezeNext</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell cols="6">No D No FT W-bit A-bit Size (MB) Top-1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Baseline</cell><cell></cell><cell>-</cell><cell>-</cell><cell>32</cell><cell>32</cell><cell>9.86</cell><cell>69.38</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ZEROQ</cell><cell></cell><cell></cell><cell></cell><cell>MP</cell><cell>8</cell><cell>1.23</cell><cell>59.23</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ZEROQ</cell><cell></cell><cell></cell><cell></cell><cell>MP</cell><cell>6</cell><cell>1.85</cell><cell>68.17</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ZEROQ</cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>8</cell><cell>2.47</cell><cell>69.17</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII :</head><label>VIII</label><figDesc>Ablation study for Distilled Data on ResNet18, ShuffleNet and SqueezeNext. We show the performance of ZEROQ with different sources of data to compute the sensitivity and determine the activation range. All quantized models have the same size as quantized models with 4-bit weights.</figDesc><table><row><cell>Method</cell><cell cols="5">W-bit A-bit ResNet18 ShuffleNet SqueezeNext</cell></row><row><cell>Baseline</cell><cell>32</cell><cell>32</cell><cell>71.47</cell><cell>65.07</cell><cell>69.38</cell></row><row><cell>Gaussian</cell><cell>MP</cell><cell>8</cell><cell>67.87</cell><cell>56.23</cell><cell>48.41</cell></row><row><cell>Training Data</cell><cell>MP</cell><cell>8</cell><cell>68.61</cell><cell>58.90</cell><cell>62.55</cell></row><row><cell>Distilled Data</cell><cell>MP</cell><cell>8</cell><cell>68.45</cell><cell>57.50</cell><cell>59.23</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please see Section A where we describe how we relax this assumption without having to perform an exponentially large computation for the sensitivity for each bit precision setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Importantly note that OCS requires access to the training data, while ZEROQ does not use any training/validation data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Here we use the standard mAP 0.5:0.05:0.95 metric on COCO dataset.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Experimental determination of precision requirements for back-propagation training of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>International Computer Science Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Post training 4-bit quantization of convolution networks for rapid-deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno>abs/1810.05723</idno>
	</analytic>
	<monogr>
		<title level="m">)</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cat: Compressionaware training for bandwidth reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11481</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<title level="m">Pact: Parameterized clipping activation for quantized neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hawq-v2: Hessian aware trace-weighted quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiyaan</forename><surname>Arfeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03852</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hawq: Hessian aware quantization of neural networks with mixed-precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Squeezenext: Hardware-aware neural network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Workshop paper in CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The knowledge within: Methods for data-free model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Haroush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01274</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Workshop paper in NIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing&apos;s energy problem (and what we can do about it)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5 MB model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Low-bit quantization of neural networks for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Kravchik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kisilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Choukroun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Co-design of deep neural nets and neural net accelerators for embedded vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th ACM/ESDA/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwon</forename><surname>Jun Haeng Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saerom</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Jo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05488</idno>
		<title level="m">Quantization for rapid deployment of deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04711</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Ternary weight networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully quantized network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2810" to="2819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring the regularity of sparse structure in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenshuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Workshop paper in CVPR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Same, same but different-recovering neural network quantization error through weight factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Meller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Almog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Grobman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01917</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Tyka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Data-free quantization through weight equalization and bias correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Value-aware quantization for training and inference of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="580" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05840</idno>
		<title level="m">Hessian based ultra low precision quantization of bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">HAQ: Hardware-aware automated quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mixed precision quantization of convnets via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00090</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LQ-Nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving neural network quantization without retraining using outlier channel splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritchie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dotzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive quantization for deep neural network</title>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<editor>Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<title level="m">Trained ternary quantization. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
