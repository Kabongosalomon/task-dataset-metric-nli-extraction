<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<email>lei.w@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<postCode>Data61/CSIRO</postCode>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
							<email>piotr.koniusz@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<postCode>Data61/CSIRO</postCode>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we build on a concept of self-supervision by taking RGB frames as input to learn to predict both action concepts and auxiliary descriptors e.g., object descriptors. So-called hallucination streams are trained to predict auxiliary cues, simultaneously fed into classification layers, and then hallucinated at the testing stage to aid network. We design and hallucinate two descriptors, one leveraging four popular object detectors applied to training videos, and the other leveraging image-and video-level saliency detectors. The first descriptor encodes the detector-and ImageNet-wise class prediction scores, confidence scores, and spatial locations of bounding boxes and frame indexes to capture the spatio-temporal distribution of features per video. Another descriptor encodes spatio-angular gradient distributions of saliency maps and intensity patterns. Inspired by the characteristic function of the probability distribution, we capture four statistical moments on the above intermediate descriptors. As numbers of coefficients in the mean, covariance, coskewness and cokurtotsis grow linearly, quadratically, cubically and quartically w.r.t. the dimension of feature vectors, we describe the covariance matrix by its leading n eigenvectors (so-called subspace) and we capture skewness/kurtosis rather than costly coskewness/cokurtosis. We obtain state of the art on five popular datasets such as Charades and EPIC-Kitchens.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action Recognition (AR) has progressed from hand-crafted video representations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84]</ref> to Convolutional Neural Networks (CNN) <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref>. The two-stream networks <ref type="bibr" target="#b69">[70]</ref>, 3D spatio-temporal features <ref type="bibr" target="#b74">[75]</ref>, spatio-temporal ResNet model <ref type="bibr" target="#b18">[19]</ref> and the new Inflated 3D (I3D) convolutions network pre-trained on Kinetics-400 <ref type="bibr" target="#b4">[5]</ref>. Often, AR combine the RGB and optical flow inputs, and benefit from a late fusion (next to the classifier) with low-level representations such as Improved Dense Trajectory (IDT) descriptors <ref type="bibr" target="#b80">[81]</ref> due to their highly complementary nature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b9">10]</ref>. Recently, AssembleNet and AssembleNet++ <ref type="bibr" target="#b66">[67]</ref>, learnt with the Neural Architecture Search (NAS) have yielded superb results.</p><p>A recent AR pipeline <ref type="bibr" target="#b84">[85]</ref>, called DEEP-HAL, used IDT descriptors encoded with Bag-of-Words (BoW) <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b11">12]</ref> and Fisher Vectors (FV) <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> to learn them by so-called hallucination streams and generate at the testing stage to boost results beyond a naive fusion of modalities. DEEP-HAL and approach <ref type="bibr" target="#b73">[74]</ref> have shown that even optical flow frames encoded by a network can be learnt by another network trained on RGB frames only, thus pointing at redundancy in training both RGB and optical flow network streams. DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> has attained the state of the art on several AR benchmarks by learning to hallucinate IDT-based BoW/FV and Optical Flow Features (OFF) from a single RGB-based I3D network stream.  <ref type="figure" target="#fig_0">Figure 1a</ref> shows bounding boxes from four detectors. The faster R-CNN detector with ResNet101 focuses on human-centric actions such as stand, watch, talk, etc. The other three detectors discover e.g., oven, sink, clock, etc. <ref type="figure" target="#fig_0">Figure 1b</ref> shows that the MNL saliency detector focuses on spatial regions. <ref type="figure" target="#fig_0">Figure 1c</ref> shows ACLNet saliency detector discovers motion regions. DEEP-HAL opens up an exciting opportunity to investigate what other representations can coregularize/self-supervise a backbone network for AR with the goal of learning to hallucinate costly representations at the training stage and simply leveraging outputs of halluciantion streams at the testing time. We build on DEEP-HAL which already includes IDT-based BoW/FV and OFF streams. However, we investigate the self-supervisory ability of object/saliency detectors in DEEP-HAL. Moreover, beyond I3D backbone, we investigate the use of AssembleNet and AssembleNet++ but we disable their (impractical to obtain) segmentation mask input.</p><p>In this paper, we design and hallucinate two kinds of descriptors, namely Object Detection Features (ODF) and Saliency Detection Features (SDF). The ODF descriptor leverages faster R-CNN detector <ref type="bibr" target="#b61">[62]</ref> based on backbones such as (i) Inception V2 <ref type="bibr" target="#b72">[73]</ref>, (ii) Inception ResNet V2 <ref type="bibr" target="#b71">[72]</ref>, (iii) ResNet101 <ref type="bibr" target="#b29">[30]</ref> and (iv) NASNet <ref type="bibr" target="#b96">[97]</ref>. The Inception V2, Inception ResNet V2 and NASNet are pre-trained on the COCO dataset <ref type="bibr" target="#b50">[51]</ref> (91 object classes), whereas the ResNet101 is pre-trained on the AVA v2.1 dataset <ref type="bibr" target="#b26">[27]</ref> (80 human AR classes). The above detectors are applied to training videos to identify humans and objects. Such detected objects together with their relevance and class labels summarized with our descriptor encourage the AR pipeline to focus on semantically important regions and actors relevant to the task of action recognition. <ref type="figure" target="#fig_0">Figure 1a</ref> shows a few of bounding boxes detected by these four detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>We build on DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> which includes I3D RGB and Optical Flow networks (the latter net. is used only during training). For AssembleNet and AssembleNet++, the backbone encodes both RGB and the optical flow, which is synthesized on the fly from RGB frames. For the I3D variant, we remove the prediction and the last 1D conv. layers from I3D RGB and optical flow streams, we feed the 1024 ? 7 feature representations X (rgb) into Bag-of-Words (BoW), Fisher Vector (FV), the Optical Flow Features (OFF) and the High Abstraction Features (HAF) streams (dashed black) followed by the Power Normalization (PN) and Sketching (SK) blocks. The OFF stream is supervised by X (opt.) . For the AssembleNet variant, we obtain the 2048 feature representations X (rgb) and do not use the OFF stream/optical flow backbone. Moreover, we introduce DET1, ...,DET4, SAL1 and SAL2 streams corresponding to our detector-and saliency-based descriptors (dashed blue). The resulting feature vectors? (?) , where (?) denotes the stream name e.g., (det1), ..., (det2) etc., are reweighted by corresponding weights w (?) (magenta lines) and aggregated (sum) by (?). All? (?) are reweighted, aggregated (sum) and fed to Prediction Network (PredNet). By , we indicate that the Mean Square Error (MSE) losses are used during training to supervise all streams outputting? (?) by the ground-truth ? (?) . By , we indicate that the MSE losses are switched off for testing and? (?) are hallucinated/fed into PredNet to obtain labels y.</p><p>The SDF leverages image-and video-level saliency detectors such as MNL <ref type="bibr" target="#b94">[95]</ref> and ACLNet <ref type="bibr" target="#b93">[94]</ref> with the goal of identifying salient regions correlating with the human gaze in spatial and temporal sense. Saliency maps extracted from training videos and summarized by our descriptor help the AR pipeline learn spatial and temporal regions correlating with actions. <ref type="figure" target="#fig_0">Figures 1b and 1c</ref> show saliency maps from region-wise and temporal saliency detectors.</p><p>IDT descriptors are fused with the majority of modern AR pipelines <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b9">10]</ref> at the classifier level for the best performance while DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> learns to hallucinate, and feeds them into the classification branch called PredNet. In this paper, we go further and prepare two compact descriptors, ODF and SDF, and hallucinate them within DEEP-HAL. We equip each hallucination branch with a weighting mechanism adjusted per epoch to attain the best results. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates DEEP-HAL at the conceptual level.</p><p>For ODF descriptors, we concatenate together per bounding box per frame (i) the one-hot detection and (ii) ImageNet <ref type="bibr" target="#b64">[65]</ref> scores, (iii) embedded confidence scores, (iv) embedded bounding box coordinates, and (v) embedded normalized frame index. For all bounding boxes, we stack such features into a matrix. Inspired by the characteristic function of the probability density fun., we extract the mean, leading eigenvectors of covariance, skewness and kurtosis. For SDF descriptors, per frame, we encode saliency via (i) kernelized descriptor on spatio-angular gradient distributions of saliency maps and (ii) intensity patterns. We obtain an ODF per detector and an SDF per saliency detector. Our contributions are as follows: / i. We propose to use the object and human detectors to enhance the performance of AR pipelines.</p><p>ii. We design two types of statistically motivated high-order compact descriptors, Object Detection</p><p>Features and Saliency Detection Features, for the use in AR pipelines.</p><p>iii. We build on the recent DEEP-HAL pipeline <ref type="bibr" target="#b84">[85]</ref> but we introduce AssembleNet and Assem-bleNet++ apart from I3D backbone. Moreover, we introduce a weight learning mechanism for hallucinated feature vectors, and ODF and SDF are hallucinated which leads to the state-of-theart performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Below, we describe handcrafted spatio-temporal video descriptors, their encoding strategies and the optical flow used by DEEP-HAL <ref type="bibr" target="#b84">[85]</ref>. We also describe deep learning pipelines for video classification. Finally, we discuss the object category and human detectors followed by the spatial and temporal saliency detectors used by us.</p><p>Early video descriptors. Early AR used on spatio-temporal interest point detectors <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b78">79]</ref> and spatio-temporal descriptors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81]</ref> which capture various appearance and motion statistics. As spatio-temporal interest point detectors are unable to capture long-term motion patterns, a Dense Trajectory (DT) <ref type="bibr" target="#b78">[79]</ref> approach densely samples feature points in each frame to track them in the video (via optical flow). Then, multiple descriptors are extracted along trajectories to capture shape, appearance and motion cues. As DT cannot compensate for the camera motion, the IDT <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b79">80]</ref> estimates the camera motion to remove the global background motion. IDT also removes inconsistent matches via a human detector. For spatio-temporal descriptors, IDT employs HOG <ref type="bibr" target="#b22">[23]</ref>, HOF <ref type="bibr" target="#b12">[13]</ref> and MBH <ref type="bibr" target="#b79">[80]</ref>. HOG <ref type="bibr" target="#b22">[23]</ref> contains statistics of the amplitude of image gradients w.r.t. the gradient orientation, thus it captures the static appearance cues. In contrast, HOF <ref type="bibr" target="#b12">[13]</ref> captures histograms of optical flow while MBH <ref type="bibr" target="#b79">[80]</ref> captures derivatives of the optical flow, thus it is highly resilient to the global camera motion whose cues cancel out due to derivatives. Thus, HOF and MBH contain the zero-and first-order optical flow statistics. Other spatio-temporal descriptors include HOG-3D <ref type="bibr" target="#b36">[37]</ref>, SIFT3D <ref type="bibr" target="#b67">[68]</ref>, SURF3D <ref type="bibr" target="#b88">[89]</ref> and LTP <ref type="bibr" target="#b91">[92]</ref>.</p><p>We use the DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> setup. We encode HOG, HOF, and MBH descriptors on the Improved Dense Trajectories <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref> via BoW <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b11">12]</ref> and FV <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>BoW/FV encoding. BoW <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b11">12]</ref> uses a k-means vocabulary to which local descriptors are assigned. Variants include Soft Assignment (SA) <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b38">39]</ref> and Localized Soft Assignment (LcSA) <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b42">43]</ref>. As we use DEEP-HAL <ref type="bibr" target="#b84">[85]</ref>, we use BoW <ref type="bibr" target="#b11">[12]</ref> with Power Normalization <ref type="bibr" target="#b42">[43]</ref>, and FV <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> which capture first-and second-order statistics of local descriptors assigned to GMM clusters. DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> setup describes how to obtain the BoW/FV global descriptors.</p><p>Optical flow. Older optical flow methods cope with small displacements <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref> while newer methods cope with larger displacements e.g., Large Displacement Optical Flow (LDOF) <ref type="bibr" target="#b3">[4]</ref>. Recent methods use non-rigid descriptor or segment matching <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b2">3]</ref>, or edge-preserving interpolation <ref type="bibr" target="#b62">[63]</ref>. We use LDOF <ref type="bibr" target="#b55">[56]</ref>.</p><p>Object detectors. Modern deep learning methods include Region-based Convolutional Neural Networks (R-CNN) <ref type="bibr" target="#b25">[26]</ref>, its faster variants <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">62]</ref>, its mask-based variants <ref type="bibr" target="#b28">[29]</ref>, and YOLO <ref type="bibr" target="#b60">[61]</ref>, YOLO v2, YOLO v3 etc., which use a single network for efficiency.</p><p>In this paper, we use the faster R-CNN detector <ref type="bibr" target="#b61">[62]</ref> with backbones such as (i) Inception V2 <ref type="bibr" target="#b72">[73]</ref>, (ii) Inception ResNet V2 <ref type="bibr" target="#b71">[72]</ref>, (iii) ResNet101 <ref type="bibr" target="#b29">[30]</ref> and (iv) NASNet <ref type="bibr" target="#b96">[97]</ref>. As the Inception V2, Inception ResNet V2 and NASNet are pre-trained on the COCO dataset <ref type="bibr" target="#b50">[51]</ref>, they detect from 91 object classes good at summarizing e.g., indoor environments and helping us associate the scene context with actions. The ResNet101 model is pre-trained on the AVA v2.1 dataset <ref type="bibr" target="#b26">[27]</ref> with 80 different human actions, thus directly helping human-centric action recognition problems.</p><p>In addition to detection scores, we describe each bounding box with ImageNet <ref type="bibr" target="#b64">[65]</ref> scores from pre-trained Inception ResNet V2 <ref type="bibr" target="#b71">[72]</ref>.</p><p>Saliency detectors. Image regions correlating with human visual attention are detected by saliency detectors in the form of saliency maps. Deep saliency models <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b31">32]</ref> outperform conventional saliency detectors <ref type="bibr" target="#b95">[96]</ref> but they require pixel-wise annotations. Recent models include MNL <ref type="bibr" target="#b94">[95]</ref> (weakly-supervised model), RFCN <ref type="bibr" target="#b85">[86]</ref> (a fully-supervised model) and a cheap non-CNN Robust Background Detector (RBD) <ref type="bibr" target="#b95">[96]</ref> (see survey <ref type="bibr" target="#b1">[2]</ref> for more details).</p><p>For the spatial saliency, we use MNL <ref type="bibr" target="#b94">[95]</ref> trained on multiple noisy labels from weak/noisy unsupervised handcrafted saliency models. For temporal saliency, we use a CNN-LSTM ACLNet <ref type="bibr" target="#b93">[94]</ref>.</p><p>Deep learning AR. Early AR CNN models use frame-wise features and average pooling <ref type="bibr" target="#b35">[36]</ref> discarding the temporal order. Thus, frame-wise CNN scores were fed to LSTMs <ref type="bibr" target="#b15">[16]</ref> while the two-stream networks <ref type="bibr" target="#b69">[70]</ref> compute representations per RGB frame and per 10 stacked optical flow frames. Finally, spatio-temporal 3D CNN filters <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b77">78]</ref> model spatio-temporal patterns.</p><p>As two-stream networks <ref type="bibr" target="#b69">[70]</ref> discard the temporal order, rank pooling <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b81">82]</ref> and higherorder pooling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40]</ref> are popular. A recent I3D model <ref type="bibr" target="#b4">[5]</ref> 'inflates' 2D CNN filters pre-trained on ImageNet to spatio-temporal 3D filters, and implements temporal pooling. PAN <ref type="bibr" target="#b92">[93]</ref> proposes a motion cue called Persistence of Appearance that enables the network to distill the motion information directly from adjacent RGB frames. Approach <ref type="bibr" target="#b52">[53]</ref> uses bootstrapping with long-range temporal context attention while approach <ref type="bibr" target="#b46">[47]</ref> proposes a graph attention model to explore the semantics. Slow-I-Fast-P (SIFP) <ref type="bibr" target="#b49">[50]</ref> for compressed AR contains the slow and fast pathways I and P, resp., receiving a sparse sampling I-frame clip and a dense sampling pseudo optical flow clip.</p><p>AssembleNet <ref type="bibr" target="#b66">[67]</ref> automatically finds a neural architecture with a good connectivity to capture spatio-temporal interactions for AR through NAS. AssembleNet++ <ref type="bibr" target="#b65">[66]</ref> further learns the interactions between raw appearance and/or motion features and spatial object information through learning dynamic attention weights and search through the inter-block attention connectivity.</p><p>We use DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> which employs a 1D convolution for temporal pooling (I3D net.). We also investigate the use of AssembleNet and AssembleNet++ as backbones to show that our proposed object and saliency descriptors are independent of the backbone. We focus on the design/ability of ODF/SDF to supervise DEEP-HAL.</p><p>Power Normalization. For BoW/FV and CNN-based streams, the so-called burstiness defined as 'the property that a given visual element appears more times in an image than a statistically independent model would predict' <ref type="bibr" target="#b33">[34]</ref> has to be tackled. Thus, we employ Power Normalization <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44]</ref> which suppresses the burstiness via the so-called MaxExp pooling <ref type="bibr" target="#b42">[43]</ref> given in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Below, we present Power Normalization <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b41">42]</ref>, count sketches <ref type="bibr" target="#b86">[87]</ref>, and the RBF feature maps which we use in our pipeline with the goal of the burstiness and dimensionality reduction, and Cartesian coordinate/frame positional encoding.</p><p>Notations. We use boldface uppercase letters to express matrices e.g., M , P , regular uppercase letters with a subscript to express matrix elements e.g., P ij is the (i, j) th element of P , boldface lowercase letters to express vectors, e.g. x, ?, ?, and regular lowercase letters to denote scalars. Vectors can be numbered e.g., x n while regular lowercase letters with a subscript express an element of vector e.g., x i is the i th element of x. Operators ';' and ',' concatenate vectors along the first and second mode, respectively, i?I</p><formula xml:id="formula_0">K v i = [v 1 ; ...; v K ] and 2 i?I K v i = [v 1 , ..., v K ]</formula><p>concatenate a group of vectors in the first and second mode, respectively, ? denotes the aggregation (sum) while I d denotes an index set of integers {1, ..., d}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Power Normalization</head><p>Proposition 1. Sigmoid (SigmE), a Max-pooling approximation <ref type="bibr" target="#b44">[45]</ref>, is an extension of the MaxExp operator defined as g(?, ?) = 1 ? (1 ? ?) ? for ? &gt; 1 to the operator with a smooth derivative, a response defined for real-valued ? (rather than ? ? 0), a parameter ? and a small constant :</p><formula xml:id="formula_1">g(?, ? ) = 2 1+e ?? ?/( ? 2 + ) ?1.</formula><p>(1)</p><p>Proof. See papers <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> for extensive considerations.</p><p>As papers <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b84">85]</ref> show that various pooling operators perform similarly, we equip our hallucination streams with SigmE followed by count sketching described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Count Sketches</head><p>Sketching vectors by the count sketch <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b86">87]</ref> is used for their dimensionality reduction which we use in this paper. </p><formula xml:id="formula_2">P ij = s i if h i = j, 0 otherwise,<label>(2)</label></formula><p>and the sketch projection p : R d ? R d is a linear operation given as p(?) = P ? (or p(?; P ) = P ? to highlight P ).</p><p>Proof. It directly follows from the definition of the count sketch e.g., see Definition 1 <ref type="bibr" target="#b86">[87]</ref>.</p><p>Remark 1. Count sketches are unbiased estimators:</p><formula xml:id="formula_3">E h,s (p(?, P (h, s)), p(? , P (h, s))) = ?, ? . As variance V h,s (p(?), p(? )) ? 1 d ?, ? 2 + ? 2 2 ? 2 2</formula><p>, the larger sketches are less noisy. Thus, for every modality, we use a separate sketch matrix P .</p><p>Proof. For the first and second property, see Appendix A of paper <ref type="bibr" target="#b86">[87]</ref> and Lemma 3 <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Positional Embedding</head><p>Let G ? (x?x ) = exp(? x?x 2 2 /2? 2 ) denote a standard Gaussian RBF kernel centered at x and having a bandwidth ?. Kernel linearization refers to rewriting this G ? as an inner-product of two infinite-dimensional feature maps. To obtain these maps, we use a fast approximation method based on probability product kernels <ref type="bibr" target="#b32">[33]</ref>. Specifically, we employ the inner product of d -dimensional isotropic Gaussians given x, x ? R d . Thus, we have:</p><formula xml:id="formula_4">G ? (x?x )= 2 ?? 2 d 2 ??R d G ?/ ? 2 (x??) G ?/ ? 2 (x ??) d?.<label>(3)</label></formula><p>Eq. <ref type="formula" target="#formula_4">(3)</ref> is then approximated by replacing the integral with the sum over Z pivots ? 1 , ..., ? Z , thus yielding a feature map ? as:</p><formula xml:id="formula_5">?(x; {? i } i?I Z ) = G ?/ ? 2 (x ? ? 1 ), ..., G ?/ ? 2 (x ? ? Z ) T ,<label>(4)</label></formula><p>and</p><formula xml:id="formula_6">G ? (x?x ) ? ? c?(x), ? c?(x ) ,<label>(5)</label></formula><p>where c is a const. Eq. (5) is the linearization of the RBF kernel. Eq. (4) is the feature map. {? i } i?I Z are pivots. As we use 1 dim. signals, we simply cover interval [0; 1] with Z equally spaced pivots. For clarity, we drop {? i } i?I Z and write ?(x), etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>Our pipeline is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. It consists of (i) streams already present in DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> such as the FV/BoW streams (black), the High Abstraction Features (HAF) stream and the Optical Flow Features (OFF) which are fed into (ii) the Prediction Network abbreviated as PredNet. In this paper we focus on two non-trivial streams, that is the Object Detection Features and Saliency Detection Features (dashed blue) (ODF and SDF for short).</p><p>BoW/FV/OFF streams take the backbone intermediate representations generated from the RGB frames and learn to hallucinate BoW/FV and the optical flow (I3D only) representations via the MSE loss between the ground-truth BoW/FV/OFF and the outputs of BoW/FV/OFF streams. For AssembleNet/AssembleNet++, RGB and optical flow are combined by the backbone, thus we remove the OFF stream. The same MSE loss is applied to the ODF and SDF streams. However, the design of compact ground-truth ODF and SDF descriptors is one of our main contributions.</p><p>The HAF stream processes the backbone representations prior to combining them with the hallucinated streams. PredNet fuses the combined BoW/FV/OFF/HAF and our new ODF and SDF to learn actions on videos. Below, we start by describing how we obtain our ODF and SDF descriptors before we describe modules of DEEP-HAL <ref type="bibr" target="#b84">[85]</ref> and our modifications. One change is that we learn weights for the weighted mean pooling (i.e., i w i ?/ i w i ) of each stream to avoid concatenation of streams (prevent overparametrization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical Motivation</head><p>Before we outline our ODF and SDF descriptors, we motivate the use of higher-order statistics. To compare videos, we want to capture a distribution of local features/descriptors e.g., detection scores. The characteristic function ? ? (?) = E ??? exp(i? T ?) describes the probability density f ? (?) of some video features (local features ? ? ? ). We obtain the Taylor expansion of the characteristic function:</p><formula xml:id="formula_7">E ??? ? r=0 i j r! ?, ? r ? 1 N N n=0 ? r=0 i r r! ? ? r ? n , ? ? r ? = (6) ? r=0 i r r! 1 N N n=0 ? ? r ? n , ? ? r ? = ? r=0 X (r) , i r r! ? ? r ? ,</formula><p>where i is the imaginary number, and a tensor descriptor X (r) = 1 N N n=0 ? ? r ? n . In principle, with infinite data and infinite moments, one can fully capture f ? (?). In practice, first-, second-and thirdorder moments are typically sufficient, however, second-and third-order tensors grow quadratically and cubically w.r.t. the size of ?. Thus, in what follows, we represent second-order moments not by a covariance matrix but by the subspace corresponding to the top n leading eigenvectors. We also make use of the corresponding eigenvalues of the signal. Finally, it suffices to notice that ? (r) = diag X (r) corresponds to the notion of order r cumulants used in calculations of skewness (r = 3) and kurtosis (r = 4) but it grows linearly w.r.t. the size of ?. Thus, in what follows, we use the 2 norm normalized mean, leading eigenvectors, trace-normalized eigenvalues, skewness and kurtosis (rather than coskewness and cokurtosis) to obtain compact representation of ODF and SDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Detection Features</head><p>Each object bounding box is described by the feature vector:</p><formula xml:id="formula_8">? = ?(y (det) ); y (inet) ; ?(?); i?I4 ?(v i ); ? t?1 ??1 ? R d ,<label>(7)</label></formula><p>where ? = [0, ..., 1, ..., 0] T is a vector with all zeros but a single 1 placed at the location y. As we have 91 object classes for detectors trained on the COCO dataset and 80 classes for a detector trained on the AVA v2.1 dataset, we simply assume y (det) ? I 91+80 , that is, the labels 0, ..., 91 describe classes from COCO while classes 92, ..., 80+91 describe classes from AVA v2.1. Moreover, y (inet) ? R 1001 is an 1 norm normalized ImageNet classification score, 0 ? ? ? 1 is the detector confidence score, v 0 , ..., v 4 are the top-left and bottom-right Cartesian coordinates of a bounding box normalized in range [0; 1], and (t ? 1)/(? ? 1) is the frame index normalized w.r.t. the video sequence length ? . For feature maps ?(?) defined in Eq. (4), we simply use Z = 7 pivots and the ? of RBF is set to 0.5. Finally, for all detections per video from a given detector, we first compute the mean ?([? 1 , ..., ? N ]) ? R d (we write ?) where N is the total number of detections. Then, we form a matrix ? ? R d?N :</p><formula xml:id="formula_9">? = 1 J 1 K1 2 i?I K 1 (? i1 ??) , ..., 1 K J 2 i?I K J (? iJ ??) ,<label>(8)</label></formula><p>where K j denotes a number of detections per frame j ? I J , from which we extract higher-order statistical moments as described below. As N is large and its size varies from video to video, hallucinating ? directly is not feasible (nor it has invariance properties).</p><formula xml:id="formula_10">Firstly, we obtain U ?V = svd (? ) rather than U ? 2 U T = eig ?? T as N d, where U = [u 1 , u 2 , ...]</formula><p>. Take X (r) {v??} N n=0 (which we abbreviate to X (r) ) and ? (r) = diag X (r) defined in Section 4.1. We form our multi-moment descriptor ? (det) ? R d(4+n ) , n ? 1:</p><formula xml:id="formula_11">? (det) = ? ||?|| 2 ; 2 i?I n u i X (2) ; ? (3) ? (2) 3/2 ; ? (4) ? (2) 2 ; diag(? 2 ) i ? 2 ii ,<label>(9)</label></formula><p>The composition of Eq. (9) is described in Section 4.1. It is easy to verify that ? (3) (? (2) ) 3/2 and ? (4) (? (2) ) 2 are the empirical versions of skewness and kurtosis given by</p><formula xml:id="formula_12">E ???( (???) 3 ) E 3/2 ??? ((???) 2 ) and E ???( (???) 4 ) E 2 ??? ((???) 2 ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Saliency Detection Features</head><p>We extract directional gradients from saliency frames by discretised gradient operators [?1, 0, 1] and [?1, 0, 1] T and obtain gradient amplitude snd orientation maps ? and ? per frame encoded by:</p><formula xml:id="formula_13">? (sal) = i?I W ,j?I H ? ij ?(? ij /(2?)) ? ? i?1 W?1 ? ? j?1 H?1 ,<label>(10)</label></formula><p>where ? is the Kronecker product and ?(?) follows Eq. (4) with the exception that the assignment to Gaussians is realized in the modulo ring to respect the periodical nature of ?. We encode ?(?) with 12 pivots which encode the orientation of gradients. The remaining maps ?(?) are encoded with 5 pivots each, which correspond to spatial binning. Note that ? (sal) (we write ? ) is similar to a single CKN layer <ref type="bibr" target="#b53">[54]</ref> but is simpler: for one dimensional variables we sample pivots (c.f . learn) for maps ?(?). Each saliency frame is described as a feature vector ? ? = [? /||? ||2; I:/||I:||1] ? R d ? , where I : is a vectorized low-resolution saliency map. Thus, ? ? captures the directional gradient statistics and the intensity-based gist of saliency maps. Subsequently, we compute the mean ?(</p><formula xml:id="formula_14">[? ? 1 , ..., ? ? J ]) ? R d ? (we simply write ?)</formula><p>where J is the total number of frames per video. Then, we obtain ? ? = ? ? 1 , ..., ? ? J /J ? R d ? ?J which is compactly described by the multi-moment Eq. (9) resulting in ? (sal) ? R d(4+n ? ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hallucinating Streams/High Abstr. Features</head><p>Each hallucinating stream takes as input the backbone intermediate representation X (rgb) of size 1024 ? 7 obtained by removing the classifier and the last 1D conv. layer of I3D pre-trained on Kinetics-400. For AssembleNet/AssembleNet++, instead of the classification layer (FC layer), we use a 2048 dimensional output from 3D AveragePooling layer. For the BoW/FV/OFF and HAL streams, we follow the steps described in the DEEP-HAL approach <ref type="bibr" target="#b84">[85]</ref>. For all streams, we use  <ref type="figure" target="#fig_2">Figure 3a a</ref> shows the stream architecture used by us for the FV, BoW, OFF, HAF, DET1, ...,DET4, SAL1 and SAL2 streams. <ref type="figure" target="#fig_2">Figure 3b</ref> shows our PredNet. Operation and their parameters are in each block e.g., conv2d and its number of filters/size, Power Normalization (PN) and Sketching (SK). We indicate the size of input and/or output under arrows.</p><p>a Fully Connected (FC) unit shown in <ref type="figure" target="#fig_2">Figure 3a</ref>. Each stream uses Power Normalization (PN) realized via SigmE and Sketching (SK) from 1000 to 512 dim via? (?) =P (?)?(?) . Outputs? can be now aligned with ground-truth ? (?) described below. The same steps are applied to High Abstraction Features (HAF), combined with other streams, and also fed into PredNet (see <ref type="figure" target="#fig_1">Fig. 2</ref>). While hallucinating streams co-supervise the backbone via external ground-truth tasks, HAF simply passes the backbone features into PredNet.</p><p>Ground-truth BoW/FV/OFF. We follow the DEEP-HAL setup <ref type="bibr" target="#b84">[85]</ref> and apply PCA to a concatenation of IDT trajectories (30 dim.), HOG (96 dim.), HOF (108 dim.), MBHx (96 dim.) and MBHy (96 dim.). The resulting 213 dim. local descriptors are encoded by FV and BoW with a 256 dim. and a 1000 dim. GMM and k-means dictionaries. For the OFF stream (not used with AssembleNet or AssembleNet++), we pre-computed I3D with LDOF X (opt.) <ref type="figure" target="#fig_1">(Fig. 2)</ref>. All ground-truth representations were Power Normalized by SigmE/sketched to 512 dim. each via ? (?) = P (?) ? (?) and fed into the MSE loss. No ground-truth testing data is used in training/testing.</p><p>Ground-truth DET1, ...,DET4/SAL1/SAL2. The ODF ground-truth training representations are of size 1214?N , where N is the total number of bounding boxes per video (50-10000). The feature dim. 1214 is composed of 80 + 91 dim. one-hot detection classes, 6 ? 7 are the ?(?)-embedded confidence, bounding box coordinates and the frame number, 1001 is the ImageNet score. We also consider a variant without the RBF embedding: ?(x) = x (1178?N size). The SDF ground-truth training repr. are of size 556?J, where J is the number of frames per video. 300 dim. (12?5?5) concern spatio-angular gradient distributions and 256 dim. (16 ? 16) concern the luminance of saliency maps. Each ODF/SDF is encoded per video with the multi-moment descriptor in Eq. (9) yielding 1178?(4+n ) and 556?(4+n ? ) compact representations (we vary n and n ? between 1 and 5). ODF and SDF are Power Normalized by SigmE/sketched to 512 dim. each via ? (?) = P (?) ? (?) and fed into the MSE loss. No ground-truth testing representations were used for training/testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Objective Function</head><p>During training, we combine MSE loss functions which co-supervise hallucination streams with the classifier:</p><formula xml:id="formula_15">* (X , y;?) = ? |H| i?H ? i ?? i 2 2 + f(? (tot) ; ? (pr) ), y; ? ( ) , where: ?i ? H,? i =P i g( (X , ? i ), ?) , ? i = P i ? i , ? (haf ) = P (haf ) g (X , ? (haf ) ), ? , ? (tot) = 1 |H * |+1 w (haf ) ? (haf ) + i?H * w i? i , ? (det) = 1 |D| i?D w i? i ,? (sal) = 1 |S| i?S w i? i .<label>(11)</label></formula><p>The above equation is a trade-off between the MSE loss functions { ? i ?? i 2 2 , i ? H} and the classification loss (?, y; ? ( ) ) with some label y ? Y and parameters ? ( ) ? {W , b}. The trade-off is controlled by ? ? 0 while MSE is computed over hall. streams i ? H, and H ? {(f v1), (f v2), (bow), (of f ), (det1), ..., (det4), (sal1), (sal2)} is our set of hallucination streams. Moreover, g(?, ?) is a Power Norm. in Eq. (1), f (?; ? (pr) ) is the PredNet module with parameters ? (pr) which we learn, { (?, ? i ), i ? H} are the hallucination streams while {? i , i ? H} are resulting hallucinated BoW/FV/OFF/ODF/SDF representations. We set ? = 1. Moreover, (?, ? (haf ) ) is the HAF stream with the sketched output ? (haf ) = P (haf ) ? (haf ) . For the hallucination streams, we learn parameters {? i , i ? H} while for HAF, we learn ? (haf ) . The full set of parameters we learn is defined as? ? ({? i , i ? H}, ? (haf ) , ? (pr) , ? ( ) ). Furthermore, {P i , i ? H} and {P i , i ? H} are the projection matrices for count sketching of streams {? i , i ? H} and the ground-truth feature vectors {? i , i ? H}. Finally, for ? (tot) is a weighted average of several streams fed into the PredNet module f . Moreover, H * ? {(f v1), (f v2), (bow), (of f ), (det), (sal)}, D ? {(det1), ..., (det4)} and S ? {(sal1), (sal2)}. Section 3.2 details how to select matrices P . Let T be set to either H * , D or S, then our weights are:</p><formula xml:id="formula_16">w i = 1 |T | max(w ? i , ?) j?T max(w ? j , ?) .<label>(12)</label></formula><p>Prior to CNN training, we train an SVM on each ground-truth stream separately (using a manageable training subset), and we set weights w proportionally to the accuracies obtained on the validation set. For the HAF stream, we simply set w (haf ) = 1 |H * |+1 and ? = 0.1. For the first few epochs (i.e., 10), we set ? = 0 so that all streams receive equal weights. Subsequently, in each epoch, we run the Golden-section search to find the best ? ? 0. We start from initial boundary values ? ? {0, 50}, we train an SVM on a manageable subset of training data and evaluate ? on the validation set, and we update boundary values for the next epoch accordingly. Eq. (12) has a nice property: for ? = 0, we</p><formula xml:id="formula_17">have w i = 1/|T |. For ? ? ?, we have w i = 1 if w i = max({w i } i?T ), otherwise w i = 0. Thus, ?</formula><p>interpolates between equalizing all weights and the winner-takes-all solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Protocols</head><p>HMDB-51 <ref type="bibr" target="#b45">[46]</ref> has 6766 internet videos/ 51 classes; each video has ?20-1000 frames. We report the mean accuracy across three splits.</p><p>YUP++ <ref type="bibr" target="#b19">[20]</ref> has 20 scene classes of video textures, 60 videos per class. Splits contain scenes captured by the static or moving camera. We use standard splits (1/9 dataset for training) for evaluation. <ref type="bibr" target="#b63">[64]</ref> contains high-resolution videos of people cooking dishes. The 64 activities from 3748 clips include coarse actions e.g., opening refrigerator, and fine-grained actions e.g., peel, slice, cut apart. We use the mean Average Precision (mAP) over 7-fold cross validation. For human-centric protocol <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, we use the faster RCNN <ref type="bibr" target="#b61">[62]</ref> to crop video around human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII Cooking Activities</head><p>Charades <ref type="bibr" target="#b68">[69]</ref> consist of of 9848 videos of daily indoors activities, 66500 clip annotations and 157 classes. <ref type="bibr" target="#b13">[14]</ref> is a multi-class egocentric dataset with 28K training videos associated with 352 noun and 125 verb classes. The dataset consists of 39,594 segments in 432 videos. We follow protocol <ref type="bibr" target="#b0">[1]</ref>. We evaluate our model on validation, standard seen (S1: 8047 videos), and unseen (S2: 2929 videos) test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EPIC-Kitchens</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluations</head><p>Below, we show the effectiveness of our method. For smaller datasets, we use the I3D backbone. For large Charades and EPIC-Kitchens, we additionally investigate AssembleNet and AssembleNet++ backbones. Firstly, we evaluate various design components.</p><p>Ground-truth ODF+SVM. Firstly, we evaluate our ODF on SVM given the HMDB-51 dataset. We set n = 3 for Eq. (9) and compare various detector backbones and pooling strategies. <ref type="table" target="#tab_0">Table 1</ref> shows that all detectors perform similarly with (det3) being slightly better than other methods. Moreover, max-pooling on ODFs from all four detectors is marginally better than the average-pooling. However, only the weighted mean (all+wei) according to Eq. <ref type="formula" target="#formula_2">(12)</ref>   the need for the robust aggregation of ODFs. Similarly, when we combine pre-trained DEEP-HAL with all detectors, the weighted mean (DEEP-HAL+all+wei) performs best. <ref type="table" target="#tab_1">Table 2</ref> shows the similar trend on YUP++. We trained SVM only on videos for which at least one detection occurred, thus a 75.74% accuracy is much lower than the main results reported on the full pipeline. <ref type="figure" target="#fig_3">Figure 4</ref> shows that ? = 1 has a positive impact on reweighting.</p><p>Ground-truth SDF. The SDF on HMDB-51 and YUP++ yielded 24.35% and 32.68% accuracy. This is expected as SDFs do not capture a discriminative information per se but they locate salient spatial and temporal regions to focus the main network on them.</p><p>Multi-moment descr. <ref type="figure">Figure 5</ref> shows that the concat. of the mean and three eigenvectors according to Eq. (9) yields good results but adding further vectors deteriorates the performance. Adding skewness and kurtosis (? and ?) further improves results, while adding eigenvalues has a limited impact.</p><p>HMDB-51. <ref type="table" target="#tab_2">Table 3</ref> shows several DEEP-HAL variants, which all hallucinate BoW/FV/OFF. DEEP-HAL with our reweighting mechanism. (DEEP-HAL+W) outperforms the original DEEP-HAL denoted as (HAF/BoW/FV hal.) <ref type="bibr" target="#b84">[85]</ref> by ? 0.8%. DEEP-HAL with our ODF and SDF descriptors (DEEP-HAL+ODF) and (DEEP-HAL+SDF) outperform (HAF/BoW/FV hal.) by ? 1.8% and ? 1.4%, resp. This shows that both ODF and SDF are effective. Combining DEEP-HAL,   ODF and SDF outperform DEEP-HAL by ? 2.7% demonstrating the complementary nature of ODF and SDF. Utilizing our weighting mechanism with DEEP-HAL, ODF and SDF denoted as (DEEP-HAL+W+ODF+SDF) outperform (HAF/BoW/FV hal.) by ? 4.6%. Finally, DEEP-HAL with weighting, and ODF and SDF with RBF feature maps from Eq. (4) outperform (HAF/BoW/FV hal.) by ? 5.1%. <ref type="table" target="#tab_3">Table 4</ref> shows that ODF is better than SDF, that is (DEEP-HAL+ODF) and (DEEP-HAL+SDF) outperform (HAF/BoW/FV hal.) by ? 0.6% and ? 0.2%, resp. This is expected as YUP++ contains dynamic scenes without objects/specific saliency regions correlating with class concepts. However, a combination of detectors/saliency (DEEP-HAL+SDF) plus weighting (DEEP-HAL+W+ODF+SDF) plus the RBF maps (DEEP-HAL+W+G+ODF+SDF) outperform (HAF/BoW/FV hal.) by ? 0.7%, ? 1.6% and ? 1.8% accuracy, resp. MPII. <ref type="table" target="#tab_5">Table 5</ref> shows a ? 3.0% mAP gain over (HAF/BoW/FV hal.) due to detectors capturing the human interaction with objects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YUP++.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ODF (variant) mean accuracy (%)</head><formula xml:id="formula_18">[?, u 1 ] [?, u 1 , u 2 ] [?, u 1 , u 2 , u 3 ] [?, u 1 , ? ? ? , u 4 ] [?, u 1 , ? ? ? , u 5 ] [?, u 1 , u 2 , u 3 , ?, ?] [?, u 1 , u 2 , u 3 , ?, ?, ? 2 ]</formula><formula xml:id="formula_19">[?, u 1 ] [?, u 1 , u 2 ] [?, u 1 , u 2 , u 3 ] [?, u 1 , ? ? ? , u 4 ] [?, u 1 , ? ? ? , u 5 ] [?, u 1 , u 2 , u 3 , ?, ?]</formula><p>[?, u 1 , u 2 , u 3 , ?, ?, ? 2 ] (b) <ref type="figure">Figure 5</ref>: ODF eval. on SVM on four detectors (the weighted mean). <ref type="figure">Fig. 5a and 5b</ref> show results on HMDB-51 and YUP++. ?, u1, ..., ui, ?, ?, and ? 2 correspond to the entries in Eq. (9).  Charades. <ref type="table">Table 6</ref> (top) presents relative gains of our hallucination pipeline (DEEP-HAL) with weighted mean pooling (W) and the RBF maps (G) denoted as (DEEP-HAL+W+G). We evaluate Object Detection Features (ODF) and Saliency Detection Features (SDF) with 512 dim. sketching (SK512) and note that (DEEP-HAL+W+G+ODF (SK512)) outperforms (DEEP-HAL+W+G+SDF (SK512)), and both methods outperform the baseline (HAF/BoW/FV hal.) <ref type="bibr" target="#b84">[85]</ref>.</p><p>Table 6 (bottom) shows that combining ODF and SDF into (DEEP-HAL+W+G+SDF+ODF (SK512)) yields 49.06% mAP which constitutes on a ? 6% gain over the baseline(HAF/BoW/FV hal.) <ref type="bibr" target="#b84">[85]</ref>. This demonstrates that ODF and SDF are highly complementary. Applying a larger sketch (DEEP-HAL+W+G+ODF+SDF (SK1024)) yields 50.14% mAP which matches the use (DEEP-HAL+W+G+ODF+SDF (exact)) that denotes a late fusion by concatenation of ODF and SDF with the stream resulting from DEEP-HAL fed into PredNet. Note that (exact) indicates that ODF and SDF are not hallucinated at the test time but they are computed. the results matching between (DEEP-HAL+W+G+ODF+SDF (SK1024)) and (DEEP-HAL+W+G+ODF+SDF (exact)) show that we can hallucinate ODF and SDF at the test time while regaining the full performance. We save computational time and hallucinate the detection and saliency features which boost results on Charades by ? 6% over the baseline. <ref type="table">Table 7</ref> shows that our idea applied to AssembleNet and AssembleNet++ yields state of the art e.g., we outperform these two networks by 4.5% and 5.6% mAP, respectively. We note that our detectors do not need to be computed at all at the test time.</p><p>In contrast, the best currently reported papers such as SlowFast networks <ref type="bibr" target="#b17">[18]</ref> and AssembleNet <ref type="bibr" target="#b66">[67]</ref> achieve 45.2% and 51.6% on Charades. As SlowFast networks and AssembleNet backbones can be used in place of I3D in our experimental setup, our approach is 'orthogonal' to these latest developments which focus on heavy mining for combinations of neural blocks/dataflow between them to obtain an 'optimal' pipeline. We achieve similar results with a simple approach based on self-supervised learning. Our pipeline is lightweight by comparison (no need for computations of the optical flow, or detections or segmentation masks at test time).</p><p>ImageNet (global score) vs. object detectors. Various scores from the object and saliency detectors which we use cannot be plugged directly into the DEEP-HAL due to the varying number of objects detected and the varying number of frames, thus we propose and use ODF and SDF descriptors. We also note that using a simplified variant of ODF which stacks up ImageNet scores per frame into a matrix (no detectors) to which we apply our multi-moment descriptor yielded ? 4% worse results on Charades than our DEEP-HAL+ODF (detectors-based approach) which yields 48.0% mAP. This is expected as ImageNet is trained in a multi-class setting (one object per image) while detectors let us model robustly distributions of object classes and locations per frame. <ref type="table" target="#tab_7">Table 8</ref> shows the experimental results. I3D and AssembleNet/AssembleNet++ learn human-like semantic features due to ODF/SDF, and there is no evidence a backbone can discover these without a guidance. By comparing MPII (3748 clips) with large EPIC-Kitchens (39594 clips) (both about cooking), SDF+ODF boost MPII from 81.8 to 84.8%, and SDF+ODF boost EPIC-Kitchens from 32.51% (DEEP-HAL) to 35.88% (on seen classes protocol), and from 22.33% (DEEP-HAL) to 27.32% (on unseen classes protocol). The boost is 3% on both MPII and EPIC-Kitchens (nearly 10? more clips than MPII).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EPIC-Kitchens.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced two simple yet effective object and saliency descriptors, which perform selfsupervision of an AR hallucination-based network. We have shown that modeling high-order statistical moments can result in small representations that can self-supervise our AR pipeline. The findings are in line with recent multi-task learning papers which argue that related tasks can co-  supervise the main task. We are the first to hallucinate object and saliency detection descriptors with clear cut improvements in accuracy, and state-of-the-art results on the large-scale Charades and EPIC-Kitchens. More importantly, we demonstrate that hallucinating object and saliency detections is an attractive proposition even for the state-of-the-art AR backbones such as AssembleNet and AssembleNet++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reweighting mechanism</head><p>In this experiment, we employ pipeline (DEEP-HAL+W+G+SDF+ODF (SK512)) explained above. Typically, we use three levels of weighting mean pooling which are applied to (i) four object detectors constituting on ODF, (ii) two saliency detectors constituting on SDF, and (iii) the final combination of HAF/BOW/FV/OFF/ODF/SDF. Thus, below we investigate the performance of a single weighting mean pooling step applied simultaneously to four object detectors, two saliency detectors and the remaining streams.    <ref type="table" target="#tab_9">Table 9</ref> shows that using a flat single level weighted mean pooling yields 86.1% accuracy on the HMDB-51 which is a ? 1.4% less compared to utilizing three levels of weighted mean pooling. We expect that having one weighted mean pooling per modality is a reasonable strategy as for instance object category detectors may yield similar responses thus they should be first reweighted for the best 'combined detector' performance before being combined with highly complementary modalities.</p><p>Finally, <ref type="figure">Figure 8</ref> (top) demonstrates how our Golden-search selects optimal ? on the validation set of MPII (split1). <ref type="figure">Figure 8</ref> (bottom) demonstrates the corresponding validation mAP (this is not the mAP score on the testing set). Note that for the first 10 epochs we use ? = 0 and we start the Golden-search from epoch 11. <ref type="table" target="#tab_0">Table 10</ref> shows basic statistics re. datasets used in our experiments. We note that Charades with 66500 uniquely annotated clips, 157 action labels and an average frame count of 300 per clip is the largest among these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset statistics and timing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="00">(a)</head><p>Oo (b) <ref type="figure">Figure 6</ref>: Visualization of the feature space (from PredNet) for DEEP-HAL in <ref type="figure">Fig. 6a</ref> and DEEP-HAL+ODF in <ref type="figure">Fig. 6b</ref> on the YUP++ dataset. For comparison, we circle regions with interesting changes. <ref type="table" target="#tab_0">Table 11</ref> introduces timing for object detectors used by our ODF descriptors during training. We note that detections with all four object detectors which we use take ? 1.47 second per frame. Thus, obtaining four ODF descriptors per clip (uniquely annotated sequence to train or classify) takes between 136 and 441 seconds. <ref type="table" target="#tab_0">Table 12</ref> introduces timing for saliency detectors used in our SDF descriptors during training. We note that detections with both saliency detectors which we use take ? 0.9 second per frame, and obtaining both SDF descriptors per clip takes between 84 and 271 seconds. We do note that the major computational cost is incurred due to detectors rather than our ODF and SDF descriptors proposed in the main paper (their cost is minimal). We further note that  <ref type="table" target="#tab_0">Table 11</ref>: Statistics of object detectors we use. We provide timings such as seconds per frame (sec. per frame) and seconds per clip (s.p.c.) for detectors used by ODF. The total time incurred by a combined detector (ODF total) is also provided. We also compute the time taken by the full SVD and all remaining ODF operations described in the main paper, assuming ? 5 detections per frame.  <ref type="table" target="#tab_0">Table 12</ref>: Statistics of saliency detectors we use. We provide timings such as seconds per frame (sec. per frame) and seconds per clip (s.p.c.) for detectors used by SDF. The total time incurred by a combined detector (SDF total) is also provided. We also compute the time taken by the descriptor in Eq. (10) and all remaining SDF operations described in the main paper. Finally, we also provide the combined ODF and SDF time (SDF+ODF total).  <ref type="figure" target="#fig_7">Fig. 7a</ref> and DEEP-HAL+ODF in <ref type="figure" target="#fig_7">Fig. 7b</ref> on the HMDB-51 dataset. For comparison, we circle regions with interesting changes. the idea of learning these costly representations during training is very valuable. While the total computations per training clip vary between 220 and 712 seconds, during testing time we obtain these representations for free (milliseconds) thanks to DET1,...,DET4 and SAL1/SAL2 units from <ref type="figure" target="#fig_1">Figure 2</ref> (the main submission). Assuming 25% of clips in charades for testing, that results in 137 days of computational savings on a single GPU (conversely, 1 day savings on 137 GPUs). Given the obtained 6% boost on Charades over the baseline without ODF and SDF, and the computational savings, we believe these statistics highlight the value of our approach.   <ref type="figure" target="#fig_7">Fig. 7a</ref>, bottom left corner contains samples from classes in red and blue colors which partially overlap. In <ref type="figure" target="#fig_7">Fig. 7b</ref>, bottom left corner contains the samples from the corresponding classes in red and blue colors. This time, the class-wise clusters seem to be more clearly delineated and samples of these classes are separated better from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visualization using UMAP</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We use detectors/saliency in hallucination descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proposition 2 .</head><label>2</label><figDesc>Let d and d denote the dimensionality of the input and sketched output vectors, respectively. Let vector h ? I d d contain d uniformly drawn integer numbers from {1, ..., d } and vector s ? {?1, 1} d contain d uniformly drawn values from {?1, 1}. Then, the sketch projection matrix P ? {?1, 0, 1} d ?d becomes:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Stream details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The impact of ? in the weighted mean on the classification results.Figure 4ashows results for HMDB-51 on (top) four detectors combined+SVM and (bottom) DEEP-HAL with four detectors com-bined+SVM.Figure 4bshows results for YUP++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(10)) (+Eq. (10)+SVD) sec. per frame 0.60 0.30 0.90 (+0.003) 2.36 (+0.1) s.p.c. HMDB-51 55.7 27.9 83.6 (+0.3) 219.2 (+0.8) s.p.c. YUP++ 83.2 41.6 124.8 (+0.4) 327.3 (+1.2) s.p.c. MPII 106.0 53.0 159.0 (+0.5) 417.0 (+1.8) s.p.c. Charades 180.3 90.1 270.4 (+0.9) 709.1 (+3.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the feature space (from PredNet) for DEEP-HAL in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 Figure 8 :</head><label>68</label><figDesc>is a visualization performed with UMAP [55] on the YUP++ dataset. In Fig. 6a, top left corner contains samples from classes in red, green, and blue colors which partially overlap. In Fig. Visualization of the Golden-search for the weighting mechanism (final level weighting). (top) Illustration of how the lower and upper estimates ? (l) and ? (u) converge as epochs progress. (bottom) For every epoch, we set ? = 0.5(? (l) +? (u) ) and obtain the corresponding validation score (mAP) on MPII (split1). As the epoch number advances, mAP improves and remains stable as the Golden-search algorithm converges.6b, top left corner contains the samples from the corresponding classes in red, green, and blue colors. This time, the samples of these three classes are well separated from each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7</head><label>7</label><figDesc>is a visualization performed with UMAP [55] on the HMDB-51 dataset. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>00% 39.74% 40.39% 40.72% det1 40.49% 40.13% 39.67% 40.09% det3 43.78% 44.05% 41.97% 43.26% det4 41.08% 39.22% 40.39% 40.23% all+avg 42.50% 41.05% 41.01% 41.52% all+max 43.25% 42.32% 42.09% 42.55% all+wei 45.80% 44.52% 44.09% 44.80% DEEP-HAL+all+avg 83.25% 82.24% 82.84% 82.77% DEEP-HAL+all+max 83.18% 81.86% 82.84% 82.62% DEEP-HAL+all+wei 84.01% 83.25% 83.10% 83.45% Evaluations of ODF on HMDB-51. (top) We evaluate backbones such as (det1) Inception V2, (det2) Inception ResNet V2, (det3) ResNet101 and (det4) NASNet. (middle) The average-pooled, max-pooled and the weighted mean combination of all detectors are given by (all+avg), (all+max) and (all+wei). (bottom) Pretrained DEEP-HAL combined with all four detectors by the average-pooling, max-pooling and the weighted mean.</figDesc><table><row><cell>outperforms (det3) which highlights</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Pooling on YUP++. Results for the average-pooled (avg), max-pooled (max) and the weighted mean (wei) of all detectors (all) vs. pre-trained DEEP-HAL combined with all detectors by the average-pooling, max-pooling and the weighted mean.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>sp1 sp2 sp3 mean acc. DEEP-HAL+W 83.94% 82.50% 83.34% 83.26% DEEP-HAL+ODF 85.03% 83.59% 84.25% 84.29% DEEP-HAL+SDF 84.64% 83.20% 83.82% 83.88% DEEP-HAL+ODF+SDF 86.14% 83.66% 85.81% 85.20% DEEP-HAL+W+ODF+SDF 87.78% 86.27% 87.06% 87.04% DEEP-HAL+W+G+ODF+SDF 88.37% 86.80% 87.52% 87.56% FV exact 82.50% [85] HAF/BoW/FV hal. 82.48% [85] Evaluations of (top) our methods and (bottom) comparisons to the state of the art on HMDB-51. FV hal. [85] 94.8% 89.6% 93.3% 92.2% 92.6% MSOE-two-stream [28] 97.0% 87.0% 91.8% 92.0% 91.9%</figDesc><table><row><cell cols="2">ADL+I3D 81.5% [82]</cell><cell>Full-FT I3D 81.3% [5]</cell></row><row><cell cols="2">EvaNet (Ensemble) 82.3% [60]</cell><cell>PA3D + I3D 82.1% [91]</cell></row><row><cell cols="3">HAF/BoW/static dynamic mixed mean mean stat/dyn all</cell></row><row><cell cols="3">DEEP-HAL+ODF DEEP-HAL+SDF DEEP-HAL+SDF+ODF DEEP-HAL+W+SDF+ODF DEEP-HAL+W+G+SDF+ODF 96.30% 92.40% 94.35% 94.4% 94.4% 95.00% 90.93% 93.52% 93.0% 93.2% 94.96% 89.93% 93.58% 92.4% 92.8% 95.10% 91.11% 93.61% 93.1% 93.3% 96.30% 92.22% 94.17% 94.3% 94.2%</cell></row><row><cell>T-ResNet [20] ADL I3D [82]</cell><cell cols="2">92.4% 81.5% 89.0% 87.0% 87.6% 95.1% 88.3% -91.7% -</cell></row><row><cell>HAF/BoW/</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluations of (top) our methods and (bottom) comparisons to the state of the art on YUP++.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>sp1 sp2 sp3 sp4 sp5 sp6 sp7 mAP DEEP-HAL+W+ODF+SDF 82.5 85.1 85.6 83.5 86.6 80.8 81.2 83.6% DEEP-HAL+W+G+ODF+SDF 83.3 87.6 85.6 83.4 86.6 83.2 83.6 84.8% KRP-FS+IDT 76.1% [9] GRP+IDT 75.5% [7] I3D+BoW/OFF MTL 79.1% [85] HAF/BoW/OFF hal. 81.8%<ref type="bibr" target="#b84">[85]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Evaluations of (top) our methods and (bottom) comparisons to the state of the art on MPII.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :Table 7 :</head><label>67</label><figDesc>Evaluations of our methods on Charades (I3D backbone). Evaluations of our methods on the Charades dataset (AssembleNet and AssembleNet++ backbones). Note that we do not use segmentation masks for AssembleNet and AssembleNet++, thus baseline results reported by us are slightly lower compared to authors' results of 55.0% and 59.8% mAP, respectively.</figDesc><table><row><cell cols="2">HAF/BoW/FV</cell><cell cols="2">DEEP-HAL+</cell><cell></cell><cell cols="2">DEEP-HAL+</cell></row><row><cell cols="2">hal. [85]</cell><cell cols="5">W+G+ODF (SK512) W+G+SDF (SK512)</cell></row><row><cell>43.1</cell><cell></cell><cell></cell><cell>47.22</cell><cell></cell><cell>45.30</cell><cell></cell></row><row><cell cols="8">DEEP-HAL+W+G+ DEEP-HAL+W+G+ DEEP-HAL+W+G+</cell></row><row><cell cols="8">ODF+SDF (SK512) ODF+SDF (SK1024) ODF+SDF (exact)</cell></row><row><cell cols="2">49.06</cell><cell></cell><cell>50.14</cell><cell></cell><cell cols="2">50.16</cell></row><row><cell cols="7">AssembleNet++ 50 (Kinetics-400 pre-training)</cell></row><row><cell cols="8">baseline ODF+SDF (SK512) ODF+SDF (SK1024) ODF+SDF (exact)</cell></row><row><cell>53.8</cell><cell>55.81</cell><cell></cell><cell cols="2">56.94</cell><cell></cell><cell>57.30</cell></row><row><cell cols="7">AssembleNet++ 50 (without pre-training)</cell></row><row><cell cols="8">baseline ODF+SDF (SK512) ODF+SDF (SK1024) ODF+SDF (exact)</cell></row><row><cell>56.7</cell><cell>60.71</cell><cell></cell><cell cols="2">61.98</cell><cell></cell><cell>62.29</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Verbs</cell><cell cols="2">Nouns</cell><cell>Actions</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">top-1 top-5 top-1 top-5 top-1 top-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Validation</cell></row><row><cell>LFB Max [90]</cell><cell></cell><cell></cell><cell>52.6</cell><cell>81.2</cell><cell>31.8</cell><cell>56.8</cell><cell>22.8</cell><cell>41.1</cell></row><row><cell>WeakLargeScale [24]</cell><cell></cell><cell></cell><cell>58.4</cell><cell>84.1</cell><cell>36.9</cell><cell>60.3</cell><cell>26.1</cell><cell>42.7</cell></row><row><cell cols="3">DEEP-HAL+ODF+SDF(SK1024)</cell><cell>55.4</cell><cell>82.9</cell><cell>33.3</cell><cell>55.1</cell><cell>21.5</cell><cell>39.7</cell></row><row><cell cols="3">AssembleNet++ ODF+SDF(SK512)</cell><cell>57.2</cell><cell>84.6</cell><cell>34.8</cell><cell>56.4</cell><cell>23.2</cell><cell>41.3</cell></row><row><cell cols="4">AssembleNet++ ODF+SDF(SK1024) 58.7</cell><cell>85.6</cell><cell>36.0</cell><cell>57.3</cell><cell>24.7</cell><cell>43.0</cell></row><row><cell cols="3">AssembleNet++ ODF+SDF(exact)</cell><cell>60.0</cell><cell>86.7</cell><cell>37.1</cell><cell>59.2</cell><cell>25.2</cell><cell>43.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test s1 (seen)</cell></row><row><cell>TSN Fusion [14]</cell><cell></cell><cell></cell><cell>48.2</cell><cell>84.1</cell><cell>36.7</cell><cell>62.3</cell><cell>20.5</cell><cell>39.8</cell></row><row><cell>LFB Max [90]</cell><cell></cell><cell></cell><cell>60.0</cell><cell>88.4</cell><cell>45.0</cell><cell>71.8</cell><cell>32.7</cell><cell>55.3</cell></row><row><cell>WeakLargeScale [24]</cell><cell></cell><cell></cell><cell>65.2</cell><cell>87.4</cell><cell>45.1</cell><cell>67.8</cell><cell>34.5</cell><cell>53.8</cell></row><row><cell cols="3">DEEP-HAL+ODF+SDF(SK1024)</cell><cell>62.2</cell><cell>85.0</cell><cell>46.1</cell><cell>69.3</cell><cell>32.5</cell><cell>53.6</cell></row><row><cell cols="4">AssembleNet++ ODF+SDF(SK1024) 65.0</cell><cell>87.8</cell><cell>48.8</cell><cell>72.5</cell><cell>35.0</cell><cell>56.1</cell></row><row><cell cols="3">AssembleNet++ ODF+SDF(exact)</cell><cell>66.2</cell><cell>88.5</cell><cell>49.3</cell><cell>72.8</cell><cell>35.8</cell><cell>56.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test s2 (unseen)</cell></row><row><cell>TSN Fusion [14]</cell><cell></cell><cell></cell><cell>39.4</cell><cell>74.3</cell><cell>22.7</cell><cell>45.7</cell><cell>10.9</cell><cell>25.3</cell></row><row><cell>LFB Max [90]</cell><cell></cell><cell></cell><cell>50.9</cell><cell>77.6</cell><cell>31.5</cell><cell>57.8</cell><cell>21.2</cell><cell>39.4</cell></row><row><cell>WeakLargeScale [24]</cell><cell></cell><cell></cell><cell>57.3</cell><cell>81.1</cell><cell>35.7</cell><cell>58.7</cell><cell>25.6</cell><cell>42.7</cell></row><row><cell cols="3">DEEP-HAL+ODF+SDF(SK1024)</cell><cell>55.3</cell><cell>79.1</cell><cell>32.6</cell><cell>55.4</cell><cell>22.3</cell><cell>39.2</cell></row><row><cell cols="4">AssembleNet++ ODF+SDF(SK1024) 58.3</cell><cell>82.1</cell><cell>35.2</cell><cell>58.2</cell><cell>25.9</cell><cell>42.9</cell></row><row><cell cols="3">AssembleNet++ ODF+SDF(exact)</cell><cell>59.0</cell><cell>83.3</cell><cell>35.7</cell><cell>59.0</cell><cell>27.3</cell><cell>44.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Experimental results on the EPIC-Kitchens.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>sp1 sp2 sp3 mean acc. wei+flat 86.47% 85.56% 86.27% 86.10% wei+3 levels 88.37% 86.80% 87.52% 87.56%</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Evaluations of the flat single level weighted mean (wei+flat) vs. three levels of weighted mean pooling (wei+3 levels) on HMDB-51.no. of av. frame no. of no. of no.</figDesc><table><row><cell>of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Statistics of datasets used in our experiemnts.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A general dense image matching framework combining direct and feature-based costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Braux-Zin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bartoli</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Selective spatio-temporal interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Gonz?lez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="396" to="410" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized rank pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Higher-order pooling of CNN features via kernel linearization for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<meeting><address><addrLine>Santa Rosa, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-linear temporal subspace representations for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>IEEE. 1, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PoTion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Hadjieleftheriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1530" to="1541" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jutta</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human Detection Using Oriented Histogram of Flow and Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>IEEE. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICCCN</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilinear attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumava</forename><surname>Kumar Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8030" to="8039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>IEEE. 12</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal residual networks for dynamic scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amir Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning end-to-end video classification with rankpooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Orientation histograms for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<idno>TR94-03</idno>
		<imprint>
			<date type="published" when="1994-12" />
			<pubPlace>Cambridge, MA 02139</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MERL -Mitsubishi Electric Research Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<idno>IEEE. 13</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Long Beach,California,USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new large scale dynamic texture dataset with application to ConvNet understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isma</forename><surname>Hadji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability product kernels. JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the Burstiness of Visual Elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Long Beach,alifornia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Spatio-Temporal Descriptor Based on 3D-Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMCV</title>
		<meeting><address><addrLine>Leeds, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tensor representations via kernel linearization for action recognition from 3D skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Netherlands</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Soft Assignment of Visual Words as Linear Coordinate Coding and Optimisation of its Reconstruction Error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno>IEEE. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2461" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tensor representations for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Higher-order Occurrence Pooling on Mid-and Low-level Features: Visual Concept Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Higher-order occurrence pooling for bags-of-words: Visual concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Comparison of Mid-Level Feature Coding Approaches And Pooling Strategies in Visual Concept Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Power normalizations in fine-grained image, few-shot image and graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A deeper look at power normalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
		<idno>IEEE. 9</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Finding achilles&apos; heel: Adversarial attack on multi-modal action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Wei</forename><surname>Seah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<editor>Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger Zimmermann</editor>
		<meeting><address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3829" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Human action recognition using multi-velocity STIPs and motion energy orientation histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailiang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="312" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A slow-i-fast-p architecture for compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno>2020. ACM. 4</idno>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<meeting><address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2039" to="2047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2014</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Lingqiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<idno>IEEE. 3</idno>
		<title level="m">Defence of Soft-assignment Coding. In ICCV</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2486" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Action recognition with bootstrapping based long-range temporal context attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Kai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<editor>Laurent Amsaleg, Benoit Huet, Martha A. Larson, Guillaume Gravier, Hayley Hung, Chong-Wah Ngo, and Wei Tsang Ooi</editor>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Heraklion, Crete</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ninh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<meeting><address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evolving spacetime neural architectures for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
		<idno>IEEE. 11</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="1793" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Providence, Rhode Island</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Assem-blenet++: Assembling modality representations via attention connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhana</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Kangaspunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Glasgow, UK, 2020</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A 3-Dimentional SIFT Descriptor and its Application to Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<meeting><address><addrLine>Augsburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">ICCV</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>San Francisco,CA,USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Hallucinating optical flow features for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianqiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<meeting><address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="926" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Ionut Cosmin Duta, Negar Rostamzadeh, and Nicu Sebe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uijlings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
	<note>Realtime Video Classification using Dense HOF/HOG</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cor</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Mark</forename><surname>Geusebroek</surname></persName>
		</author>
		<title level="m">Visual word ambiguity. TPAMI</title>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Cheng-Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dense Trajectories and Motion Boundary Descriptors for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning discriminative video representations using adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Analysis and evaluation of Kinect-based action recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>School of the Computer Science and Software Engineering, The University of Western Australia</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A comparative review of recent kinect-based action recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Hallucinating IDT descriptors and I3D optical flow features for action recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">PA3D: Pose-action 3D machine for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Local trinary patterns for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lahav</forename><surname>Yeffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno>IEEE. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="492" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">PAN: persistent appearance network with an efficient motion cue for fast action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<editor>Laurent Amsaleg, Benoit Huet, Martha A. Larson, Guillaume Gravier, Hayley Hung, Chong-Wah Ngo, and Wei Tsang Ooi</editor>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Few-shot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>Long Beach California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deep unsupervised saliency detection: A multiple noisy labeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Salt Lake City,UT,USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>IEEE. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
