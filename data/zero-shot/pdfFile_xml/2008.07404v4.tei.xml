<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer Vision and Image Understanding Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Plizzari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<addrLine>Via Giuseppe Ponzio 34/5</addrLine>
									<postCode>20133</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Politecnico di Torino</orgName>
								<orgName type="institution" key="instit2">Corso Duca degli Abruzzi</orgName>
								<address>
									<addrLine>24</addrLine>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cannici</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<addrLine>Via Giuseppe Ponzio 34/5</addrLine>
									<postCode>20133</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Matteucci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<addrLine>Via Giuseppe Ponzio 34/5</addrLine>
									<postCode>20133</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer Vision and Image Understanding Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton data has demonstrated being robust to illumination changes, body scales, dynamic camera views, and complex background. In particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated to be effective in learning both spatial and temporal dependencies on non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem, especially when it comes to extracting effective information from joint motion patterns and their correlations. In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self--attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network, whose performance is evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving backbone results. Compared with methods that use the same input data, the proposed ST-TR achieves state-of-the-art performance on all datasets when using joints' coordinates as input, and results on-par with state-of-the-art when adding bones information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human Action Recognition is achieving increasing interest in recent years for the progress achieved in deep learning and computer vision and for the interest of its applications in human-computer interaction, eldercare and healthcare assistance, as well as video surveillance. Recent advances in 3D depth cameras such as Microsoft Kinect <ref type="bibr" target="#b59">(Zhang (2012)</ref>) and Intel RealSense <ref type="bibr" target="#b23">(Keselman et al. (2017)</ref>) sensors, and advanced human pose estimation algorithms <ref type="bibr" target="#b5">(Cao et al. (2019)</ref>) made it possible to estimate 3D skeleton coordinates quickly and accurately with cheap devices. Nevertheless, several aspects of skeleton-based action recognition still remain open ; <ref type="bibr" target="#b0">Aggarwal and Ryoo (2011)</ref>; <ref type="bibr" target="#b41">Ren et al. (2020)</ref>). The most widespread method to perform skeletonbased action recognition has nowadays become Graph Neural Networks (GNNs), and in particular, Graph Convolutional Networks (GCNs) since, being an efficient representation of non-Euclidean data, they are able to effectively capture spatial (intra-frame) and temporal (inter-frame) information. Models making use of GCN were first introduced in skeleton-based action recognition by <ref type="bibr" target="#b55">Yan et al. (2018)</ref> and they are usually referred to as Spatial-Temporal Graph Convolutional Networks (ST-GCNs). These models process spatial information by operating on skeleton bone-connections along space, and temporal information by considering additional time-connections between each skeleton joint along time. Despite being proven to perform very well on skeleton data, ST-GCN models have some structural limitations, some of them already addressed by <ref type="bibr" target="#b44">Shi et al. (2019a)</ref>; <ref type="bibr" target="#b47">Simonyan and Zisserman (2014)</ref>; <ref type="bibr" target="#b7">Cheng et al. (2020)</ref>; <ref type="bibr" target="#b35">Liu et al. (2020)</ref>.</p><p>First of all, the topology of the graph representing the human body is fixed for all layers and all the actions; this may prevent the extraction of rich representations for the skeleton movements during time, especially if graph links are directed and information can only flow along a predefined path. Secondly, both Spatial and Temporal Convolution are implemented starting from a standard 2D convolution. As such, they are lim-q k v (1)</p><p>(2) (3) (4) <ref type="figure">Fig. 1</ref>. Self-attention on skeleton joints. (1) For each body joint, a query q, a key k and a value vector v are calculated.</p><p>(2) Then, the dot product ( ) between the query of the joint and the key of all the other nodes is performed, representing the connection strength between each pair of nodes.</p><p>(3) Finally, each node is scaled by its correlation w.r.t. the current node, (4) whose new features are obtained summing the weighted nodes together.</p><p>ited to operate in a local neighborhood, somehow restricted by the convolution kernel size. And finally, as a consequence of the previous, correlations between body joints not linked in the human skeleton, e.g., the left and right hands, are underestimated even if relevant in actions such as "clapping". In this paper, we face all these limitations by employing a modified Transformer self-attention operator, as depicted in <ref type="figure">Figure 1</ref>. Despite being originally designed for Natural Language Processing (NLP) tasks, Transformer self-attention has shown remarkable results on a broad range of computer vision tasks, spanning from classical classification and detection <ref type="bibr" target="#b12">(Dosovitskiy et al. (2020)</ref>; <ref type="bibr" target="#b1">Bello et al. (2019a)</ref>; <ref type="bibr" target="#b54">Wang et al. (2018)</ref>; <ref type="bibr" target="#b6">Carion et al. (2020)</ref>), to more complex tasks such as those involving point clouds <ref type="bibr" target="#b60">(Zhao et al. (2020)</ref>), generative modeling <ref type="bibr" target="#b38">(Oord et al. (2016)</ref>; <ref type="bibr" target="#b39">Parmar et al. (2018)</ref>) and captioning ). In our setting, the sequentiality and hierarchical structure of human skeleton sequences, as well as the flexibility of Transformer self-attention <ref type="bibr" target="#b50">(Vaswani et al. (2017)</ref>) in modeling long-range dependencies, make the Transformer a perfect solution to tackle ST-GCN weaknesses. In our work, we aim to apply Transformer to spatial-temporal skeleton-based architectures, and in particular to joints representing the human skeleton, with the goal of modeling long-range interactions within human actions both in space, through a Spatial Self-Attention (SSA) module, and time, through a Temporal Self-Attention (TSA) module. Main contributions of this paper are summarized as follows:</p><p>? We propose a novel two-stream Transformer-based model for skeleton activity recognition tasks, employing selfattention on both the spatial and the temporal dimensions</p><p>? We design a Spatial Self-Attention (SSA) module to dynamically build links between skeleton joints, representing the relationships between human body parts, conditionally on the action and independently from the natural human body structure. On the temporal dimension, we introduce a Temporal Self-Attention (TSA) module to study the dynamics of a joint along time. We made both layers publicly available for experiments replication and further use 1</p><p>? Our model outperforms ST-GCN <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>) and 1 Code at https://github.com/Chiaraplizz/ST-TR A-GCN <ref type="bibr" target="#b45">(Shi et al. (2019b)</ref>) consistently improving backbone results on all datasets and achieving state-of-the-art performance when using joint information, and results onpar with state-of-the-art when bones information is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Skeleton-based Action Recognition</head><p>Most of the early studies in skeleton-based action recognition relied on handcrafted features <ref type="bibr" target="#b17">(Hu et al. (2015)</ref>; <ref type="bibr" target="#b51">Vemulapalli et al. (2014)</ref>; <ref type="bibr" target="#b19">Hussein et al. (2013)</ref>) exploiting relative 3D rotations and translations between joints. Deep learning revolutionized activity recognition by proposing methods capable of increased robustness ) and able to achieve unprecedented performance. Methods that fall into this category rely on different aspects of skeleton data: (1) Recurrent neural network (RNN) based methods <ref type="bibr" target="#b26">(Lev et al. (2016)</ref>; <ref type="bibr" target="#b52">Wang and Wang (2017)</ref>; <ref type="bibr" target="#b32">Liu et al. (2017b)</ref>; <ref type="bibr" target="#b13">Du et al. (2015)</ref>) leverage on the sequentiality of joint coordinates, treating input skeleton data as time series. (2) Convolutional neural network (CNN) based methods <ref type="bibr" target="#b8">(Ch?ron et al. (2015)</ref>; <ref type="bibr" target="#b47">Simonyan and Zisserman (2014)</ref>; <ref type="bibr" target="#b11">Ding et al. (2017)</ref>; <ref type="bibr" target="#b33">Liu et al. (2017c)</ref>; <ref type="bibr" target="#b27">Li et al. (2017)</ref>) leverage spatial information, in a complementary way to RNNbased ones. Indeed, 3D skeleton sequences are mapped into a pseudo-image, representing temporal dynamics and skeleton joints respectively in rows and columns. (3) Graph neural network (GNN) based methods <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>; <ref type="bibr" target="#b28">Li et al. (2019)</ref>; <ref type="bibr">Shi et al. (2019b,a)</ref>; <ref type="bibr" target="#b7">Cheng et al. (2020)</ref>; <ref type="bibr" target="#b35">Liu et al. (2020)</ref>), make use of both spatial and temporal data by exploiting information contained in the natural topological graph structure of the human skeleton. These latter methods have demonstrated to be the most expressive among the three, and among these, the first model capturing the balance between spatial and temporal dependencies has been the Spatio-Temporal Graph Convolutional Network (ST-GCN) <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>).</p><p>In this work, we used ST-GCN as the baseline model; its functioning is presented in details in Section 3.2. Specifically, we propose to substitute regular graph convolutions on both space and time with the transformer self-attention operator. <ref type="bibr" target="#b9">Cho et al. (2020)</ref> also proposed a Self-Attention Network (SAN) in which embeddings are extracted by segmenting the action sequence in temporal clips, and self-attention is applied among them in order to model long-term semantic information. However, since it applies self-attention on course-grained embeddings rather than skeleton joints, it hardly captures low-level joints' correlations within and between frames. Instead, by applying self-attention directly on nodes in the graph, both intraand inter-frame, we efficiently model both spatial and temporal dependencies of the skeleton sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Neural Networks</head><p>Geometric deep learning <ref type="bibr" target="#b3">(Bronstein et al. (2017)</ref>) refers to all emerging techniques attempting to generalize deep learning models to non-Euclidean domains such as graphs. The notion of Graph Neural Network (GNN) was initially outlined by <ref type="bibr" target="#b14">Gori et al. (2005)</ref> and further elaborated by <ref type="bibr" target="#b42">Scarselli et al. (2008)</ref>.</p><p>The intuitive idea underlying GNNs is that nodes in a graph represent objects or concepts while edges represent their relationships. Due to the success of Convolutional Neural Networks, the concept of convolution has later been generalized from grid to graph data. GNNs iteratively process the graph, each time representing nodes as the result of applying a transformation to nodes' and their neighbors' features. The first formulation of CNNs on graphs is due to <ref type="bibr" target="#b4">Bruna et al. (2014)</ref>, who generalized convolution to signals using a spectral construction. This approach had computational drawbacks that have been subsequently addressed by <ref type="bibr" target="#b16">Henaff et al. (2015)</ref> and <ref type="bibr" target="#b10">Defferrard et al. (2016)</ref>. The latter has been further simplified and extended by <ref type="bibr" target="#b24">Kipf and Welling (2017)</ref>. A complementary approach is the spatial one, where graph convolution is defined as information aggregation <ref type="bibr" target="#b36">(Micheli (2009);</ref><ref type="bibr" target="#b37">Niepert et al. (2016)</ref>; <ref type="bibr" target="#b49">Such et al. (2017)</ref>). In this work we make use of the spectral construction proposed by <ref type="bibr" target="#b24">Kipf and Welling (2017)</ref>, whose formulation is provided in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transformers in Computer Vision</head><p>The Transformer is the leading neural model for Natural Language Processing (NLP), proposed by <ref type="bibr" target="#b50">Vaswani et al. (2017)</ref> as an alternative to recurrent networks. It has been designed to face two key problems: (i) the processing of very long sequences, which are often intractable both for LSTMs and RNNs, and (ii) the limitations in parallelizing sentence processing, which is usually performed sequentially, word by word, in standard RNNs architectures. The Transformer follows a usual encoder-decoder structure, but it relies solely on multihead self-attention <ref type="bibr" target="#b50">(Vaswani et al. (2017)</ref>). Recently, Transformer self-attention has been applied in many popular computer vision tasks. <ref type="bibr" target="#b54">Wang et al. (2018)</ref> proposed a differentiable non-local operator based on self-attention, which allows to capture long-range dependencies both in space and time for a more accurate video classification. After the first attempt of <ref type="bibr" target="#b1">Bello et al. (2019a)</ref> to use self-attention as an alternative to convolutional operators, <ref type="bibr" target="#b12">Dosovitskiy et al. (2020)</ref> proposed a Vision Transformer (ViT), which shows how Transformers can effectively replace standard convolutions on images. <ref type="bibr" target="#b15">He et al. (2020)</ref> proposed a novel image transformer architecture for the image captioning task. <ref type="bibr" target="#b6">Carion et al. (2020)</ref> made the first attempt to use a Transformer model to tackle detection problems, namely the Detection Transformer (DeTR). <ref type="bibr" target="#b60">Zhao et al. (2020)</ref> proposed Point Transformer, a model which uses transformer self-attention to encode relations between point clouds, exploiting their permutation invariant nature. Other applications of Transformers in segmentation tasks ), multi-modal tasks <ref type="bibr" target="#b25">(Lee et al. (2020)</ref>) and generative modeling <ref type="bibr" target="#b38">(Oord et al. (2016)</ref>; <ref type="bibr" target="#b39">Parmar et al. (2018)</ref>) have been recently developed, showing the potential of Transformer models on a broad range of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In this section, Spatial-Temporal Graph Convolutional Networks (ST-GCN) by <ref type="bibr" target="#b55">Yan et al. (2018)</ref> and the original Transformer self-attention by <ref type="bibr" target="#b50">Vaswani et al. (2017)</ref> are summarized, being the basic blocks of the model we propose in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Skeleton Sequences Representation</head><p>Given a sequence of skeletons, we define V as the number of joints representing each skeleton and T as the total number of skeletons composing the sequence, also named frames in the following. In order to represent the sequence, a spatial temporal graph is built, i.e., G = (N, E), where N = {v ti |t = 1, ..., T, i = 1, ..., V} represents the set of all the nodes v ti of the graph, i.e., the body joints of the skeleton along all the time sequence, and E represents the set of all the connections between nodes. E consists of two subsets; the first subset</p><formula xml:id="formula_0">E S = {(v ti , v t j ) | i, j = 1, . . . , V, t = 1, .</formula><p>. . , T } is composed by the intra-skeleton connections at each time interval t, for any pair of joints (i, j) connected by a bone in the human skeleton.</p><p>The subset E S of intra-skeleton connections is commonly further divided into K disjoint partitions, based on some criterion <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>) (e.g., distance from the center of gravity), and encoded using a set of adjacency matrices? k ? {0, 1} V?V . The second subset</p><formula xml:id="formula_1">E T = {(v ti , v (t+1)i ) | i = 1, . . . , V, t = 1, . . . , T }</formula><p>consists of all the inter-frame connections between joints along consecutive time frames. The result is a graph extending on both the spatial and the temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Temporal Graph Convolutional Networks</head><p>Spatial Temporal Graph Convolutional Networks (ST-GCN) have been introduced by <ref type="bibr" target="#b55">Yan et al. (2018)</ref>. A ST-GCN is structured as a hierarchy of stacked spatial-temporal blocks, which are internally composed of a spatial convolution (GCN) followed by a temporal convolution (TCN).</p><p>The spatial sub-module uses the Graph Convolution formulation proposed by <ref type="bibr" target="#b24">Kipf and Welling (2017)</ref>, which can be summarized as it follows:</p><formula xml:id="formula_2">f out = K s k (f in A k )W k ,<label>(1)</label></formula><formula xml:id="formula_3">A k = D ? 1 2 k (? k + I)D ? 1 2 k , D ii = K s k (? i j k + I ij ),<label>(2)</label></formula><p>where K s is the kernel size on the spatial dimension,? k is the adjacency matrix of the undirected graph representing intrabody connections, I is the identity matrix and W k is a trainable weight matrix. The temporal convolution sub-module (TCN) is implemented as a 1 ? K t 2D convolution operating on (V, T ) dimensions of the (C in , V, T ) input volume, where K t is the number of frames considered within the kernel receptive field. As shown in Equation 1, the graph structure is predefined, being the adjacency matrix fixed. In order to make it adapative, <ref type="bibr" target="#b45">Shi et al. (2019b)</ref> introduced the Adaptive Graph Convolutional Network (A-GCN), where the GCN formulation in Equation 1 is replaced by the following:</p><formula xml:id="formula_4">f out = K s k f in (A k + B k + C k )W k ,<label>(3)</label></formula><p>where A k is the same as the one in Equation 1, B k is learned during training, and C k determines whether two vertices are connected or not through a similarity function.</p><p>(a) Spatial Self-Attention (b) Temporal Self-Attention <ref type="figure">Fig. 2</ref>. Spatial Self-Attention (SSA) and Temporal Self-Attention (TSA). Self-attention operates on each pair of nodes, by computing a weight for each of them which represents the strength of their correlation. Those weights are then used to score the contribution of each body joint v ti , proportionally to how relevant the node is w.r.t. to all the others. Please notice that on SSA (a), the procedure is illustrated only of a group of five nodes for simplicity, while in practice it operates on all the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transformer Self-Attention</head><p>The original Transformer model of <ref type="bibr" target="#b50">Vaswani et al. (2017)</ref> employs self-attention, i.e., a non-local operator originally designed to operate on words in NLP tasks with the goal of enriching the embedding of each word based on the surrounding context. In the Transformer, new word embeddings are computed by comparing pairs of words an then mixing their embeddings together based on how much a word is relevant w.r.t. the others. By gathering clues from the surrounding context, selfattention enables to extract a better meaning from each word, dynamically building relations within and between phrases.</p><p>In particular, for each word embedding w i ? W = {w 1 , ..., w n }, a query q ? R d q , a key k ? R d k and a value vector v ? R d v are computed through trainable linear transformations, independently. Then, a score for each word embedding is obtained by taking the dot product ? i j = q i ? k T j ?i, j = 1, ..., n, where n is the total number of nodes being considered. This score represents how much the word j is relevant for word i. To compute the final embedding for word i, a weighted sum is computed by first multiplying the value vector of each other word v j by the corresponding score ? i j , scaled through the softmax function, and then summing these vectors together. This process, also called scaled dot-product attention, can be written in matrix form as it follows:</p><formula xml:id="formula_5">Attention(Q, K, V) = so f tmax QK T ? d k V,<label>(4)</label></formula><p>where Q, K, and V are matrices containing the predicted query, key and value vectors, respectively, packed together and d k is the channel dimension of the key vectors. The division by ? d k is performed in order to increase gradients stability during training. In order to obtain better performance, a mechanism called multi-headed attention is usually applied, which consists in applying attention, i.e., a head, multiple times with different learnable parameters and then finally combining the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spatial Temporal Transformer Network</head><p>We propose the Spatial Temporal Transformer (ST-TR) network, an architecture which uses Transformer self-attention to operate on both space and time. We propose to achieve this goal using two modules, the Spatial Self-Attention (SSA) and the Temporal Self-Attention (TSA) modules, each one focusing on extracting correlations on one of the two dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motivation</head><p>The idea behind the original Transformer self-attention is to allow the encoding of both short-and long-range correlations between words in the sentence. Our intuition is that the same approach can be applied to skeleton-based action recognition as well, as correlations between nodes are crucial both on the spatial and on the temporal dimension. We consider the joints comprising the skeleton as a bag-of-words and make use of the Transformer self-attention to extract node embeddings encoding the relation between surrounding joints, just like words in a phrase in NLP. Contrary to a standard graph convolution, where only the adjacent nodes are compared, we discard any predefined skeleton structure and instead let the Transformer selfattention automatically discover joint relations which are relevant for predicting the current action. The resulting operation acts similarly to a graph convolution, but in which the kernel values are dynamically predicted based on the discovered joint relations. The same idea is also applied at the sequence level, by analyzing how each joint changes during the action and building long-range relations that span different frames, similarly to how relations between phrases are built in NLP. The resulting operator is capable of obtaining a dynamical representation extending both on the spatial and the temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatial Self-Attention (SSA)</head><p>The Spatial Self-Attention module applies self-attention inside each frame to extract low-level features embedding the relations between body parts. This is achieved by computing correlations between each pair of joints in every single frame independently, as depicted in <ref type="figure">Figure 2a</ref>. Given the frame at time t, for each node v ti of the skeleton, a query vector q t i ? R dq , a key vector k t i ? R dk and a value vector v t i ? R dv are first computed by applying trainable linear transformations to the node  <ref type="figure">Fig. 3</ref>. Illustration of two 2s-ST-TR architecture. On each stream, the first three layers extract low level features through standard ST-GCN <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>) layers. At each successive layer, on the S-TR stream (coloured in green), SSA is used to extract spatial information, followed by a 2D convolution on time dimension (TCN), while on the T-TR stream (coloured in blue), TSA is used to extract temporal information, while spatial features are extracted by a standard graph convolution (GCN).</p><formula xml:id="formula_6">features n t i ? R C in with parameters W q ? R C in ?dq , W k ? R C in ?dk , W v ? R C in ?dv ,</formula><p>body nodes (v ti , v t j ), a query-key dot product is applied to obtain a weight ? t i j = q t i ? k t j T ? R, ?t ? T representing the correlation strength between the two nodes. The resulting score ? t i j is used to weight each joint value v t j , and a weighted sum is computed to obtain a new embedding z t i for node v ti , as in the following:</p><formula xml:id="formula_7">z t i = j so f tmax j ? ? ? ? ? ? ? ? t i j ? d k ? ? ? ? ? ? ? v t j ,<label>(5)</label></formula><p>where z t i ? R C out (with C out the number of output channels) constitutes the new embedding of node v ti .</p><p>Multi-head attention is applied by repeating this embedding extraction process N h times, each time with a different set of learnable parameters. The set (z t i 1 , ..., z t i H ) of node embeddings thus obtained, all referring to the same node v ti , is then combined with a learnable transformation, i.e., concat(z t i 1 , ..., z t i H ) ? W o , and constitutes the output features of SSA.</p><p>As shown in <ref type="figure">Figure 2a</ref>, the relations between nodes (i.e., the ? t i j scores) are dynamically predicted in SSA; the correlation structure in the skeleton is then not fixed for all the actions, but it changes adaptively for each sample. SSA operates similar to a graph convolution on a fully connected graph where, however, the kernel values (i.e., the ? t i j scores) are predicted dynamically based on the skeleton pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Temporal Self-Attention (TSA)</head><p>With the Temporal Self-Attention (TSA) module, the dynamics of each joint is studied separately along all the frames, i.e., each single joint is considered as independent and correlations between frames are computed by comparing the change in the embeddings of the same body joint along the temporal dimension (see <ref type="figure">Figure 2b)</ref>. The formulation is symmetrical to the one reported in Equation (5) for SSA:</p><formula xml:id="formula_8">? v tu = q v t ? k v u ?v ? V, z v t = j so f tmax u ? v tu ? d k v v u ,<label>(6)</label></formula><p>where v ti , v ui indicate the same joint v in two different instants t, u, ? i tu ? R is the correlation score, q i t ? R dq is the query Reshape Reshape BatchNorm Conv1D Conv1D <ref type="figure">Fig. 4</ref>. Illustration of a SSA module (the implementation of TSA is the same, with the only difference that the dimension V corresponds to T and viceversa). The input f in is reshaped by moving T in the batch dimension, such that self-attention operates on each time frame separately. SSA is implemented as a matrix multiplication, where Q, K and V are the query, key and value matrix respectively, and ? denotes the matrix multiplication.</p><p>associated to v ti , k i u ? R dk and v i u ? R dv are the key and value associated to joint v ui (all computed using trainable linear transformations as in SSA), and z i t ? R C out is the resulting node embedding. Note that the notation used in this section is opposite w.r.t. the one used in Section 4.2; subscripts indicate time while superscripts indicate the joint. Multi-head attention is applied in TSA as in SSA. An example of TSA is depicted in <ref type="figure">Figure 2b</ref>.</p><p>The TSA module, by extracting inter-frame relations between nodes in time, can learn how to correlate frames apart from each other (e.g., nodes in the first frame with those in the last one), capturing discriminant features that are not otherwise possible to capture with a standard ST-GCN convolution, being this limited by the kernel size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Two-Stream Spatial Temporal Transformer Network</head><p>To combine the SSA and TSA modules, a two-stream architecture named ST-TR is used, as similarly proposed by <ref type="bibr" target="#b45">Shi et al. (2019b)</ref> and <ref type="bibr" target="#b44">Shi et al. (2019a)</ref>. In our formulation, the two streams differentiate on the way the proposed self-attention mechanisms are applied: SSA operates on the spatial stream (named S-TR), while TSA on the temporal one (named T-TR). On both streams, node features are first extracted by a threelayers residual network, where each layer processes the input on the spatial dimension through graph convolution (GCN), and on the temporal dimension through a standard 2D convolution (TCN), as done by <ref type="bibr" target="#b55">Yan et al. (2018)</ref> 2 . SSA and TSA are then applied on the S-TR and on the T-TR streams in the subsequent layers in substitution to the GCN and TCN feature extraction modules respectively <ref type="figure">(Figure 3)</ref>. The S-TR stream and T-TR stream are end-to-end trained separately along with their corresponding feature extraction layers. The sub-networks outputs are eventually fused together by summing up their softmax output scores to obtain the final prediction, as proposed by <ref type="bibr" target="#b45">Shi et al. (2019b)</ref> and <ref type="bibr" target="#b44">Shi et al. (2019a)</ref>.</p><p>Spatial Transformer Stream (S-TR). In the spatial stream, self-attention is applied at the skeleton level through a SSA module, which focuses on spatial relations between joints. The output of the SSA module is passed to a 2D convolutional module with kernel K t on the temporal dimension (TCN), as done by <ref type="bibr" target="#b55">Yan et al. (2018)</ref>, in order to extract temporally relevant features, as shown in <ref type="figure">Figure 3</ref> and expressed in the following:</p><formula xml:id="formula_9">S-TR(x) = Conv 2D(1?K t ) (SSA(x)).<label>(7)</label></formula><p>Following the original Transformer, the input is passes through a Batch Normalization layer <ref type="bibr" target="#b20">(Ioffe and Szegedy (2015)</ref>; Nguyen and Salazar <ref type="formula" target="#formula_2">(2019)</ref>), and skip connections are used to sum the input to the output of the SSA module (see <ref type="figure">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Transformer Stream (T-TR). The temporal stream, instead, focuses on discovering inter-frame temporal relations.</head><p>Similarly to the S-TR stream, inside each T-TR layer, a standard graph convolution sub-module <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>) is followed by the proposed Temporal Self-Attention module:</p><p>T-TR(x) = TSA(GCN(x)).</p><p>TSA operates on graphs linking the same joint along all the time dimension (e.g., all left feet, or all right hands).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Implementation of SSA and TSA</head><p>The matrix implementation of SSA (and of TSA) is based on the implementation of Transformer on pixels by <ref type="bibr" target="#b2">Bello et al. (2019b)</ref>. As shown in <ref type="figure">Figure 4</ref>, given an input tensor of shape (C in , T, V), where C in is the number of input features, T is the number of frames and V is the number of nodes, a matrix X V ? R T ?C in ?V is obtained by rearranging the input. Here the T dimension is moved inside the batch dimension, effectively implementing parameter sharing along the temporal dimension and applying the transformation separately on each frame:</p><formula xml:id="formula_11">head h (X V ) = S o f tmax ? ? ? ? ? ? ? ? ? ? ? ? (X V W q )(X V W k ) T d h k ? ? ? ? ? ? ? ? ? ? ? ? (X V W v ) S el f Attention V = Concat(head 1 , ..., head N h )W o ,<label>(9)</label></formula><p>where the product with</p><formula xml:id="formula_12">W q ? R C in ?N h ?d h q , W k ? R C in ?N h ?d h k and W v ? R C in ?N h ?d h v gives rise respectively to Q ? R T ?N h ?d h q ?V , K ? R T ?N h ?d h k ?V and V ? R T ?N h ?d h v ?V ,</formula><p>being N h the number of heads, and W o a learnable linear transformation combining the heads outputs. The output of the Spatial Transformer is then rearranged back into R C out ?T ?V . The TSA matrix implementation has the same expression as Equation <ref type="formula" target="#formula_11">(9)</ref>, differing only in the way the input X is processed. Indeed, in order to be processed by each TSA module, the input is reshaped into a matrix X T ? R V?C in ?T , where the V dimension has been moved in the first position and aggregated to the batch dimension, not reported here explicitly, in order to operate separately on each joint along the time dimension. The formulation is analogous to Equation <ref type="formula" target="#formula_11">(9)</ref>, differing only in the shape of matrices, which become</p><formula xml:id="formula_13">Q ? R V?N h ?d h q ?T , K ? R V?N h ?d h k ?T and V ? R V?N h ?d h v ?T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model Evaluation</head><p>To understand the impact of both the Spatial and Temporal Transformer streams, we analyze their performance separately and in different configurations through extensive experiments on NTU-RGB+D 60 ) (see <ref type="table" target="#tab_1">Table 1</ref>-3). Then, for a comparison with the state-of-the-art, we test the resulting best configurations on the Kinetics dataset <ref type="bibr" target="#b21">(Kay et al. (2017)</ref>) and on the NTU-RGB+D 120 dataset ), which represents to date one of the most complex skeleton-based action recognition benchmarks (see <ref type="table">Table 4</ref>-5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>NTU RGB+D 60 and NTU RGB+D 120. The NTU RGB+D 60 (NTU-60) dataset is a large-scale benchmark for 3D human action recognition collected using Microsoft Kinect v2 by <ref type="bibr" target="#b43">Shahroudy et al. (2016)</ref>. Skeleton information consists of 3D coordinates of 25 body joints and a total of 60 different action classes. The NTU-60 dataset follows two different criteria for evaluation. The first one, called Cross-View Evaluation (X-View), uses 37, 920 training and 18, 960 test samples, split according to the camera views from which the action is taken. The second one, called Cross-Subject Evaluation (X-Sub), is composed instead of 40, 320 training and 26, 560 test samples. Data collection has been performed with 40 different subjects performing actions and divided into two groups, one for training and the other for testing. NTU RGB+D 120 ) (NTU-120) is an extension of NTU-60, which adds 57, 367 new skeleton sequences representing 60 new actions. To perform the evaluation, the extended dataset follows two criteria: the first one is the Cross-Subject Evaluation (X-Sub), the same used for NTU-60, while the second one is called Cross-Setup Evaluation (X-Set), which substitutes Cross-View by splitting training and testing samples based on the parity of the camera setup IDs.</p><p>Kinetics. The Kinetics skeleton dataset <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>) is obtained by extracting skeleton annotations from videos composing the Kinetics 400 dataset <ref type="bibr" target="#b21">(Kay et al. (2017)</ref>), by using the OpenPose toolbox <ref type="bibr" target="#b5">(Cao et al. (2019)</ref>). It consists of 240, 436 training and 19, 796 testing samples, representing a total of 400 action classes. Each skeleton is composed by 18 joints, each one provided with the 2D coordinates and a confidence score. For each frame, a maximum of 2 people are selected based on the highest confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model Complexity</head><p>We perform an analysis on the complexity of the different self-attention modules we designed, and compare them to ST-GCN modules <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>), based on standard convolution, and to 1s-AGCN <ref type="bibr" target="#b45">(Shi et al. (2019b)</ref>) modules, based on adaptive graph convolution. First, we compare in <ref type="figure" target="#fig_0">Figure 5a</ref> singularly, a layer of standard convolution with our transformer mechanism, setting C in = C out channels. This results in the same number of parameters for both TSA and SSA, since the convolutions performed internally have the same kernel dimensions and both the query-key dot product and the logit-value product are parameter free. It can be seen that SSA introduces less parameters than GC, especially when dealing with a large number of channels, where the maximum ? GC?S S A , i.e., the decrease in terms of parameters, is 1.1 ? 10 5 . When dealing with adaptive modules (AGC), an additional number of parameters has to be considered, resulting in a difference with respect to SSA of ? AGC?S S A = 5 ? 10 5 . On the temporal dimension ? TC?T S A reaches a value of 16.8 ? 10 5 . Temporal convolution in <ref type="bibr" target="#b55">Yan et al. (2018)</ref> is implemented as a 2D convolution with filter 1 ? F, where F is the number of frames considered along the time dimension, and it is usually set to 9, striding along T = 300 frames. Thus, substituting it with a self-attention mechanism results in a great complexity reduction, in addition to better performance, as reported in the next sections. Finally, in <ref type="figure" target="#fig_0">Figure 5b</ref> we also compare the entire stream architectures, i.e., ST-GCN <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>) and 1s-AGCN <ref type="bibr" target="#b45">(Shi et al. (2019b)</ref>) with the proposed S-TR and T-TR streams in terms of parameters. As expected from the considerations above, the biggest improvement in parameters reduction is achieved by substituting temporal convolution with TSA, i.e., in T-TR, with a ? S T ?GCN??T ?T R = 16.7 ? 10 5 . On the spatial dimension the difference in terms of parameters is not as pronounced as in temporal dimension, but it is still significant, with a ? S T ?GCN??S ?T R = 1.07?10 5 and ? 1s?AGCN??S ?T R = 5.0?10 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Settings</head><p>Using PyTorch <ref type="bibr" target="#b40">(Paszke et al. (2019)</ref>) framework, we trained our models for a total of 120 epochs with batch size 32 and SGD as optimizer on NTU-60 and NTU-120, while on Kinetics we trained our models for a total of 65 epochs, with batch size 128. The learning rate is set to 0.1 at the beginning and then reduced by a factor of 10 at the epochs {60, 90} and {45, 55} for NTU and Kinetics respectively. These schedulings have been selected as they have been shown to provide good results on ST-GCN networks used by <ref type="bibr" target="#b44">Shi et al. (2019a)</ref>. When using adaptive AGCN modules, we performed a linear warmup of the learning rate during the first epoch. Moreover, we preprocessed the data with the same procedure used by <ref type="bibr" target="#b45">Shi et al. (2019b)</ref> and <ref type="bibr" target="#b44">Shi et al. (2019a)</ref>. In order to avoid overfitting, we also used DropAttention, a particular dropout technique introduced by <ref type="bibr" target="#b56">Zehui et al. (2019)</ref> for regularizing attention weights in Transformer networks, that consists in randomly dropping columns of the attention logits matrix. In all of these experiments, the number of heads for multi-head attention is set to 8, and d q , d k , d v embedding dimensions to 0.25?C out in each layer, as done in <ref type="bibr" target="#b2">Bello et al. (2019b)</ref>. We did not perform grid search on these parameters. As far as it concerns the model architecture, each stream is composed by 9 layers, of channel dimension <ref type="bibr">64, 64, 64, 128, 128, 128, 256, 256 and 256</ref>. Batch normalization is applied to input coordinates, a global average pooling layer is applied before the softmax classifier and each stream is trained using the standard cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results</head><p>To verify in a fair way the effectiveness of our SSA and TSA modules, we compare the S-TR and T-TR streams individually against the ST-GCN <ref type="bibr" target="#b55">(Yan et al. (2018)</ref>) baseline (whose results are reported using our learning rate scheduling) and other models that modify its basic GCN module (see <ref type="table" target="#tab_1">Table 1</ref>): (i) ST-GCN (fc): we implemented a version of ST-GCN whose adjacency matrix is composed of all ones (referred as A f c ), to simulate the fully-connected skeleton structure underlying our SSA module and verify the superiority of self-attention over graph convolution on the spatial dimension; (ii) 1s-AGCN: Adaptive Graph Convolutional Network (AGCN) <ref type="bibr" target="#b45">(Shi et al. (2019b)</ref>) (see Section 3.2), as it demonstrated in the literature to be more robust than standard ST-GCN, in order to remark the robustness of our SSA module over more recent methods; (iii) 1s-AGCN w/o A: 1s-AGCN without the static adjacency matrix, to verify the effectiveness of our SSA over graph convolution in a similar setting where all the links between joints are exclusively learnt. All these methods use the same implementation of convolution on the temporal dimension (TCN). We make a comparison both in terms of model accuracy and number of parameters.</p><p>Regarding SSA, the performance of S-TR is superior to all methods mentioned above, demonstrating that self-attention can be used in place of graph convolution, increasing the network performance while also decreasing the number of parameters. In fact, as it can be seen from <ref type="table" target="#tab_1">Table 1</ref>, S-TR introduces 0.3 ? 10 5 parameters less then ST-GCN and 4 ? 10 5 less than 1s-AGCN, with a performance increment w.r.t. all GCN configu-  rations. Similarly, regarding TSA, what emerges from the comparison between T-TR and the ST-GCN baseline adopting standard convolution, is that by using self-attention on the temporal dimension the model is significantly lighter (13.4 ? 10 5 less parameters), and achieves an increment in accuracy of 0.9%.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> we first analyze the performance of the S-TR stream, T-TR stream and their combination by using input data consisting of joint information only. As it can be seen from <ref type="table" target="#tab_2">Table 2a</ref>, on NTU-60 the S-TR stream achieves slightly better performance (+0.4%) than the T-TR stream, on both X-View and X-Sub. This can be motivated by the fact that SSA in S-TR operates on 25 joints only, while on temporal dimension the number of correlations is proportional to the huge number of frames. Again, as shown in <ref type="table" target="#tab_1">Table 1a</ref>, applying self-attention instead of convolution clearly benefits the model on both spatial and temporal dimensions. The combination of the two streams achieves 88.7% of accuracy on X-Sub and 95.6% of accuracy on X-View, outperforming the baseline ST-GCN and surpassing other two-stream architectures (see <ref type="table" target="#tab_3">Table 3</ref>).</p><p>As adding the differential of spatial coordinates (bones information) demonstrated to lead to better results in previous works <ref type="bibr" target="#b44">(Shi et al. (2019a)</ref>; <ref type="bibr" target="#b47">Simonyan and Zisserman (2014)</ref>), we also studied our Transformer modules on combined joint and bones information. For each node v 1 = (x 1 , y 1 , z 1 ) and v 2 = (x 2 , y 2 , z 2 ), the bone connecting the two is calculated as b v 1 ,v 2 = (x 2 ? x 1 , y 2 ? y 1 , z 2 ? z 1 ). Both joint and bone information are concatenated along the channel dimension, and then fed to the network. At each layer, the dimension of the input and output channels are doubled as done by <ref type="bibr" target="#b44">Shi et al. (2019a)</ref> and <ref type="bibr" target="#b47">Simonyan and Zisserman (2014)</ref>. Results are shown again in <ref type="table" target="#tab_2">Table 2a</ref>, where all previous configurations improve when bones information is added as input. This highlights the flexibility of our method, which is capable of adapting to different input types and network configurations.</p><p>To further test its flexibility, we also perform experiments in which the GCN module is substituted by the AGCN adaptive module on the temporal stream. As it can be seen from <ref type="table" target="#tab_2">Table  2a</ref>, these configurations (T-TR-agcn) achieve better results than the one using standard GCN on both X-Sub and X-View.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effect of Applying Self-Attention since Feature Extraction</head><p>We designed our streams to operate starting from high-level features, rather than directly from coordinates, extracted using a sequence of residual GCN and TCN modules as reported in Section 4.4. This set of experiments validates our design choice. In these experiments SSA (TSA) substitutes GCN (TCN) on the S-TR (T-TR) stream, from the very first layer. The configurations reported in <ref type="table" target="#tab_2">Table 2b</ref> (named S-TR-alllayers), performs worse than the corresponding ones in <ref type="table" target="#tab_2">Table  2a</ref>, while still outperforming the baseline ST-GCN <ref type="bibr" target="#b44">(Shi et al. (2019a)</ref>) (see <ref type="table" target="#tab_3">Table 3</ref>). Indeed, self-attention has demonstrated being more efficient when incorporated in later stages of the network <ref type="bibr" target="#b6">(Carion et al. (2020)</ref>; <ref type="bibr" target="#b18">Huang et al. (2019)</ref>; <ref type="bibr" target="#b54">Wang et al. (2018)</ref>). Notice that on T-TR, in order to deal with the great number of frames in the very first layers (T = 300), we divided them into blocks within which SSA is applied, and then gradually reduce the number of blocks (d block = 10 where C out = 64, d block = 10 where C out = 128, and a single block of d block = T l on layers l with C out = 256).</p><p>The standard protocol used in recent works <ref type="bibr" target="#b7">(Cheng et al. (2020)</ref>; <ref type="bibr" target="#b45">Shi et al. (2019b)</ref>) that propose alternative modules for ST-GCN based networks is to keep the original ST-GCN backbone architecture fixed in terms of layers composition. Following these works, we kept the three original feature extraction layers for a fair comparison. We further conduct some targeted experiments in which we vary their number k <ref type="table" target="#tab_2">(Table 2b</ref>). As it can be seen, performance is not sensitive to variations of k, confirming the effectiveness of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Effect of Augmenting Convolution with Self-Attention</head><p>Motivated by the results in <ref type="bibr" target="#b2">Bello et al. (2019b)</ref>, we studied the effect of applying the proposed Transformer mechanism as an augmentation procedure to the original ST-GCN modules. In this configuration, 0.75 ? C out features result from GCN (TCN) and they are concatenated to the remaining 0.25 ? C out features from SSA (TSA), a setup that has proven to be effective in <ref type="bibr" target="#b2">Bello et al. (2019b)</ref>. To compensate the reduction of attention channels, wide attention is used, i.e., half of the attention channels are assigned to each head, then recombined together while merging heads. The results are reported in <ref type="table" target="#tab_2">Table 2b</ref> (referred as ST-TR-augmented). Graph convolution is the one that benefits the most from SSA attention (S-TR-augmented, 94.5%), to be compared with S-TR's 94% in <ref type="table" target="#tab_2">Table 2a</ref>. Nevertheless, the lower number of output features assigned to self-attention prevent temporal convolution improving on T-TR stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Effect of combining SSA and TSA in a single stream</head><p>We tested the efficiency of the model when SSA and TSA are combined in a single stream architecture (see <ref type="table" target="#tab_2">Table 2b</ref>, referred as S-TR-1s). In this configuration, feature extraction is still performed by the original GCN and TCN modules, while from the 4th layer on, each layer is composed by SSA followed by TSA, i.e., ST-TR-1s(x) = TSA(SSA(x)).</p><p>We also tested this configuration on NTU-60, obtaining an accuracy of 93.3%, slightly lower than the 95.6% accuracy of 2s-ST-TR (see <ref type="table" target="#tab_1">Table 1a</ref>, ST-TR). However, it should be noted  38.0 60.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparison with State-Of-The-Art Results</head><p>In addition to NTU-60, we compare our methods on NTU-120 and Kinetics. For a fair comparison, we compare the ST-TR configurations on methods trained on the same input data (either with joint information only, or both joint and bones information). On NTU-60 <ref type="table" target="#tab_3">(Table 3)</ref>, the proposed ST-TR, when using joint information only, outperforms all the state-of-theart models using the same type of information. In particular, it outperforms SAN <ref type="bibr" target="#b9">(Cho et al. (2020)</ref>), another method employing self-attention in skeleton-based action recognition, by up to 3%. When using bones information, the proposed transformer based architecture outperforms 2s-AGCN <ref type="bibr" target="#b45">(Shi et al. (2019b)</ref>) on both X-Sub and X-Set and our best configuration making use of the AGCN backbone (ST-TR-agcn) reaches performance on-par with state-of-the-art. We further compare ST-TR against 4s Shift-GCN <ref type="bibr" target="#b7">(Cheng et al. (2020)</ref>) which, in addition to the joint and bones streams, also comprises two extra streams making use of additional temporal information, and DGNN <ref type="bibr" target="#b44">(Shi et al. (2019a)</ref>), which also makes use of motion information besides joint and bones. We report 4s Shift-GCN and DGNN in <ref type="table" target="#tab_3">Table 3</ref>-5 with a different color to highlight the difference in the input. ST-TR-agcn outperforms DGNN on both X-Sub and X-View and crucially, although 4s Shift-GCN uses extra input data and combines two additional streams, ST-TR-agcn still achieves on-par results but with a simpler design.</p><p>On NTU-120 <ref type="table">(Table 4)</ref>, the model only based on joints outperforms all state-of-the-art methods that use the same information. When adding bones, both ST-TR and ST-TR-agcn outperform 2s-AGCN by up to 3% on both X-Sub and X-Set. Moreover, ST-TR-agcn's results are on-par with 2s Shift-GCN <ref type="bibr" target="#b7">(Cheng et al. (2020)</ref>) on X-Sub, while they improve on X-Set. Our network has only slightly lower performance than MS-G3D ), which represents to date a very strong baseline in skeleton-based action recognition. Considering that MS-G3D features a multi-path design with multi-scale graph convolutions, the performance obtained by ST-TR is remarkable given that the latter is based on a simpler backbone. Finally, on Kinetics <ref type="table" target="#tab_4">(Table 5)</ref>, our model using only joints outperforms the ST-GCN baseline by 5% and all previous methods using only joint information. When bones information is added, it outperforms both 2s-AGCN and DGNN, and achieves results on-par with the very recent state-of-the-art method MS-G3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Qualitative Results</head><p>In <ref type="figure">Figure 6</ref>, we report some actions and the corresponding Spatial Self-Attention maps. On the top we draw the skeleton of the subjects, where the radius of the circles in correspondence to each joint is proportional its relevance predicted by the self-attention. The heatmaps on the bottom represent the attention scores of the last layer; these are 25?25 matrices, where each row and each column represents a body joint. An element in position (i, j) represents the predicted correlation between joint i and joint j in the same frame. As it can be observed, depending on the action, different parts of the body are activated. In <ref type="figure" target="#fig_3">Figure 7</ref> are shown the same heatmaps at each layer. In the first layers, self-attention captures low-level correlations between body joints, as highlighted by the sparsity of the activations. While going deeper through the network, the global importance of each node emerges instead, as highlighted by the vertical lines corresponding to the most relevant joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>In this paper we propose a novel approach that introduces Transformer self-attention in skeleton activity recognition as an alternative to graph convolution. Through extensive experiments on NTU-60, NTU-120 and Kinetics, we demonstrated that our Spatial Self-Attention module (SSA) can replace graph convolution, enabling more flexible and dynamic representations. Similarly, Temporal Self-Attention module (TSA) overcomes the strict locality of standard convolution, enabling the extraction of long-range dependencies across the action. Moreover, our final Spatial-Temporal Transformer network (ST-TR) achieves state-of-the-art performance on all dataset w.r.t. methods using same input joint information and stream setup, and results on-par with state-of-the-art methods when bones information is added. As configurations only involving self-attention modules revealed to be sub-optimal, a possible future work is to search for a unified Transformer architecture able to replace graph convolution in a variety of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reading</head><p>Hand  <ref type="figure">Fig. 6</ref>. Skeleton of the subjects performing the action (top) and the corresponding SSA heatmaps (bottom). We display with red boxes the joints which the network identifies as the most relevant, while the corresponding spatial self-attention scores are highlighted in the attention maps. </p><note type="other">Waving Touch Chest</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Difference in terms of parameters between Graph Convolution (GC), Adaptive Convolution (AGC), Spatial Self-Attention (SSA) modules of C in = C out channels, and between Temporal Convolution (TC) and Temporal Self-Attention (TSA) modules; (b) parameters comparison between ST-GCN, 1s-AGCN and our novel S-TR and T-TR. Best viewed in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Layer visualization. Each column represent the k-th layer, along with the corresponding spatial self-attention heatmap on top, and the skeleton of the subjects performing the action on the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>shared across all nodes. Then, for each pair of</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ST-GCN module (layer i-th)</cell></row><row><cell cols="2">feature extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(ST-GCN layers)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">S-TR modules</cell><cell>S-TR stream</cell><cell></cell></row><row><cell>S-TR stream</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">S-TR i-th module</cell></row><row><cell>3,64,1 64,64,1</cell><cell>64,64,1</cell><cell>64,128,2</cell><cell>128,128,1 128,128,1</cell><cell>128,256,2 256,256,1 256,256,1</cell><cell>T-TR stream</cell><cell>T-TR i-th module</cell></row><row><cell>T-TR stream</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">T-TR modules</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison between the baseline and our self-attention modules in terms of both performance (accuracy (%)) and efficiency (number of parameters) on NTU-60 (X-View)</figDesc><table><row><cell>Method</cell><cell cols="3">GCN TCN Params [?10 5 ] Top-1</cell></row><row><cell>ST-GCN</cell><cell></cell><cell>31.0</cell><cell>92.7</cell></row><row><cell>ST-GCN-fc</cell><cell>A f c</cell><cell>26.5</cell><cell>93.7</cell></row><row><cell>1s-AGCN</cell><cell>A k , B k , C k</cell><cell>34.7</cell><cell>93.7</cell></row><row><cell cols="2">1s-AGCN w/o A B k , C k</cell><cell>33.1</cell><cell>93.4</cell></row><row><cell>S-TR</cell><cell>SSA</cell><cell>30.7</cell><cell>94.0</cell></row><row><cell>T-TR</cell><cell>TSA</cell><cell>17.6</cell><cell>93.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">a) Comparison of S-TR and T-TR streams, and their combination</cell></row><row><cell cols="2">(ST-TR) on NTU-60, w and w/o bones. b) Ablations on different model</cell></row><row><cell>configurations</cell><cell></cell></row><row><cell>Method</cell><cell>Bones X-Sub X-View</cell></row><row><cell>S-TR</cell><cell>86.4 94.0</cell></row><row><cell>T-TR</cell><cell>86.0 93.6</cell></row><row><cell>T-TR-agcn</cell><cell>86.9 94.7</cell></row><row><cell>ST-TR</cell><cell>88.7 95.6</cell></row><row><cell>ST-TR-agcn</cell><cell>89.2 95.8</cell></row><row><cell>S-TR</cell><cell>87.9 94.9</cell></row><row><cell>T-TR</cell><cell>87.3 94.1</cell></row><row><cell>T-TR-agcn</cell><cell>88.6 94.7</cell></row><row><cell>ST-TR</cell><cell>89.9 96.1</cell></row><row><cell>ST-TR-agcn</cell><cell>90.3 96.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art accuracy (%) on NTU-60. Best for both configurations w/ and w/o bones in bold.</figDesc><table><row><cell>NTU-60</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Bones X-Sub X-View</cell></row><row><cell>STA-LSTM (Song et al. (2017))</cell><cell></cell><cell>73.4</cell><cell>81.2</cell></row><row><cell>VA-LSTM (Zhang et al. (2017))</cell><cell></cell><cell>79.4</cell><cell>87.6</cell></row><row><cell>AGC-LSTM (Si et al. (2019))</cell><cell></cell><cell>89.2</cell><cell>95.0</cell></row><row><cell>ST-GCN (Yan et al. (2018))</cell><cell></cell><cell>81.5</cell><cell>88.3</cell></row><row><cell>AS-GCN (Li et al. (2019))</cell><cell></cell><cell>86.8</cell><cell>94.2</cell></row><row><cell>1s-AGCN (Shi et al. (2019b))</cell><cell></cell><cell>86.0</cell><cell>93.7</cell></row><row><cell>SAN (Cho et al. (2020))</cell><cell></cell><cell>87.2</cell><cell>92.7</cell></row><row><cell>1s Shift-GCN (Cheng et al. (2020))</cell><cell></cell><cell>87.8</cell><cell>95.1</cell></row><row><cell>ST-TR (Ours)</cell><cell></cell><cell>88.7</cell><cell>95.6</cell></row><row><cell>ST-TR-agcn (Ours)</cell><cell></cell><cell>89.2</cell><cell>95.8</cell></row><row><cell>2s-AGCN (Shi et al. (2019b))</cell><cell></cell><cell>88.5</cell><cell>95.1</cell></row><row><cell>DGNN (Shi et al. (2019a))</cell><cell></cell><cell>89.9</cell><cell>96.1</cell></row><row><cell>2s Shift-GCN (Cheng et al. (2020))</cell><cell></cell><cell>89.7</cell><cell>96.0</cell></row><row><cell>4s Shift-GCN (Cheng et al. (2020))</cell><cell></cell><cell>90.7</cell><cell>96.5</cell></row><row><cell>MS-G3D (Liu et al. (2020))</cell><cell></cell><cell>91.5</cell><cell>96.2</cell></row><row><cell>ST-TR (Ours)</cell><cell></cell><cell>89.9</cell><cell>96.1</cell></row><row><cell>ST-TR-agcn (Ours)</cell><cell></cell><cell>90.3</cell><cell>96.3</cell></row><row><cell cols="4">Table 4. Comparison with state-of-the-art accuracy (%) of S-TR, T-TR,</cell></row><row><cell cols="4">and their combination (ST-TR) on NTU-120. Best for both configurations</cell></row><row><cell>w/ and w/o bones in bold.</cell><cell></cell><cell></cell></row><row><cell>NTU-120</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Bones X-Sub X-Set</cell></row><row><cell>ST-LSTM (Liu et al. (2016))</cell><cell></cell><cell cols="2">55.7 57.9</cell></row><row><cell>GCA-LSTM (Liu et al. (2017a))</cell><cell></cell><cell cols="2">61.2 63.3</cell></row><row><cell cols="2">RotClips+MTCNN (Ke et al. (2018))</cell><cell cols="2">62.2 61.8</cell></row><row><cell cols="2">Pose Evol. Map (Liu and Yuan (2018))</cell><cell cols="2">64.6 66.9</cell></row><row><cell>1s Shift-GCN (Cheng et al. (2020))</cell><cell></cell><cell cols="2">80.9 83.2</cell></row><row><cell>S-TR (Ours)</cell><cell></cell><cell cols="2">78.6 80.7</cell></row><row><cell>T-TR (Ours)</cell><cell></cell><cell cols="2">78.4 80.5</cell></row><row><cell>T-TR-agcn (Ours)</cell><cell></cell><cell cols="2">80.1 82.1</cell></row><row><cell>ST-TR (Ours)</cell><cell></cell><cell cols="2">81.9 84.1</cell></row><row><cell>ST-TR-agcn (Ours)</cell><cell></cell><cell cols="2">82.7 85.0</cell></row><row><cell>2s-AGCN (Shi et al. (2019b))</cell><cell></cell><cell cols="2">82.9 84.9</cell></row><row><cell>2s Shift-GCN (Cheng et al. (2020))</cell><cell></cell><cell cols="2">85.3 86.6</cell></row><row><cell>4s Shift-GCN (Cheng et al. (2020))</cell><cell></cell><cell cols="2">85.9 87.6</cell></row><row><cell>MS-G3D (Liu et al. (2020))</cell><cell></cell><cell cols="2">86.9 88.4</cell></row><row><cell>S-TR (Ours)</cell><cell></cell><cell cols="2">81.0 83.6</cell></row><row><cell>T-TR (Ours)</cell><cell></cell><cell cols="2">80.4 83.0</cell></row><row><cell>T-TR-agcn (Ours)</cell><cell></cell><cell cols="2">82.7 84.9</cell></row><row><cell>ST-TR (Ours)</cell><cell></cell><cell cols="2">84.3 86.7</cell></row><row><cell>ST-TR-agcn (Ours)</cell><cell></cell><cell cols="2">85.1 87.1</cell></row><row><cell cols="4">that S-TR-1s presents 17.4 ? 10 5 parameters, drastically reduc-</cell></row><row><cell cols="4">ing the complexity of the baseline ST-GCN which consists in</cell></row><row><cell cols="4">31 ? 10 5 parameters. Moreover, it outperforms the ST-GCN</cell></row><row><cell cols="2">baseline by 0.6% using half of the parameters.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison with state-of-the-art accuracy (%) of S-TR, T-TR, and their combination (ST-TR) on Kinetics. Best for both configurations w/ and w/o bones in bold.</figDesc><table><row><cell>Kinetics</cell><cell></cell></row><row><cell>Method</cell><cell>Bones Top-1 Top-5</cell></row><row><cell>ST-GCN (Yan et al. (2018))</cell><cell>30.7 52.8</cell></row><row><cell>AS-GCN (Li et al. (2019))</cell><cell>34.8 56.5</cell></row><row><cell>SAN (Cho et al. (2020))</cell><cell>35.1 55.7</cell></row><row><cell>S-TR (Ours)</cell><cell>32.4 55.3</cell></row><row><cell>T-TR (Ours)</cell><cell>32.4 55.2</cell></row><row><cell>T-TR-agcn (Ours)</cell><cell>34.4 57.1</cell></row><row><cell>ST-TR (Ours)</cell><cell>34.5 57.6</cell></row><row><cell>ST-TR-agcn (Ours)</cell><cell>36.1 58.7</cell></row><row><cell>2s-AGCN (Shi et al. (2019b))</cell><cell>36.1 58.7</cell></row><row><cell>DGNN (Shi et al. (2019a))</cell><cell>36.9 59.6</cell></row><row><cell>MS-G3D (Liu et al. (2020))</cell><cell>38.0 60.9</cell></row><row><cell>S-TR (Ours)</cell><cell>35.4 57.9</cell></row><row><cell>T-TR (Ours)</cell><cell>33.1 55.86</cell></row><row><cell>T-TR-agcn (Ours)</cell><cell>34.7 56.4</cell></row><row><cell>ST-TR (Ours)</cell><cell>37.0 59.7</cell></row><row><cell>ST-TR-agcn (Ours)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In principle other features, e.g., visual features, could be added here but we want in this paper to focus on pure skeleton base action recognition and we leave this option for future investigations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral networks and locally connected networks on graphs. International Conference on Learning Representations (ICLR2014)</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ch?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3218" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-attention network for skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maqbool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigation of different skeleton features for cnn-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshops</title>
		<imprint>
			<publisher>ICMEW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image captioning through image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graphstructured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-third international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Intel realsense stereoscopic depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iselin Woodfill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grunnet-Jepsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhowmik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Parameter efficient multimodal transformers for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04124</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rnn fisher vectors for action recognition and image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="833" to="850" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multiscale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Actionalstructural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Skeletonbased human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05895</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning convolutional neural networks for graphs</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05328</idno>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning, PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
	<note>Image transformer</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A survey on 3d skeleton-based action recognition using learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05907</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust spatial filtering with graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dom?nguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A comparative review of recent kinect-based action recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00813</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition. Thirty-second AAAI conference on artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Dropattention: A regularization method for fully-connected self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zehui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A comprehensive survey of vision-based human action recognition methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1005</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
