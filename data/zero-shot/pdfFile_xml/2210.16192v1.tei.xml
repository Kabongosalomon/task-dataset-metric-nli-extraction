<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUPERVISED CONTRASTIVE LEARNING FOR RESPIRATORY SOUND CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilyass</forename><surname>Moummad</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab-STICC</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6285</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<postCode>F-29238</postCode>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">OSO-AI</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Farrugia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab-STICC</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6285</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<postCode>F-29238</postCode>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUPERVISED CONTRASTIVE LEARNING FOR RESPIRATORY SOUND CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Respiratory sound classification</term>
					<term>deep learning</term>
					<term>supervised contrastive learning</term>
					<term>ICBHI dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic respiratory sound classification using machine learning is a challenging task, due to large biological variability, imbalanced datasets, as well as a diversity in recording techniques used to capture the respiration signal. While datasets with annotated respiration cycles have been proposed, methods based on supervised learning using annotations only may be limited in their generalization capability. In this study, we address this issue using supervised contrastive learning, relying both on respiration cycle annotations and a spectrogram frequency and temporal masking method SpecAugment to generate augmented samples for representation learning with a contrastive loss. We demonstrate that such an approach can outperform supervised learning using experiments on a convolutional neural network trained from scratch, achieving the new state of the art. Our work shows the potential of supervised contrastive learning in imbalanced and noisy settings. Our code is released at https: //github.com/ilyassmoummad/scl_icbhi2017</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Respiratory sound classification is the task of identifying the diagnosis of a breathing cycle whether it's normal or abnormal. In recent years, Machine Learning (ML) has found its application in different fields, especially in the healthcare sector. Thanks to the recent progress of ML, computer assistance for disease detection or prevention is becoming more popular. Diagnosing a respiratory pathology using ML on audio data would reduce the work overload for physicians and medical experts, and make medical examination less prone to error.</p><p>The International Conference on Biomedical and Health Informatics (ICBHI) released a public respiratory sound dataset <ref type="bibr" target="#b0">[1]</ref> for their 2017 challenge that consists of distinguishing between normal breathing, crackle, wheeze, and a combination of these two anomalies. The ICBHI dataset contains recordings of thousands of breathing cycles of varying durations, in different respiratory classes (crackle, wheeze, This work was co-funded by the AI@IMT program and the company OSO-AI. occurrence of both, normal) with an imbalanced class distribution. Recordings were made with four different devices (microphone and electronic stethoscopes) on seven different chest locations. These properties make respiratory sound classification on this dataset a challenging task. Many ICBHI challenge participants used ML techniques such as Hidden Markov Model, Gaussian Mixture Model and Support Vector Machine <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The common extracted features are computed by either Fourier or Wavelet transforms. These methods reached a score of around 50 %, corresponding to a high specificity and low sensitivity. These approaches relied heavily on carefully chosen features, making it difficult to account appropriately for the variability in the training data.</p><p>Recent methods based on Deep Learning (DL), especially Convolutional Neural Networks (CNNs), have the ability to learn how to extract and combine relevant representations directly from data. Early DL works on ICBHI, such as Lung-BRN <ref type="bibr" target="#b3">[4]</ref>, computed both STFT and wavelet features and used two ResNet <ref type="bibr" target="#b4">[5]</ref> CNNs to extract relevant information from each modality. Follow-up work explored data augmentation to address the data class imbalance (LungRN+NL <ref type="bibr" target="#b5">[6]</ref>), as well as attention mechanism into ResNet blocks (Lun-gAttn <ref type="bibr" target="#b6">[7]</ref>) to improve classification accuracy of respiratory sounds. In addition to data augmentation, RespireNet <ref type="bibr" target="#b7">[8]</ref> uses a model pretrained on ImageNet, with a device specific finetuning strategy. A very recent work <ref type="bibr" target="#b8">[9]</ref> instead proposed spectrum correction to scale the frequency responses of the recording devices, as well as a co-tuning strategy to learn the relationship between source and target categories to improve transfer learning.</p><p>Recent interesting ideas from Self-Supervised Learning (SSL) emerged in Supervised Learning (SL) such as Supervised Contrastive Learning (SCL) <ref type="bibr" target="#b9">[10]</ref> to remedy the drawbacks of the classification cross-entropy loss, such as poor classification margins between samples of different classes <ref type="bibr" target="#b10">[11]</ref> and sensitivity to noisy labels <ref type="bibr" target="#b11">[12]</ref>. SSL consists of learning useful representations of data without using labels. The most common class of SSL is Contrastive Learning (CL) <ref type="bibr" target="#b12">[13]</ref> where the goal is to learn similar representations of different augmented versions of a sample (called positive samples) while learning different representations of different (augmented) samples (called negative samples). In the work of SCL <ref type="bibr" target="#b9">[10]</ref>, label information is used to incorporate more positive (augmented) samples. Because of class imbalance and different recording settings in ICBHI, there may be hard samples, and training using the cross-entropy loss may be affected by these samples. In this work, we compare cross-entropy training and supervised contrastive training, as done for environmental sound classification task in Sound-CLR <ref type="bibr" target="#b13">[14]</ref>, and show that the combination of both frameworks can further improve respiratory sound classification scores.</p><p>Ideas from SCL have already been tested in the context of ICBHI. Song et al. <ref type="bibr" target="#b14">[15]</ref> used a simplified SCL framework, by sampling a first batch of anchors, and according to their class labels, a second batch is constructed so that each anchor has a corresponding positive example. For negative samples, a fixed number of samples with different class labels is chosen from all other samples in the two batches. However they did not report results on the official split of ICBHI. In this paper, we adapt the original formulation of SCL to construct positive and negative pairs, and outperform cross-entropy (CE) training. We also test whether a combination of SCL with CE can further boost the classification scores on ICBHI official split. Our main contributions are summarized as follows :</p><p>1. We show that using a small model with transfer learning from AudioSet combined with simple data augmentation is competitive with current state-of-the-art large neural networks pretrained on ImageNet on respiratory sound classification task.</p><p>2. We show that supervised contrastive learning outperforms the cross-entropy training for respiratory sound classification when finetuning a model pretrained on AudioSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A hybrid approach combining supervised contrastive learning with cross-entropy outperforms supervised learning, and obtains the best score reported so far when considering approaches with no pretraining on an external dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head><p>Let f be a Neural Network (NN) encoder, g a NN classifier, x i ? X the input breathing cycle, and y i ? Y the class label, the cross entropy loss (CE) is calculated as follows :</p><formula xml:id="formula_0">L CE = ? i y i log(g(f (A(x i ))))<label>(1)</label></formula><p>where A is a stochastic augmentation function and i ? {1...N } with N being batch size. Here, we train g ? f to predict the respiratory breathing class for a given respiratory cycle <ref type="figure" target="#fig_1">(Fig 2a)</ref>. Supervised contrastive learning (SCL) consists of learning a classification task in two steps : first, the feature extractor is trained to pull together in the embedding space samples with the same label, and to push away samples with different label. Second, the linear classifier is trained on the frozen representations learned in the first step <ref type="figure" target="#fig_1">(Fig 2b)</ref>. Formally, the first step consists in adding to the encoder f a shallow NN h called a projector (usually a MLP with one hidden layer) that maps representations to the space where the contrastive loss is applied. In the second step, h is discarded (representations before the non linear projector contains more information <ref type="bibr" target="#b12">[13]</ref>), then a classifier g is trained on the frozen representations (output of f ) trained in the first step. The supervised contrastive loss (SCL) is calculated as follows :</p><formula xml:id="formula_1">L SCL = i?I ?1 |P (i)| p?P (i) log exp (z i ? z p /? ) n?N (i) exp (z i ? z n /? )<label>(2)</label></formula><p>where i ? I = {1...2N } the index of an augmented sample within a training batch. z i = h(f (A(x i ))) ? R D P where D P is the projector's dimension. P (i) = {p ? I : y p = y i } is the set of indices of all positives in the multiviewed batch distinct from i sharing similar label with i, and |P (i)| is its cardinality, N (i) = {n ? I : y n = y i } is the set of indices of all negatives in the multiviewed batch having dissimilar label with i, the ? symbol denotes the dot product, and ? ? R + * is a scalar temperature parameter. We explain the application of this framework on ICBHI in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>For the hybrid approach, we combine both CE loss and SCl loss, and minimize the folowing hybrid loss term :</p><formula xml:id="formula_2">L Hybrid = ?L CE + (1 ? ?)L SCL<label>(3)</label></formula><p>where ? is a hyper-parameter. Here, we train f with both the head of CE training, and the head of SCL <ref type="figure" target="#fig_1">(Fig 2c)</ref>. cycle duration is 2.7s). 3642 respiratory cycles contain normal breathing, 1864 contain crackles, 886 contain wheezes, and 506 contain both crackles and wheezes, in 920 audio samples from 126 subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing</head><p>The audio recordings are sampled with a rate that varies from 4 kHz to 44.1 kHz. We re-sample all recordings to 16 kHz mono as some previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref>. Among the 6898 respiratory cycles, only 15 cycles last longer than 8 seconds and the majority contain artefacts, we then uniformly limit the maximum duration of a respiratory cycle to 8 seconds : for longer cycles we take only the first 8 seconds, as for cycles shorter than 8 seconds, we repeat the cycle while adding fade in/out, to create more realistic breathing cycles, until we have the desired duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extraction and Data Augmentation</head><p>We convert the audio signal into the time-frequency representation Mel-spectrogram, with 64 Mel filterbanks, a window size of 1024 over a hop size of 512, with a minimum and a maximum frequency of 50 and 2000 Hz respectively, because wheezes and crackles are in this interval <ref type="bibr" target="#b1">[2]</ref>. We also transform the Mel-spectrogram from the power scale to the decibel scale and apply min-max normalization to scale all the inputs individually in the range (0,1), and then standardize the inputs using mean and standard deviation of the training set. Some works on ICBHI use different data augmentations on the waveform such as pitch shift, time shift, white noise adding, and time stretching <ref type="bibr" target="#b14">[15]</ref>. Others augment the time-frequency representation using vocal tract length perturbation (VTLP) and spectrogram (horizontal) flipping <ref type="bibr" target="#b8">[9]</ref>.</p><p>Here, we use SpecAugment <ref type="bibr" target="#b15">[16]</ref> that has been widely used in deep learning for audio processing. SpecAugment simply consists of masking blocks of frequency channels and time steps, followed by time warping, that help the network learn features that are robust to partial loss of frequency and time information, and to deformation in the time direction. <ref type="figure" target="#fig_1">(Fig 2)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model</head><p>We used a model <ref type="bibr" target="#b18">[19]</ref> introduced for the dataset AudioSet, a large dataset that contains 2 million sounds including respiratory classes. Importantly, we compare results with and without pretraining on Audioset. We report results with the CNN6 model, that consists in 4 blocks, each containing a 2D convolution with a kernel size of 5, a batch normalization and an average pooling with a kernel size of 2. CNN6 contains 4.3 million parameters, compared to other works on ICBHI that use ResNet34, ResNet50, and ResNet101 containing approximately 21, 25, and 44 million parameters, respectively <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation Metrics</head><p>We adopt the same evaluation metrics as the official ICBHI 2017 challenge. The score (Sc) is calculated as an arithmetic mean of sensitivity Se and specificity Sp scores:</p><formula xml:id="formula_3">Se = P c + P w + P b T c + T w + T b ; Sp = P n T n ; Sc = Se + Sp 2<label>(4)</label></formula><p>where P c , P w , P n , P b stand for the numbers of correct predictions of crackle, wheeze, normal and both classes, respectively, and T c , T w , T n , T b correspond to the total number of instances of each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Experimental Setting</head><p>We experiment with three different setups : CE, SCL, and a hybrid training combining the two frameworks, with and without AudioSet pretraining. We only report here results with the CNN6 architecture, although exploratory experiments with larger models (CNN10 and CNN14) showed a similar pattern of results (more results are presented on the github repository). We modified the original CNN6 model as follows: we remove dropout, keep all convolutional and pooling layers, then add a simple linear layer for the classifier in the CE setting and the first head of the hybrid setting, and add a MLP with one hidden layer with 128 neurons for the SCL and the second head of the hybrid setting. We train all models on a single Nvidia TITAN RTX GPU using Adam optimizer with a momentum of 0.9, a batch size of 128 with initial learning rate of 10 ?4 for pretrained models and a learning rate of 10 ?3 for models trained from scratch, and a weight decay of 10 ?4 . We use cosine annealing as a learning rate schedule without warm restarts for all the experiments, except the second phase of supervised contrastive where we train a linear classifier on frozen representations using a learning rate of 0.1 without scheduling. We performed a grid search to find the best parameters for SpecAugment on this dataset. The best and most stable results were obtained without time warping, when masking two blocks of twenty consecutive mel frequency channels, and two blocks of forty consecutive time steps. We also performed a grid search to find the best value of ? = 0.06 in SCL as well as ? = 0.5 for hybrid training. We report results on the official challenge split (the dataset was divided into 60% for training and 40% for testing). In order to be comparable with previous approaches, and in the absence of validation split, results correspond to the epoch with the best Sc on the test split as done by previous works such as RespireNet <ref type="bibr" target="#b7">[8]</ref>. For stability and robustness, we report results over ten identical runs.  <ref type="bibr" target="#b8">[9]</ref>, in which they introduced co-tuning technique and obtains a score of 58.29, while our approach compares well with their vanilla finetuning and StochNorm scores. In terms of computational complexity, our approach has up to five times less parameters than previously proposed approaches with networks finetuned from Imagenet. Finally, it is worth noting that our approach with CNN6 trained from scratch with the hybrid loss achieves the new state of the art on ICBHI when not considering pretraining on an external dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We have shown the potential of supervised contrastive learning for respiratory sound classification, an imbalanced and noisy setting, using SpecAugment and a contrastive loss. We demonstrate that such an approach can outperform crossentropy using experiments on a small CNN trained from scratch. We also show that such an approach can be used to finetune a model pretrained on AudioSet, to obtain performances comparable to previous work that used ImageNet pretrained networks but with up to five times less parameters. For future work, we want to investigate incorporating metadata in the contrastive learning framework for positive/negative samples construction to better adress the variability in the dataset, and explore data augmentation techniques better suited for respiratory sounds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Supervised Contrastive Framework : Respiratory cycles sharing the same label are pulled together in the feature space while respiratory cycles with different label are pushed appart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>ICBHI 2017<ref type="bibr" target="#b0">[1]</ref> is a dataset of the 2017 ICBHI challenge that consists of 5.5 hours of recordings containing 6898 respiratory cycles with a duration ranging from 0.2s to 16.2s (mean Overview of the proposed framework : cross-entropy training (a), supervised contrastive learning (b), hybrid training (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance analysis.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Sp</cell><cell>Se</cell><cell>Sc</cell><cell cols="2"># of Parameters (in M) Ext. Dataset</cell></row><row><cell cols="2">LungBRN[4]</cell><cell>69.2</cell><cell>31.1</cell><cell>50.2</cell><cell>1.6</cell><cell>None</cell></row><row><cell cols="2">LungRN+NL[6]</cell><cell>63.2</cell><cell>41.3</cell><cell>52.3</cell><cell>-</cell><cell>None</cell></row><row><cell cols="2">LungAttn[7]</cell><cell>71.44</cell><cell>36.36</cell><cell>53.9</cell><cell>0.7</cell><cell>None</cell></row><row><cell cols="2">Wang et al.[17]</cell><cell>70.4</cell><cell>40.2</cell><cell>55.3</cell><cell>25 (estimated)</cell><cell>ImageNet</cell></row><row><cell cols="2">RespireNet[8]</cell><cell>72.3</cell><cell>40.1</cell><cell>56.2</cell><cell>21 (estimated)</cell><cell>ImageNet</cell></row><row><cell cols="2">ARSC-Net[18]</cell><cell>67.13</cell><cell>46.38</cell><cell>56.76</cell><cell>-</cell><cell>ImageNet</cell></row><row><cell cols="2">Nguyen et al.[9] (Vanilla)</cell><cell>76.33</cell><cell>37.37</cell><cell>56.85</cell><cell>23 (estimated)</cell><cell>ImageNet</cell></row><row><cell cols="2">Nguyen et al.[9] (StochNorm)</cell><cell>78.86</cell><cell>36.40</cell><cell>57.63</cell><cell>23 (estimated)</cell><cell>ImageNet</cell></row><row><cell cols="2">Nguyen et al.[9] (CoTuning)</cell><cell>79.34</cell><cell>37.24</cell><cell>58.29</cell><cell>23 (estimated)</cell><cell>ImageNet</cell></row><row><cell>Backbone</cell><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Our Results (10 runs)</cell></row><row><cell></cell><cell>CE</cell><cell cols="3">76.72?3.97 31.12?3.72 53.92?0.71</cell><cell>4.3</cell><cell>None</cell></row><row><cell>CNN6</cell><cell>SCL</cell><cell cols="3">76.17?3.84 27.97?3.92 52.08?1.06</cell><cell>4.3</cell><cell>None</cell></row><row><cell></cell><cell>Hybrid</cell><cell cols="3">75.35?5.47 33.84?5.67 54.74?0.5</cell><cell>4.3</cell><cell>None</cell></row><row><cell></cell><cell>CE</cell><cell cols="3">70.09?3.08 40.39?2.97 55.24?0.43</cell><cell>4.3</cell><cell>AudioSet</cell></row><row><cell>CNN6</cell><cell>SCL</cell><cell cols="3">75.95?2.31 39.15?1.89 57.55?0.81</cell><cell>4.3</cell><cell>Audioset</cell></row><row><cell></cell><cell>Hybrid</cell><cell cols="3">70.47?2.07 43.29?1.83 56.89?0.55</cell><cell>4.3</cell><cell>Audioset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>is composed of 2 panels : the upper panel shows state-of-the-art performance on the ICBHI official split, and the bottom panel shows our results across 10 identical runs using cross-entropy training, supervised contrastive training, and hybrid training for CNN6 both from scratch and with AudioSet pretraining. Our results show that supervised contrastive learning or hybrid learning surpass cross-entropy training when training from scratch or by finetuning from AudioSet. A fine-tuned CNN6 reaches a score of 57.55 in the supervised contrastive setting outperforming all previous work except the setting of the recent work of Nguyen et al.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An open access database for the evaluation of respiratory sound classification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Bruno M Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?s</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorkem</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sezer Ulukaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yasemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nik?a</forename><surname>Kahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jakovljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tatjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turukalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perantoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological measurement</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35001</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hidden markov model based respiratory sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jakovljevi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lon?ar-Turukalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Precision Medicine Powered by pHealth and Connected Health, Nicos Maglaveras</title>
		<editor>Ioanna Chouvarda, and Paulo de Carvalho</editor>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An automated lung sound preprocessing and classification system based onspectral analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorkem</forename><surname>Serbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sezer</forename><surname>Ulukaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><forename type="middle">P</forename><surname>Kahya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lungbrn: A smart digital stethoscope for detecting respiratory disease using bi-resnet deep learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Biomedical Circuits and Systems Conference (BioCAS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Lungrn+nl: An improved adventitious lung sound classification using nonlocal block resnet neural network with mixup data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfu</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">08</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lungattn: advanced lung sound classification using attention mechanism with dual tqwt and triple stft spectrogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liebin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Measurement</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">105006</biblScope>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Respirenet: A deep neural network for accurately detecting abnormal lung sounds in limited data setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Gairola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lung sound classification using co-tuning and stochastic normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2872" to="2882" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>in International conference on machine learning. PMLR, 2020</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Soundclr: Contrastive learning of representations for improved environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01929</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive embeddind learning method for respiratory sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1275" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A domain transfer based data augmentation method for automated respiratory classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9017" to="9021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arsc-net: Adventitious respiratory sound classification network using parallel paths with channel-spatial attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hulin</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1125" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Panns: Largescale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
