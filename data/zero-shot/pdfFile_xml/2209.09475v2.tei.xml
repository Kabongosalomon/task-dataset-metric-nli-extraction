<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Image Pyramid Structure for High Resolution Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehun</forename><surname>Kim</surname></persName>
							<email>dkim@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE</orgName>
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunhee</forename><surname>Kim</surname></persName>
							<email>kunkim@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE</orgName>
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonyeong</forename><surname>Lee</surname></persName>
							<email>joonyeonglee@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE</orgName>
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Cha</surname></persName>
							<email>cardongmin@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE</orgName>
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiho</forename><surname>Lee</surname></persName>
							<email>jiholee@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE</orgName>
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE</orgName>
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Image Pyramid Structure for High Resolution Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/plemeri/InSPyReNet.git</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Salient object detection (SOD)  has been in the spotlight recently, yet has been studied less for high-resolution (HR) images. Unfortunately, HR images and their pixel-level annotations are certainly more labor-intensive and time-consuming compared to low-resolution (LR) images and annotations. Therefore, we propose an image pyramidbased SOD framework, Inverse Saliency Pyramid Reconstruction Network (InSPyReNet), for HR prediction without any of HR datasets. We design InSPyReNet to produce a strict image pyramid structure of saliency map, which enables to ensemble multiple results with pyramidbased image blending. For HR prediction, we design a pyramid blending method which synthesizes two different image pyramids from a pair of LR and HR scale from the same image to overcome effective receptive field (ERF) discrepancy. Our extensive evaluations on public LR and HR SOD benchmarks demonstrate that InSPyReNet surpasses the State-ofthe-Art (SotA) methods on various SOD metrics and boundary accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While there are many successful works for SOD in low-resolution (LR) images, there are many demands on high-resolution (HR) images. One can argue that methods trained with LR datasets produce decent results on HR images by resizing the input size ( <ref type="figure">Fig. 1a</ref>), but the quality in terms of the high-frequency details of prediction still remains poor in that way. Moreover, previous studies on HR prediction have been working on developing complex architectures and proposing laborious annotations on HR images <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>  <ref type="figure">(Fig. 1b, c)</ref>.</p><p>In this paper, we focus on only using LR datasets for training to produce highquality HR prediction. To do so, we mainly focus on the structure of saliency prediction, which enables to provide high-frequency details from the image regardless of the size of the input. However, there is still another problem to be solved where the effective receptive fields (ERFs) <ref type="bibr" target="#b4">[5]</ref> of HR images are different from the LR images in most cases. To alleviate the aforementioned issues, we propose two solid solutions which are mutually connected to each other.  <ref type="figure">Fig. 1</ref>. Different approaches for HR SOD prediction. Areas denoted as a dashed box are trained with supervision. (a): Resizing HR input to LR, then up-sample. Works for any methods, lack of details. (b): Requires multiple training sessions, and HR datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. (c): Can overcome ERF discrepancy, but the architecture is complex, requires HR datasets <ref type="bibr" target="#b3">[4]</ref>. (d): Works without HR dataset training. We predict multiscale results with single network and synthesize HR prediction with pyramid blending.</p><p>First is to design a network architecture which enables to merge multiple results regardless of the size of the input. Therefore, we propose Inverse Saliency Pyramid Reconstruction Network (InSPyReNet), which predicts the image pyramid of the saliency map. Image pyramid is a simple yet straightforward method for image blending <ref type="bibr" target="#b5">[6]</ref>, so we design InSPyReNet to produce the image pyramid of the saliency map directly. Previous works have already used image pyramid prediction, but results did not strictly follow the structure, and hence unable to use for the blending <ref type="figure" target="#fig_0">(Fig. 2)</ref>. Therefore, we suggest new architecture, and new supervision techniques to ensure the image pyramid structure which enables stable image blending for HR prediction.</p><p>Second, to solve the problem of ERF discrepancy between LR and HR images, we design a pyramid blending technique for the inference time to overlap two image pyramids of saliency maps from different scales. Recent studies of HR SOD methods use two different scales of the same image, by resizing HR image to LR, to alleviate such problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, but the network should be complicated and large <ref type="figure">(Fig. 1c</ref>). Simply forwarding HR images to the InSPyReNet, or other LR SOD networks fail to predict salient region since they are not trained with HR images. Nevertheless, we notice the potential of enhancing details for highquality details from HR prediction, even the result shows a lot of False Positives (HR prediction in <ref type="figure">Fig. 3</ref>). To combine the robust saliency prediction and details from LR and HR predictions, we blend the two image pyramids of saliency maps.</p><p>InSPyReNet does not require HR training and datasets, yet produces highquality results on HR benchmarks. A series of quantitative and qualitative results on HR and LR SOD benchmarks show that our method shows SotA performance, yet more efficient than previous HR SOD methods in terms of training resources, annotation quality, and architecture engineering. . However, our InSPyReNet shows almost identical results compared to the ground truth across each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Salient Object Detection. Edge-Based Models are studied in SOD for better understanding of the structure of the salient object by explicitly modeling the contour of the saliency map. Methods with auxiliary edge estimator require additional edge GT, or extra training process with extra edge datasets. For instance, EGNet <ref type="bibr" target="#b7">[8]</ref> has an additional edge estimation branch which is supervised with additional edge-only dataset. However, the effect of edge branch is limited to the encoder network (backbone), expecting better representation with robust edge information, because the estimated edge from the edge branch is not directly used to the detection. Also, LDF <ref type="bibr" target="#b8">[9]</ref> designed an alternative representation for the edge information. They divided the saliency map into 'body' and 'detail', which corresponds to the edge part. Unlike EGNet, they utilized both 'body' and 'detail' for the saliency prediction in the inference stage. However, to achieve the disentanglement of 'body' and 'detail' components, it requires multiple training stages and ground truth generation. Unlike auxiliary edge models, we embedded the image pyramid structure to the network for saliency prediction, which does not require additional training process nor extra datasets, and the decoder network is implicitly trained to predict the Laplacian of the saliency map, high-frequency details of the larger scales, which implicitly includes edge information. Thanks to this simple structure, we also do not require additional training stages. Image Segmentation for HR Images. Pixel-wise prediction tasks such as SOD resize input images into a pre-defined shape (e.g., 384 ? 384) for batched and memory efficient training. This is plausible since the average resolution of training datasets are usually around 300 to 400 for both width and height. For example, the average resolution is 378 ? 469 for ImageNet <ref type="bibr" target="#b9">[10]</ref>, and 322 ? 372 for DUTS <ref type="bibr" target="#b10">[11]</ref>. After training, resizing input images into a pre-defined shape is often required, especially when the input image is relatively larger than the pre-defined shape ( <ref type="figure">Fig. 1a</ref>). However, down-sampling large images causes severe information loss, particularly for high-frequency details. We can overcome this problem by not resizing images, but current SotA SOD methods fail to predict  <ref type="figure">Fig. 3</ref>. Illustration of effective receptive field (ERF) <ref type="bibr" target="#b4">[5]</ref> discrepancy between LR and HR images from InSPyReNet. LR prediction shows successful saliency prediction, but lack of details. While HR prediction shows better details but due to the ERF discrepancy (red boxes), it over detects objects. With pyramid blending, we can capture the global dependency from LR prediction while enhance local details from HR prediction at the same time. Best viewed by zooming in.</p><p>appropriate saliency map because they are neither trained with HR dataset nor designed to produce HR prediction. Most likely, the problem is the discrepancy between the effective receptive fields <ref type="bibr" target="#b4">[5]</ref> of the same corresponding pixel from the original and resized images ( <ref type="figure">Fig. 3)</ref>.</p><p>CascadePSP <ref type="bibr" target="#b11">[12]</ref> first tackled this problem in semantic segmentation by approaching HR segmentation by a refinement process. They trained their model with coarse segmentation masks as an input with a set of augmentation techniques, and used the model to refine an initial segmentation mask with multiple global steps and local steps in a recursive manner. However, they need an initial prediction mask from standalone models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, which is definitely not resourcefrendly. Zeng et al . <ref type="bibr" target="#b0">[1]</ref> first proposed HR dataset for SOD task with a baseline model which consists of separate LR, HR and fusion networks. They combined global (GLFN) and local (LRN) information by two separate networks dedicated for each of them. Tang et al . <ref type="bibr" target="#b1">[2]</ref> also designed LR and HR networks separately, where the branch for the HR (HRRN) gets an image and a predicted saliency map from the LR branch (LRSCN). PGNet <ref type="bibr" target="#b3">[4]</ref> first proposed a standalone, endto-end network for HR prediction by combining features from LR and HR images with multiple backbone networks.</p><p>Aforementioned methods require HR datasets for training, complex model architecture, multiple training sessions for submodules ( <ref type="figure">Fig. 1b, c)</ref>. Unlike previous methods, InSPyReNet does not require HR datasets for training, yet predicts fine details especially on object boundary. Image Pyramid in Deep Learning Era. Studies of pixel-level prediction tasks have shown successful application of image pyramid prediction. LapSRN <ref type="bibr" target="#b14">[15]</ref> first applied a Laplacian image prediction for Super Resolution task and since then, most end-to-end supervised super resolution methods adopt their structure. LRR <ref type="bibr" target="#b15">[16]</ref> first applied a Laplacian image pyramid for the semantic seg-mentation task in the prediction reconstruction process. Then, Chen et al . <ref type="bibr" target="#b6">[7]</ref> adopted LRR prediction strategy for the SOD with reverse attention mechanism, and UACANet <ref type="bibr" target="#b16">[17]</ref> extended self-attention mechanism with uncertainty area for the polyp segmentation. As the above methods have already proved that without any training strategy, we can expect the network to implicitly predict the image pyramid by designing the architecture. However, without extra regularization strategy for the supervision to follow the image pyramid structure rigorously, we cannot make sure that the Laplacian images from each stage truly contains high-frequency detail <ref type="figure" target="#fig_0">(Fig. 2)</ref>.</p><p>We revisit this image pyramid scheme for prediction, and improve the performance by setting optimal stage design for image pyramid and regularization methods to follow pyramidal structure. Also, to the best of our knowledge, In-SPyReNet is the first attempt to extend image pyramid prediction for multiple prediction ensembling by image blending technique. This is because previous methods' Laplacian images did not strictly follow actual high-frequency detail. Rather, they focus more on correcting errors from the higher stages ( <ref type="figure" target="#fig_0">Fig. 2</ref>). Unlike previous methods, we adopt scale-wise supervision ( <ref type="figure" target="#fig_1">Fig. 4b</ref>), Stop-Gradient and pyramidal consistently loss (Sec. 3.2) for regularization which enables consistent prediction, and hence we are able to use blending technique by utilizing multiple results to facilitate more accurate results on HR benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Overall Architecture. We use Res2Net <ref type="bibr" target="#b17">[18]</ref> and Swin Transformer <ref type="bibr" target="#b18">[19]</ref> for the backbone network, but for HR prediction, we only use Swin as a backbone. We provide a thorough discussion (Sec. 5) for the reason why we use only Swin Transformer for HR prediction.</p><p>From UACANet <ref type="bibr" target="#b16">[17]</ref>, we use Parallel Axial Attention encoder (PAA-e) for the multiscale encoder to reduce the number of channels of backbone feature maps and Parallel Axial Attention decoder (PAA-d) to predict an initial saliency map on the smallest stage (i.e., Stage-3). We adopt both modules because they capture global context with non-local operation, and it is efficient thanks to the axial attention mechanism <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Refer to the stage design in <ref type="figure" target="#fig_1">Fig. 4</ref>, previous pyramid-based methods for pixellevel prediction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> started with Stage-5, and ended at Stage-2. However, there are still two remaining stages to reconstruct for previous methods, which makes the reconstruction process incomplete in terms of the boundary quality. Thus, we claim that starting image pyramid from Stage-3 is sufficient, and should reconstruct until we encounter the lowest stage, Stage-0 for HR results. To recover the scale of non-existing stages (Stage-1, Stage-0), we use bi-linear interpolation in appropriate locations ( <ref type="figure" target="#fig_1">Fig. 4)</ref>.</p><p>We locate a self-attention-based decoder, Scale Invariant Context Attention (SICA), on each stage to predict a Laplacian image of the saliency map (Lapla-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-2 Laplacian</head><p>Sailency Map cian saliency map). From the predicted Laplacian saliency maps, we reconstruct saliency maps from higher-stages to the lower-stages ( <ref type="figure" target="#fig_1">Fig. 4a</ref>). Scale Invariant Context Attention. Attention-based decoder for pixel-wise prediction shows great performance due to its non-local operation with respect to the spatial dimension <ref type="bibr">[22,</ref><ref type="bibr" target="#b22">23]</ref>. However, when the size of the input image gets larger than the training setting (e.g., 384 ? 384), it usually fails to produce an appropriate result for the following reason. Because as the size of input image is large enough, there exist a train-inference discrepancies for a non-local operation which flattens the feature map according to the spatial dimension and does a matrix multiplication. For instance, the magnitude of the result from the nonlocal operation varies depending on the spatial dimension of the input image. Moreover, the complexity of non-local operation increases quadratically as the input size increases.</p><p>To this end, we propose SICA, a scale invariant context attention module for robust Laplacian saliency prediction. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, the overall operation of SICA follows OCRNet <ref type="bibr" target="#b22">[23]</ref>. We found that computing object region representation causes train-inference discrepancy, so we resize input feature maps x and context maps c according to the shape from training time (h, w). Because in the training step, images are already reshaped to the fixed shape, we do not have to resize them. For context maps, unlike OCRNet, we can only access to the saliency map which is insufficient, so we generate several context maps following <ref type="bibr" target="#b16">[17]</ref>. Further details of the context map selection and equations may be found in the supplementary material. With SICA, we can compute Laplacian saliency from encoder from decoder maps more precisely for HR images, and hence can apply pyramid blending for HR prediction (Sec. 3.3).</p><formula xml:id="formula_0">? ? ! " ? $ " ? % ? ? ! " ? $ " ? &amp; : number of contexts ? ? &amp;? % ? ? ! " ? $ " ? &amp; ? ? ? ' " ? ( " ? % ? ? ? ' " ? ( " ? &amp; , ? height &amp; width</formula><p>Inverse Saliency Pyramid Reconstruction. Laplacian pyramid <ref type="bibr" target="#b23">[24]</ref> is an image compression technique that stores the difference between the low-pass filtered image and the original image for each scale. We can interpret the Laplacian image as a remainder from the low-pass filtered signal or, in other words, high-frequency details. Inspired by this technique, we revisit the image pyramid structure by designing our network to construct a Laplacian pyramid to concentrate on the boundary details and reconstruct the saliency map from the smallest stage to its original size. We start with the saliency map from the uppermost stage (Stage-3) for the initial saliency map and aggregate high-frequency details from the Laplacian saliency maps. Formally, we denote the saliency map and Laplacian saliency map of the jth stage as S j and U j , respectively. To reconstruct the saliency map from the j + 1th stage to the jth stage, we apply EXPAND operation <ref type="bibr" target="#b23">[24]</ref> as follows,</p><formula xml:id="formula_1">S j e (x, y) = 4 3 m=?3 3 n=?3 g(m, n) ? S j+1 ( x ? m 2 , y ? n 2 )<label>(1)</label></formula><p>where (x, y) ? I j are pixel coordinates and I j is a lattice domain of Stage-j. Also, g(m, n) is a Gaussian filter where the kernel size and standard deviation are empirically set to 7 and 1 respectively. To restore the saliency details, we add Laplacian saliency map from SICA as follows,</p><formula xml:id="formula_2">S j = S j e + U j .<label>(2)</label></formula><p>We repeat this process until we obtain the lowest stage, Stage-0 as shown in <ref type="figure" target="#fig_1">Fig. 4a</ref>, and use it as a final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervision Strategy and Loss Functions</head><p>A typical way to supervise a network with multi-stage side outputs is to use bi-linear interpolation for each stage's prediction and compute the loss function with the ground-truth. However, the predicted saliency map from higher-stage is small regarding its spatial dimension, and this may cause stage-scale inconsistency, especially for the boundary area of salient objects. Instead, we focus on "Do what you can with what you have where you are". In fact, the saliency output from Stage-3 cannot physically surpass the details from Stage-2, so we choose to provide each stage a suitable ground-truth. To do so, we create an image pyramid of the ground-truth ( <ref type="figure" target="#fig_1">Fig. 4b</ref>).</p><p>First, we obtain the ground-truth G j for Stage-j from G j?1 with REDUCE operation <ref type="bibr" target="#b23">[24]</ref> as follows,</p><formula xml:id="formula_3">G j (x, y) = 3 m=?3 3 n=?3 g(m, n) ? G j?1 (2x + m, 2y + n).<label>(3)</label></formula><p>From the largest scale, we deconstruct the ground-truth until we get groundtruths for each stage of our network. For loss function, we utilize binary cross entropy (BCE) loss with pixel position aware weighting strategy L wbce <ref type="bibr" target="#b24">[25]</ref>. Moreover, to encourage the generated Laplacian saliency maps to follow the pyramid structure, we deconstruct S j?1 to the jth stage,S j by REDUCE operation. Then, we reinforce the similarity between S j and reduced saliency mapS j with pyramidal consistency loss L pc as follows,</p><formula xml:id="formula_4">L pc (S j ,S j ) = (x,y)?I j ||S j (x, y) ?S j (x, y)|| 1 .<label>(4)</label></formula><p>L pc regularizes the lower-stage saliency maps to follow the structure of the image pyramid through the training process. We define the total loss function L as follows,</p><formula xml:id="formula_5">L(S, G) = 3 j=0 ? j L wbce (S j , G j ) + ? 2 j=0 ? j L pc (S j ,S j )<label>(5)</label></formula><p>where ? is set to 10 ?4 and ? j = 4 j for balancing the magnitude of loss across stages. Finally, we include Stop-Gradient for the saliency map input of SICA and reconstruction process from higher-stages to force each stage saliency output to focus on each scale during training time and only affect each other in the inference time ( <ref type="figure" target="#fig_1">Fig. 4</ref>). This strategy encourages the stage-wise ground-truth scheme by explicitly preventing the gradient flow from lower-stages affecting the higher-stages. Thus, supervisions with high-frequency details will not affect higher-stage decoder, which are intended only to have the abstract shape of the salient objects. While this strategy might affect the performance in terms of the multiscale scheme, we use feature maps from the different stages for multiscale encoder and SICA to compensate for this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pyramid Blending</head><p>While SICA enables saliency prediction for various image sizes, when the image gets larger, there still exists ERF discrepancies ( <ref type="figure">Fig. 3</ref>). Thankfully, one very straightforward application for our saliency pyramid outputs is assembling multiple saliency pyramids from different inputs. We first generate saliency pyramids with InSPyReNet for original and resized images as shown in <ref type="figure">Fig. 6</ref>, namely LR and HR saliency pyramids. Then, instead of reconstructing the saliency map from the HR pyramid, we start from the lowest stage of the LR pyramid. Intuitively speaking, the LR pyramid is extended with the HR pyramid, so they construct a 7 stage saliency pyramid.</p><p>For the HR pyramid reconstruction, similar to <ref type="bibr" target="#b15">[16]</ref>, we compute the dilation and erosion operation to the previous stage's saliency map and subtract them to obtain the transition area for and multiply with the Laplacian saliency map. Transition area is used to filter out the unwanted noises from the HR pyramid, since the boundary details we need to apply should exist only around the boundary area. Unlike <ref type="bibr" target="#b15">[16]</ref>, it is unnecessary for the LR branch since we train InSPyReNet with methods in Sec. 3.2, results in the saliency pyramid are guaranteed to be consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Implementation Details. We train our method with widely used DUTS-TR, a subset of DUTS <ref type="bibr" target="#b10">[11]</ref> for training. We use Res2Net <ref type="bibr" target="#b17">[18]</ref> and Swin Transformer <ref type="bibr" target="#b18">[19]</ref> backbones which are pre-trained with ImageNet-1K and ImageNet-22K <ref type="bibr">[</ref> from -10 to 10 degrees, and random image enhancement (contrast, sharpness, brightness) for the data augmentation. We set the batch size to 6 and maximum epochs to 60. We use Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with initial learning rate 1e-5, and follow the default PyTorch settings. Finally, we use poly learning rate decay for scheduling <ref type="bibr" target="#b12">[13]</ref> with a factor of (1 ? ( iter itermax ) 0.9 ) and linear warm-up for the first 12000 iterations. Evaluation Datasets and Metrics. We evaluate our method on five LR benchmarks, DUTS-TE, a subset of DUTS for evaluation, DUT-OMRON <ref type="bibr" target="#b26">[27]</ref>, ECSSD <ref type="bibr" target="#b27">[28]</ref>, HKU-IS <ref type="bibr" target="#b28">[29]</ref>, and PASCAL-S <ref type="bibr" target="#b29">[30]</ref>. Furthermore, we evaluate our method on three HR benchmarks, DAVIS-S <ref type="bibr" target="#b30">[31]</ref>, HRSOD-TE <ref type="bibr" target="#b0">[1]</ref>, and UHRSD-TE <ref type="bibr" target="#b3">[4]</ref>. From <ref type="bibr" target="#b31">[32]</ref>, we report S-measure (S ? ) <ref type="bibr" target="#b32">[33]</ref>, maximum F-measure (F max ) <ref type="bibr" target="#b33">[34]</ref>, and Mean Absolute Error (MAE) <ref type="bibr" target="#b34">[35]</ref>. Since F-measure requires a binary map, it is computed with thresholds in a range of [0, 255] and the maximum value is used for the evaluation. With the above metrics, we also report mean boundary accuracy (mBA) <ref type="bibr" target="#b35">[36]</ref> for boundary quality measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Resizing Factor L. We use the resizing method for HR images from <ref type="bibr" target="#b35">[36]</ref> for pyramid blending since current GPUs cannot deal with large sizes such as 4K images as is. So, we choose a maximum length of the shorter side of the image as L. For instance, if an input size is 1920 ? 1080 and L = 810, then we resize the input into 1440 ? 810. Moreover, we do not deploy pyramid blending process for inputs where the shorter side length is less than 512 because the difference between LR and HR pyramid is not enough for blending. We compare S ? and <ref type="table">Table 1</ref>. Ablation study of InSPyReNet (SwinB) with and without SICA and pyramid blending on HR benchmarks. mBA on three HR datasets by varying L from 784 to 1536 ( <ref type="figure">Fig. 7)</ref>. We choose L = 1280 since mBA almost converges after that. SICA and pyramid blending. To demonstrate the necessity of SICA, we evaluate InSPyReNet with and without SICA. Since SICA only takes place when the input image is large enough to make train-inference discrepancy, we demonstrate results only on HR benchmarks. Please note that all evaluation is done with resizing method mentioned above, except for the LR resolution (Tab. 1).</p><p>In Tab. 1, InSPyReNet without SICA shows the worst performance, especially for the mBA. Since mBA only considers boundary quality, InSPyReNet with SICA and without pyramid blending shows the best performance in terms of mBA measure, yet shows poor results on other SOD metrics 1 . This is because even with SICA, InSPyReNet cannot overcome the discrepancy in effective receptive fields <ref type="bibr" target="#b4">[5]</ref> between HR and LR images. For the setting without SICA, InSPyReNet with pyramid blending shows inferior results compared to the InSPyReNet without pyramid blending, meaning that the pyramid blending technique is meaningless without SICA since it worsen the results. Thus, SICA is crucial to be included in InSPyReNet, especially for the HR pyramid in the pyramid blending. Compared to the LR setting (i.e., resizing into 384 ? 384), using both SICA and pyramid blending shows better performance for all four metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art methods</head><p>Quantitative Comparison. First, we compare InSPyReNet with 12 SotA LR SOD methods. In this experiment, we resize images same as for training. We either download pre-computed saliency maps or run an official implementation with pre-trained model parameters provided by the authors to evaluate with the same evaluation code for a fair comparison. Moreover, we re-implement Chen et al . <ref type="bibr" target="#b6">[7]</ref>, F 3 Net [25], LDF <ref type="bibr" target="#b8">[9]</ref>, MINet <ref type="bibr" target="#b39">[40]</ref>, and PA-KRN <ref type="bibr" target="#b40">[41]</ref> with same backbones we use to demonstrate how much the training settings affects the performance for other methods compared to InSPyReNet. We choose the above methods since <ref type="table">Table 2</ref>. Quantitative results on five LR benchmarks. The first and the second best results for each metric are colored red and blue. ? indicates larger the better, and ? indicates smaller the better. ? indicates our re-implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithms</head><p>Backbones</p><formula xml:id="formula_6">DUTS-TE DUT-OMRON ECSSD HKU-IS PASCAL-S S? ? Fmax ? MAE? S? ? Fmax ? MAE? S? ? Fmax ? MAE? S? ? Fmax ? MAE? S? ? Fmax ? MAE? CNN backbone Models (ResNet, ResNext, Res2Net)</formula><p>PoolNet <ref type="bibr" target="#b36">[37]</ref> ResNet50 0.887 0.865 0.037 0.831 0.763 0.054 0.926 0.937 0.035 0.909 0.912 0.034 0.865 0.858 0.065 BASNet <ref type="bibr" target="#b37">[38]</ref> ResNet34 0.866 0.838 0.048 0.836 0.779 0.056 0.916 0.931 0.037 0.909 0.919 0.032 0.838 0.835 0.076 EGNet <ref type="bibr" target="#b7">[8]</ref> ResNet50 0.874 0.848 0.045 0.836 0.773 0.057 0.918 0.928 0.041 0.915 0.920 0.032 0.848 0.836 0.075 CPD <ref type="bibr" target="#b11">[12]</ref> ResNet50 Moreover, to verify the effectiveness of pyramid blending, we compare our method with SotA methods on HR and LR benchmarks (Tab. 3). Among HR methods, our method shows great performance among other methods, even though we use only DUTS-TR for training. Note that previous SotA HR methods show inferior results on LR datasets and vice versa, meaning that generalizing for both scales is difficult, while our method is robust for both scales. For instance, while PGNet trained with HR datasets (H, U) shows great performance on HR benchmarks, but shows more inferior results than other methods and even LR methods on LR benchmarks, while our method shows consistent results on both benchmarks. This is because LR datasets do not provide high-quality boundary details, while HR datasets lack of global object saliency.</p><p>Qualitative Comparison. We provide a visual comparison of our method in <ref type="figure">Fig. 8</ref> and <ref type="figure" target="#fig_5">Fig. 9</ref> on HR benchmarks. Overall, previous SotA methods are sufficient for detecting salient objects, but shows degraded results for complex scenes. Results show that InSPyReNet can produce accurate saliency prediction for the complex, fine details thanks to the pyramid blending. Moreover, even though we train our method only with LR dataset, DUTS-TR, InSPyReNet consistently shows accurate results compared to other methods. <ref type="table">Table 3</ref>. Quantitative results on three HR and two LR benchmarks. Beckbones; V: VGG16, R18: ResNet18, R50: ResNet50, S: SwinB. Datasets; D: DUTS-TR, H: HRSOD-TR, U: UHRSD-TR. The first and the second best results for each metric are colored red and blue. ? indicates larger the better, and ? indicates smaller the better. ? indicates our re-implementation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>Weakness: Backbone Network. We do not use Res2Net50 for HR benchmark due to the following reason. As shown in <ref type="figure" target="#fig_6">Fig. 10</ref>, HR prediction from Res2Net50 backbone produces saliency map with numerous unnecessary artifacts. This is because CNN backbones are vulnerable to its ERF size, which is highly dependent on its training dataset. Unlike traditional CNN backbones, there are many works to minimize the above issue such as Fast Fourier Convolution <ref type="bibr" target="#b43">[44]</ref>, or ConvNeXt <ref type="bibr" target="#b44">[45]</ref>. We found that those methods are helpful for reducing such artifacts for HR prediction, but not enough for detail reconstruction. However, Vision Transformers like SwinB have larger ERFs and consist of non-local operation for regarding global dependencies, which are suitable for our method. Thus, even the HR prediction shows some False Positives (second column, second row in <ref type="figure" target="#fig_6">Fig. 10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Method Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Backbone Networks</head><p>In this section, we describe how the feature maps from the backbone network is retrieved. Since we adopt two different types of backbone networks, we explain the minor details of each model. For Res2Net <ref type="bibr" target="#b17">[18]</ref> backbone network, we use 26w?8s setting. Because the main difference between ResNet <ref type="bibr" target="#b45">[46]</ref> and Res2Net is a building block, the overall architecture is identical. So, to explain where we extract the feature maps from the Res2Net backbone, we refer to the ResNet paper. The feature map for the Stage-1 is extracted from conv1, and for Stage-j, where j &gt; 1, we extract feature maps from the last layer which has a name of convj _x (e.g., conv4_6 of Res2Net50 for Stage-4).</p><p>For Swin Transformer <ref type="bibr" target="#b18">[19]</ref>, it is slightly different from conventional CNNbased models in the fact that they divide an image into tokens. Unlike ResNet (or Res2Net), there is a layer which works similar to the stem layer (conv_1), a patch partition layer which generates a size of 4 ? 4 patches and aggregates to its spatial dimension for each embedding. The feature map for the Stage-1 is extracted from the patch partition layer, and for Stage-j, where j &gt; 1, we extract feature maps from the last layer of stage j 2 . Since Vision Transformers interpret an image as a sequence of patches, we rearrange patches to a 2-dimensional feature map for each stage.</p><p>Overall, the hierarchical structure is identical to each other, so we can easily adopt Swin Transformer to other methods, so as we mention in the main paper, we implement 5 SotA models, Chen et al . <ref type="bibr" target="#b6">[7]</ref>, F 3 Net [25], LDF <ref type="bibr" target="#b8">[9]</ref>, MINet <ref type="bibr" target="#b39">[40]</ref> and PA-KRN <ref type="bibr" target="#b40">[41]</ref>, and conduct experiments with Res2Net and Swin Transformer backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Scale Invariant Context Attention</head><p>In this section, we provide more details of SICA. Since the overall computation of SICA is similar to the OCRNet, we have to obtain soft object regions (or, context maps), which is a coarse segmentation maps corresponds to each class. However, salient object detection is a class-agnostic segmentation task, so we only have a salient region, and its inverse area. UACANet <ref type="bibr" target="#b16">[17]</ref> first proposed uncertainty area to provide additional context maps, which are often related to the object boundary region. We notice that the saliency map has an object contextual information, but the Laplacian saliency map also has a boundary context. Therefore, we design SICA with additional context clues from the Laplacian saliency map. However, while UACA set the threshold value as 0.5, we argue that a fixed threshold does not guarantee the optimal saliency map. F-measure based loss (FLoss) <ref type="bibr" target="#b46">[47]</ref> claims that the optimal threshold obtained with an exhaustive search on the test dataset is impractical for real-world application. While they still used exhaustive search after applying FLoss to find the optimal threshold, it is still clear that 0.5 is not the optimal threshold for most cases. We also claim that the threshold needs to be adaptive for extracting context information, and needs to be different for each stage because the saliency prediction from each stage may have different statistics due to the scale differences. So, we choose to learn threshold values for each stage and use them to extract meaningful context regions. We illustrate the context map configuration details in <ref type="figure" target="#fig_7">Fig. 11a</ref>.</p><p>First, we compute context maps as follows,</p><formula xml:id="formula_7">S f = max(S ? ? S , 0), S b = max(? S ? S, 0), S u = ? S ? abs(S ? ? S ),<label>(6)</label></formula><formula xml:id="formula_8">U f = max(U ? ? U , 0), U b = max(? U ? U, 0),<label>(7)</label></formula><p>where we denote trainable threshold ? S and ? U for input saliency map S and Laplacian saliency map U , respectively, and initialize them with 0.5. S f , S b , and S u are foreground, background, and uncertainty context maps from S. Likewise, U f and U b are foreground and background context map from U . Note that because there is no Laplacian saliency map in Stage-3, there are no U f and U b for SICA in Stage-2. Then, we aggregate context maps for simplicity as follows,</p><formula xml:id="formula_9">c = [S f , S b , S u ], if Stage-2. [S f , S b , S u , U f , U b ], otherwise.<label>(8)</label></formula><p>c consists of three or five context maps depending on its stage (see <ref type="figure" target="#fig_7">Fig. 11a</ref>). Then, the input feature map from the encoder and the saliency map from the decoder are resized to the size from the training session. We denote the size of the input image as H ? W , the output stride of the current stage as s, and the number of channels and context maps as C and N respectively. So, the input feature map x ? R </p><formula xml:id="formula_10">f k = (x,y)?I c k (x, y)x(x, y),<label>(9)</label></formula><p>where I is a lattice domain of S and U . Since the matrix multiplication is done on the spatial dimension, f has a same shape with or without resize, yet has better representation ability. Subsequently, we compute the attention score w by computing the similarity score between f and x(x, y),</p><formula xml:id="formula_11">w k (x, y) = exp(T x (x(x, y)) T f (f k )) K l=1 exp(T x (x(x, y)) T f (f l )) ,<label>(10)</label></formula><p>where T x (?) and T f (?) denotes transformation functions implemented by consecutive convolution layer, batch normalization, and ReLU activation. Lastly, with context representation vector f and attention map w as a weighting factor, we compute a context enhanced feature map y as follows,</p><formula xml:id="formula_12">y(x, y) = T y ( K l=1 w l (x, y)T f (f l )),<label>(11)</label></formula><p>where T y (?) and T f (?) are transformation functions and K = 3 if Stage-2, otherwise, K = 5 (see Eq. (8) and <ref type="figure" target="#fig_7">Fig. 11a</ref>). The input and output feature maps of SICA are concatenated and forwarded to a simple decoder with convolution blocks to predict the Laplacian saliency map <ref type="figure" target="#fig_7">(Fig. 11b)</ref>.</p><p>A.3 Implementation of Image Pyramid Operations.</p><p>We implement EXPAND and REDUCE operations <ref type="bibr" target="#b23">[24]</ref> in Pytorch <ref type="bibr" target="#b47">[48]</ref>. We provide a source code of both operations in Algorithm 1. We obtain the 1D Gaussian kernel with cv2.getGaussianKernel from OpenCV <ref type="bibr" target="#b48">[49]</ref>, and use outer operation to generate the 2D Gaussian kernel. For EXPAND operation, we use pixel-shuffle operation from PyTorch, and we used even indices for REDUCE operation.</p><p>For pyramid blending, we use cv2.getStructuringElement function with cv2.MORPH_ELIPSE argument for dilation and erosion. Also, we use dilation and erosion functions from kornia.morphology <ref type="bibr" target="#b49">[50]</ref>. The kernel size for dilation and erosion is set to 5, 9, and 17 for Stage-2, Stage-1, and Stage-0 respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Ablation Studies</head><p>Gaussian Filter in Image Pyramid. Even the kernel size k and the standard deviation ? of the Gaussian kernel g for image pyramid operation is usually set to 5 and 1 respectively, we compare our image pyramid operations with different kernel sizes and standard deviations. Results in Tab. 4 shows that when the ? gets larger, the overall performance decreases. Furthermore, when k is 7 and ? is set to 1, we obtain the best result among different settings.</p><p>Training Strategies. To demonstrate the effect of our training strategies such as supervision under image pyramid structure (pred and gt), pyramidal consistency loss (L pc ), stop-gradient(S.G.), we train InSPyReNet with different settings and evaluate on HR benchmarks. Results in Tab. 4 shows that without image pyramid structure on prediction branch (pred ) shows unsatisfactory results in terms of mBA. This is because while other strategies (gt, S.G., L pc ) are considered as supervision strategies, pred is embedded as a model architecture.</p><p>With other strategies, we can notice that S.G. provides some improvements in terms of SOD metrics, which means it gives more stable results for pyramid blending. Also, L pc shows extra improvements for mBA, which means it ensures the image pyramid structure in a sense of high-frequency residual information of Laplacian image. SICA. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, methods without SICA generates unpleasant artifacts on the boundary areas, which can be interpreted as the failure of Laplacian images of the saliency map from SICA. On the other hand, InSPyReNet with SICA shows detailed predictions, but without pyramid blending, it misses some major object parts. With SICA and pyramid blending shows best results, showing less failure in terms of both capturing the whole salient object body parts and high-frequency object boundary details.  Moreover, to understand the potential of InSPyReNet as is, we trained our method with HR datasets in HR scale. In this case, we do not deploy pyramid blending since we are not training in LR scale, and hence there is no LR pyramid to merge with. For HR training, we follow the training size from <ref type="bibr" target="#b3">[4]</ref> (i.e., 1024?1024). As shown in Tab. 6, our method shows great results with fully supervised manner for HR prediction, even we do not specifically design InSPyReNet for HR prediction without pyramid blending. This experiment shows that with simple image pyramid structure from our method can further be utilized for HR prediction with HR datasets. Overall, results trained with HR scale shows better performance especially for mBA, but slightly worse on LR benchmarks than results trained on LR scale. From this experiment, although the performance on LR and HR benchmarks tends to prefer the corresponding training scale, InSPyReNet well adopts to each other. We also provide qualitative results in <ref type="figure" target="#fig_1">Fig. 14.</ref> Compared to the same setting of PGNet <ref type="bibr" target="#b3">[4]</ref> trained with HRSOD-TR and UHRSD-TR, our method substantially outperforms the quality of high-frequency details of saliency maps. Moreover, we can notice that without UHRSD-TR whether we train with LR or HR scale, we cannot expect good results for complex scenes (first and third samples in <ref type="figure" target="#fig_1">Fig. 14)</ref>. This is because while DUTS-TR and HRSOD-TR are in favor of "centered" objects (e.g., our methods without UHRSD-TR in the first sample), while UHRSD-TR more focus on complex details which usually cover the whole image like thrid sample in <ref type="figure" target="#fig_1">Fig. 14.</ref> C Discussion <ref type="table">Table 7</ref>. Comparison of boundary quality measures (BDE <ref type="bibr" target="#b51">[52]</ref>, mBA <ref type="bibr" target="#b35">[36]</ref>, BIoU <ref type="bibr" target="#b52">[53]</ref>) with HR SOD methods. D: DUTS-TR, H: HRSOD-TR, U: UHRSD-TR. Three best results in order except our method are colored as red, blue, and green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithms</head><p>Train Datasets </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS-S HRSOD-TE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Selection of Boundary Metrics</head><p>Although many SOD methods dedicated to the HR benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref> use Boundary Displacement Error (BDE) <ref type="bibr" target="#b51">[52]</ref>, we suggest to use mean boundary accuracy (mBA) <ref type="bibr" target="#b35">[36]</ref> instead for following reasons. First, it is substantially outdated metric for measuring boundary quality since BDE was proposed in 2002, when image segmentation methods highly depended on low-level signal analysis of the given image. Now we're in the era of deep learning. We can easily generate more accurate, high-quality segmentation results, and hence need to use metrics like mBA or Boundary IoU (BIoU) <ref type="bibr" target="#b52">[53]</ref>. Second, we could not find any of official, non-official implementation related to the BDE, and the only source that we found is unable to access. While, mBA and BIoU have official implementations. We report mBA in the main paper because it does not require modification for SOD since <ref type="bibr" target="#b35">[36]</ref> also works for a binary segmentation map like SOD, while BIoU requires major modification since the official code only provides BIoU embedded in the evaluation codes for Average Precision and Panoptic Quality. Third, the evaluation results with BDE shows inconsistent while mBA and BIoU shows consistent results as shown in Tab. 7. On the other hand, mBA and BIoU shows consistent results across different HR methods. Thus, we use mBA for boundary metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Prediction Ground Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Potential Vulnerability of InSPyReNet</head><p>While we show that our InSPyReNet can produce high-quality results in HR benchmarks, we can notice that there are some failure cases in terms of two different aspects. First, as shown in the first row from <ref type="figure" target="#fig_2">Fig. 15</ref>, if the LR branch in pyramid blending fails to predict the saliency object, it suffers from global context failure. Second, even if the LR branch in pyramid blending successfully predict the saliency branch, we still have a chance to fail reconstructing local details when the HR branch fails to generate high-frequency details. In the second row from <ref type="figure" target="#fig_2">Fig. 15</ref>, the LR branch detected the body part of the bicycle, but from HR branch, it fails to predict spokes of the wheel and details of the front basket.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of image pyramid based saliency map between (a) Chen et al . [7], (b) InSPyReNet, and image pyramid of (c) ground truth. Compared to the image pyramid of ground truth saliency map, Chen et al . shows distorted results especially for the higher stages (e.g., Stage-3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Invariant Context Attention EXPAND : EXPAND operation REDUCE : REDUCE operation :output stride k The architecture of proposed InSPyReNet. (a) The initial saliency map from Stage-3 and Laplacian saliency maps from higher-stages are combined with EXPAND operation to be reconstructed to the original input size. (b) The ground-truth is deconstructed to the smaller stages for predicted saliency maps from each stage by REDUCE operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of Scale Invariant Context Attention (SICA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>S? ? Fmax ? MAE? mBA? S? ? Fmax ? MAE? mBA? S? ? Fmax ? MAE? mBA? w/o pyramid blending 384 ? 384 -0.953 0.949 0.013 0.705 0.945 0.941 0.019 0.700 0.927 0.932 0.032 0.724 L = 1280 w/o SICA 0.396 0.602 0.497 0.504 0.373 0.416 0.530 0.512 0.242 0.395 0.645 0.506 L = 1280 w/ SICA 0.873 0.821 0.037 0.774 0.886 0.873 0.043 0.750 0.809 0.819 0.092 0.751 w/ pyramid blending L = 1280 w/o SICA 0.860 0.883 0.023 0.537 0.863 0.869 0.029 0.531 0.834 0.863 0.052 0.521 L = 1280 w/ SICA 0.962 0.959 0.009 0.743 0.952 0.949 0.016 0.738 0.932 0.938 0.029 0.741</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>0.869 0.840 0.043 0.825 0.754 0.056 0.918 0.926 0.037 0.905 0.911 0.034 0.848 0.833 0.071 GateNet [39] ResNeXt101 0.897 0.880 0.035 0.849 0.794 0.051 0.929 0.940 0.035 0.925 0.932 0.029 0.865 0.855 0.064 ? Chen et al . [7] Res2Net50 0.890 0.869 0.040 0.834 0.769 0.061 0.931 0.943 0.035 0.921 0.927 0.034 0.871 0.862 0.060 ? F 3 Net [25] Res2Net50 0.892 0.876 0.033 0.839 0.771 0.048 0.915 0.925 0.040 0.915 0.925 0.030 0.856 0.842 0.065 ? LDF [9] Res2Net50 0.897 0.885 0.032 0.848 0.788 0.045 0.928 0.943 0.033 0.924 0.935 0.027 0.868 0.863 0.059 ? MINet [40] Res2Net50 0.896 0.883 0.034 0.843 0.787 0.055 0.931 0.942 0.031 0.923 0.931 0.028 0.865 0.858 0.060 ? PA-KRN [41] Res2Net50 0.898 0.888 0.034 0.853 0.808 0.050 0.930 0.943 0.032 0.922 0.935 0.027 0.863 0.859 0.063 Ours Res2Net50 0.904 0.892 0.035 0.845 0.791 0.059 0.936 0.949 0.031 0.929 0.938 0.028 0.876 0.869 0.056 Transformer backbone Models (Swin, T2T-ViT) VST [42] T2T-ViT-14 0.896 0.878 0.037 0.850 0.800 0.058 0.932 0.944 0.033 0.928 0.937 0.029 0.872 0.864 0.061 Mao et al . [43] SwinB 0.917 0.911 0.025 0.862 0.818 0.048 0.943 0.956 0.022 0.934 0.945 0.022 0.883 0.883 0.050 ? Chen et al . [7] SwinB 0.901 0.883 0.034 0.860 0.810 0.052 0.937 0.948 0.030 0.928 0.935 0.029 0.876 0.868 0.058 ? F 3 Net [25] SwinB 0.902 0.895 0.033 0.860 0.826 0.053 0.937 0.951 0.027 0.932 0.944 0.023 0.868 0.864 0.059 ? LDF [9] SwinB 0.896 0.881 0.036 0.854 0.809 0.052 0.931 0.942 0.032 0.933 0.941 0.024 0.861 0.851 0.065 ? MINet [40] SwinB 0.906 0.893 0.029 0.852 0.798 0.047 0.935 0.949 0.028 0.930 0.938 0.025 0.875 0.870 0.054 ? PA-KRN [41] SwinB 0.913 0.906 0.028 0.874 0.838 0.042 0.941 0.956 0.025 0.933 0.944 0.023 0.873 0.872 0.056 Ours SwinB 0.931 0.927 0.024 0.875 0.832 0.045 0.949 0.960 0.023 0.944 0.955 0.021 0.893 0.893 0.048 they provided source code with great reproducibility and consistent results. As shown in Tab. 2, our SwinB backbone model consistently shows outstanding performance across three metrics. Moreover, our Res2Net50 backbone model shows competitive results regarding its number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative results of InSPyReNet (SwinB) compared to PGNet on UHRSD-TE. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Visual comparison of LR, HR prediction, and pyramid blended results of InSPyReNet with (a) Res2Net50 and (b) SwinB backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Context map configuration (a) and details of SICA (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>s</head><label></label><figDesc>?C and input context maps c ? R H s ? W s ?N are resized according to the shape from training time h ? w with bi-linear interpolation. The resized feature map x ? R h s ? w s ?C and context map c' ? R h s ? w s ?N is used to compute object region representation f ? R N ?C .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Table 4 .</head><label>4</label><figDesc>Left: Ablation study of kernel size k and the standard deviation ? of the Gaussian kernel G(k, ?) for image pyramid operations of InSPyReNet (Res2Net50) on DUTS-TE and DUT-OMRON. Right: Ablation study of training strategies for InSPyReNet (SwinB) on DAVIS-S and HRSOD-TE. pred and gt denotes the image pyramid structure applied in prediction and ground truth respectively. S.G. denotes Stop-Gradient. Lpc denotes pyramidal consistency loss. k ? DUTS-TE DUT-OMRON S? Fmax MAE S? Fmax MAE 5 1 0.902 0.890 0.037 0.839 0.783 0.059 5 3 0.897 0.882 0.041 0.837 0.779 0.059 5 5 0.888 0.876 0.042 0.834 0.770 0.060 7 1 0.904 0.892 0.035 0.845 0.791 0.059 7 3 0.897 0.884 0.037 0.841 0.777 0.058 7 5 0.888 0.878 0.038 0.833 0.774 0.061 pred gt S.G. Lpc DAVIS-S HRSOD-TE S? ? Fmax ? MAE? mBA? S? ? Fmax ? MAE? mBA? 0.935 0.937 0.016 0.693 0.931 0.933 0.023 0.682 0.937 0.939 0.014 0.714 0.934 0.937 0.019 0.712 0.938 0.935 0.016 0.699 0.932 0.931 0.022 0.695 0.945 0.942 0.014 0.719 0.944 0.942 0.017 0.715 0.955 0.959 0.010 0.727 0.947 0.948 0.018 0.729 0.962 0.959 0.009 0.743 0.952 0.949 0.016 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .Algorithm 1</head><label>121</label><figDesc>Visual demonstration for ablation study of SICA. The sample is taken from AIM-500<ref type="bibr" target="#b50">[51]</ref>. PyTorch pseudocode of image pyramid operations.# Module for image pyramid operations ( EXPAND , REDUCE ) # ksize : kernel size for Gaussian filter # sigma : standard deviation for Gaussian filter # channels : number of channels for pyramid operation # ( For saliency map , set 1. For RGB image , set 3.) class ImagePyramid : def __init__ ( self , ksize =7 , sigma =1 , channels =1): self . ksize = ksize self . sigma = sigma self . channels = channels k = cv2 . g e t G a u s s i a n K e r n e l ( ksize , sigma ) k = np . outer (k , k ) k = torch . tensor ( k ). float () self . kernel = k . repeat ( channels , 1 , 1 , 1) # call to use GPU def cuda ( self ): self . kernel = self . kernel . cuda () return self # EXPAND operation def expand ( self , x ): z = torch . zeros_like ( x ) x = torch . cat ([ x , z , z , z ] , dim =1) x = F . pixel_shuffle (x , 2) x = F . pad (x , ( self . ksize // 2 , ) * 4 , mode = ' reflect ') x = F . conv2d (x , self . kernel * 4 , groups = self . channels ) return x # REDUCE operation def reduce ( self , x ): x = F . pad (x , ( self . ksize // 2 , ) * 4 , mode = ' reflect ') x = F . conv2d (x , self . kernel , groups = self . channels ) return x [: , : , ::2 , ::2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Visual comparison of PGNet [4] (H, U) and our methods trained with HR datasets. Results are ordered as image, ground truth, PGNet, and Ours(D, H * ) in the first row, and Ours(H * , U * ), Ours(D, H * , U * ), Ours(D, H), Ours(H, U) in the second row from left to right for each sample. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Visual illustration for the failure case (first row: global context failure, second row: local detail failure) of InSPyReNet. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2209.09475v2 [cs.CV] 26 Sep 2022</figDesc><table><row><cell>?</cell><cell>LR SOD Network</cell><cell>?</cell><cell>?</cell><cell>LR SOD Network</cell><cell>?</cell><cell>C</cell><cell>HR SOD Network</cell></row><row><cell></cell><cell cols="2">(a) LR methods</cell><cell></cell><cell cols="4">(b) Two-stage methods</cell></row><row><cell></cell><cell>HR SOD</cell><cell></cell><cell></cell><cell>InSPyRe</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Network</cell><cell>LR HR Fusion</cell><cell></cell><cell>-Net</cell><cell>shared</cell><cell></cell><cell>Pyramid</cell></row><row><cell>?</cell><cell>LR SOD Network</cell><cell>Network</cell><cell>?</cell><cell>InSPyRe -Net</cell><cell></cell><cell></cell><cell>Blending</cell></row><row><cell cols="3">(c) Multiscale methods</cell><cell></cell><cell cols="2">(d) Ours</cell><cell></cell><cell>? ?</cell><cell>: Down-sample : Up-sample</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 6. Illustration of pyramid blending of InSPyReNet for HR prediction.</figDesc><table><row><cell cols="2">Image Resized</cell><cell>? ?</cell><cell></cell><cell></cell><cell>Image ?</cell></row><row><cell>LR</cell><cell></cell><cell>LR</cell><cell></cell><cell></cell><cell>HR</cell><cell>Transition Area</cell><cell>HR</cell></row><row><cell>Saliency</cell><cell cols="3">Saliency Pyramid</cell><cell></cell><cell>Saliency Pyramid</cell><cell>Saliency</cell></row><row><cell>Pyramid</cell><cell cols="3">Reconstruction</cell><cell></cell><cell>Blending &amp; Reconstruction</cell><cell>Pyramid</cell></row><row><cell cols="3">:InSPyReNet</cell><cell></cell><cell></cell></row><row><cell cols="3">:EXPAND</cell><cell>:Resize</cell><cell>?</cell><cell>: Original image size</cell><cell>:Element-wise multiplication</cell></row><row><cell cols="4">:Dilation ? Erosion</cell><cell>? ?</cell><cell>: Image size from training</cell><cell>:Element-wise summation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>10] respectively. Images are resized to 384 ? 384 for training, and we use a random scale in a range of [0.75, 1.25] and crop to the original size, random rotation</figDesc><table><row><cell></cell><cell>0.9591</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.733</cell></row><row><cell>S</cell><cell>0.9588 0.9589 0.9590</cell><cell></cell><cell></cell><cell></cell><cell>DAVIS-S</cell><cell></cell><cell></cell><cell>mBA S</cell><cell>0.715 0.721 0.727 mBA</cell></row><row><cell></cell><cell></cell><cell>784</cell><cell>896</cell><cell>1024</cell><cell>1152</cell><cell>1280</cell><cell>1408</cell><cell>1536</cell><cell></cell></row><row><cell></cell><cell>0.9529</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.735</cell></row><row><cell>S</cell><cell>0.9487 0.9501 0.9515</cell><cell></cell><cell></cell><cell></cell><cell>HRSOD-TE</cell><cell></cell><cell></cell><cell>mBA S</cell><cell>0.703 0.714 0.724 mBA</cell></row><row><cell></cell><cell></cell><cell>784</cell><cell>896</cell><cell>1024</cell><cell>1152</cell><cell>1280</cell><cell>1408</cell><cell>1536</cell><cell></cell></row><row><cell></cell><cell>0.9253</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.734</cell></row><row><cell></cell><cell>0.9251</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.724</cell></row><row><cell>S</cell><cell>0.9247 0.9249</cell><cell>784</cell><cell>896</cell><cell>1024</cell><cell>1152 L UHRSD-TE</cell><cell>1280</cell><cell>1408</cell><cell>1536 mBA S</cell><cell>0.705 0.715</cell></row></table><note>mBA Fig. 7. Performance measure (S? and mBA) of InSPyReNet with pyramid blending by changing L on three HR benchmarks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fmax ? MAE? mBA? S? ? Fmax ? MAE? mBA? S? ? Fmax ? MAE? mBA? S? ? Fmax ? MAE? S? ? Fmax ?MAE? Qualitative results of InSPyReNet (SwinB) compared to SotA HR methods on HRSOD-TE. Best viewed by zooming in.</figDesc><table><row><cell cols="10">Algorithms S? ? ? Chen et al . [7] Backbone Train Datasets S D 0.934 0.925 0.018 0.697 0.915 0.907 0.032 0.684 0.915 0.919 0.034 0.712 0.901 0.883 0.034 0.860 0.810 0.052 HR benchmarks LR benchmarks DAVIS-S HRSOD-TE UHRSD-TE DUTS-TE DUT-OMRON</cell></row><row><cell>? F 3 Net [25]</cell><cell>S</cell><cell>D</cell><cell cols="7">0.931 0.922 0.017 0.681 0.912 0.902 0.034 0.674 0.920 0.922 0.033 0.708 0.902 0.895 0.033 0.860 0.826 0.053</cell></row><row><cell>? LDF [9]</cell><cell>S</cell><cell>D</cell><cell cols="7">0.928 0.918 0.019 0.682 0.905 0.888 0.036 0.672 0.911 0.913 0.038 0.702 0.896 0.881 0.036 0.854 0.809 0.052</cell></row><row><cell>? MINet [40]</cell><cell>S</cell><cell>D</cell><cell cols="7">0.933 0.930 0.017 0.673 0.927 0.917 0.025 0.670 0.915 0.917 0.035 0.694 0.906 0.893 0.029 0.852 0.798 0.047</cell></row><row><cell>? PA-KRN [41]</cell><cell>S</cell><cell>D</cell><cell cols="7">0.944 0.935 0.014 0.668 0.927 0.918 0.026 0.653 0.919 0.926 0.034 0.673 0.913 0.906 0.028 0.874 0.838 0.042</cell></row><row><cell>PGNet [4]</cell><cell>S+R18</cell><cell>D</cell><cell cols="7">0.935 0.931 0.015 0.707 0.930 0.922 0.021 0.693 0.912 0.914 0.037 0.715 0.911 0.903 0.027 0.855 0.803 0.045</cell></row><row><cell>Zeng et al . [1]</cell><cell>V</cell><cell>D,H</cell><cell cols="4">0.876 0.889 0.026 0.618 0.897 0.892 0.030 0.623</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.824 0.835 0.051 0.762 0.743 0.065</cell></row><row><cell>Tang et al . [2]</cell><cell>R50</cell><cell>D,H</cell><cell cols="4">0.920 0.935 0.012 0.716 0.920 0.915 0.022 0.693</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.895 0.888 0.031 0.843 0.796 0.048</cell></row><row><cell>PGNet [4]</cell><cell>S+R18</cell><cell>D,H</cell><cell cols="7">0.947 0.948 0.012 0.716 0.935 0.929 0.020 0.714 0.912 0.915 0.036 0.735 0.912 0.905 0.028 0.858 0.803 0.046</cell></row><row><cell>PGNet [4]</cell><cell>S+R18</cell><cell>H,U</cell><cell cols="7">0.954 0.956 0.010 0.730 0.938 0.939 0.020 0.727 0.935 0.930 0.026 0.765 0.861 0.828 0.038 0.790 0.727 0.059</cell></row><row><cell>Ours</cell><cell>S</cell><cell>D</cell><cell cols="7">0.962 0.959 0.009 0.743 0.952 0.949 0.016 0.738 0.932 0.938 0.029 0.741 0.931 0.927 0.024 0.875 0.832 0.045</cell></row><row><cell>Image</cell><cell cols="3">Ground Truth</cell><cell>HRSOD</cell><cell>DHQSOD</cell><cell cols="2">PGNet (D)</cell><cell></cell><cell>PGNet (D, H)</cell><cell>PGNet (H, U)</cell><cell>InSPyReNet (Ours)</cell></row><row><cell cols="3">Fig. 8. Image</cell><cell></cell><cell>Ground Truth</cell><cell>PGNet (D)</cell><cell cols="2">PGNet (D, H)</cell><cell></cell><cell>PGNet (H, U)</cell><cell>InSPyReNet (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>), we can easily remove them while enhance boundary details via pyramid blending. Future Work and Conclusion. Starting from previous works with Laplacian pyramid prediction<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>, we have shown that InSPyReNet shows noticeable improvements on HR prediction without any HR training datasets or complex architecture. In a series of experiments, our method shows great performance on HR benchmarks while robust again LR benchmarks as well. Although we only utilize a concept of pyramid-based image blending for merging two pyramids with different scales, we hope our work can extend to the multi-modal input such as RGB-D SOD or video SOD with temporal information.</figDesc><table><row><cell>Acknowledgement. This work was supported by Institute of Information &amp;</cell></row><row><cell>communications Technology Planning &amp; Evaluation(IITP) grant funded by the</cell></row><row><cell>Korea government(MSIT) (No.2017-0-00897, Development of Object Detection</cell></row><row><cell>and Recognition for Intelligent Vehicles) and (No.B0101-15-0266, Development</cell></row><row><cell>of High Performance Visual BigData Discovery Platform for Large-Scale Real-</cell></row><row><cell>time Data Analysis)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Quantitative results of applying pyramid blending for previous pyramid-based SOD method (Chen et al .<ref type="bibr" target="#b6">[7]</ref>) on three HR and two LR benchmarks. P.B. denotes pyramid blending. ? indicates larger the better, and ? indicates smaller the better. Visual comparison of applying pyramid blending for previous pyramid-based SOD method (Chen et al .) and InSPyReNet. Samples are taken from UHRSD<ref type="bibr" target="#b3">[4]</ref>. Best viewed by zooming in.B.2 Applying pyramid blending to previous Image Pyramid based ModelIt is easy to apply pyramid blending if the base model outputs image pyramid of saliency map same as InSPyReNet. We choose Chen et al .<ref type="bibr" target="#b6">[7]</ref> since it has a great reproducibility and has a pyramid structure. We trained Chen et al . with same backbone (SwinB) for fair comparison. As shown in Tab. 5, pyramid blending for Chen et al . worsen results especially for the mBA measure which is a major reason for pyramid blending. We also provide some qualitative results of Chen et al . with pyramid blending inFig. 13, which shows some clear degradation. Thus, without a well-defined image pyramid based model designed for image blending, it cannot be used for HR prediction.B.3 Training InSPyReNet with HR datasetsTo demonstrate the potential of our method, we utilize HR datasets (HRSOD-TR, UHRSD-TR) alongside DUTS-TR for our training dataset. First, we use HR datasets for training with fixed LR scale (i.e., 384 ? 384), meaning that we do not use HR annotations and regard them as another LR datasets. Results show that our method well generalizes to the HR prediction with extra LR datasets (Tab. 6). Note that we do not include this experiments for our final results since even we resize HR datasets into LR scale, annotations are still remains highquality thanks to the interpolation method, so it is not fair to claim that we are using only LR datasets.</figDesc><table><row><cell>Algorithms</cell><cell>DAVIS-S S? ? Fmax ? MAE?</cell><cell>mBA?</cell><cell>HRSOD-TE S? ? Fmax ? MAE?</cell><cell>mBA?</cell><cell>UHRSD-TE S? ? Fmax ? MAE?</cell><cell>mBA?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>w/o pyramid blending</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Chen et al . [7] 0.934 0.925 0.018</cell><cell>0.697</cell><cell>0.915 0.907 0.032</cell><cell>0.684</cell><cell>0.915 0.919 0.034</cell><cell>0.712</cell></row><row><cell>Ours</cell><cell>0.953 0.949 0.013</cell><cell>0.705</cell><cell>0.945 0.941 0.019</cell><cell>0.700</cell><cell>0.927 0.932 0.032</cell><cell>0.724</cell></row><row><cell></cell><cell></cell><cell></cell><cell>w/ pyramid blending</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Chen et al . [7] 0.916 0.893 0.023 0.583 (-16.4%) 0.909 0.891 0.033 0.590 (-13.7%) 0.903 0.906 0.042 0.590 (-17.1%)</cell></row><row><cell>Ours</cell><cell cols="6">0.962 0.958 0.009 0.732 (+3.8%) 0.952 0.955 0.015 0.732 (+4.6%) 0.932 0.938 0.029 0.741 (+2.3%)</cell></row><row><cell cols="2">RAS (SwinB)</cell><cell></cell><cell>RAS (SwinB) w/ pyramid blending</cell><cell></cell><cell></cell><cell></cell></row></table><note>InSPyReNet Fig. 13.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Quantitative results on three HR and two LR benchmarks for training with extra HR datasets. D: DUTS-TR, H: HRSOD-TR, U: UHRSD-TR. The best results for each metric are denoted as bold. ? indicates larger the better, and ? indicates smaller the better. * indicates that the dataset is resized into LR scale. Fmax ? MAE? mBA? S? ? Fmax ? MAE? mBA? S? ? Fmax ? MAE? mBA? S? ? Fmax ? MAE? S? ? Fmax ? MAE?</figDesc><table><row><cell>Train Datasets</cell><cell>HR benchmarks HRSOD-TE S? ? PGNet [4] DAVIS-S</cell><cell>UHRSD-TE</cell><cell>LR benchmarks DUTS-TE DUT-OMRON</cell></row><row><cell>H,U</cell><cell cols="3">0.954 0.956 0.010 0.730 0.938 0.939 0.020 0.727 0.935 0.930 0.026 0.765 0.861 0.828 0.038 0.790 0.727 0.059</cell></row><row><cell></cell><cell cols="2">Ours Trained with LR scale (i.e., 384 ? 384)</cell><cell></cell></row><row><cell cols="4">D, H  *  0.963 0.966 0.008 0.744 0.958 0.958 0.014 0.752 0.937 0.945 0.027 0.754 0.936 0.934 0.022 0.878 0.836 0.044</cell></row><row><cell cols="4">H  *  ,U  *  0.963 0.967 0.008 0.732 0.947 0.945 0.020 0.741 0.949 0.956 0.020 0.765 0.925 0.922 0.028 0.874 0.835 0.048</cell></row><row><cell cols="4">D,H  *  ,U  *  0.970 0.972 0.007 0.743 0.951 0.951 0.018 0.748 0.950 0.957 0.020 0.767 0.931 0.928 0.024 0.880 0.837 0.042</cell></row><row><cell></cell><cell cols="2">Ours Trained with HR scale (i.e., 1024 ? 1024)</cell><cell></cell></row><row><cell>D,H</cell><cell cols="3">0.972 0.976 0.007 0.770 0.960 0.957 0.014 0.766 0.936 0.938 0.028 0.785 0.934 0.927 0.023 0.859 0.799 0.049</cell></row><row><cell>H,U</cell><cell cols="3">0.973 0.977 0.007 0.770 0.956 0.956 0.018 0.771 0.953 0.957 0.020 0.812 0.936 0.932 0.024 0.872 0.823 0.046</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>BDE mBA BIoU BDE mBA BIoU Zeng et al . [1] D, H 44.359 0.618 0.662 88.017 0.623 0.659 Tang et al . [2] D, H 14.266 0.716 0.785 46.495 0.693 0.744</figDesc><table><row><cell>PGNet [4]</cell><cell>D</cell><cell cols="4">34.957 0.707 0.769 46.923 0.693 0.749</cell></row><row><cell>PGNet [4]</cell><cell cols="5">D, H 14.463 0.716 0.790 45.292 0.714 0.772</cell></row><row><cell>PGNet [4]</cell><cell cols="5">H, U 12.725 0.730 0.814 57.147 0.727 0.781</cell></row><row><cell>Ours</cell><cell>D</cell><cell>-</cell><cell>0.743 0.850</cell><cell>-</cell><cell>0.738 0.826</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This phenomenon shows that mBA itself cannot measure the performance of saliency detection, rather it only measures the quality of boundary itself.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the term 'stage j' is from<ref type="bibr" target="#b18">[19]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">23</biblScope>
			<pubPlace>1, 2, 4, 10</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Disentangled high quality salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">23</biblScope>
			<pubPlace>1, 2, 4</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Looking for the detail and context devils: High-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3204" to="3216" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pyramid grafting network for one-stage high resolution saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05041</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multiresolution spline with application to image mosaics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="236" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Label decoupling framework for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uacanet: Uncertainty augmented context attention for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ACM MM</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<title level="m">Axial attention in multidimensional transformers</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">F 3 net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>Bengio, Y., LeCun, Y., eds.: ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Saliency detection via graphbased manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Suppress and balance: A simple gated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Locate globally, segment locally: A progressive architecture with knowledge review network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Visual saliency transformer. In: ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Transformer transforms salient object detection and camouflaged object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10127</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fast fourier convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4479" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Optimizing the f-measure for threshold-free salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep automatic natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="800" to="806" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Yet another survey on image segmentation: Region and boundary information integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mart?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cuf?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
