<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MobileStereoNet: Towards Lightweight Deep Networks for Stereo Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faranak</forename><surname>Shamsafar</surname></persName>
							<email>faranak.shamsafar@</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tuebingen WSI Institute for Computer Science</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Woerz</surname></persName>
							<email>samuel.woerz@student.</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tuebingen WSI Institute for Computer Science</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafia</forename><surname>Rahim</surname></persName>
							<email>rafia.rahim@</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tuebingen WSI Institute for Computer Science</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
							<email>andreas.zell@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tuebingen WSI Institute for Computer Science</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MobileStereoNet: Towards Lightweight Deep Networks for Stereo Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent methods in stereo matching have continuously improved the accuracy using deep models. This gain, however, is attained with a high increase in computation cost, such that the network may not fit even on a moderate GPU. This issue raises problems when the model needs to be deployed on resource-limited devices. For this, we propose two light models for stereo vision with reduced complexity and without sacrificing accuracy. Depending on the dimension of cost volume, we design a 2D and a 3D model with encoder-decoders built from 2D and 3D convolutions, respectively. To this end, we leverage 2D Mo-bileNet blocks and extend them to 3D for stereo vision application. Besides, a new cost volume is proposed to boost the accuracy of the 2D model, making it performing close to 3D networks. Experiments show that the proposed 2D/3D networks effectively reduce the computational expense (27%/95% and 72%/38% fewer parameters/operations in 2D and 3D models, respectively) while upholding the accuracy. Our code is available at https://github.com/ cogsys-tuebingen/mobilestereonet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Stereo matching is one of the techniques for depth perception of a scene, which is established based on the displacement of the matching points in a binocular camera setup. Given a pair of rectified left/right images, we can compute depth by redirecting to disparity map estimation. Depth prediction is used in many real-world applications, like self-driving cars <ref type="bibr" target="#b14">[15]</ref>, robotics <ref type="bibr" target="#b25">[26]</ref>, and object detection <ref type="bibr" target="#b24">[25]</ref>. Compared to other techniques for depth perception, like LiDAR and Time-of-Flight sensors, passive stereo vision is more desirable in real-world scenarios because of inherent problems in other techniques, such as sparsity of depth data, incompetency in sunlight or reflective/absorbing surfaces, and limited operating depth range.  A stereo matching algorithm has three main components: feature extraction, regularization, and disparity selection. Before deep learning, numerous algorithms proposed different schemes for each step, like local descriptors (Sum of Absolute Difference or Census Transform <ref type="bibr" target="#b29">[30]</ref>) for feature extraction, Semi-global Matching (SGM) <ref type="bibr" target="#b6">[7]</ref> for regularization, and winner-take-all (WTA) for final disparity selection. Nowadays, similar to other computer vision tasks, stereo matching has also benefited from deep networks. Primarily, the research incorporated deep learning in individual components of the pipeline, like <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17]</ref> in matching costs and <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> in regularization. Later, the research stirred towards end-to-end frameworks, taking all the fundamental components into one network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Accordingly, in end-to-end pipelines, depending on the dimension of the cost volume built on top of the unary features, the subsequent convolutional layers in the regularization part (encoder-decoder) can be either 2D convolutions (for 3D cost volume) <ref type="bibr" target="#b12">[13]</ref> or 3D convolutions (for 4D cost volume) <ref type="bibr" target="#b9">[10]</ref>. We dub the former and the latter group as "2D" and "3D" models, respectively. While the 3D end-toend networks introduce highly boosted accuracy in disparity estimation, they are computationally costly due to more complex networks. As such, some of these networks cannot fit even on a moderate GPU, raising an out-of-memory (OOM) state. On the other hand, there are many embedded platforms with memory constraints on which the networks should fit and execute efficiently.</p><p>In order to reduce the complexity of 3D models, some works reconsider the configuration of the networks. For instance, DeepPruner <ref type="bibr" target="#b3">[4]</ref> develops a PatchMatch module to ignore the cost volume evaluation for most of the disparity range. In <ref type="bibr" target="#b28">[29]</ref>, authors establish an aggregation module on top of a cost volume computed by traditional local descriptors. Thus, skipping the convolutional feature extraction, the network benefits from a lighter learning paradigm. Also, it creates a 3D cost to avoid the curse of 3D convolutions.</p><p>In this work, we develop two end-to-end stereo matching networks, which exploit MobileNet-V1 <ref type="bibr" target="#b7">[8]</ref> and MobileNet-V2 <ref type="bibr" target="#b22">[23]</ref> blocks to mitigate the computational burden in favor of real embedded platforms, like FPGAs or mobile devices. Depending on the cost volume dimension, i.e. 3D or 4D, we propose a 2D and a 3D network. Moreover, for the 3D cost volume, a new learning construction module is devised based on interlacing the features from two viewpoints. Although reducing the computation cost is usually accompanied by a degradation in performance, we show that the proposed architectures are competitive with their state-ofthe-art counterparts <ref type="figure" target="#fig_1">(Fig. 1)</ref>.</p><p>Overall, our main contributions are as follows: i) Two lightweight models (2D/3D) are designed and proposed for stereo matching using MobileNet blocks without sacrificing accuracy. ii) We raise MobileNet blocks from the originally proposed 2D convolutions to 3D for the application of stereo matching. Also, by analyzing their costs, we prove their merit in reducing the computational load when processing 4D data. iii) We introduce a learnable cost volume module for the 2D model to keep the accuracy comparable with over-parameterized 3D models. iv) Extensive experiments for analyzing the accuracy/complexity trade-off in different design choices are conducted. Our findings in the design choice can be applicable to similar 2D/3D networks to reduce their complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Stereo vision is one of the popular techniques for estimating the depth from images. In the last decade, machine learning and deep learning approaches have wellprogressed in computer vision tasks, including stereo matching. Deep learning-based methods can be categorized into two groups: methods that focus on transferring only one or some of the general pipeline components into a deep learning framework <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1]</ref>, and approaches that formulate the whole process in an end-to-end scheme <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32]</ref>. Most of the end-to-end methods are developed based on 3D convolutional layers. Although these architectures achieve a substantial increase in accuracy, they require a high amount of memory usage, making them impractical for mobile and real-time applications, such as robotics and autonomous vehicles.</p><p>Lighter networks. Lightweight architectures have become an active research domain, ringing the bell that it is getting impractical to slide through complex networks without considering the load of computations. Generally, convolutions entail the most considerable computational load. Some deep networks for stereo reconstruction have been developed to achieve less complexity while being competitive in terms of accuracy with heavy 3D architectures. In <ref type="bibr" target="#b28">[29]</ref>, an initial matching cost is constructed based on the traditional cost computation. After reducing the channel dimension through 1 ? 1 convolutions, the data is fed into a U-Net to regress the disparity map. In another work, DeepPruner <ref type="bibr" target="#b3">[4]</ref> mitigates the computational complexity of 3D convolutional layers by calculating the matching cost for a subset of possible disparity values. Recently, <ref type="bibr" target="#b19">[20]</ref> proposed to use feature-wise and feature-disparity-wise separable convolutions for optimizing the 3D stereo models. On the whole, according to the results in <ref type="bibr" target="#b28">[29]</ref>, the 2D architectures can make a better trade-off between accuracy and speed.</p><p>Other works mainly focus on reducing the complexity of 3D convolutional layers for other 3D vision tasks. Qiu et al. <ref type="bibr" target="#b18">[19]</ref> developed pseudo-convolutions that decouple a 3D convolution into 3D convolutions equivalent to a 2D convolution and a light 3D convolution, which aggregates information only across the third domain. The network is demonstrated on video classification. In <ref type="bibr" target="#b27">[28]</ref>, authors made use of 3D depth-wise convolutions for 3D reconstruction. Recently, <ref type="bibr" target="#b4">[5]</ref> proposed progressively forward expanding a 2D tiny network along multiple axes to get fewer parameters for video classification and detection.</p><p>Cost volume computation. In a stereo model, measuring the similarity of left/right features is just as important as feature extraction and regularization. Traditional algorithms utilized simple calculations, like absolute difference, Hamming distance, or correlation. Similar solutions carried on to deep learning-based networks. Specifically, correlation is used on top of unary features to compute a 3D cost volume <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9]</ref>. Later, Kendall et al. <ref type="bibr" target="#b9">[10]</ref> proposed to concatenate unary features to make a 4D cost, requiring 3D convolutions in the following. After that, this approach was mainly adapted for 3D models with some modifications to enhance the accuracy, e.g. variance-based <ref type="bibr" target="#b20">[21]</ref>, groupwise correlation <ref type="bibr" target="#b5">[6]</ref> and pyramid <ref type="bibr" target="#b26">[27]</ref> cost volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As a first step, we reformulate two common light blocks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> to raise them from 2D convolutions to 3D for the application of stereo matching. We also analyze their computation cost w.r.t. the standard 2D/3D convolution coun- terparts. The computation cost of a deep network is measured by the number of operations in MACs (Multiply-Accumulate) and the number of parameters. While the number of parameters is fixed for a model, MACs depend on the input size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Light blocks replacing 2D/3D convolutions</head><p>As a pioneer work, MobileNet-V1 [8] employs depthwise and point-wise convolutions to produce an output with the same size as the output of a standard convolution, but with fewer computations. Later, MobileNet-V2 <ref type="bibr" target="#b22">[23]</ref> was introduced, which formulates its block with point-wise, depth-wise, and once again, point-wise layers. The number of input and output channels are specific for each layer, such that the channel dimension is expanded with an expansion factor (t) within the block. In a non-downsampling layer, a skip connection is included as well, making it a socalled Inverted Residual block. In <ref type="figure" target="#fig_2">Fig. 2</ref>, MobileNet-V1 and MobileNet-V2 blocks (shortened to v1 and v2 hereon, for simplicity) are shown.</p><p>Raising MobileNet blocks to 3D for stereo network. Originally, v1 and v2 blocks are designed to replace 2D convolutions and are proved mainly for sparse prediction tasks, like image classification. Still, many other computer vision problems require 3D convolutions to operate on 4D data, e.g. tasks with temporal input besides the spatial data. Likewise, dense disparity estimation is such a topic, which explores the space for the 3rd dimension of the scene. This outlook of using 3D convolutions for stereo matching has emerged recently, where they are utilized for processing 4D cost data. Hence, we take v1 and v2 blocks to their 3D counterparts for stereo vision application and show their necessity for light stereo vision models.</p><p>For this, just as in 2D convolutions, we commit the depth-wise and point-wise convolutions in the channel (feature) dimension in 3D convolutions. To be more precise, to raise the convolutions from 2D to 3D, the input data is extended from C ? H ? W to C ? D ? H ? W , where H and W indicate the input height and width, respectively; D is the new third dimension, and C is the number of channels. In our formulation for stereo matching, the 4D data of cost <ref type="bibr">Operator</ref> Operations -MACs <ref type="table" target="#tab_11">Table 1</ref>: Computation cost of the standard convolutions vs. MobileNet-V1 (v 1 ) and MobileNet-V2 (v 2 ) blocks in 2D and 3D counterparts. In MACs, (D?) belongs to 3D types. We have ignored the computation cost of batch normalization, ReLU and residual connection of v 2 . Example for reduction factor (w.r.t. the standard convolution counterparts) is computed for k = 3, C in = 32, C out = 64, and t = 2.</p><formula xml:id="formula_0">((D?)H ? W ?) Example for Reduction Factor Std. Conv. k ? k ? C in ? Cout - 2D v 1 Block (k ? k ? C in )+ (C in ? Cout) 7.9x v 2 Block (C in ? t ? C in ) (k ? k ? t ? C in )+ (t ? C in ? Cout) 2.7x Std. Conv. k ? k ? k ? C in ? Cout - 3D v 1 Block (k ? k ? k ? C in )+ (C in ? Cout) 18.9x v 2 Block (C in ? t ? C in ) (k ? k ? k ? t ? C in )+ (t ? C in ? Cout) 7.0x</formula><p>volume <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6]</ref> </p><formula xml:id="formula_1">is likewise of size C ? D ? H ? W ,</formula><p>where D is the predefined disparity range for building the cost volume. Accordingly, we raise kernels from 2D to 3D, considering the same size for the added dimension, i.e. if the 2D kernel is 3?3, the 3D kernel is 3?3?3. Therefore, taking v1 and v2 blocks to 3D is straightforward by applying depth-wise and point-wise convolutions in the channel dimension. <ref type="figure" target="#fig_2">Figure 2</ref> displays the new v1 and v2 blocks raised to 3D. In Tab. 1, we compute the cost of the standard 2D/3D convolutions and 2D/3D v1 and v2 blocks. Comparing to standard convolution counterparts, there is a reduction factor in computation cost that depends on the kernel size k, channels (C in and C out ) and expansion factor (t) in v2 blocks. The example in the last column shows that v1 blocks are lighter than v2 in both 2D and 3D versions, as expected. Moreover, exploiting v1 and v2 blocks in 3D type is more desirable as they show the capability for further reduction in operations compared to standard counterparts.</p><p>Additionally, we examine the impact of the expansion factor of v 2 blocks in <ref type="figure" target="#fig_3">Fig. 3</ref>. We consider the common cases in an hourglass module (e.g. in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>) for evaluation, i.e. C in = C out = {32, 64, 128} with k = 3. We see that by increasing t, the reduction factor decreases until a point where the block becomes heavier than a standard convolution counterpart. Also, the 2D block is more sensitive to t, such that the cost is increased beyond the convolution after t = 5. Here, once again, we can verify the merit of MobileNet architectures in our reformulation for 3D convolutional layers of 3D networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed models</head><p>Here, we describe two end-to-end baselines (2D and 3D) and design MobileStereoNets in reliance on these two mod-   <ref type="bibr" target="#b5">[6]</ref> and it is processed with 3D convolutions. els and 2D/3D v1 and v2. Following the common pipeline in recent work, we first feed the rectified left/right images to the feature extraction backbone and obtain the unary features. The backbone is shared for the left and right images. The results are passed into a cost volume construction module to merge the data from two viewpoints. Finally, an encoder-decoder (hourglass) is applied on top of the cost volume to estimate the disparity map.</p><p>3D baseline. For this case, we adopt GwcNet-g <ref type="bibr" target="#b5">[6]</ref> with only one hourglass ( <ref type="figure" target="#fig_4">Fig. 4)</ref> as it is performing superior to other similar designs, e.g. GCNet <ref type="bibr" target="#b9">[10]</ref>, PSMNet <ref type="bibr" target="#b1">[2]</ref>, and GA-Net <ref type="bibr" target="#b31">[32]</ref>. This baseline utilizes a ResNet-like backbone and an encoder-decoder with 3D convolutions. Namely, a 4D cost volume is constructed by group-wise correlation of unary features, requiring 3D convolutions afterward. The encoder-decoder consists of an hourglass <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> outputting a downsampled disparity map, which after upsampling is compared against the ground-truth with a smooth-L1 loss function. For the detailed architecture, we refer the reader to the appendix.</p><p>2D baseline. In order to develop a much lighter stereo network, we modify the 3D baseline such that it uses an encoder-decoder with 2D convolutions <ref type="figure" target="#fig_4">(Fig. 4</ref>). This approach contrasts with the recent trend, where 3D convolutions are deployed to add a feature dimension for disparity via a 4D cost volume. With a ResNet-like backbone similar to the 3D baseline, an input image with resolution H ? W is turned into a feature of size 320 ? (H/4) ? (W/4). We add further processing by four successive point-wise convolutions to reduce the number of channels and attain a size of 32 ? (H/4) ? (W/4). In order to aggregate two unary features to form a cost volume, which indicates a similarity measurement in the left/right images across the disparity dimension, we propose a new Interlacing Cost Volume. Note that we need a 3D cost volume to retain the encoder-decoder with 2D convolutions. Ignoring the feature dimension for disparity, 3D cost volume is of size</p><formula xml:id="formula_2">(d max /4) ? (H/4) ? (W/4),</formula><p>where d max is the maximum disparity level. Finally, the 3D cost volume is taken into the encoder-decoder module after passing through two convolutions as a pre-hourglass module.</p><p>Interlacing cost volume construction. Traditionally, a cost volume for stereo matching is computed for comparing the descriptors of binocular images across the disparity dimension, mainly as 3D data as following:</p><formula xml:id="formula_3">C 3D (d, x, y) = G(f L (x, y), f R (x ? d, y)),<label>(1)</label></formula><p>where (x, y) and d are the spatial location and the disparity value within a range of (0,</p><formula xml:id="formula_4">d max ), respectively. f R (x?d, y)</formula><p>is the traversed right feature for a specific disparity level. G(., .) indicates a similarity measurement function that was conventionally chosen as correlation or Hamming distance. With the advent of deep learning in stereo vision, as the unary features raised into 3D data, i.e. f (x, y) ? f (., x, y), DispNetC <ref type="bibr" target="#b12">[13]</ref> proposed to use a correlation layer (dot product) to merge these data by f L (., x, y) f R (., x ? d, y). Later, <ref type="bibr" target="#b9">[10]</ref> introduced a cost volume as 4D data, C 4D (d, ., x, y), by concatenating the unary features.</p><p>These 3D or 4D cost volumes are obtained in an unparameterized module of the network. Thus, we propose a subnetwork, named Interlacing Cost Volume Construction with the motivations as following: i) In order to achieve a better aggregation of the two unary features, we propose a parameterized subnetwork to learn the aggregation. ii) By interlacing the left/right unary features, the corresponding features maps are distilled by a kernel. iii) we aim to retain the encoder-decoder with only 2D convolutions (and not 3D, which is the case in recent work), as this can contribute to significant reduction of operations.</p><p>Namely, given the left and traversed right unary features, each of size C ? H ? W , we first interlace them across the channel dimension to form a data with double-sized channels, 2C ? H ? W <ref type="figure">(Fig. 5)</ref>. After unsqueezing the output and raising it to 4D data (1 ? 2C ? H ? W ), a 3D convolutional layer is applied such that the 3D kernels cover a <ref type="figure">Figure 5</ref>: Left: Interlacing cost volume construction at a particular disparity from the left (red) and right (blue) unary features. The kernels of the first layer take a group of non-overlapping interlaced features (here, four channels from each unary feature, Interlaced 4 ). Right: Processing the data with more convolutional layers to yield the aggregated feature for that disparity level. The numbers above and below the arrows indicate the kernel size and the stride, respectively. specific number of left/right feature pairs. That is , a kernel of size 2i ? 3 ? 3 covers i channel from each of the unary features. By increasing the i, the kernel covers more features and thus, integrates more information. The two following layers <ref type="figure">(Fig. 5</ref>) are also 3D convolution with double and same number of kernels' of the first layer. In general, the kernels are of size m ? 3 ? 3 with strides as m ? 1 ? 1, showing that they are covering non-overlapping channels. Finally, we convert the data to a single channel and pass through one 2D convolutional layer. Note that similar to other methods for cost volume, the spatial resolution is unchanged. We can write the general formula for a certain disparity level as Eg. 2. In Sec. 4, we show that the inclusion of learnable weights in stereo network contributes to better aggregation of the left/right features.</p><formula xml:id="formula_5">C 3D (d, x, y) = Interlace{f L (., x, y), f R (., x ? d, y)} (2)</formula><p>MobileStereoNets. In our baselines, the feature extraction and channel reduction are essentially processed with 2D convolutions. On the other hand, the hourglass employs 2D or 3D convolutions depending on the constructed cost volume. In order to obtain lighter networks, we replace these components with 2D and 3D counterparts of v1 and v2 blocks. There are different design choices when exploiting these blocks in individual modules of the networks. Thus, extensive experiments are conducted to answer the following questions: i) Can we replace different modules in the 2D/3D baselines with 2D/3D v1 and v2 to achieve lighter stereo networks and keep the error rates low? ii) If so, which modules should be replaced with them for a better compromise? iii) Which block type performs better in terms of accuracy and computational load?</p><p>Our experiments (cf. Sec. 4) lead to our MobileStere-oNets by modifying the baselines as follows: ? First Convolutions: Each of the three initial 3 ? 3 convolutions are replaced with one v2. Using an expansion factor of t = 3 provides a favorable trade-off between the performance and computational complexity. ? Feature Extraction: We retained the original layer architecture and block structure, consisting of two 3 ? 3</p><p>convolutions and a residual connection between each block. Substituting these convolutions with v1 keeps performance competitive while reducing the computational complexity significantly. ? Channel Reduction: In the 2D baseline, we keep this module, i.e. four 1?1 convolutions, unchanged with standard convolutions as replacing them with lighter blocks deteriorates the performance. ? Pre-hourglass: In the 2D baseline, we replace the two 3 ? 3 convolutions in both of the blocks with v2. For 3D-MobileStereoNet, we use our extension of v2 to 3D instead of 3D convolutions. In both models, we choose expansion factor as t = 3. ? Hourglass: We employ a stack of three hourglasses for both 2D and 3D models. While the 3D network uses the same channel dimension as GwcNet <ref type="bibr" target="#b5">[6]</ref> (32), the hourglass width is increased to 48 in the 2D model. 2D-MobileStereoNet uses v2 instead of 2D convolutions. In 3D-MobileStereoNet, we once again swap the 3D convolutions for our extension of v2 to 3D. For both models, the expansion factor is t = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results and discussion</head><p>Here, we first evaluate the performance of the proposed interlaced cost volume. Then, extensive experiments for taking the 2D/3D v1 and v2 blocks into the baselines are elaborated to show the path taken to reach the final architectures. Finally, we compare our developed networks with other methods. Note that d max is 192 in all cases.</p><p>In order to analyze different design choices, we use the SceneFlow "final pass" dataset <ref type="bibr" target="#b12">[13]</ref>, consisting of 35,454/4,370 training/test samples with 540 ? 960 resolution. This dataset can also help to pretrain the networks for limited real datasets. The quantitative evaluation for SceneFlow images is mainly reported with End-Point-Error (EPE), the mean average disparity error in px. Two more errors are also reported, i.e. px-3 and D1, which are percentages of the outliers with disparity errors larger than 3px and max(3px, 0.05 ? ground-truth), respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cost volume construction</head><p>To verify the performance of the interlacing cost volume, we replace the corresponding module in 2D baseline ( <ref type="figure" target="#fig_4">Fig. 4)</ref> with correlation, which is adopted by 2D networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref>. <ref type="table" target="#tab_6">Table 5</ref> shows the evaluation of this baseline against the model embedded with our interlacing cost volume. Interlaced i indicates that i channels are taken from each unary feature, and the kernel of the subsequent layer is of size 2i ? 3 ? 3. For instance, when i = 4, four channels from each feature data are combined by 8 ? 3 ? 3 kernels. Therefore, cases of i &gt; 1 can be interpreted as group-wise interlacing. We observe that better similarity measurements between the left/right features are achieved by introducing this learnable cost volume construction, resulting in lower EPE. According to the table, the best case is achieved when i = 4 (also depicted in <ref type="figure">Fig. 5)</ref>, and hence, we consider this case for further experiments in the 2D baseline.</p><p>We also investigate the effect of interlacing against direct concatenation of left/right unary features. According to the table, the error has increased in this case, showing that direct concatenation is not efficiently distilling the corresponding left/right features, which is essential for stereo matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effect of incorporating MobileNet blocks</head><p>In this section, we incorporate v 1 and v 2 blocks (either 2D or 3D, depending on the type of convolutional module) in various components of the 2D and 3D baselines. In addition to error metric, we monitor the reduction of computational complexity to help us choose lighter models. We analyze replacing the fundamental modules of the network, i.e. feature extraction and hourglass with lighter blocks. The results of substituting these modules with v 1 and v 2 (t = 2) for 2D and 3D models are tabulated in <ref type="table" target="#tab_3">Tables 3a and 3b</ref>. The best model is selected according to the least EPE obtained in 20 epochs. Also, the input resolution for computing MACs is 256?512. In these cases, the first convolutions of feature extraction are kept in standard type.</p><p>We can conclude that: i) In both 2D and 3D baselines, feature extraction is responsible for much of the computational load. ii) Substituting feature extraction with v 1 and hourglass with v 2 yields a better compromise between accuracy and computational complexity. For the 2D baseline, "FE" and "HG" stand for feature extraction and hourglass.  this combination results in the least EPE. We consider this combination for both 2D and 3D models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FE</head><p>We also examine replacing other modules with lighter blocks to make the network even lighter. Namely, the first convolutional layers in feature extraction and pre-hourglass modules are replaced with v2. The reason for choosing v2 instead of v1 is the higher accuracy v2 can maintain after substituting for standard convolutions. We observed that in this case, for the 2D baseline, both the complexity and the error are reduced (tables are available in the appendix). We also tried replacing other modules with MobileNet blocks, i.e. the channel reduction module and the convolutions in interlacing cost volume construction in the 2D baseline. However, since these replacements deteriorate the learning capability of the network, they are kept unchanged with standard convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative and qualitative results</head><p>The discussed experiments support our design choice for the final 2D and 3D models, i.e. 2D-MobileStereoNet and 3D-MobileStereoNet. To increase the accuracy, we utilize a stack of three hourglasses. Also, we found out that higher t values make the network less accurate and heavier.</p><p>SceneFlow dataset. The evaluation of the proposed models on SceneFlow are presented in Tables 4a and 4b. Note that 2D-MobileStereoNet has more parameters than 3D-MobileStereoNet due to i) channel reduction module, ii) parameterized cost volume construction, and iii) wider hourglass. Nevertheless, with the least operations, it can be practical on systems with limited computation capacities. Compared to other 2D models, a lower error is achieved by 2D-MobileStereoNet with 16.6x fewer param-Left image/Disparity 2D-MobileStereoNet 3D-MobileStereoNet <ref type="figure">Figure 6</ref>: Qualitative performance on SceneFlow: Every two rows correspond to a test sample. In the left-most column, the samples and the ground-truth disparity maps are illustrated. The following two columns show the disparity and error maps (embedded with error values) estimated by 2D-MobileStereoNet and 3D-MobileStereoNet.    of real-world driving scenarios <ref type="bibr" target="#b13">[14]</ref>, with 376 ? 1236 resolution. To evaluate our models on this dataset, which has only ground-truth available for 200 for training samples, we use a 160/40 training/validation split. We finetune the networks that are pretrained on SceneFlow. For a fair comparison, we also train and evaluate other methods with the same data split. As shown in Tab. 5, 2D-MobileStereoNet attains comparable results to PSMNet, which is a 3D model, with much less computational load (2.3x/8x fewer parameters/operations). Also, 3D-MobileStereoNet is outperforming PSMNet and GA-Net-11, and is competitive with GA-Net-deep. Compared to GwcNet-g, 3D-MobileStereoNet is lighter with 3.6x/1.6x fewer parameters/operations. Additionally, we submitted the results of our finetuned models to the KITTI 2015 benchmark. To this end, we finetuned the epoch with the best cross-domain generalizability from SceneFlow to KITTI 2015. <ref type="table" target="#tab_7">Table 6</ref> shows that 2D-MobileStereoNet is surpassing GCNet (a 3D model) with 27%/95% fewer parameters/operations. Also, we can verify that 3D-MobileStereoNet shows superior performance when compared to GCNet and PSMNet and it is surpassing GwcNet-g (in D1 f g and in D1 all in all pixels) with 72%/38% fewer parameters/operations. <ref type="figure">Figure 7</ref> visualizes the results from KITTI 2015 benchmark. Comparing 2D-MobileStereoNet and 3D-MobileStereoNet, we observe that 3D-MobileStereoNet obtains crisp edges due to deploying 3D convolutions in the encoder-decoder. In other words, in the upsampling/downsampling process in encoder-decoder, 3D convolutions can better preserve finer details compared to an encoder-decoder with 2D convolutions. Nevertheless, 2D-MobileStereoNet achieves visually similar outputs to GCNet <ref type="bibr" target="#b9">[10]</ref> PSMNet <ref type="bibr" target="#b1">[2]</ref> GwcNet-g <ref type="bibr" target="#b5">[6]</ref> 2D-MobileStereoNet 3D-MobileStereoNet <ref type="figure">Figure 7</ref>: Qualitative performance (disparity images together with error maps) from KITTI 2015 benchmark. Warmer colors in error maps denote larger values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>GA-Net-deep <ref type="bibr" target="#b31">[32]</ref> GA-Net-11 <ref type="bibr" target="#b31">[32]</ref> GwcNet-gc <ref type="bibr" target="#b5">[6]</ref> GwcNet-g <ref type="bibr" target="#b5">[6]</ref> PSMNet <ref type="bibr">[</ref>  3D models while requiring considerably fewer operations (87% fewer operations compared to GwcNet-g). Also, 3D-MobileStereoNet visually achieves competitive or better results compared to other methods.</p><p>Model size. We also report the memory requirement of our models in <ref type="table" target="#tab_9">Table 7</ref>. Both of the proposed methods show smaller memory sizes, which is promising for memoryconstrained chips and their power consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presented lightweight stereo networks to alleviate high memory usage on embedded or mobile devices. Namely, we proposed two models (2D and 3D) with the primary goal of reducing the cost (in terms of parameters, operations, and model size) by using MobileNet blocks. To increase the accuracy of the 2D model, we also designed a new cost volume to learn the similarity of unary features. Yielding a favorable accuracy/complexity trade-off, these MobileStereoNets are promising for deploying end-to-end stereo networks on edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobileStereoNet: Towards Lightweight Deep Networks for Stereo Matching (Appendix)</head><p>This appendix provides more details of our work. Here, we have the following sections: A: Detailed baseline architectures, B: More qualitative results, C: Incorporating light blocks in other modules, D: Implementation details, and E: Analyzing the complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed baseline architectures</head><p>The detailed architectures of the 2D and 3D baseline models are displayed in <ref type="figure" target="#fig_1">Fig. 1</ref>. The numbers in the blocks indicate the output size of each particular layer/module. The feature extraction step is the same for the two models. The architecture of hourglass and its intraconnections are also similar, except that in the 2D baseline, the convolutions are all in 2D type, while there are 3D convolutions in hourglass of the 3D baseline. These two models differ in the cost volume construction and the channel reduction module as well. GA-Net-11 <ref type="bibr" target="#b31">[32]</ref> GA-Net-deep <ref type="bibr" target="#b31">[32]</ref> GwcNet-g <ref type="bibr" target="#b5">[6]</ref> 2D-MobileStereoNet 3D-MobileStereoNet <ref type="figure" target="#fig_3">Figure 3</ref>: Qualitative performance on KITTI 2015 validation set: From top to bottom, the left image, the ground-truth disparity map and the estimated disparity maps by PSMNet <ref type="bibr" target="#b1">[2]</ref>, GA-Net-11 <ref type="bibr" target="#b31">[32]</ref>, GA-Net-deep <ref type="bibr" target="#b31">[32]</ref>, GwcNet-g <ref type="bibr" target="#b5">[6]</ref>, 2D-MobileStereoNet and 3D-MobileStereoNet are illustrated. For a fair comparison, we trained all the models with a 160/40 split of KITTI 2015 training test. Warmer colors in error maps denote higher errors.</p><p>From <ref type="figure" target="#fig_3">Fig. 3</ref>, once again, we can verify that 2D-MobileStereoNet shows close performance to 3D models with the least number of operations. Also, 3D-MobileStereoNet obtains competitive or better accuracy with the least number of parameters among other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Incorporating light blocks in other modules</head><p>As mentioned in the paper, in order to further reduce the complexity, the first convolutions in the feature extraction and the pre-hourglass convolutions (cf. <ref type="figure" target="#fig_1">Fig. 1</ref>) are replaced with MobileNet-V2 (v 2 ). The experimental results are reported in Tables 1 and 2. Note that the first convolutions are of the 2D type for both 2D and 3D baselines; however, the pre-hourglass comes in 2D or 3D convolutions depending on the baseline. We can observe that in 2D-MobileStereoNet, when the two modules are replaced with MobileNet-V2 (v 2 ), the network obtains the least EPE. In 3D-MobileStereoNet, this combination yields slightly higher EPE. However, due to the nice reduction in the computation cost, we consider the same design choice for the 3D network. It is noteworthy that we have examined MobileNet-V1 (v 1 ) for these modules as well. However, as it deteriorates the performance, we ignore v 1 for these modules, albeit it shows much decrease in the cost.   <ref type="table" target="#tab_1">Table 2</ref>: Performance evaluation for the selected variant of 3D baseline (FE 2D :v 1 , HG 3D :v 2 ) from Tab. 3b of the paper, when replacing other components with v 2 block (t = 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation details</head><p>We used PyTorch for implementation and conducting experiments. All the trainings are executed on 4 ? NVIDIA GeForce GTX 1080 Ti. We adapt the Adam optimizer with ? 1 = 0.9 and ? 2 = 0.999. On the SceneFlow dataset, the networks are trained for 20 epochs, starting with a learning rate of 0.001. The learning rate is halved after epoch 10, 12, 14, and 16. The best model is selected based on the least EPE value. In the experiments on the KITTI 2015 validation set, we finetune the best SceneFlow model for 400 epochs, reducing the initial learning rate 0.001 by a factor of 10 after 200 epochs. To submit the results to the KITTI 2015 benchmark, we finetune starting from a SceneFlow checkpoint showing the best generalization performance from the SceneFlow to the KITTI 2015 images. For the 3D-MobileStereoNet, we used a batch size of 4, and for 2D-MobileStereoNet, the batch size is 8. <ref type="table" target="#tab_3">Table 3</ref> shows the computation cost of the main modules, i.e. feature extraction and encoder-decoder, in baselines (with standard convolutions) and in MobileStereoNets. Note that feature extraction is the same in 2D and 3D models. We see our design choice for feature extraction is significantly reducing the complexity both in operation (from 52.07 to 7.84 GigaMACs) and in parameters (from 7.84 to only 0.39 million). We also observe that the cost of the encoder-decoder modules, either in 2D or 3D, is reduced in lighter networks in both number of operations and parameters. Evidently, the major bottleneck for the 3D models is the encoder-decoder with 3D convolutions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analyzing the complexity</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Equal contribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Performance vs. computation cost on SceneFlow test set (Left) and KITTI 2015 validation set (Right): Metrics are EPE, number of parameters (?10 6 ), and number of operations (MACs in log scale). For all, the lower is better. By using a new parameterized cost volume, 2D-MobileStereoNet shows closer performance to 3D models with the least MACs. 3D-MobileStereoNet obtains competitive accuracy with the least number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: MobileNet-V1 block and its extension to 3D, Right: MobileNet-V2 block and its extension to 3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Reduction factor of 2D/3D v2 blocks with varying expansion factor (t) w.r.t. standard convolution counterparts. The right hand numbers indicate the channel numbers (C in = C out and k = 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Top (2D Baseline): After feature extraction, data is reduced to 32 channels. A 3D cost volume, (dmax/4) ? (H/4) ? (W/4), is generated using the proposed interlacing cost volume construction and it is processed by 2D convolutions. Bottom (3D Baseline): A 4D cost volume, 40 ? (dmax/4) ? (H/4) ? (W/4), is computed using Gwc40</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :Figure 2 Figure 2 :</head><label>122</label><figDesc>Top: 2D baseline, Bottom: 3D baseline. The numbers in the blocks indicate the output size of each particular layer/module. B. More qualitative results depicts more qualitative results on SceneFlow dataset. We have also shown qualitative comparison on KITTI 2015 validation set inFig. 3.Left image/Disparity2D-MobileStereoNet 3D-MobileStereoNet Qualitative performance on SceneFlow: Every two rows correspond to a test sample. In the left-most column, the samples and the ground-truth disparity maps are illustrated. The following two columns show the disparity and error maps (embedded with error values) estimated by 2D-MobileStereoNet and 3D-MobileStereoNet. Warmer colors in error maps denote higher errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance evaluation on SceneFlow test set for the proposed 3D cost volume: The costs created by Interlaced i contribute to lower error rates.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance evaluation on SceneFlow test set for variants of (a) 2D and (b) 3D baselines with 1 ? HG.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Comparison on SceneFlow test set for (a) 2D and</cell></row><row><cell>(b) 3D models. "Red." indicates the reduction factor of our</cell></row><row><cell>models compared to other methods.</cell></row><row><cell>eters. Moreover, we observe that in 3D-MobileStereoNet,</cell></row><row><cell>significantly fewer parameters are achieved, while the per-</cell></row><row><cell>formance is competitive with or better than other methods.</cell></row><row><cell>Compared to GwcNet-gc with the best EPE metric in the</cell></row><row><cell>table, 3D-MobileStereoNet uses 1.7x fewer parameters (in</cell></row><row><cell>millions) and 3.9x fewer GigaMACs. Note that although</cell></row><row><cell>DeepPruner-Fast [4] obtains the least number of operations,</cell></row><row><cell>it is still over-parametrized and this causes issues when fine-</cell></row><row><cell>tuning on smaller datasets like KITTI (cf. Table 6). Figure</cell></row><row><cell>6 shows the disparity estimation results.</cell></row><row><cell>KITTI 2015 dataset. This dataset consists of images</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison on KITTI 2015 validation set. D1 all D1 bg D1 f g D1 all</figDesc><table><row><cell cols="2">All(%) D1 bg D1 f g MC-CNN[31] Methods 2.89 8.88 3.89</cell><cell>Noc(%) 2.48 7.64 3.33</cell></row><row><cell>Fast DS-CS[29]</cell><cell>2.83 4.31 3.08</cell><cell>2.53 3.74 2.73</cell></row><row><cell>GCNet[10]</cell><cell>2.21 6.16 2.87</cell><cell>2.02 5.58 2.61</cell></row><row><cell>DeepPruner-Fast[4]</cell><cell>2.32 3.91 2.59</cell><cell>2.13 3.43 2.35</cell></row><row><cell>PSMNet[2]</cell><cell>1.86 4.62 2.32</cell><cell>1.71 4.31 2.14</cell></row><row><cell>AutoDispNet-CSS[22]</cell><cell>1.94 3.37 2.18</cell><cell>1.80 2.98 2.00</cell></row><row><cell>DeepPruner-Best[4]</cell><cell>1.87 3.56 2.15</cell><cell>1.71 3.18 1.95</cell></row><row><cell>GwcNet-g[6]</cell><cell>1.74 3.93 2.11</cell><cell>1.61 3.49 1.92</cell></row><row><cell>2D-MobileStereoNet</cell><cell>2.49 4.53 2.83</cell><cell>2.29 3.81 2.54</cell></row><row><cell>3D-MobileStereoNet</cell><cell>1.75 3.87 2.10</cell><cell>1.61 3.50 1.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Comparison on KITTI 2015 benchmark. 3D-</cell></row><row><cell>MobileStereoNet requires 98% and 72% fewer parame-</cell></row><row><cell>ters compared to AutoDispNet-CSS and GwcNet-g, respec-</cell></row><row><cell>tively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison of model size. Our two proposed methods yield more compact models compared to other works.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 :</head><label>1</label><figDesc>Performance evaluation for the selected variant of 2D baseline (FE 2D :v 1 , HG 2D :v 2 ) from Tab. 3a of the paper, when replacing other components with v 2 block (t = 2).</figDesc><table><row><cell>first-conv 2D</cell><cell>pre-HG 3D</cell><cell cols="3">EPE(px) ? MACs(G) ? Params(M ) ?</cell></row><row><cell>conv.</cell><cell>conv.</cell><cell>0.99</cell><cell>105.01</cell><cell>0.98</cell></row><row><cell>conv.</cell><cell>v 2</cell><cell>1.01</cell><cell>69.44</cell><cell>0.89</cell></row><row><cell>v 2</cell><cell>conv.</cell><cell>0.99</cell><cell>104.44</cell><cell>0.97</cell></row><row><cell>v 2</cell><cell>v 2</cell><cell>1.01</cell><cell>68.86</cell><cell>0.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3 :</head><label>3</label><figDesc>Analyzing the computation cost in terms of MACs and number of parameters for the main modules.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is supported by the German Federal Ministry of Education and Research (BMBF) for the project DeepStere-oVision (FRE: 01I518024B).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CBMV: A coalesced bidirectional matching volume for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Batsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2060" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepPruner: Learning efficient stereo matching via differentiable patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4384" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">looking at the right stuff&quot;-guided semantic-gaze for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwesan</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11883" to="11892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Look wider to match image patches with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haesol</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1788" to="1792" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging stereo matching with learning-based confidence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Gyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Separable convolutions for optimizing 3d stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafia</forename><surname>Rahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faranak</forename><surname>Shamsafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NLCA-Net: A non-local context attention network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AutoDispNet: Improving disparity estimation with automl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1812" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MobileNetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SGM-Nets: Semi-global matching with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihito</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Point-GNN: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic stereo matching with pyramid cost volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7484" to="7493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d depthwise convolution: Reducing model parameters in 3d vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongtian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="186" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast deep stereo with 2d convolutional processing of cost signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2287" to="2318" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GA-Net: Guided aggregation net for endto-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
