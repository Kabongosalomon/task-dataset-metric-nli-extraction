<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucien</forename><surname>Martin-Gaff?</surname></persName>
							<email>lucien@anotherbrain.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anotherbrain</forename><surname>Paris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wacongne</surname></persName>
							<email>catherine@anotherbrain.ai</email>
						</author>
						<title level="a" type="main">DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a new approach for feature fusion between RGB and LWIR Thermal images for the task of semantic segmentation for driving perception. We propose DooDLeNet, a double DeepLab architecture with specialized encoder-decoders for thermal and color modalities and a shared decoder for final segmentation. We combine two strategies for feature fusion: confidence weighting and correlation weighting. We report state-of-the-art mean IoU results on the MF dataset [1].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Thermal images can improve the performance of computer vision algorithms, particularly during night time or in harsh weather conditions, when visibility becomes limited. On the other hand, color images provide a richer source of visual information during day time. Hence it can be highly beneficial for vision models to combine both color and thermal images into a RGBT (red, green, blue, thermal) image pipeline.</p><p>Multimodality or the combination of different imaging sources becomes crucial to alleviate perception and ensure robustness under poor visibility conditions, but the combination of different cameras and sensors comes with alignment and synchronization challenges. Calibration of thermal images is a painstaking task where special heated or masked calibration patterns need to be employed. Alignment of color and thermal images is also a challenging problem, not only because of common differences in field of view and resolution between the color and thermal cameras but also because of the huge differences in image appearance between the two imaging modalities.</p><p>In this paper we consider the task of semantic segmentation of paired color RGB and thermal LWIR (Long Wave Infra-Red) images. Most previous research in this field has been focused in searching for good fusion strategies between the color and thermal images. However, previous work on semantic color-thermal segmentation has barely considered how to fuse the images taking into account the feature disagreement due to poor alignment.</p><p>Inspired by recent advances in semantic matching, here we consider an end-to-end deep learning strategy which indirectly takes into account the alignment of RGB and LWIR thermal images through a matching correlation layer.</p><p>We show the benefits of the correlation and confidence aware feature fusion for the task of semantic segmentation on RGB-thermal data.</p><p>The main contributions of this paper are the following:</p><p>? We propose a new CNN model for color-thermal semantic segmentation. We show the interest of learning deep features that are specialized for both thermal images and color images with specialized encoderdecoder paths.</p><p>? We show that semantic segmentation performance on color-thermal image pairs is increased by the aid of two different weighted feature fusion strategies:</p><p>-For matching correlation weighting, we explicitly estimate the feature matching at the last specialized decoder layer by computing feature correlations.</p><p>-For confidence weighting, we estimate the segmentation confidence at both color and thermal features, and this confidence is used to weight differently the color and thermal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic segmentation</head><p>Autonomous driving requires understanding the scene ahead.</p><p>The release of datasets BDD100K <ref type="bibr" target="#b1">[2]</ref> and arXiv:2204.10266v1 [cs.</p><p>LG] 21 Apr 2022</p><p>Cityscapes <ref type="bibr" target="#b2">[3]</ref> has boosted the interest of using semantic segmentation as a way to process images for driving applications. Semantic segmentation consists in assigning a class to every pixel of an image. Early efforts in deep-learning based semantic segmentation used fully convolutional network (FCN) <ref type="bibr" target="#b3">[4]</ref> which allowed for dense pixel classification of arbitrary sized images. However, FCN generated pixel labels from high level feature maps, resulting in relatively imprecise object boundaries. Subsequently, one of the main challenge of this task was to propose methods that extracted global context while providing pixel precise information to recover both the semantic categories present in the image and the pixel level boundaries of each object. Several approaches were developed to do so. Modifying the original FCN design, encoder/decoder architectures were proposed, that compress information to capture the global context before decompressing it using either deconvolution <ref type="bibr" target="#b4">[5]</ref>, pooling indexes (SegNet) <ref type="bibr" target="#b5">[6]</ref> or skip connections (U-net) <ref type="bibr" target="#b6">[7]</ref> to recover the fine spatial information.</p><p>Other methods use pyramids of features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> extracted by an encoder model applied to the same image at different resolutions. The features maps can then be upsampled to match full scale the image size. More recent work proposed to use atrous convolutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> to capture features at different spatial scales without the need to upsample the feature maps. Combining atrous convolutions and spatial pyramid pooling, Deeplabv3+ <ref type="bibr" target="#b9">[10]</ref> proposes a simple and effective encoder-decoder FCN architecture.</p><p>Another approach consists in introducing attention mechanisms <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> to combine different levels of feature maps and better capture long-range image dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multimodality and driving perception</head><p>The release of the KAIST Multispectral dataset <ref type="bibr" target="#b14">[15]</ref> can be seen as an important step in the direction of combining color and thermal images for driving perception. The dataset provides physically aligned color and thermal images with bounding boxes annotations for a few semantic classes representing humans, such as person, people and cyclist.</p><p>Besides the release of the KAIST dataset, other works have considered the problem of multimodality in driving perception. The work of <ref type="bibr" target="#b15">[16]</ref> proposed a cycle consistent GAN which generates synthetic color images from LWIR and synthetic LWIR from color images while enforcing that object detection remains accurate in both modalities. The authors obtain the best object detection accuracy when combining synthetic RGB and true LWIR images.</p><p>It should be noted that thermal and color images are often not perfectly aligned, due to the parallax effect, poor calibration or registration. This implies that naive input fusion, which consists in simply stacking the color and thermal im-ages together by forming a 4-channel RGBT data input is rarely the most effective fusion strategy.</p><p>A number of works investigated better strategies to fuse different image modalities together in one model. Fusenet <ref type="bibr" target="#b16">[17]</ref> was one of the first works dealing indirectly with the multimodal fusion problem, by proposing a double encoder architecture for the fusion of depth maps and color images.</p><p>The work of <ref type="bibr" target="#b0">[1]</ref> proposed the MFNet (Multi-spectral Fusion Networks), a method for semantic segmentation on RGBT data. Their model consists of separate encoders for RGB and thermal data and fusion at the decoder level. The authors also published a semantic segmentation dataset on RGBT data, the MF dataset. The RTFNet method <ref type="bibr" target="#b17">[18]</ref> performs semantic segmentation on RGBT data by fusing RGB and thermal features at multiple network depths, the method was also evaluated on the MF dataset <ref type="bibr" target="#b0">[1]</ref>. Fusion of features extracted at multiple CNN layers as performed by <ref type="bibr" target="#b17">[18]</ref> can be seen a strategy to attenuate feature disagreements, as the misalignment magnitude is decreased in deeper network layers.</p><p>FuseSeg-161 <ref type="bibr" target="#b18">[19]</ref>, a multi-modal fusion model has been able to fuse feature from thermal and RGB images in one end-to-end model. It consists of two encoders to extract features from input, one for RGB and another for thermal images with a two-stage fusion (TSF) strategy. In the first stage, the thermal feature maps are hierarchically added with the RGB feature maps in the RGB encoder. The fused feature maps are then concatenated with the corresponding decoder feature maps in the second fusion stage. TSF method achieved over state-the-art semantic segmentation performance under various lighting conditions.</p><p>More recently, FEANet <ref type="bibr" target="#b19">[20]</ref> proposed a new way to extract detail spatial information with a two-stage Feature-Enhanced Attention Network. The Feature-Enhanced Attention Module (FEAM) improve excavation feature capacity from both the channel and spatial views, involving a better preservation of spatial information. Thus, the model shift more attention to high-resolution features which outperform other state-of-the-art techniques.</p><p>Another approach consists in training a deep neural network that weights differently thermal and color features. A first work in this direction was done by <ref type="bibr" target="#b20">[21]</ref>, where the model learns to estimate a position shift between pedestrian bounding boxes in color and thermal modalities.</p><p>Our work is related to the method proposed by <ref type="bibr" target="#b20">[21]</ref> in the sense of performing a sort of confidence feature weighting. However, while <ref type="bibr" target="#b20">[21]</ref> is focused on bounding box retrieval and weighting, we compute dense confidence and correlation weights that are better suited for semantic segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>Given a dataset of paired color and thermal images with pixel-level target labels, we seek to perform semantic segmentation on this data by fusing color and thermal images in a manner that maximizes overall segmentation performance. Here we propose a mid-level feature fusion approach, which aims to limit the feature disagreements caused by misalignment between the thermal and color images.</p><p>Differently than the work of <ref type="bibr" target="#b17">[18]</ref> which performs fusion of RGB and thermal features directly at the RGB encoder, we employ completely specialized thermal and color encoders.</p><p>We rely upon the DeepLabV3+ [10] model backbone, which has dilated (atrous) convolutions, limiting the loss of spatial resolution and better preserving structural details. The main decoder ingredient is the ASPP (Atrous Spatial Pyramid Pooling) which obtains multi-scale features obtained by atrous convolutions with different dilation rates.</p><p>We propose a double encoder-decoder variant of DeepLabV3+ where fusion is performed at two different feature resolutions at the decoder side, with a shared decoder for the final segmentation. A motivation for our approach comes from the observation that rich encoder features from pretrained networks such as ResNet <ref type="bibr" target="#b21">[22]</ref> could be adapted as seamless as possible for multiple deep learning tasks, in order to minimize training time and maximize prediction performance. <ref type="figure">Figure 2</ref>. Encoder-decoder path for modality confidence-aware feature weighting based on DeepLabv3+ architecture. The illustrated procedure is computed for both color and thermal images. Resulting re-weighted features from color and thermal are concatenated together as input for the shared decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Confidence weighted fusion</head><p>It can be claimed that direct fusion of RGB and thermal features may be problematic depending on the time of day, weather and visibility conditions of the captured scene.</p><p>For instance, during day time and favorable weather, RGB features could provide the best features to perform segmentation, as thermal images are only one-channel and texture details from objects are hardly noticeable.</p><p>On the other hand, during night time and at low light, thermal images may provide better features to segment hardly visible objects, in particular the ones with distinguishable changes in temperature compared to the background such as pedestrians and cars.</p><p>If we always give equal importance for features coming from both the modalities, the model may achieve lower performance in segmenting objects when one of the modalities provide unreliable features.</p><p>To alleviate this issue, we propose a confidence-based weighting before performing feature fusion, such that RGB and the thermal features receive difference importance based on their reliability. The decoder and the segmentation heads of each modality are able to perform a preliminary segmentation, from which a confidence can be estimated. Based on the model confidence of the preliminary segmentation, we may be able to give more weight to the more reliable features. <ref type="figure">Figure 2</ref> illustrates how confidence is computed for each imaging modality.</p><p>Formally, we denote the color image as I c and thermal image as I t . Each image modality I m , with m P tc, tu has a set of features computed by a specialized encoder f m " e m pI m q and a modality prediction computed by specialized decoder y m " d m pf m q. It can be seen that the label for the pedestrian class (overlayed in yellow) is not perfectly aligned with the pedestrians on the thermal image. Such misalignment can cause the RGB and the thermal encoder features to disagree. We propose a correlation feature weighting to alleviate this issue.</p><p>The confidence weight matrix C m for modality m is given by an estimation of the confidence on the most probable prediction:</p><formula xml:id="formula_0">C mi " max?e xppy mi q ? j exppy mj q?( 1)</formula><p>where y mi denotes the one-hot logits for modality m at spatial position i.</p><p>The confidence map C m is used to re-weight the features. Color and thermal features receive different weights, based on the confidence of the specialized color and thermal decoders. Weighting is performed by point-wise Hadamard product, and resulting weighted color and thermal features are concatenated together:</p><formula xml:id="formula_1">f 1 c " C c d f c (2) f 1 t " C t d f t (3) f 1 ct " f 1 c ' f 1 t .<label>(4)</label></formula><p>where d denotes the Hadamard product and ' denotes concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Matching correlation weighted fusion</head><p>Another possible issue of directly fusing color and thermal features is the spatial misalignment between both images. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates a sample image from MF dataset <ref type="bibr" target="#b0">[1]</ref> where feature disagreement can be caused due to image misalignment.</p><p>As it may be hard to achieve perfect pixel by pixel alignment between the different imaging modalities, we propose to evaluate the alignment through a matching layer.</p><p>Inspired by the work of <ref type="bibr" target="#b22">[23]</ref> on deep image alignment, we propose to employ a correlation layer which accounts to evaluate the matching between features coming from the two different encoders.</p><p>The correlation layer computes pair-wise feature similarities between the color and thermal decoder and the ob-tained scores are normalized such that weak matched features are suppressed by down weighting.</p><p>In order to deal with strong appearance differences between color and thermal features, we estimate semantic matching between the last specialized decoder, such that correlation is invariant to appearance changes. Formally, given y c " d c pf c q corresponding to the color decoder predictions and y t " d t pf t q corresponding to the thermal decoder predictions, the correlation weight matrix M ct P R h?w is obtained by computing modality-wise prediction correlations (normalized scalar product of a pair of predictions), followed by a channel compressing convolutional block:</p><formula xml:id="formula_2">M ct " cp}?p s y t T ? y c q} 2 q<label>(5)</label></formula><p>where ? denotes ReLU activation, s . denotes spatially flattening (y m P R k?h?w , ? y m P R k?N , N " h?w, k being the number of predicted classes) , }.} 2 denotes L 2 vector normalization and c is a convolutional block performing channel compression (N?h?w input dimensions, h?w output dimensions). Note that c acts in analogy to a feature regressor estimating parametric alignment <ref type="bibr" target="#b22">[23]</ref>, but here we instead estimate alignment-aware spatial weights.</p><p>Then, the concatenated confidence-weighted features obtained by Eq. 4 are re-weighted by the correlation map:</p><formula xml:id="formula_3">f 2 ct " M ct d f 1 ct . (6)</formula><p>Finally, the obtained feature map f 2 ct is used as input to the shared decoder head to produce the final semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we perform semantic segmentation experiments that shows the advantages of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We perform semantic segmentation experiments on the MF dataset released by the authors of the method MFNet <ref type="bibr" target="#b0">[1]</ref>. To the best of our knowledge, this is the only public dataset with paired color and thermal images and dense annotated masks in the context of urban street scenes. The dataset has eight hand-labeled object classes (car, person, bike, curve, car stop, guardrail, color cone, bump) and one unlabelled background class. This dataset contains 1569 pairs of RGB and thermal images, in which 820 taken at daytime and 749 taken at nighttime. We follow the dataset splitting scheme proposed in <ref type="bibr" target="#b0">[1]</ref>, with the training set consisting of 784 pairs of images. The validation set consists of 392 pairs of images, and the test set contains 392 image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative comparison</head><p>In this section, we compare our model with previous works on color-thermal semantic segmentation: FRRN <ref type="bibr" target="#b23">[24]</ref>, BiSeNet <ref type="bibr" target="#b24">[25]</ref>, DFN <ref type="bibr" target="#b25">[26]</ref>, HRNetV2 <ref type="bibr" target="#b26">[27]</ref>, MFNet <ref type="bibr" target="#b0">[1]</ref>, FuseNet <ref type="bibr" target="#b16">[17]</ref>, DACNN <ref type="bibr" target="#b27">[28]</ref>, RTFNet(50-152) <ref type="bibr" target="#b17">[18]</ref>, FuseSeg-161 <ref type="bibr" target="#b18">[19]</ref> and FEANet <ref type="bibr" target="#b19">[20]</ref>.</p><p>For comparison with previous work, we compute the mean IoU (Intersection Over Union) and the mean accuracy. The IoU (also called Jaccard index) is arguably the most adapted metric to evaluate semantic segmentation, it measures the overlap between predicted and ground truth mask, while the accuracy simply measures the total percentage of correct predictions.</p><p>In <ref type="table">Table 1</ref>, it can be seen that our model improves mean IoU over all classes in the MF dataset. It improves IoU on three classes and notably on the person class, which is arguably one of the most important category for driving perception.</p><p>It should be noted that similarly to other datasets for driving perception, MF dataset is extremely class unbalanced. This implies that the IoU metric is a better indicator of segmentation performance than accuracy for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative comparison</head><p>In <ref type="figure">Fig. 4</ref>, we show a number of semantic segmentation results obtained with our method. It can be seen that segmentation of important details are improved compared to previous work. In the zoomed in parts of columns 4 and 5, it can be seen that our method segments better the pedestrian legs. In the last two columns, it can also be seen that our method is the only one able to retrieve the person and the bike objects, which are hardly visible in the color image due the small size of the object and a saturated highlight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In <ref type="table" target="#tab_1">Table 2</ref>, we consider a number of variants of our model, all of them also based on the DeepLabv3+ backbone. We evaluate the individual contributions of the confidence weighting, the concatenated decoder-level fusion and correlation weighting.</p><p>We also evaluate the model performance in different lighting conditions, only day, and only night. It can be seen that our proposed model obtains the highest mIoU in all lighting conditions, compared to its simpler variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Learned features visualization</head><p>In <ref type="figure">Fig. 5</ref>, we visualize the mean output feature responses of our proposed model before and after semantic segmentation training on RGB-T data. It can be seen that for the given image sample, the shallow second encoder layer gives somewhat similar responses before and after training. On the other hand, the deeper feature response at deeper layer <ref type="table">Table 1</ref>. Comparison with previous work on the MF test set for Acc (%) and IoU (%). We denote 3-channel RGB and 4-channel RGBthermal data respectively as 3c and 4c. Note that mAcc and mIoU are calculated as a mean over all classes, including unlabelled. Bold font highlights the highest score in each column, underlined font highlights the second highest score. We report an improvement of 2% in overall mIoU compared to previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Car Person Bike Curve Carstop Guardrail Cone Bump mAcc mIoU Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU FRRN-4c <ref type="bibr" target="#b23">[24]</ref> 81 seems to vary more in its response after training, by learning to spot at the two pedestrians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Implementation and setup details</head><p>Our method is implemented using the pytorch library <ref type="bibr" target="#b28">[29]</ref> and the torch segmentation models library <ref type="bibr" target="#b29">[30]</ref>. For all experiences with our method, we use ResNet-101 model <ref type="bibr" target="#b21">[22]</ref> pretrained on Imagenet as backbone. We perform concatenation and confidence weighting with features from layer 2 and layer 4 of DeepLabv3+ encoder. Bilinear interpolation is used to downsample or upsample confidence map C m and matching map M ct whenever needed to fit the encoder feature spatial resolutions from layer 2 and layer 4.</p><p>For a fair comparison, we perform similar training setup as compared to previous methods. We adopt mostly the same hyperparameters as employed by the authors of the RTFNet method <ref type="bibr" target="#b17">[18]</ref>. We use random cropping and random flipping as image augmentation during training.</p><p>For optimization, we use SGD with momentum set to 0.9, weight decay set to 0.0005, initial learning rate set to 0.01 and a maximum of 50 training epochs. We employ exponential decaying learning rate scheduler with gamma set to 0.95. As commonly done in the field, we minimize the cross entropy loss between the predicted and the target labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have presented a new feature fusion approach for the task of semantic segmentation of paired color RGB and thermal LWIR images. We proposed Doo-DLeNet, a double DeepLab net which fuses multimodal images taking into account the feature disagreements and feature confidences. We have shown that this approach improves semantic segmentation performance compared to previous works, in particular for the correct segmentation of pedestrians. Future work may be focused on evaluating different model backbones and to refine further segmentation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This research is part of Heliaus project 1 , which has received funding from the ECSEL Joint Undertaking (JU) under grant agreement No 826131. The JU receives support from the European Union's Horizon 2020 research and innovation programme and France, Germany, Ireland, Italy 2 . <ref type="figure">Figure 4</ref>. Qualitative results where the robustness of our method can be appreciated for daytime and nighttime scenes of MF test set split. In the last two columns (original and zoomed in images), it can be seen that our method is the only segmenting the person and the bike objects. <ref type="figure">Figure 5</ref>. Illustration of features learned by color encoder (left two columns) and thermal encoder (right two columns) at encoder layer 2 and layer 4 (second and third rows) before (pretrained on Imagenet) and after training for semantic segmentation. Note that at the deeper level the feature responses seem less invariant to the imaging modality after training. nor the granting authority can be held responsible for them. Completed by local funding from the French region Auvergne Rh?ne Alpes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Summarized illustration of model architecture for weighted fusion semantic segmentation. The RGB and thermal encoderdecoder paths are specialized and both provide a rough segmentation used to compute a spatial confidence score. The confidence and matching correlation is used to weight the RGB and thermal features before fusing them in the final shared decoder. Solid arrows represent concatenation-based skip connections, dashed arrows represent data flow. Weighted concatenated fusion is performed with features coming from two different encoder depths (layer 2 and layer 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example of misalignment between color and thermal images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>RTFNet-152<ref type="bibr" target="#b17">[18]</ref> 93.0 87.4 79.3 70.3 76.8 62.7 60.7 45.3 38.5 29.8 0.0 0.0 45.5 29.1 74.7 55.7 63.1 53.2 FuseSeg-161 [19] 93.1 87.9 81.4 71.7 78.5 64.6 68.4 44.8 29.1 22.7 63.7 6.4 55.8 46.9 66.4 47.9 70.6 54.5 FEANet [20] 93.3 87.8 82.7 71.1 76.7 61.1 65.5 46.5 26.6 22.1 70.8 6.6 66.6 55.3 77.3 48.9 73.2 55.3 Ours 91.7 86.7 81.3 72.2 76.6 62.5 58.9 46.7 36.2 28.0 35.2 5.1 56.9 50.7 74.8 65.8 67.9 57.3</figDesc><table><row><cell></cell><cell>.9 74.7 66.2 60.8 62.8 50.3 41.2 35.0 12.5 11.5 0.0 0.0 37.2 34.0 35.2 34.6 48.5 44.2</cell></row><row><cell>FRRN-3c [24]</cell><cell>80.0 71.2 53.0 46.1 65.1 53.0 34.0 27.1 21.6 19.1 0.0 0.0 34.7 32.5 36.2 30.5 47.1 41.8</cell></row><row><cell cols="2">BiSeNet-4c [25] 89.7 84.1 72.0 63.2 74.1 60.1 45.1 36.7 34.2 25.3 18.2 5.0 47.4 42.2 39.8 35.9 57.7 50.0</cell></row><row><cell cols="2">BiSeNet-3c [25] 90.0 84.5 65.0 54.3 75.0 61.4 32.1 25.7 32.3 26.2 3.2 0.9 49.6 43.3 48.1 40.5 54.9 48.2</cell></row><row><cell>DFN-4c [26]</cell><cell>90.0 84.4 73.2 65.0 75.5 60.9 54.0 40.4 38.9 25.7 10.2 2.7 48.3 42.5 55.8 47.4 60.5 52.0</cell></row><row><cell>DFN-3c [26]</cell><cell>90.7 81.4 67.7 52.8 71.5 57.5 49.2 34.9 35.1 23.8 4.1 1.4 44.2 31.0 54.6 47.5 57.3 47.5</cell></row><row><cell cols="2">HRNet2-4c [27] 92.8 87.6 79.3 71.0 78.3 63.4 59.8 42.5 25.7 19.1 18.8 0.0 56.5 49.8 63.5 44.5 63.7 53.2</cell></row><row><cell cols="2">HRNet2-3c [27] 92.2 86.6 73.1 59.8 74.9 61.3 47.0 33.2 23.8 28.7 7.3 0.0 54.6 47.2 61.5 46.2 60.9 51.3</cell></row><row><cell>MFNet [1]</cell><cell>77.2 65.9 67.0 58.9 53.9 42.9 36.2 29.9 19.1 9.9 0.1 8.5 30.3 25.2 30.0 27.7 45.1 39.7</cell></row><row><cell>FuseNet [17]</cell><cell>81.0 75.6 75.2 66.3 64.5 51.9 51.0 37.8 28.7 15.0 0.0 0.0 31.1 21.4 51.9 45.0 52.4 45.6</cell></row><row><cell>DACNN [28]</cell><cell>85.2 77.0 61.7 53.4 76.0 56.5 40.2 30.9 9.9 29.3 22.8 6.4 32.9 30.1 36.5 32.3 55.1 46.1</cell></row><row><cell cols="2">RTFNet-50 [18] 91.3 86.3 78.2 67.8 71.5 58.2 69.8 43.7 32.1 24.3 13.4 3.6 40.4 26.0 73.5 57.2 62.2 51.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study: comparison of mAcc (%) and mIoU (%) on the day-only, night-only and regular test set of MF dataset for different model variants. For a fair comparison, all of the variants are trained with the same optimizer parameters and are based on DeepLabv3+ with Resnet-101 pretrained backbone. It can be noted the advantage of our approach -simpler model variants based on DeepLabv3+ result in lower mIoUs for all lighting conditions.</figDesc><table><row><cell>Variants</cell><cell>Daytime</cell><cell></cell><cell>Nighttime</cell><cell></cell><cell>Both</cell><cell></cell></row><row><cell></cell><cell>mAcc</cell><cell>mIoU</cell><cell>mAcc</cell><cell>mIoU</cell><cell>mAcc</cell><cell>mIoU</cell></row><row><cell>Only RGB</cell><cell>54.3</cell><cell>44.4</cell><cell>56.5</cell><cell>48.3</cell><cell>61.2</cell><cell>50.7</cell></row><row><cell>Only thermal</cell><cell>48.2</cell><cell>39.0</cell><cell>64.0</cell><cell>51.9</cell><cell>65.6</cell><cell>50.1</cell></row><row><cell>Input fusion -stacked RGBT</cell><cell>48.7</cell><cell>42.8</cell><cell>56.0</cell><cell>51.0</cell><cell>57.1</cell><cell>51.1</cell></row><row><cell>Unweighted fusion</cell><cell>60.7</cell><cell>49.0</cell><cell>61.4</cell><cell>53.6</cell><cell>64.9</cell><cell>54.7</cell></row><row><cell>Only-confidence weighting</cell><cell>63.3</cell><cell>49.6</cell><cell>61.2</cell><cell>54.1</cell><cell>65.4</cell><cell>55.2</cell></row><row><cell>Ours</cell><cell>58.8</cell><cell>50.1</cell><cell>64.1</cell><cell>55.7</cell><cell>67.9</cell><cell>57.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Moreover, we perform a weighted feature fusion which takes into account feature agreement and feature confidence. Our proposed architecture is illustrated inFig. 1. We detail the two weighting strategies in the following subsections.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">European Union's Heliaus project website: https://heliaus. eu/ 2 Disclaimer: Funded by the European Union. Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or ECSEL JU. Neither the European Union</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MFNet: Towards real-time semantic segmentation for autonomous vehicles with multispectral scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5108" to="5115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BDD100K: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2633" to="2642" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Foundation / IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12416" to="12425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection: Benchmark dataset and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised object detection via LWIR/RGB translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Del Rinc?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2020</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="407" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RTFNet: RGB-Thermal Fusion Network for Semantic Segmentation of Urban Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fuseseg: Semantic segmentation of urban scenes based on rgb and thermal data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feanet: Feature-enhanced attention network for rgbthermal real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>abs/2110.08988, 2021. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly aligned cross-modal learning for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="5126" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1611.08323</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno>abs/1808.00897</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno>abs/1804.09337</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highresolution representations for labeling pixels and regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1904.04514</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth-aware CNN for RGB-D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<idno>abs/1803.06791</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yakubovskiy</surname></persName>
		</author>
		<ptr target="https://github.com/qubvel/segmentation_models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
