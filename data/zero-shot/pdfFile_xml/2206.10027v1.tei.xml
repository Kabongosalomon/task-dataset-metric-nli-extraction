<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DNA: Proximal Policy Optimization with a Dual Network Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Aitchison</surname></persName>
							<email>matthew.aitchison@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penny</forename><surname>Sweetser</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DNA: Proximal Policy Optimization with a Dual Network Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores the problem of simultaneously learning a value function and policy in deep actor-critic reinforcement learning models. We find that the common practice of learning these functions jointly is sub-optimal, due to an order-ofmagnitude difference in noise levels between these two tasks. Instead, we show that learning these tasks independently, but with a constrained distillation phase, significantly improves performance. Furthermore, we find that the policy gradient noise levels can be decreased by using a lower variance return estimate. Whereas, the value learning noise level decreases with a lower bias estimate. Together these insights inform an extension to Proximal Policy Optimization we call Dual Network Architecture (DNA), which significantly outperforms its predecessor. DNA also exceeds the performance of the popular Rainbow DQN algorithm on four of the five environments tested, even under more difficult stochastic control settings.</p><p>1 For simplicity, we refer to actor-critic policy gradient as policy gradient, even though some early policy gradient approaches, such as REINFORCE [31], do not learn a value function.</p><p>2 PPO [27], A3C [21], and Muslie [13] all use a joint network when training on vision-based discrete action environments. PPG [9] uses a dual network with an auxiliary task, which we discuss in more detail in Section 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Combining deep neural networks with reinforcement learning has produced impressive results on challenging problems, such as playing Chess <ref type="bibr" target="#b27">[28]</ref>, Atari games <ref type="bibr" target="#b21">[22]</ref>, and robotics tasks <ref type="bibr" target="#b26">[27]</ref>. However, results have bifurcated between two competing approaches: Q-learning-based approaches that require learning an (action conditioned) value estimate and actor-critic policy gradient (AC-PG, or just PG) 1 approaches that learn both a policy and value estimate. PG offers many theoretical advantages over Q-learning, such as natural support for continuous control problems and the ability to learn stochastic policies. However, until recently, PG approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9]</ref>, while strong on continuous control problems, have under-performed Q-learning approaches on complex vision-based problems <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref>, restricting the use of PG in this domain. This paper aims to close the gap between PG and Q-learning methods by showing that the common practice of jointly learning value and policy with a shared representation negatively affects the algorithm's performance. <ref type="bibr" target="#b1">2</ref> We also demonstrate an order-of-magnitude difference in noise levels between these two tasks and argue that this makes these two tasks poorly aligned.</p><p>In light of this result, we introduce a new algorithm, based on Proximal Policy Optimization (PPO) <ref type="bibr" target="#b26">[27]</ref>, called Dual Network Architecture (DNA). We test DNA empirically on a subset of the Arcade Learning Environment (ALE) <ref type="bibr" target="#b4">[5]</ref> called Atari-5. <ref type="bibr" target="#b2">3</ref> , Our results show a strong increase in performance compared to both PPO and another dual network model Phasic Policy Gradient (PPG) <ref type="bibr" target="#b8">[9]</ref>. Our model also outperforms Rainbow DQN <ref type="bibr" target="#b13">[14]</ref> on four of the five games tested, even under more challenging environmental settings.</p><p>We summarize our contributions. First, we provide empirical results showing an order-of-magnitude difference in the noise scale between policy gradient and value learning. Second, we give evidence for the benefit of using low bias value estimates for value learning, but low variance estimates for advantage estimates. Finally, we introduce and justify our dual network constrained distillation algorithm DNA with empirical results on ALE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Work</head><p>Multi-task learning Multi-task learning is the process of learning a shared representation that can be used to solve multiple tasks in the hope that constructive interference between the tasks will lead to better performance <ref type="bibr" target="#b23">[24]</ref>. However, this is not always the case. When tasks are unaligned destructive interference, can instead reduce the performance of the model <ref type="bibr" target="#b31">[32]</ref>. We argue that large differences between the noise scale in value learning and policy gradient make these tasks poorly unaligned.</p><p>TD(?) Return Estimation Policy gradient algorithms often make use of a value estimate as a baseline <ref type="bibr" target="#b20">[21]</ref>, as well as for estimating the value of truncated trajectories <ref type="bibr" target="#b6">[7]</ref>. To better facilitate control over the noise levels for policy and value learning, our work makes use of two different return estimations, both using TD(?) <ref type="bibr" target="#b28">[29]</ref>. Given value estimates V (?) from the value network we define the n-step value estimate for some state s t taken at time t, and their exponentially weighted sum as</p><formula xml:id="formula_0">NSTEP (?,k) (s t ) := k?1 i=0 ? i r t+i + ? k V (s t+k ),<label>(1)</label></formula><formula xml:id="formula_1">TD (?,?) (s t ) := (1 ? ?) ? k=1 ? k?1 NSTEP (?,k) (s t ).<label>(2)</label></formula><p>For values of ? close to 1, more weight is assigned to longer n-step return estimates, and less to shorter ones. There has been a long-standing belief that shorter n-step returns generate more biased estimates, whereas longer n-step estimates, due to summing over many stochastic rewards, have higher variance. <ref type="bibr" target="#b3">4</ref> For a more thorough discussion on this topic see <ref type="bibr" target="#b16">[17]</ref>.</p><p>Noise Scale. Noise scale is a measure of 'signal to noise' in stochastic gradient descent (SGD). While SGD provides unbiased gradient estimates, these estimates are typically very noisy. Each sampled gradient estimate? can be thought of as a sum of the true gradient G and some noise vector ?. A method for estimating the ratio of the magnitude of this implied noise vector to the magnitude of the true gradient was proposed by <ref type="bibr" target="#b19">[20]</ref> who show that their efficient-to-calculate simple noise scale is a good match for the noise scale. They also show that the noise scale provides useful information about the choice of mini-batch size to use when estimating gradients.</p><p>Phasic Policy Gradient. Most similar to our work is Phasic Policy Gradient (PPG) <ref type="bibr" target="#b8">[9]</ref>. Like DNA, PPG has three distinct phases during training and uses two independent networks. However, there are several important distinctions in our work. First, we forgo the large replay buffer, relying instead on learning entirely from recent experience, significantly reducing the memory requirements of our algorithm. Second, we make use of a distillation phase rather than an auxiliary phase. <ref type="bibr" target="#b4">5</ref> Finally, our work reduces gradient noise by using two different return estimators calibrated for the properties of each task. We assess the impact of these differences on the performance of the agent in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Noise Properties of Value Learning and Policy Gradient</head><p>Here we examine the noise properties of the loss functions for policy gradient and value learning and show that policy gradient has a much higher noise level than value learning. This implies different challenges in the optimization problem. Specifically, that policy should be learned with a larger mini-batch size than value learning and that return estimates should be adapted appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivating Example</head><p>To motivate our investigation into noise levels, we consider the task of learning two independent functions F 1 (x) := sin(5x) + N(0, ? 2 1 ) and F 2 (x) := cos(5x) + N(0, ? 2 2 ), where N(?, ? 2 ) is Gaussian noise with mean ? and standard deviation ?, over the domain [??, ?]. We fix ? 2 = 1, and vary ? 1 on a log scale from 0.1 to 100. We trained two 3-layer multi-layer perceptions (MLPs) on this problem. <ref type="bibr" target="#b5">6</ref> The first was a joint model, using a first hidden layer of 1024 units and a second hidden layer of 2048 units, followed by two output heads. Our second model was a dual network consisting of two independent MLPs with 1024 units on each hidden layer. <ref type="bibr" target="#b6">7</ref> All models used ReLU activations in between linear layers, and error was measured as mean-squared-error (MSE) between the predicted value and the noise free true value. The results are presented in <ref type="figure">Figure 1</ref>. At low noise levels, the tasks did not interfere, but as the noise of the first task increased, performance on the second task eventually degraded for the joint network, but not the dual network. While simple, this experiment demonstrates the impact one noisy task can have on another when learned jointly by a deep neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSE</head><p>Dual LT2 Joint LT2 <ref type="figure">Figure 1</ref>: A depiction of the problem of destructive interference on a toy problem. L T 1 and L T 2 refer to the mean-squared-error on the first and second task respectively.Shading indicates standard error over 100 seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noise Scale in Reinforcement Learning</head><p>To assess the differences in noise levels in our experiments, we measure the gradient noise scale of the policy gradient and value loss gradient. We did this by learning an estimate of the noise scale, developed by <ref type="bibr" target="#b19">[20]</ref> called simple noise scale defined as</p><formula xml:id="formula_2">B simple := tr(?) | G| 2 ,<label>(3)</label></formula><p>where ? is the gradient covariant matrix, and G is the true gradient if the entire batch was used. For convenience we also use the notation ? := B simple , which can interpreted as the ratio of the length of the implied noise vector to the length of the true gradient vector. That is, a kind of noise-to-signal ratio. As <ref type="bibr" target="#b19">[20]</ref> have shown, an unbiased estimate of B simple can be found efficiently by generating gradient estimates G Bsmall using a small mini-batch of size B small as well as the gradient G Bbig using a large mini-batch of size B big , then calculating <ref type="bibr" target="#b5">6</ref> It is worth noting that we did not find the same result when using shallow networks. That is, dual and joint models performed similarly when using a 2-layer MLP. <ref type="bibr" target="#b6">7</ref> This number of hidden units was chosen so that both models had the same number of parameters.  To evaluate the noise scale of value learning and policy gradient we trained our dual network model (described fully in section 4) on the three Atari games from the Atari-3 validation set. <ref type="bibr" target="#b7">8</ref> Like <ref type="bibr" target="#b19">[20]</ref>, we found it necessary to smooth out noise by maintaining an exponential moving average of | G Bbig | 2 . We used B small = 16 and B big = 16, 384 in all our experiments.</p><formula xml:id="formula_3">|G| 2 := 1 B big ? B small (B big | G Bbig | 2 ? B small | G Bsmall | 2 )<label>(4)</label></formula><formula xml:id="formula_4">S := 1 1/B small ? 1/B big (| G Bsmall | 2 ? | G Bbig | 2 ),<label>(5)</label></formula><p>The results are presented in <ref type="figure" target="#fig_2">Figure 2</ref>. We found that policy noise (? ? ) reduced by about half during training and that value noise (? v ) remained constant. Both noise levels varied very little between the three environments. However, a large variation was observed between the value loss and policy gradient, consistent between environments. The noise levels, measured at the end of training and averaged over all three environments, were found to be ? ? = 220.5 and ? V = 17.5, representing a 12.6 times difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dual Network Architecture</head><p>Based on our experimental results in Section 3 we propose an architecture which takes into account the large difference in noise levels between the two tasks. This architecture consists of three improvements to PPO. First, to reduce negative interference from the noisy policy gradient, policy and value should be learned by independent networks with different hyperparameters (e.g. training epochs and mini-batch size). Second, the variance/bias trade-off in the return estimations should be calibrated to the properties of each task. Finally, a constrained distillation phase should account for any constructive interference between the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Independent Networks</head><p>Important to DNA is the use of a dual network architecture. <ref type="bibr" target="#b8">9</ref> Not only does this setup allow for learning of the value and policy without destructive interference, it also enables a specialized set of hyperparameters to be calibrated for the distinct tasks of policy and value learning. Specifically, the calibration of the mini-batch size, which it has been suggested is best set roughly proportional to ? <ref type="bibr" target="#b19">[20]</ref>. The policy network outputs a policy ? and a value estimate V ? , whereas the value network only outputs a value V V , as depicted in <ref type="figure" target="#fig_3">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoupled Return Estimation</head><p>To better facilitate control over the noise levels for policy and value learning, DNA makes use of two different return estimations, both using TD(?) <ref type="bibr" target="#b28">[29]</ref> as follows</p><formula xml:id="formula_5">V targ (s t ) := TD (?,? V ) (s t ),<label>(6)</label></formula><formula xml:id="formula_6">V adv (s t ) := TD (?,??) (s t ),<label>(7)</label></formula><p>where ? V and ? ? are hyperparameters controlling the variance / bias trade-off of each estimate. The V targ estimates are used as targets for training the value function, whereas V adv are used for advantage estimates used for policy gradient estimates given b?</p><formula xml:id="formula_7">A t := V adv (s t ) ? V (s t ).<label>(8)</label></formula><p>This formulation of the advantages is equivalent to the general advantage estimation (GAE) <ref type="bibr" target="#b25">[26]</ref>, that is, for any</p><formula xml:id="formula_8">? ? [0..1], ? ? [0..1) we have, 10 A GAE(?,?) t = TD (?,?) (s t ) ? V (s t ).<label>(9)</label></formula><p>Because of this, it is common when using PPO to generate value targets, V targ , for the value function by adding the network's value estimates to the (unnormalized) advantages, implicitly settings ? V = ? ? . <ref type="bibr" target="#b10">11</ref> We hypothesize that due to bootstrapping, estimates used for V targ benefit from being low bias, conversely, because estimates used for advantage estimation contribute to the noise of the policy gradient, they would benefit from being low variance. This suggests ? ? &lt; ? V , a thought which we test empirically in section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Distillation</head><p>Because value learning is a simpler task in terms of noise level, it makes sense to try to transfer some of the knowledge learned by the value network to the policy network. If the value network learns to identify important features in the environment, we would like the policy network to identify those features also. To facilitate this, we employ a constrained distillation update <ref type="bibr" target="#b14">[15]</ref>. Distillation between two identical networks has the property that the student's target function since an identical network generated it, is guaranteed to be in the model's solution space. Also, because the distillation targets are deterministic, <ref type="bibr" target="#b11">12</ref> the process is also very low noise, which we verify in Appendix D.</p><p>We contrast this with the auxiliary update of PPG which trains both the value function and the policy on an auxiliary task. This auxiliary task equates to learning 1</p><formula xml:id="formula_9">N N i=0 V ??i targ where V ??i</formula><p>targ is a value estimate for the policy from i updates prior. For quickly changing policies, and large N , these updates may cause tension with the value estimates of the current policy. 13 <ref type="bibr" target="#b9">10</ref> A proof for this claim is provided in Appendix E <ref type="bibr" target="#b10">11</ref> For example, see line 65 of the popular baselines implementation of PPO. https://github.com/openai/ baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/ppo2/runner.py <ref type="bibr" target="#b11">12</ref> That is to say, Vtarg(st) are samples drawn from the random variable R(st), where R is a (noisy) return estimator, whereas V (st) is a deterministic function that produces an estimate of E[R(st)] <ref type="bibr" target="#b12">13</ref> A better way to do auxiliary tasks would be to train a separate value network output head, dedicated to the auxiliary task. This way the agent could learn the value of the current policy, along with the moving average value. We believe, however, that the better solution to overfitting the value function to recent data is simply to train less. An idea which we discuss further in Section 7.</p><p>We use V V (s) as the distillation targets, with the input states s being taken from the current rollout. However, unlike policy and value training, distillation state inputs need not be generated on-policy. We explored other distillation targets, covered in Appendix J, but found V V (s) to be the best of those tried. Distillation is performed, like PPG's auxiliary task, using mean squared error, and under a soft constraint on the policy network's policy, specifically</p><formula xml:id="formula_10">L D t (?) :=? t (V ? (s t ) ? V V (s t )) 2 + ? ?? t [KL(? old (?|s t ), ?(?|s t ))<label>(10)</label></formula><p>where ? is the policy constraint coefficient, and ? old is a copy of the policy before the distillation update. During updates gradients are only propagated through the policy network, and not the value network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>Like Phasic Policy Gradient (PPG) <ref type="bibr" target="#b8">[9]</ref>, DNA splits training into three distinct phases, but unlike PPG, rather than using a large replay buffer, we perform all updates on-policy on the current batch of rollout data. Each of the three phases optimizes a single objective, for some number of epochs, using its own optimizer, with a unique set of hyperparameters. The optimization objective for the policy network of DNA is the clipped surrogate object from PPO <ref type="bibr" target="#b26">[27]</ref> including the entropy bonus</p><formula xml:id="formula_11">L CLIP t :=? t min(? t (?)? t , clip(? t (?), 1 ? , 1 + )? t ) + c eb ? S[?(s t )]<label>(11)</label></formula><p>where S is the entropy in nats, ? t is the ratio ?(at|st) ?old(at|st) at time t, is the clipping coefficient, and c eb is the entropy bonus coefficient. For value loss we use the squared-error loss,</p><formula xml:id="formula_12">L V F t :=? t (V V (s t ) ? V targ (s t )) 2 .<label>(12)</label></formula><p>In summary, the DNA algorithm separates the tasks of policy learning and value learning into a network designed to handle the high noise of policy learning and a separate network designed to handle the lower noise task of value learning. Knowledge from the value learning network is transferred to the policy network through a separate constrained distillation phase, which allows for constructive interference between the learning tasks while minimizing the destructive. We formalize the algorithm as follows.</p><p>Algorithm 1 Proximal Policy Optimization with Dual Network Architecture 1: procedure PPO-DNA 2: 3:</p><p>for t = 1 to N do 4:</p><formula xml:id="formula_13">for i = 1 to A do 5:</formula><p>Run policy ? t in environment i for T timesteps 6:</p><formula xml:id="formula_14">Compute V targ ? TD (?,? V )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Compute? ? TD (?,??) ? V adv 8:</p><formula xml:id="formula_15">for i = 1 to E ? do 9:</formula><p>Optimize L CLIP wrt ? ? 10:</p><formula xml:id="formula_16">for i = 1 to E V do 11: Optimize L V F wrt ? V 12:</formula><p>? old ? ? t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>for i = 1 to E D do <ref type="bibr">14:</ref> Optimize L D wrt ? ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>To evaluate the performance of our algorithm, we used the Atari-5 benchmark. Scores on this subset, when properly weighted, correlate well with the median score performance of an algorithm if it had 6 been run on all 57-games. In all cases, we fit hyperparameters to the 3-game validation set and only used the 5-game test set for final evaluations. To verify the statistical significance of our results, we performed multiple seeded runs.</p><p>We opted for the more difficult stochastic ALE settings recommended as best practice by <ref type="bibr" target="#b18">[19]</ref>. However, to better understand our results in the context of prior work, we also provide detailed results under the simpler deterministic settings in Appendix F, and additionally provide for reference a single seed evaluation on the full 57-game set in Appendix K. Unless otherwise specified, agents were scored according to their average performance over the previously completed 100-episodes at the end of training.</p><p>A coarse hyperparameter sweep found initial hyperparameters for our model on the Atari-3 validation set. Notably, we found the optimal mini-batch size for policy and distillation to be the minimum we tested (256), while the optimal mini-batch size for policy was the largest tested (2048). For optimization, we used Adam <ref type="bibr" target="#b17">[18]</ref>, over the standard 200 million frames. Full hyperparameter details for our experiments are given in Appendix B.</p><p>In order to understand the impact of the hyperparameters introduced by our algorithm, namely E ? , E V , E D and ? ? , ? V , we performed the following experiments on the Atari-3 validation set. We started by setting <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, and selected the best E V . We then searched over E ? and E D while keeping the other two parameters constant. We also performed a similar experiment on the impact of ? V and ? ? by setting ? V to 0.95 then sweeping across ? ? ? [0.6, 0.8, 0.9, 0.95, 0.975], then setting ? V to the best ? ? and repeating. This process allowed us to verify if the best settings occur at ? V = ? ? or at ? V = ? ? . During our ? tuning, we also recorded noise levels for one of the seeds, the results of which we discuss in Section 6.2.</p><formula xml:id="formula_17">E ? = E V = E D = 2, then searched over E V ? [1, 2,</formula><p>We evaluated our algorithm DNA, against PPO, and PPG. All models used the 'NatureCNN' encoder from <ref type="bibr" target="#b21">[22]</ref>. However, because DNA and PPG use two networks and thus twice the parameters, we doubled the number of channels of the PPO encoder which we refer to as PPO (2x) and which is very similar to the encoder used by <ref type="bibr" target="#b2">[3]</ref>. All models have have approximately 3.5 million parameters in total. For fairness we repeated the same sweep on PPO for the number of training epochs (E PPO ) and ? used for GAE (? PPO ). To verify that our settings did not inadvertently degrade the performance of PPO we also tested against the settings used by <ref type="bibr" target="#b26">[27]</ref>, and refer to this as PPO (original). These results are included in the ablation study in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Main Experimental Results</head><p>In this section we present the results of our main study comparing PPO to DNA and PPG on Atari-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Impact of Training Epochs</head><p>We found that using less than three policy epochs for DNA dramatically increased the performance of the agent while also decreasing the computation required to train the agent ( <ref type="figure" target="#fig_5">Figure 4</ref>). We also found that DNA was robust to the choice of value and distillation epochs but that overtraining on value (E V = 4) marginally decreased performance. This was not the case for distillation updates. The distillation phase demonstrated some benefit as indicated by the E D = 0 results under-perform the others. We selected E ? = 2, E V = 1, E D = 2 as the optimal settings and used these for the remainder of the experiments. These settings require a total of four updates for the policy network, but only one update for the value network. The training curves also suggest that DNA may benefit from training beyond the standard 200-million frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Return Estimation</head><p>We found that the choice of ? ? made a large difference to the performance of our agent. As was hypothesised the optimal settings for ? ? differed to those of ? V with ? ? preferring a lower variance, higher bias value of 0.8, while the value targets, ? V preferred a lower bias, higher variance setting of 0.95. Our results are presented in <ref type="figure" target="#fig_6">Figure 5</ref>. Notable is that the non-homogeneous setting ? V = 0.95, ? ? = 0.8 outperformed both homogeneous choices ? ? = ? V = 0.8 and ? ? = ? V = 0.95. We found, as expected, that noise scale can be reduced by selecting a low value for ? ? . However, against our expectations, setting ? V lower actually increased, not decreased the    noise level of the value learning task. Once we discovered this we ran addition experiments for a broader choice of ? V , and include the results in <ref type="figure" target="#fig_6">Figure 5</ref>.</p><formula xml:id="formula_18">V = 0.8 V = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparision to PPO and PPG</head><p>We found DNA to outperform both PPO and PPG on the Atari-5 dataset by a wide margin ( <ref type="figure" target="#fig_7">Figure 6</ref>). We give Atari-5 summary scores, as well as training plots for each individual game. We also include references for Rainbow DQN, although note that these scores were generated under the simpler deterministic settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Reduced Training as an Alternative to Experience Replay In this subsection, we discuss the surprising result that less training, both on policy and value updates, typically improves the agent's performance. One explanation for this could be, as <ref type="bibr" target="#b8">[9]</ref> have suggested, that in the value case, this might be due to overfitting to recent experience. <ref type="bibr" target="#b13">14</ref> In terms of policy updates, where the largest performance degradation was observed, this could be due to increased training epochs causing large changes in policy, which may suggest a better constraint than PPO's clipping is required. <ref type="bibr" target="#b14">15</ref> That being said, limiting the training epochs to one or two provides straightforward solution to the problem while having the additional benefit of decreasing the computational resources required to train an agent. Even in Deep Q-learning, others have found that, beyond some point, increasing the number of times an example is processed has a negative effect unless the learning rate is proportionally reduced <ref type="bibr" target="#b0">[1]</ref>.</p><p>Choice of ? Our results have shown strong evidence for the benefit of selecting ? ? &lt; ? V . As expected, picking lower values of ? ? reduces the policy gradient noise. However, counterintuitively, noise on value learning reduces with higher choices of ? V . That is, policy learning prefers low variance, but value learning prefers low bias. This result may explain why clipping, which introduces bias, performs poorly on value learning but quite well on policy learning. <ref type="bibr" target="#b15">16</ref> This result also suggests that an even higher choice of ? V than the 0.95 we used in our experiments may be appropriate.</p><p>Performance of PPG on Atari Our algorithms shares a lot in common with the PPG algorithm, which has shown strong results on the Procgen <ref type="bibr" target="#b7">[8]</ref> environment. However, PPG performed quite poorly on the Atari games tested, only marginally outperforming the PPO reference. We suspect that this is due to the auxiliary task involving learning stale values estimates. We verify in the Appendix C that the poor performance was not due to hyperparameter differences.</p><p>Performance of PPO on Atari Perhaps the most surprising result of our study was that PPO could perform well on Atari by simply increasing the mini-batch size, and reducing the training epochs down to 1. These settings resulted in an algorithm that was 4-times faster to train than the settings used by <ref type="bibr" target="#b26">[27]</ref>, and produce much stronger results (Appendix C).</p><p>Limitations Our paper's primary focus was improving PG results on discrete action vision-based problems. We do not cover continuous control problems and leave this for future work.</p><p>Broader Impact In our work, we have shown that deep actor-critic reinforcement learning models can effectively solve complex vision-based, discrete-action environments, even without the use of large replay buffers. Surprisingly this can be achieved using less not more policy and value network updates. Because Q-learning approaches generally produce deterministic policies, algorithms based on them may 'lock in' to a decision based on minimal differences in outcome. This is not always the case with stochastic policies, which are able to randomly split over actions of roughly equal quality. This may lead to fairer outcomes. We do not foresee any direct negative societal impacts from this research.</p><p>Future work Our algorithm is already highly competitive with Rainbow DQN, and could likely be extended with orthogonal improvements such as adding recurrence <ref type="bibr" target="#b11">[12]</ref>, distributional value learning <ref type="bibr" target="#b3">[4]</ref>, residual encoders <ref type="bibr" target="#b10">[11]</ref>, better exploration bonuses <ref type="bibr" target="#b5">[6]</ref>, and making use of a learned model <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper, we have highlighted noise level as a key difference between policy and value learning.</p><p>We have introduced an algorithm that accounts for this order-of-magnitude difference by limiting negative interference through two independent networks for value and policy but retaining constructive interference through a constrained distillation process. Furthermore, we have shown that the variance/bias tradeoff differs for value learning and policy gradient and that return estimation should cater for this. Together these changes result in a novel algorithm, DNA, that outperforms its predecessor PPO, and even surpasses the popular Q-learning approach Rainbow DQN, while under more challenging environmental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>The implementation details often matter with reinforcement learning <ref type="bibr" target="#b9">[10]</ref>. For this reason, full source code is provided in the supplementary material for all experiments. This section details some of the more important implementation decisions made, most of which match those found in <ref type="bibr" target="#b1">[2]</ref>. We ran all experiments (PPO, DNA, PPG) using these same implementation choices to confirm that performance differences were not due differences in implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward normalization</head><p>We normalized rewards such that returns have unit variance, as is common place with PPO. <ref type="bibr" target="#b16">17</ref> Even though we used two separate models, and therefore had less reason to balance the magnitude of the value and policy loss, we still kept reward normalization so that distillation loss, and its interaction with the policy constraint, would be of a similar scale between environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation normalization</head><p>We also adopted observation normalization. Each state s was normalized by s = clip((s ? s ? )/s ? , ?3, 3) where s ? was the element wide mean over states seen by the agent so far, and s ? was the standard deviation. Normalization constants were shared between the policy and value networks.</p><p>Repeat action penalty We found that our policy would occasionally get stuck repeating a single action, causing the game to freeze until the time limit occurred. This could occur if the agent mistakenly thought it would get a slightly negative score for continuing and therefore acted to postpone that reward as long as possible. <ref type="bibr" target="#b17">18</ref> Q-learning algorithms, such as DQN, which make use of -greedy, do not experience this problem so long as &gt; 0. To address this, we implemented a reward penalty of 0.25 (normalized) if the agent repeated the same action more than 100 times. We leave finding a better solution to this problem for future work. <ref type="bibr" target="#b18">19</ref> Integrating time and action information We added a watermark to the least recent frame in the 4-frame stack indicating the proportion of time which has occurred as a 'progress bar' as well as markers on each frame indicating which action the agent selected on the previous frame. Inclusion of time is necessary to avoid violation of the Markovian property in time-limited environments <ref type="bibr" target="#b22">[23]</ref>. Action indicators were added to allow the agent to understand when it repeated the same action multiple times, which is not always possible to determine from the state itself (due to multiple actions causing identical outcomes).</p><p>Warmup / desyncing environments When initializing our environments, we ran each of the parallel 128 environments for t ? U (1, 1000) interactions with actions selected uniformly over the action space. This served two purposes: to provide initial normalization parameters and to desynchronize the environments. We found that if we did not do this, agents would terminate around the same time in some environments, causing parallel rollouts to become correlated. While we found this made very little difference to the agent's performance, it removed oscillating scoring artefacts found early in training in some environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters and Environmental Settings</head><p>We selected initial hyperparameters from an initial coarse hyperparameter search on the Atari-3 validation set. In some cases, where only small differences in performances were observed, we prefered settings that had been used in previous papers or were likely to be more efficient. For example, our search found a mini-batch size of 256 optimal for value and distil updates. However, we selected 512 as the difference was not large and found this a more computationally efficient mini-batch size when trained on a GPU. A full list of hyperparameters are given in <ref type="table">Table 1</ref>. We also provide hyperparameters for our PPG experiments in <ref type="table">Table 2</ref>. The environmental settings we used are given in <ref type="table">Table 3</ref>.  <ref type="table">Table 3</ref>: Environmental Settings used in experiments. 'hard' mode settings follow best practice by <ref type="bibr" target="#b18">[19]</ref>, whereas 'easy' mode correspond to those used in the Rainbow DQN paper <ref type="bibr" target="#b13">[14]</ref>, with the exception that we do not apply the domain specific reward clipping modification. Training frames includes skipped frames, that is our agents performed 50M interactions with the environment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Study</head><p>This appendix quantifies the performance contribution of several important components of DNA. We considered the following changes:</p><p>? DNA (mb=512) Our analysis of noise levels suggested a much larger mini-batch size for policy updates than for value updates. We measure the impact of this change by evaluating DNA with mini-batch sizes for all three training objectives set to 512. ? DNA (no distil) Validation scores indicated an improvement in performance using distillation over no distillation. We verify that this result is replicated on our test set. ? DNA (? V = ? ? = 0.95) We measured the impact of using non-homogeneous values for ? V and ? ? by testing with these both set to 0.95.</p><p>We also include the reference run from the main study and a "PPO (basic)" run, which was a single network with the 'Nature-CNN' encoder, and a mini-batch size of 512, and can be thought of as DNA with all novel components turned off. Results are provided in <ref type="figure" target="#fig_9">Figure 7</ref>, along with the score, and performance regression in <ref type="table" target="#tab_2">Table 4</ref>. We found non-homogeneous values for ? V and ? ? , and a larger policy mini-batch size, to be the most significant changes, with distillation also providing some benefit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Noise scale for distillation learning</head><p>In this appendix, we present the noise scale results for distillation learning. We expected distillation to have a low noise level because the targets are drawn from the relatively noise-free value network estimations and not from the higher variance value targets. Our results in <ref type="figure" target="#fig_11">Figure 8</ref> confirm this hypothesis. Of note is that the difference in noise scale between environments was much more significant for distillation loss than it was for the value or policy loss. These results indicate that distillation may benefit from smaller mini-batch sizes. However, the decreased efficiency of processing these smaller batches on GPU hardware may out-weight any potential advantages.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of relationship between GAE and TD(?)</head><p>We provide a proof that calculating the General Advantage Estimate <ref type="bibr" target="#b25">[26]</ref> is equivalent to calculating TD(?) returns, then subtracting the state value estimate. Concretely, for some ? ? [0..1) and ? ? [0..1] we have from <ref type="bibr" target="#b25">[26]</ref> A (k) t</p><formula xml:id="formula_19">:= k?1 l=0 ? l ? V t+l = ?V (s t ) + k?1 i=0 ? i r t+i + ? k V (s t+k )<label>(13)</label></formula><p>= ?V (s t ) + NSTEP (?,k) (s t ).</p><p>The GAE advantage estimate is defined as an exponentially weighted sum of these A (k) 's as follow?</p><formula xml:id="formula_21">A GAE(?,?) t := ? i=0 (1 ? ?)? i? (i+1) t (15) = (1 ? ?) ? i=0 ? i (?V (s t ) + NSTEP (i+1) (s t ))<label>(16)</label></formula><formula xml:id="formula_22">= (1 ? ?) ? i=0 ? i (?V (s t )) + (1 ? ?) ? i=0 ? i NSTEP (i) (s t )<label>(17)</label></formula><formula xml:id="formula_23">= ?V (s t ) + TD (?,?) (s t )<label>(18)</label></formula><p>as required.</p><p>F Results under Rainbow DQN style environmental settings.</p><p>In our main study, we compared DNA to PPO on the Atari-5 benchmark under the recommended settings given by <ref type="bibr" target="#b18">[19]</ref>. Many prior results have been generated using the simpler, non-stochastic version of the environments and domain-specific knowledge, such as loss of life as a terminal state and a custom clipped reward modifier. We evaluated DNA and our implementation of PPO here in the simplified environments and found a modest improvement under these settings. We provide these results for better comparison against previous works.</p><p>In these experiments we did not use reward clipping. Reward clipping is a domain-specific reward modification that reduces all positive rewards to +1 and all negative rewards to -1. We were concerned that clipping rewards would bias the algorithm, as the agent is optimizing a reward structure that may not match the true rewards of the game. It might be that reward clipping 20 may be necessary for Deep Q-learning approaches to reduce high variance returns. However, we have not found this an advantage over reward normalization for PPO or DNA.</p><p>We found that DNA outperformed Rainbow DQN on all five environments, and obtained a better Atari-5 score after just 49M environmental steps. Proximal Policy Optimization, with a single policy update, also outperformed Rainbow DQN on this task.  <ref type="figure">Figure 9</ref>: Results on the Atari-5 benchmark, with 'easy' environmental settings matched to those used by Rainbow DQN <ref type="bibr" target="#b13">[14]</ref>. Shaded regions indicate standard error over three seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Supplementary Results</head><p>We investigated some supplementary questions. Specifically, we wanted to validate that the performance improvement of DNA compared to PPO and PPG did not result solely from the hyperparameter choices. We, therefore, evaluated PPO and PPG under a range of alternative hyperparameters on Atari-5 and note the results here.  We tested a variety of alternatives for PPO as described below. We found that none of the alternative settings resulted in improved performance <ref type="figure" target="#fig_14">(Figure 10</ref> left).</p><p>? PPO (E ? = 2) Our tuning process found a single epoch optimal for PPO, but two epochs optimal for DNA, would PPO have performed better if it was given two epochs instead of one? We found that while initial performance was stronger, this change ultimately regressed the performance.</p><p>? PPO (? V = 0.95 ? ? = 0.8) Using separate return estimations for advantages and value targets does not require a dual network setup. Therefore we check if the performance of PPO can be improved by using the non-homogeneous ? values used in our DNA experiment. We found these settings regressed performance.</p><p>? PPO (1x) In our experiments DNA and PPO used different network architectures (PPO used twice as many channels). It is possible that increasing the parameters made training more difficult for PPO, therefore we tested PPO with the standard NatureCNN encoder here. We found this change regressed performance.</p><p>? PPO (orig) Our settings for PPO deviated from those used by <ref type="bibr" target="#b26">[27]</ref>. For completeness we also provide results using these settings. We found that while the these original settings performed well initially, they eventually unperformed the reference by a large margin. We also note that these settings took much longer to train (see Appendix H).</p><p>We also evaluated PPG with alternative settings <ref type="figure" target="#fig_14">(Figure 10</ref> right).</p><p>? PPG (tuned) In our main experiment we evaluated PPG using E ? = 1, E V = 1, E aux = 6 taken from <ref type="bibr" target="#b8">[9]</ref>. These differ significantly from those used for DNA. We therefore reevaluated PPG using E ? = 2, E V = 1, E aux = 2 which more closely match the settings used by DNA. We found this change had little impact on the performance of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Training Time</head><p>DNA makes use of two independent networks and three training phases, which may have a negative effect on training time. We examine this here. All times are approximate and for comparative purposes only. Rainbow DQN times are on different hardware and using a different codebase.</p><p>Our implementation of DNA ran very quickly and is faster than PPO when PPO is configured as per <ref type="bibr" target="#b26">[27]</ref>. This is due to our use of more parallel agents (128 vs 8) coupled with a larger mini-batch size. We give approximate training times in <ref type="table" target="#tab_3">Table 5</ref> which were taken from a 24-core machine with four 2080-TIs. We found we could train 8 DNA models in 8-hours on our 4-GPU machine, giving a rate of 4 GPU hours per game learned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Tuning for Proximal Policy Optimization</head><p>We repeated the same hyperparameter sweep on PPO as we did for DNA for a fair comparison. We found that, like DNA, PPO also benefited greatly from reduced epochs during training. We present the results here for 1,2,3 and 4 epochs, along with a search over the choice for ? used in the General Advantage Estimate. Results from the main study used the best performing model, which was found to be ? = 0.95, and E ? = 1. These hyperparameters differ from those used by <ref type="bibr" target="#b26">[27]</ref>, and achieve a significant improvement in performance with less computation (see Appendix F, H).</p><p>We found the performance of PPO to plateau after a point in training which decreases with the number of training epochs. This is consistent with <ref type="bibr" target="#b26">[27]</ref>, who trained for 40M frames and whose results show performance levelling off around 20M. However, when fewer epochs are used, performance continues to increase after these points. We also performed some quick experiments using partial epochs (0.5, 0.75) but found these under-performed a single epoch and have not included the results here. We also found that the single network setup of PPO did not benefit as much as DNA from tuning the ? parameter and that the commonly used ? = 0.95 was optimal for our training set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Distillation Targets</head><p>We evaluated two distillation targets for our distillation phase: a random projection into R 16 , and the value networks value estimates as well as a third strategy, feature matching. <ref type="bibr" target="#b21">22</ref> Random projection and value estimate outperformed the no distillation baseline, and feature matching underperformed the baseline. Results are from a single seed on the Atari-3 validation dataset. In all cases, distillation was trained on two passes of trajectories sampled from the rollout and used a policy constraint.  <ref type="figure" target="#fig_2">Figure 12</ref>: Performance of the four distillation strategies trailed. Only a single seed was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Results on ALE</head><p>Our main study used Atari-5 to allow enough time for seeded runs and because it provides an established training/test split. Because Atari-5 is a new benchmark, we thought it important to validate our algorithm's performance on the full 57-game suite. We, therefore, provide results for both PPO (2x) and DNA on all 57 games in the ALE using both the 'easy' settings similar to <ref type="bibr" target="#b13">[14]</ref> and the more difficult settings used in our main study.</p><p>We measured the median score as the median human-normalized score over the past 100-episodes and report the final median scores as the average for this measure over the last 10M frames (5% of training frames). Individual game scores are also reported as the average over the final 10M frames.   We found, under both the hard and the easy settings, DNA outperformed PPO by a wide margin (see <ref type="table" target="#tab_4">Table 6</ref>). DNA also outperformed Rainbow DQN on the easy settings after just 85.5M training frames <ref type="figure" target="#fig_3">(Figure 13</ref>). Training plots are provided in <ref type="figure" target="#fig_5">Figures 14, 15</ref>. Results for each game are given in <ref type="table" target="#tab_5">Tables 7, 8</ref>. Most surprising is that when trained with only a single policy epoch, a larger batch size, and more parallel agents, PPO becomes comparable with Rainbow DQN on the easy settings while arguably being a much simpler algorithm, while also being 80-times faster to train.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Noise scale for the two learning objectives.For clarity all values are further smoothed using an exponential rolling average (with non-smoothed values presented in a lighter shade). Smoothing may introduce bias, as while |G| 2 and S are unbiased estimators, their ratio may not be. where E[|G| 2 ] = | G| 2 and E[S] = tr(?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of DNA (left) compared to a single network setup (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Training curves for DNA with various epochs. The DNA algorithm is most sensitive to the number of policy epochs, and is optimal on our validation set with E ? = 2. Applying policy updates 3 or 4 times significantly reduces the agents performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Performance of DNA across a range of ? values for return estimations. Left: The optimal settings for ? V and ? ? differed. Shading indicates standard error over 5 seeds. Mid/right: ? ? increased with ? ? where as ? V decreased with ? V . Shading indicates standard deviation over the environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Left: Results on the Atari-5 benchmark. DNA outperforms PPO by a wide margin, and even exceeds the Rainbow algorithm despite the more difficult settings used in our experiment. Right: Individual training curves for each of the five games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Training curves for the ablation studies. Shading indicates standard error over 3 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Noise level of the three tasks, policy learning, value learning, and distillation, over the three games in our validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Training curves for the supplementary studies. Shading indicates standard error over 3 seeds. Reference runs are from the main study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Training Curves for Proximal Policy Optimization over various epoch counts, and settings for ? GAE . Shaded area indicates standard error over three seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 :</head><label>13</label><figDesc>Median Score over all 57 games for DNA and PPO with 2x parameters. Left: performance under easy settings. DNA matches Rainbow DQN performance after just 86.5M frames. Right: performance under hard settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Training plots for DNA on all 57 games in the Atari-57 benchmark under 'hard' settings. Results are from a single seed, with smoothed results in bold, and non-smoothed results shown faded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 15 :</head><label>15</label><figDesc>Training plots for DNA on all 57 games in the Atari-57 benchmark under 'easy' settings. Results are from a single seed, with smoothed results in bold, and non-smoothed results shown faded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :Table 2 :</head><label>12</label><figDesc>Summary of hyperparameters found in coarse hyperparameter search. Epoch counts, and ? * values were further fine-tuned as detailed in the main study. For PPO orig , ? was linearly annealed over training from<ref type="bibr" target="#b0">[1,</ref> 0]. Summary of hyperparameters used in the Phasic Policy Gradient experiments. All other hyperparameters were set according to the DNA settings inTable 1.</figDesc><table><row><cell>Setting</cell><cell>DNA</cell><cell>PPO</cell><cell>PPO orig</cell></row><row><cell>Entropy bonus (c eb )</cell><cell>0.001</cell><cell>0.001</cell><cell>? ? 0.001</cell></row><row><cell>Rollout horizon (N)</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Parallel agents (A)</cell><cell>128</cell><cell>128</cell><cell>8</cell></row><row><cell>PPO epsilon</cell><cell>0.2</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>Discount gamma (?)</cell><cell>0.999</cell><cell>0.999</cell><cell>0.99</cell></row><row><cell>Learning Rate</cell><cell cols="3">2.5 ? 10 ?4 2.5 ? 10 ?4 ? ? 2.5 ? 10 ?4</cell></row><row><cell>Policy lambda (? ? )</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell></row><row><cell>Value lambda (? V )</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell></row><row><cell>Policy epochs (E ? )</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell>Value epochs (E V )</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>Distil epochs (E D )</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>Distil beta (?)</cell><cell>1.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Policy mini-batch size</cell><cell>2048</cell><cell>2048</cell><cell>256</cell></row><row><cell>Value mini-batch size</cell><cell>512</cell><cell>-</cell><cell>-</cell></row><row><cell>Distil mini-batch size</cell><cell>512</cell><cell>-</cell><cell>-</cell></row><row><cell>Repeated action penalty</cell><cell>0.25</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell>Global gradient clipping</cell><cell>5.0</cell><cell>5.0</cell><cell>5.0</cell></row><row><cell>Setting</cell><cell></cell><cell cols="2">PPG PPG (tuned)</cell></row><row><cell cols="2">Policy epochs (E ? )</cell><cell>1</cell><cell>2</cell></row><row><cell cols="2">Value epochs (E V )</cell><cell>1</cell><cell>1</cell></row><row><cell cols="2">Distil epochs (E D )</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Auxiliary epochs (E aux )</cell><cell>6</cell><cell>2</cell></row><row><cell cols="2">Auxiliary Period (N ? )</cell><cell>32</cell><cell>32</cell></row><row><cell>Setting</cell><cell></cell><cell>Easy</cell><cell>Hard</cell></row><row><cell cols="2">Terminal on Loss of Life</cell><cell>True</cell><cell>False</cell></row><row><cell>Action Space</cell><cell></cell><cell cols="2">Minimal Full</cell></row><row><cell cols="2">Repeat Action Probability</cell><cell>0.0</cell><cell>0.25</cell></row><row><cell cols="2">Training frames</cell><cell>200M</cell><cell></cell></row><row><cell cols="2">Color / grayscale</cell><cell cols="2">Grayscale</cell></row><row><cell>Frame stacked</cell><cell></cell><cell>4</cell><cell></cell></row><row><cell cols="2">Action repetitions</cell><cell>4</cell><cell></cell></row><row><cell cols="2">Reward clipping</cell><cell>No</cell><cell></cell></row><row><cell cols="2">Episode timeout</cell><cell>108K</cell><cell></cell></row><row><cell>Resolution</cell><cell></cell><cell cols="2">84 ? 84</cell></row><row><cell>Noop Starts</cell><cell></cell><cell>1-30</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Atari-5 scores for each of the ablation runs.</figDesc><table><row><cell>Run</cell><cell cols="2">Atari-5 Score Regression</cell></row><row><cell>DNA (ref)</cell><cell>252</cell><cell>0.0</cell></row><row><cell>DNA (no distil)</cell><cell>226</cell><cell>-10.2%</cell></row><row><cell>DNA (mb=512)</cell><cell>195</cell><cell>-22.5%</cell></row><row><cell cols="2">DNA (? V = ? ? = 0.95) 191</cell><cell>-24.3%</cell></row><row><cell>PPO (basic)</cell><cell>81</cell><cell>-67.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Approximate training times for the algorithms used in this paper.</figDesc><table><row><cell>Algorithm</cell><cell>GPU hours per game</cell></row><row><cell>PPO (our settings)</cell><cell>3</cell></row><row><cell>DNA</cell><cell>4</cell></row><row><cell>PPG</cell><cell>4.5</cell></row><row><cell>PPO ([27] settings)</cell><cell>7.5</cell></row><row><cell>Rainbow DQN</cell><cell>240 21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Summary of results on the Atari-57 benchmark.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Median (easy) Median (hard)</cell></row><row><cell>Rainbow</cell><cell>223</cell><cell>-</cell></row><row><cell>PPO</cell><cell>224</cell><cell>155</cell></row><row><cell>DNA (ours)</cell><cell>311</cell><cell>207</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Final scores for all 57 games in ALE under 'hard' settings. Reported as mean score over the</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">While there are cases (i.e. where rewards have negative covariance) where this is not the case, empirical experiments confirm the intuition that longer n-step estimates generally have higher variance and lower bias.<ref type="bibr" target="#b4">5</ref> PPG refer to their third phase as a distillation phase, however, because the update trains both the policy and value networks on value estimates generated from rollouts, this is better thought of as an auxiliary task. We discuss this in more detail in Section 4.3</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">See our parallel Atari-5 paper, included in the supplementary material.<ref type="bibr" target="#b8">9</ref> By dual network, we mean two independent networks. Not to be confused with a dual-head network (which we refer to as a joint network), or with dualing networks<ref type="bibr" target="#b29">[30]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">This is plausible, especially with high variance returns<ref type="bibr" target="#b14">15</ref> It is worth noting that PPO does clip the entropy bonus, and therefore updates with a non-zero entropy bonus could make arbitrarily large changes to the policy regardless of the choice of .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">By value clipping, we mean the value trust-region suggested by<ref type="bibr" target="#b25">[26]</ref>, but which has been shown hurt performance by<ref type="bibr" target="#b1">[2]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">We also clipped normalized rewards to [?5, 5] but found that this occurred exceedingly rarely, especially after the first 1-million frames.<ref type="bibr" target="#b17">18</ref> An example of this would be the agent failing to press the reset button after losing a life in Breakout.<ref type="bibr" target="#b18">19</ref> Adding some kind of exploration strategy, such as Random Network Distillation<ref type="bibr" target="#b5">[6]</ref> would likely solve this as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">Or squashing the value function, see Appendix A of<ref type="bibr" target="#b2">[3]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">There are faster ways of training DQN like algorithms, for example Ape-X<ref type="bibr" target="#b15">[16]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">That is, the distillation step minimized the mean squared error between the features outputted by the policy network and the features output by the value network. As with our other targets, gradients were only propagated through the policy network.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimal use of experience in first person shooter environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aitchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Games (CoG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What matters for on-policy deep actor-critic methods? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raichuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sta?czyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Agent57: Outperforming the atari human benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vitvitskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12894</idno>
		<title level="m">Exploration by random network distillation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Truncating temporal differences: On the efficient implementation of td (lambda) for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cichosz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="287" to="318" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging procedural generation to benchmark reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2048" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Phasic policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2020" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Implementation matters in deep rl: A case study on ppo and trpo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Janoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1407" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep recurrent q-learning for partially observable mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 AAAI fall symposium series</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Muesli: Combining improvements in policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4214" to="4226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00933</idno>
		<title level="m">Distributed prioritized experience replay</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bias-variance error bounds for temporal difference updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An empirical model of large-batch training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06162</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Time limits in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Levdik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kormushev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4045" to="4054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mastering atari, go, chess and shogi by planning with a learned model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="issue">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding and improving information transfer in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
