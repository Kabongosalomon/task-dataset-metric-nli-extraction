<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intention-aware Feature Propagation Network for Interactive Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuyu</forename><surname>Zhang</surname></persName>
							<email>zhangchy2@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Intention-aware Feature Propagation Network for Interactive Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to tackle the problem of point-based interactive segmentation, in which two key challenges are to infer user's intention correctly and to propagate the user-provided annotations to unlabeled regions efficiently. To address those challenges, we propose a novel intention-aware feature propagation strategy that performs explicit user intention estimation and learns an efficient clickaugmented feature representation for high-resolution foreground segmentation. Specifically, we develop a coarse-to-fine sparse propagation network for each interactive segmentation step, which consists of a coarse-level network for more effective tracking of user's interest, and a fine-level network for zooming to the target object and performing fine-level segmentation. Moreover, we design a new sparse graph network module for both levels to enable efficient long-range propagation of click information. Extensive experiments show that our method surpasses the previous state-of-the-art methods on all popular benchmarks, demonstrating its efficacy. * These authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interactive image segmentation plays a vital role in a broad range of human-in-the-loop vision tasks, such as image editing <ref type="bibr" target="#b7">[8]</ref>, medical image analysis <ref type="bibr" target="#b31">[32]</ref> and dense image annotation <ref type="bibr" target="#b29">[30]</ref>. There has been a long history of interactive segmentation in vision literature, in which a variety of interaction strategies have been explored, including points <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>, scribbles <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, and bounding boxes <ref type="bibr" target="#b26">[27]</ref>. In this work, we mainly focus on the point-based interactive segmentation that only provides point clicks to indicate foreground or background on image, which typically requires less effort from human annotators.</p><p>A key characteristic of interactive segmentation is the diversity in the regions of user's interest. Unlike semantic segmentation with a predefined label space, each instance in an interactive segmentation task may produce different foreground regions according to the user's intent. Consequently, the main challenges of point-based interactive segmentation are to figure out the user intention <ref type="bibr" target="#b30">[31]</ref> and to propagate the user annotation (i.e., clicks) to other unlabeled pixels <ref type="bibr" target="#b6">[7]</ref> based on a limited number of user interactions. Most previous work focus on modeling each individual interactive step and mainly rely on the sparsely-annotated clicks to infer the user intention <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>. Such a memoryless strategy tends to be less efficient in capturing foreground as it ignores predictions from previous steps. To alleviate this, recent work <ref type="bibr" target="#b30">[31]</ref> takes the last-step prediction as an additional input to zoom into inferred target regions, leading to more effective foreground estimation. However, such a simple fusion method may produce erroneous information propagation due to potential inaccurate mask predictions in previous steps (as shown in <ref type="figure" target="#fig_0">Fig.1</ref>). For the challenge of label propagation, previous methods typically utilize stacked convolutions to propagate user annotation in an implicit manner <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21]</ref>. This has a limited capacity in capturing long-range dependency and often leads to incomplete foreground masks. More recently, Chen et al. <ref type="bibr" target="#b6">[7]</ref> adopt fully-connected graph networks <ref type="bibr" target="#b32">[33]</ref> to facilitate information propagation from both global and local perspective. Nonetheless, the fully-connected graph network can only cope with relatively lowresolution feature map due to its high-computation complexity, which can result in inaccurate foreground boundaries. Moreover, both propagation stages are susceptible to noisy affinity estimation that relies on previous predictions or color similarity.</p><p>To address the aforementioned limitations, we propose a novel intention-aware feature propagation strategy for interactive image segmentation. Our main ideas are twofolds: 1) to learn a click-augmented feature representation based on a sparse graph neural network (GNN), which allows efficient long-range information propagation at high spatial resolution, and thus enables us to generate more accurate object boundaries; and 2) to improve user intention estimation by integrating previous foreground prediction and current user clicks at each step, which can better localize target regions and hence results in an effective zoom-in mechanism for coping with scale variation.</p><p>To this end, we develop a coarse-to-fine sparse propagation network for each interactive segmentation step, consisting of a coarse-level network for foreground estimation and a fine-level network for detailed segmentation. Specifically, at each step, our coarselevel network first generates an refined foreground mask based on the entire image, user clicks and the last-step mask prediction. The resulting mask, which encodes the user intention more accurately, is used to zoom in to a selected region of interest. We then use the fine-level network to perform foreground segmentation at a fine resolution on the zoomed-in region, of which the output is remapped onto the full image and sent to the next step. In each cascade stage, we design a new sparse graph network to propagate the user click information to the unlabeled region in a non-local and yet efficient manner. In particular, our sparse GNNs compute a set of click-augmented feature representations at high spatial resolution in linear complexity, which are highly efficient and can preserve more detailed information for foreground mask predictions.</p><p>We adopt a stage-wise training strategy for our network cascade which trains the coarse-and fine-level networks sequentially by simulating interactive steps <ref type="bibr" target="#b30">[31]</ref>. We conduct extensive experiments on GrabCut, Berkeley, DAVIS, COCO, SBD, and PAS-CAL datasets, and the results demonstrate that our method achieves the state-of-the-art performance. To summarize, our contribution is three-folds:</p><p>-We propose an effective intention-aware feature propagation strategy for interactive image segmentation, which employs a cascaded network for estimating user intention and performing long-range information propagation. -We develop a novel click propagation strategy based on two sparse GNNs, capable of capturing long-range dependency on high-resolution feature maps and generating more accurate object boundaries. -Our method achieves the state-of-the-art results on most public benchmarks, demonstrating the effectiveness of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works 2.1 Interactive segmentation</head><p>Interactive image segmentation has attracted much attention in computer vision research, and a variety of interaction strategies have been studied, which are based on bounding boxes, scribbles, or points. While the bounding-box based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34]</ref> can localize the target object quickly, and the scribble based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2]</ref> provide richer user-input cues, they often involve more user interactions. By contrast, the point-based, where a user provides points to indicate foreground or background on the image, requires less effort from human annotators <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>. Consequently, we mainly focus on the point-based methods in the discussion below. Many point-based works have emerged since Xu et al <ref type="bibr" target="#b35">[36]</ref> first propose a CNNbased method, which can be largely grouped into two categories. While one trend focuses on annotating object boundaries <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref>, most deep learning methods perform region-based segmentation, aiming to better leverage user clicks in each interactive step. In particular, Liew et al <ref type="bibr" target="#b18">[19]</ref> attempt to refine local regions based on pairs of positive and negative clicks. Majumder et al <ref type="bibr" target="#b23">[24]</ref> generate a content-aware guidance map for exploiting the hierarchical structural information in the image. Lin et al <ref type="bibr" target="#b20">[21]</ref> argue that the first click is more important than others and design a first-click attention mechanism. Hao et al <ref type="bibr" target="#b12">[13]</ref> improve the usage of interactive information from user clicks with edge-guided flow. To better adapt to test cases, recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref> develop a backpropagating refinement scheme to correct the mislabeled user clicks in the test time. Despite their promising performances, these approaches typically concentrate on a single interactive step and hence are inefficient in capturing user intention. To remedy this, Sofiiuk</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-Level Network Fine-Level Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zoom-In Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pos/Neg Clicks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Image</head><p>Probability Map et al <ref type="bibr" target="#b30">[31]</ref> utilize the last-step prediction to facilitate localizing the target region, which however tends to produce erroneous information propagation due to inaccuracy in previous predictions. By contrast, we propose a coarse-to-fine strategy that better tracks the user intention from the coarse-level and perform effective segmentation based on both global and local image cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph neural network</head><p>Graph neural networks (GNN) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> have been widely adopted to model long-range dependencies and there exists a large body of literature on this research topic <ref type="bibr" target="#b34">[35]</ref>. However, only a few works utilize GNNs in the task of interactive segmentation. In particular, based on the non-local networks <ref type="bibr" target="#b32">[33]</ref>, Chen et al <ref type="bibr" target="#b6">[7]</ref> propose a conditional diffusion network for interactive segmentation, which performs non-local feature propagation on the global convolutional features and local-level pixels using color similarity. This mixed strategy tends to suffer from inaccurate foreground boundaries due to the low-resolution deep feature map and/or the noisy graph affinity estimated based on previous foreground masks or color similarity. In contrast, we propose a simple sparse attention-based non-local graph network to propagate the click information, which can be applied to a high-resolution feature map and generate more accurate masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce our intention-aware feature propagation method for pointbased interactive image segmentation. To this end, we first present the problem setup of interactive segmentation and outline our method in Sec. 3.1. Subsequently, we detail the design of two main components, including intention-aware framework (IAF) and feature propagation module (FPM) in Sec. 3.2 and Sec. 3.3, respectively. Finally, we formally describe the training strategy of our feature propagation network in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup and Method Overview</head><p>The goal of interactive segmentation is to correctly infer the region of user's interest and segment the target object with as few clicks as possible. This sequential estimation task is typically converted into a series of foreground segmentation problems, each of which aims to output a foreground mask as accurate as possible given the current set of user's clicks in an interactive step.</p><p>In this work, we consider a learning strategy that trains a neural network to predict the foreground mask in each step <ref type="bibr" target="#b35">[36]</ref>. Formally, we assume a set of image-click pairs is generated as our training data D (See Sec. 3.4 for more detail). It comprises data tuples</p><formula xml:id="formula_0">{(I i , U i , Y i )} |D| i=1 where I i indicates an input image, U i = {(u i,j , l i,j )} Mi</formula><p>j=1 is a set of user clicks represented by their pixel indices u i,j and labels l i,j ? {pos, neg} , M i is the number of clicks and Y i denotes the groundtruth mask. Our goal is to learn a deep network M ? based on D, which can be formulated as follows:</p><formula xml:id="formula_1">min ? E (Ii,Ui,Yi)?D L(M ? (I i , U i ), Y i )<label>(1)</label></formula><p>where L denotes the loss function, and ? indicates the network parameters.</p><p>To tackle the problem in Eq <ref type="formula" target="#formula_1">(1)</ref>, we focus on two key aspects of click-guided foreground mask prediction: 1) to capture the user's intent effectively and 2) to efficiently propagate the click labels to unlabeled regions. To this end, we develop a novel intention-aware feature propagation network, which performs a coarse-to-fine foreground refinement at each interaction as shown in <ref type="figure" target="#fig_1">Fig.2</ref>. Specifically, our network consists of two main modules: a coarse-level network in the first stage which updates the foreground mask on the full image based on the last-step prediction and the up-todate click inputs, and a subsequent fine-level network which zooms into the foreground region adaptively and performs segmentation at a fine resolution. Both stages employ newly-designed sparse GNNs to facilitate propagation of the user's input information. Below, we will first introduce the overall intention-aware pipeline, followed by the details of our feature propagation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intention-Aware Framework (IAF)</head><p>We now present our intention-aware propagation framework which aims to better infer the region of user's interest and produce a foreground segmentation with accurate object boundaries. To achieve this goal, we adopt a coarse-to-fine strategy and develop a network cascade architecture for each interactive step as shown in <ref type="figure" target="#fig_1">Fig.2</ref>.</p><p>Our network cascade first use a coarse-level network to fuse the last-step foreground map with the current user clicks and to update the foreground estimation at the image level. Formally, at each step t, we denote the input image as I, the current-step user click set as U t and the last-step network prediction as P t?1 . The coarse-level network takes those three inputs and generates an updated foreground probability map for the entire image, which can be formulated as follows:</p><formula xml:id="formula_2">P t c = M c (I, P t?1 , F enc (U t ); ? c )<label>(2)</label></formula><p>where M c and ? c denote the coarse-level network and its parameters respectively, and P t c is the output probability map; F enc is an encoding function for representing user's click inputs, which generates two image-sized heatmaps for the positive and negative clicks respectively. Here we adopt a common encoding strategy <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref> in which each click is represented by a click-centered Gaussian kernel or a disk transform function with a fixed radius. Subsequently, a fine-level network as the second module, zooms in onto the foreground region and performs segmentation with a fine-level feature representation. Specifically, according to the image-level foreground estimation, we adaptively crop and resize the image, the click encoding maps and the foreground map, which are then fed into the fine-level network. The fine-level network generates a foreground probability map for the zoomed-in region and map it back to the full image view. This second-stage process can be written as follows,</p><formula xml:id="formula_3">I,P t c ,? t = ZoomIn(I, P t c , U t )<label>(3)</label></formula><formula xml:id="formula_4">P t = M f (?,P t c , F enc (? t ); ? f )<label>(4)</label></formula><p>where?,P t c ,? t denotes the image, foreground map and user clicks after the zoom-in step, P t is the output of the fine-level network M f and ? f are its parameters.</p><p>The function ZoomIn determines the region for cropping according to the coarselevel foreground prediction P t g . In particular, we first derive a bounding box of the target from binarizing P t g , and then expand the box by an adaptive margin. The margin size is inversely proportional to the size of the initial box so that large objects have a relatively tight bounding box which excludes distracting background while small objects take a relatively loose bounding box for including more context cues.</p><p>Our coarse-to-fine framework provides several key benefits for inferring foreground regions. First, it begins with an update of the foreground probability in the full image view, leading to a more accurate localization of the target region. This enables us to zoom in on small objects to extract more details and track the user's intention in a more reliable manner. Moreover, the updated foreground provides a high-quality initial labeling for the fine-level network, which is essential for refining the segmentation in local regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Propagation Module (FPM)</head><p>Both coarse-and fine-level network aim to propagate the user-provided information to unlabeled regions in their own input images. Due to the sparsity of user's clicks, this typically requires modeling long-range feature relations across the image plane. To achieve efficient information propagation, we introduce a new graph neural network module, which augments a base CNN segmentation network for predicting the foreground mask. In contrast to previous non-local design <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7]</ref>, our graph network module are built on a sparse graph topology, which enables us to compute a user-input-aware representation on a high-resolution feature map and hence produces detailed foreground segmentation with accurate boundaries.</p><p>Specifically, we first use a CNN-based segmentation network to compute a stack of feature maps, from which a higher-level and a lower-level map are selected. The higher-level feature map, denoted as F, typically encodes more semantics but has a low resolution while the lower-level map, denoted as F h , has a high resolution and preserves more object boundary cues * . In order to better exploit both the higher-and lowerlevel features, we employ a two-stage design: a sparse graph network first augments the higher-level feature map with the user-click features, which is further integrated with the lower-level features in a high-res sparse graph network at the second stage. Below we describe the details of those two graph networks in turn.</p><p>Sparse Graph sub-Module (SGM): Our sparse graph submodule performs feature propagation on the higher-level feature maps to disseminate the user-click information to all features. Formally, we represent the higher-level feature map F as {f n } H l ?W l n=1 , where H l and W l are the height and width of the low-res feature map respectively. At each location, f n ? R c is a c-channel feature vector and n is the spatial location index. To build the sparse graph, we select the feature vectors at the location of user clicks in U , denoted as F u = {f ui } M i=1 , and connect them to each location on the feature map. Given the graph, we perform feature augmentation by passing messages from the click nodes F u to each feature location as follows (See <ref type="figure" target="#fig_2">Fig. 3</ref> for illustration):</p><formula xml:id="formula_5">f n = f n + M j=1 ?(f n , f uj )W ? c f uj , ?f n ? F (5) ?(f n , f uj ) = e ?(fn) ? ?(fu j ) /Z n (F)<label>(6)</label></formula><p>wheref n is the updated feature, W c ? R c?c is a weight matrix for feature transform, and ? denotes an attention function in which ? and ? are linear transforms and Z n (F) is the normalization factor. Intuitively, the augmentation moves all the features towards * In this work, we adopt the DeeplabV3+ <ref type="bibr" target="#b5">[6]</ref> with a ResNet backbone as our base segmentation network. We select the feature map after the ASPP block as the higher-level F, and the conv features after the first block in the ResNet as the lower-level F h . The size of F and F h are 1/4 and 1/2 of the original image.</p><p>the click-annotated representations, which reduces in-class variation and improve foreground prediction. However, these augmented features, denoted asF, are built on the low-res feature map F, which tends to produce coarse segmentation masks. To remedy this, we introduce a second graph network to refine them as below.</p><p>High-res Sparse Graph sub-Module (HSGM): The second graph network generates a click-augmented feature representation with a high spatial resolution. To this end, we integrate the first-stage outputF with the lower-level feature map F h , followed by another pass of click-to-feature propagation. Specifically, we denote the lower-level feature map as</p><formula xml:id="formula_6">{f h n } H h ?W h n=1</formula><p>where H h and W h are the height and width of the highres feature map respectively. Similar to the SGM, we select the click-annotated feature, represented as F h u = {f h uj } M j=1 , and link them to every feature location on F h . Given the high-res graph, we first upsample the previous outputF so that it has the same spatial dimension as the lower-level feature map F h . We then perform feature integration and click-aware augmentation by a message passing as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Formally, denoting the upsampled feature as {? n } H h ?W h n=1 , we update the high-res feature representation as follows,</p><formula xml:id="formula_7">f h n = ?(f h n ?? n ) + M j=1 ?(? n ,? uj )W ? f ?(f h uj ?? uj )<label>(7)</label></formula><p>wheref h n denotes the high-res augmented feature, ? indicates feature concatenation, ?(?) is a transformation function and W f is a weight matrix for feature transform. Note that we use the higher-level features {? n } to compute the attention weights so that the information propagation is less susceptible to variations in the lower-level feature F h . Moreover, our sparse graphs only perform message passing from M selected nodes to N feature nodes, which can be computed efficiently with a complexity O(M N ) where M ? N in the interaction.</p><p>Given the high-res augmented features {f h n } H h ?W h n=1 , we finally generate the foreground probability map by applying a two-layer conv-block followed by the Sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>To train our deep network for interactive segmentation, we first utilize a simulation process to generate a set of image-click pairs from a foreground segmentation dataset as in <ref type="bibr" target="#b30">[31]</ref>, which assumes the user clicks on the center of maximum error regions. Given the training dataset, we then adopt a stage-wise strategy to train our coarse-level and fine-level networks in turn.</p><p>Specifically, we first train the coarse-level network, which is then kept fixed during the second-stage training. The fine-level network is then initialized with the coarse-level model parameters and fine-tuned afterwards. Following <ref type="bibr" target="#b29">[30]</ref>, both networks employ the Normalized Focal Loss (NFL) <ref type="bibr" target="#b28">[29]</ref> in their training objectives as shown in Eqn (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first depict the experiment setting and implementation details, then compare our model with existing works, followed by ablation study to validate each component. Finally, we demonstrate some qualitative results to show the model efficacy, and analyze our model's parameters and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation and Implementation Details</head><p>Datasets: We evaluate our method over a wide range of datasets including GrabCut, Berkeley, DAVIS, COCO, PASCAL VOC and SBD, by following the standard evaluation protocol.</p><p>GrabCut <ref type="bibr" target="#b26">[27]</ref> is a typical interaction segmentation dataset, which contains 50 images with distinguishable foreground and background.</p><p>Berkeley <ref type="bibr" target="#b24">[25]</ref> contains 96 images with 100 object masks from its test subset. DAVIS <ref type="bibr" target="#b25">[26]</ref> is originally introduced for video segmentation. Only 345 randomly sampled frames with finely labeled objects are used in our method by following <ref type="bibr" target="#b14">[15]</ref>.</p><p>COCO <ref type="bibr" target="#b19">[20]</ref> is a typical semantic segmentation dataset, containing more complex scene and multiscale objects. Following <ref type="bibr" target="#b35">[36]</ref>, we split the dataset into COCO(seen) and COCO(unseen) according to their object class whether in PASCAL VOC or not. And finally, 10 images are sampled randomly for each category. For simplicity, we denote COCO(seen) and COCO(unseen) as COCO s and COCO u PASCAL VOC <ref type="bibr" target="#b8">[9]</ref> contains 1449 images with 3427 object masks from its validation set. We ignore evaluating the precision of object boundaries since they are marked.</p><p>SBD <ref type="bibr" target="#b13">[14]</ref> contains 6671 object masks for 2820 images. Metric: To mimic the real user clicks in evaluation, we follow <ref type="bibr" target="#b35">[36]</ref> to click the center of maximum error region to correct the output mask continuously. The interaction process will terminate when the IoU between prediction and groundtruth mask exceeds threshold ? , or reaching the maximum number of interactions. Due to large scene diversity in different datasets, we typically set ? as 85% or 90%, and set maximum number of interactions to 20 as previous works. The number of clicks (NoC) and number of failure (NoF) to meet the termination conditions is used for evaluation metric. For example, NoC@90 means the average number of clicks for test set is needed to reach 90% IoU under 20 maximum interactions, and NoF@90 means the number of failure case that doesn't reach 90% IoU with 20 maximum interactions.</p><p>Due to the randomness of the training, we report the average NoC by running three times for the same experimental setting to obtain more stable results Implementation Details: We adopt DeeplabV3+ <ref type="bibr" target="#b5">[6]</ref> as our coarse-and fine-level network. Besides, we follow <ref type="bibr" target="#b30">[31]</ref> to utilize Conv1S to fuse click maps and foreground estimation, then sum the fused feature with image features at the output of the first convolutional block.</p><p>During training, we follow the same iterative sampling strategy in <ref type="bibr" target="#b30">[31]</ref> to generate positive and negative clicks to alleviate the gap between training and inference stages. For clicks encoding, we adopt the disk encoding strategy proposed in <ref type="bibr" target="#b3">[4]</ref> with a fixed radius of 5. Our network is trained over the SBD training set and COCO+LVIS dataset with common data augmentation approaches, including random cropping and scaling, horizontal flipping, and optimized by Adam optimizer with batch size 28. The image is resized with a size 320 ? 480 after augmentation.</p><p>We adopt the stage-wise training strategy for our network. Specifically, for the first training stage, the coarse-level network is trained for 120 epochs and utilized to initialize the fine-level network. The learning rate is set as 5e-4 and decays 10 times at 100th, 115th epochs. For the second stage training, we fix the parameters in coarse-level network and train our fine-level network for another 30 epochs with learning rate 5e-7 on the zoomed-in image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results</head><p>We compare our framework with several existing approaches for evaluation, in terms of the following metrics: standard average number of clicks (NoC) and results with maximum 100 clicks.</p><p>Main results: As shown in Tab.1, we compare our method with previous approaches over a wide range of benchmarks covered by existing works. When trained on the SBD dataset, our method achieves the strongest performance over different datasets, and outperforms all existing approaches under the same backbone setting. For Berkeley and COCO datasets, our method achieves the best average number of clicks under HRNet-18 backbone, which only requires average 2.86 clicks to reach 90% IoU on Berkeley, 3.0 clicks to reach 85% IoU on COCO s datasets. The result on COCO u shows that our framework can generalize to unseen classes greatly. Specifically, for the challenging SBD dataset and fine-grained DAVIS dataset, we achieve average 5.8 and 4.83 NoC@90 respectively on the ResNet-101 backbone, improve 0.98 and 0.5 compared with RITM <ref type="bibr" target="#b30">[31]</ref>. It indicates that our intention-aware framework with a click information propagation module can better segment objects with detailed boundaries. For GarbCut and PascalVOC datasets, we improve a little since the results are close to the upper bound. It is worth noting that the difficulty of interactive segmentation increases rapidly along with the number of clicks decreasing. Therefore, the improvements on all datasets are significant. For further comparisons, we train our model on the COCO+LVIS dataset. The results in the last two lines show that we still achieve a great improvement on fine-grained DAVIS. Meanwhile, same as RITM <ref type="bibr" target="#b30">[31]</ref>, we also observe a drop on SBD datasets compared with trained on SBD datasets. The improvement or drop on Berkeley, Pascal VOC, and GrabCut is slight because their performance are nearly saturated.</p><p>IOU@k Analysis: We analysis the performance varying over number of clicks k. As shown in the <ref type="figure" target="#fig_3">Fig.4</ref>, our method is consistently better than RITM, and about 2% higher in mIOU for the first five clicks, which validates the effectiveness of our method.</p><p>Success samples with different number of clicks: To further analysis our improvement, we report the distribution of success samples with different number of clicks on the challenging fine-grained DAVIS dataset, which contains 345 samples in total. As in <ref type="figure" target="#fig_4">Fig.5</ref>, our approach can successfully segment 71.3% (246) samples within 5 clicks, which greatly outperforms the RITM by 8.1%. It indicates that we can better utilize user-provided sparse click information. Also, we can know hard case in the tail drags down NoC metrics.</p><p>Results with maximum 100 clicks: Following <ref type="bibr" target="#b29">[30]</ref>, we report NoC with the maximum number of clicks limited to 100 on the DAVIS dataset with ResNet-50 backbone. In Tab.2, it shows that our method improves a lot on NoC@90. And we improve 0.91 on NoC 100 @90 metric compared with CDNet. This is because our sparse graph can be propagated on high-resolution feature maps to preserve more detail information, while CDNet's cannot.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>In this section, we perform several ablative experiments to evaluate the effectiveness of each component in our approach. All ablation experiments are evaluated using NoC@90 metric on DAVIS and SBD datasets, which are more challenging than other datasets.  graph module (FDM) in CDNet <ref type="bibr" target="#b6">[7]</ref>, which is actually a fully-connected graph network. For a fair comparison, all experiments are built upon the Baseline* adopted in CDNet. As in Tab 4, we can observe our 'Baseline* + FPM' achieves significant improvement in both metrics. It is worth noting that our advanced sparse graph design makes it feasible to conduct message propagation in both low &amp; high resolution feature maps. In addition, when applying FDM in both low &amp; high scale feature maps, we find that the overall network consumes amount of compuatation (1510. <ref type="bibr" target="#b15">16</ref> GFLOPs v.s. 531.42 GFLOPs), which is unacceptable in real systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Propagation Module (FPM):</head><p>We conduct incremental ablation experiments to study the effectiveness of each component in the FPM on the baseline. As shown in Tab.5, the introduction of Sparse Graph sub-Module (SGM) brings 0.23 and 0.1 NoC@90 improvements on DAVIS and SBD respectively. It indicates that the feature propagation of SGM is helpful. Moreover, the introduction of HSGM improves performance further, which means that feature propagation on high-resolution does help to preserve more precise boundary information. The results show that each module in FPM is effective. Specifically, we analyze the impact of the FPM module when there are all positive clicks. Since it is difficult to compare the case where both models have N positive points, we compare the case with only one positive point. In such case, Baseline+FPM improves mIOU on DAVIS (0.68 ? 0.71) compared to Baseline. Conceptually, if the user inputs N consecutive positive clicks, it indicates that the model tends to label the target object as background. In such cases, adding positive features to all points would shift the pixel representations towards the foreground side so that the model can predict more pixels as foreground, which could be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization analysis</head><p>As shown in <ref type="figure" target="#fig_5">Fig.6</ref>, we visualize several samples to show the effectiveness of our method. In <ref type="figure" target="#fig_5">Fig.6(a)</ref>, the blue rectangle represents the user intention estimated by the model. As we can see, the baseline focus on the incomplete object region due to the inaccurate user intention encoded in the last step prediction. Our method can correctly focus on the entire object area and obtain better segmentation results since our model updates user intention at coarse-level. In <ref type="figure" target="#fig_5">Fig.6(b)</ref>, we observe our method can obtain more precise boundaries even with fewer user clicks. It shows the effectiveness of the coarse-to-fine strategy and FPM module, which help us to preserve more accurate boundary information. In <ref type="figure" target="#fig_5">Fig.6(c)</ref>, the results indicate that our sparse graph neural network is able to capture more reliable long-range dependencies and make the prediction more complete.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameters and Inference time analysis</head><p>To study the influence of network parameters, we report the amount of parameters of our model and RITM <ref type="bibr" target="#b30">[31]</ref>, and compare their performance under the comparable parameter setting. As in Tab.6, we observe that the performance of RITM with ResNet-50 and ResNet-101 backbone shows that it is not easy to improve NoC by simply increasing the amount of parameters, but our stage-wise intention-aware framework with a sparse graph network can achieve significant improvement. The results indicate our improvements are not from more parameters but our delicately designed network structure.</p><p>Moreover, we test the inference time using the DAVIS dataset with ResNet-50 backbone on the Intel Xeon 2.20GHz CPU and a single NVIDIA Titan RTX GPU. The seconds per click (SPC) is 0.217, which is sufficiently fast for users and is much less than the average time cost of user interaction (3 seconds <ref type="bibr" target="#b3">[4]</ref>) in each step.</p><p>In practical applications, we can save considerable user annotation time when there are billion images, approximately 4.17 ? 10 5 hours per billion. In addition, as shown in <ref type="figure" target="#fig_4">Fig.5</ref>, when there are fewer human clicks, we significantly improves compared with baseline, which is very friendly to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have developed a novel intention-aware feature propagation strategy for interactive image segmentation. Our method is capable of better inferring user intention and effectively propagating user-provided sparse annotations to the entire input image. To achieve this, we introduce a coarse-to-fine sparse propagation network consisting of a coarse-level and a fine-level sub-networks. Our coarse-level network is able to zoom in onto target regions more accurately and hence mitigates the impact of scale variation. Jointly with the fine-level network, they can capture long-range dependencies at a high spatial resolution with a newly-designed sparse GNN module and hence generate more accurate object boundaries. We evaluated our method on several public benchmarks, in which our approach outperforms the prior works by a sizable margin, achieving state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparison of baseline with our approach. The blue box and orange box represent estimated user intention and zoomed-in details. The red points represent positive user clicks. The green finger represents user's current click action. (a) The baseline prediction misses a part of the cat due to inaccurately estimated intention. (b) Our approach captures complete user intention by intention update and preserves more accurate object boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The overall architecture of intention-aware feature propagation network. Our pipeline consists of a coarse-level and fine-level network. In each interactive step, we first update foreground estimation at image level by coarse-level network (blue box). The updated foreground probability map uses to zoom in onto user interested region. Subsequently, the fine-level network(yellow box) performs foreground segmentation at zoomed-in level. In each network, feature propagation module(FPM) is used to propagate user-provided information to the unlabeled regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The structure of sparse graph sub-module(SGM) and high-resolution sparse graph sub-module(HSGM) in FPM. H and W represent the height and width of the feature map. C and C ? represent the channel of higher-level and lower-level feature maps, respectively. M is the number of user clicks. For simplicity, we ignore the reshape operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>mIOU varying over number of clicks k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The distribution of the number of clicks on DAVIS dataset. The experiments are on the ResNet-50 backbone. The left of red dotted line is the success samples and the right of red dotted line is the failed samples, which can't reach 90% IoU within 20 clicks. 1-5 means the number of samples that need at least one click and at most five clicks to reach 90% IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization analysis: The odd and even columns show the prediction result of the baseline<ref type="bibr" target="#b30">[31]</ref> and our method. Row (a) indicates that our method maintain more complete user intention, Row (b)indicates that our method preserves more accurate boundary information, and Row (c) indicates that our method captures more reliable long-range dependencies # BS SGM HSGM DAVIS SBD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with SOTA. The bold means the best results across different backbones, and the underline means the best results under the same backbone. * means that we implement the results.</figDesc><table><row><cell cols="2">Method</cell><cell cols="5">GrabCut Berkeley COCO s COCO u PascalVOC</cell><cell>DAVIS</cell><cell>SBD</cell></row><row><cell></cell><cell></cell><cell cols="7">NoC@90 NoC@90 NoC@85 NoC@85 NoC@85 NoC@85 / 90 NoC@85 / 90</cell></row><row><cell>DOS[36]</cell><cell>FCN</cell><cell>6.08</cell><cell>8.65</cell><cell>8.31</cell><cell>7.82</cell><cell>6.88</cell><cell cols="2">9.03 / 12.58 9.22 / 12.80</cell></row><row><cell>RIS[19]</cell><cell>-</cell><cell>5.00</cell><cell>6.03</cell><cell>5.98</cell><cell>6.44</cell><cell>5.12</cell><cell>-/ -</cell><cell>6.03 / -</cell></row><row><cell>LD[18]</cell><cell>-</cell><cell>4.79</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-/ 9.57</cell><cell>-/ -</cell></row><row><cell>ITIS[23]</cell><cell>-</cell><cell>5.60</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.80</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>BRS[15]</cell><cell>DenseNet</cell><cell>3.60</cell><cell>5.08</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">5.58 / 8.24 6.59 / 9.78</cell></row><row><cell>CMG[24]</cell><cell>FCN</cell><cell>3.58</cell><cell>5.60</cell><cell>5.40</cell><cell>6.10</cell><cell>3.62</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell cols="2">CDNet[7] ResNet-50</cell><cell>2.64</cell><cell>3.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.17 / 6.66</cell><cell>4.37 / 7.87</cell></row><row><cell>Ours</cell><cell>ResNet-50</cell><cell>2.31</cell><cell>3.35</cell><cell>2.46</cell><cell>3.69</cell><cell>2.38</cell><cell>4.52 / 5.83</cell><cell>3.08 / 4.98</cell></row><row><cell cols="3">IS+SA[16] ResNet-101 3.07</cell><cell>4.94</cell><cell>4.08</cell><cell>5.01</cell><cell>3.18</cell><cell>5.16 / -</cell><cell>-/ -</cell></row><row><cell cols="3">FCA [21] ResNet-101 2.14</cell><cell>4.19</cell><cell>4.45</cell><cell>5.33</cell><cell>2.96</cell><cell>-/ 7.90</cell><cell>-/ -</cell></row><row><cell cols="3">f-BRS-B[30] ResNet-101 2.72</cell><cell>4.57</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.04 / 7.41</cell><cell>4.81 / 7.73</cell></row><row><cell cols="3">RITM*[31] ResNet-101 2.31</cell><cell>3.50</cell><cell>2.54</cell><cell>3.61</cell><cell>2.48</cell><cell>5.09 / 6.78</cell><cell>3.33 / 5.33</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-101 2.15</cell><cell>3.20</cell><cell>2.27</cell><cell>3.50</cell><cell>2.31</cell><cell>4.51 / 5.8</cell><cell>2.98 / 4.83</cell></row><row><cell cols="2">RITM[31] HRNet-18</cell><cell>2.04</cell><cell>3.22</cell><cell>2.40</cell><cell>3.61</cell><cell>2.51</cell><cell>4.94 / 6.71</cell><cell>3.39 / 5.43</cell></row><row><cell>Ours</cell><cell>HRNet-18</cell><cell>1.75</cell><cell>2.86</cell><cell>2.23</cell><cell>3.00</cell><cell>2.49</cell><cell>4.68 / 6.01</cell><cell>3.33 / 5.25</cell></row><row><cell cols="2">RITM[31]  ? HRNet-18</cell><cell>1.54</cell><cell>2.26</cell><cell>-</cell><cell>-</cell><cell>2.28</cell><cell>4.36 / 5.74</cell><cell>3.80 / 6.06</cell></row><row><cell>Ours  ?</cell><cell>HRNet-18</cell><cell>1.68</cell><cell>2.12</cell><cell>2.17</cell><cell></cell><cell>2.40</cell><cell>4.03 / 5.22</cell><cell>3.67 / 5.91</cell></row></table><note>? denotes the model are trained on COCO+LVIS, and others model are trained on SBD dataset. -denotes the results are not available. The lower is the better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Analysis of 100 clicks.</figDesc><table><row><cell>Method</cell><cell cols="3">Components DAVIS SBD FPM IAF</cell></row><row><cell cols="2">Baseline -</cell><cell>-</cell><cell>6.85 5.67</cell></row><row><cell></cell><cell>?</cell><cell>-</cell><cell>6.39 5.36</cell></row><row><cell></cell><cell>-</cell><cell>?</cell><cell>6.39 5.34</cell></row><row><cell>Ours</cell><cell>?</cell><cell>?</cell><cell>6.05 5.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Each component analysis.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Baseline:We adopt previous state-of-the-art RITM<ref type="bibr" target="#b30">[31]</ref> on ResNet-34 backbone as our baseline which achieves decent performance as shown in Tab.1.Effectiveness of each component:The results in Tab.3 show that each component plays a critical role in the entire framework and contributes to the final results. For the Feature propagation module (FPM), it achieves 0.46 and 0.31 NoC@90 improvements on DAVIS and SBD dataset respectively. The improvement on fine-grained DAVIS dataset reveals that the FPM can better capture accurate boundary details. Besides, the gain on SBD dataset indicates that the FPM can accurately segment objects with severe scale variation. Further, the performance is further improved on two datasets by incorporating our Intention-aware framework (IAF), which indicates that the IAF can better estimate foreground in coarse-level and segment the object accurately in fine-level. To validate the efficacy of sparse graph design in feature propagation module (FPM), we conduct more experiments to compare with feature diffusion</figDesc><table><row><cell>Baseline* Baseline* + FDM [7] Baseline* + FDM in both low &amp; high res. Sparse Graph Analysis: Method Baseline* + FPM</cell><cell>Params(M) Flops(G) NoC20@85 NoC20@90 31.4 508.72 6.60 8.42 31.42 513.82 5.40 7.64 31.44 1510.16 ? ? 31.5 531.42 5.05 7.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Graph design analysis on the DAVIS dataset. Low resolution denotes 1/4 feature map while high resolution represents and 1/2 scale feature map.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of FPM.</figDesc><table><row><cell cols="4">Method Backbone Params(M) DAVIS SBD</cell></row><row><cell cols="2">RITM*[31] ResNet-50</cell><cell>31.41</cell><cell>6.74 5.39</cell></row><row><cell cols="2">RITM*[31] ResNet-101</cell><cell>58.44</cell><cell>6.78 5.33</cell></row><row><cell>Ours</cell><cell>ResNet-50</cell><cell>64.70</cell><cell>5.83 4.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Parameter analysis. The parameters of our method are the sum of coarse and fine level networks.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-clicks</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive full image segmentation by considering all regions jointly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Error-tolerant scribbles based interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Annotating object instances with a polygonrnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional diffusion for interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Repfinder: finding approximately repeated scene elements for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Edgeflow: Achieving practical interactive segmentation with edge-guided flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive image segmentation via backpropagating refinement scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Continuous adaptation for interactive object segmentation by learning from corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive boundary prediction for object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV). pp</title>
		<meeting>the European Conference on Computer Vision (ECCV). pp</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regional interactive image segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with first click attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast interactive object annotation with curvegcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iteratively trained interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Content-aware multi-level guidance for interactive instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comparative evaluation of interactive segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">grabcut&quot; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">f-brs: Rethinking backpropagating refinement for interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reviving iterative training with mask guidance for interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06583</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deepigeos: a deep interactive geodesic framework for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Milcut: A sweeping line multiple instance learning paradigm for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<title level="m">A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interactive object segmentation with insideoutside guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
