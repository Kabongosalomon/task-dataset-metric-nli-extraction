<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting R-CNN: Reweighting R-CNN Samples by RPN&apos;s Error for Underwater Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinhao</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengteng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhui</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting R-CNN: Reweighting R-CNN Samples by RPN&apos;s Error for Underwater Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>underwater object detection hard example mining uncertainty modeling</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Complicated underwater environments bring new challenges to object detection, such as unbalanced light conditions, low contrast, occlusion, and mimicry of aquatic organisms. Under these circumstances, the objects captured by the underwater camera will become vague, and the generic detectors often fail on these vague objects. This work aims to solve the problem from two perspectives: uncertainty modeling and hard example mining. We propose a two-stage underwater detector named boosting R-CNN, which comprises three key components. First, a new region proposal network named RetinaRPN is proposed, which provides high-quality proposals and considers objectness and IoU prediction for uncertainty to model the object prior probability. Second, the probabilistic inference pipeline is introduced to combine the first-stage prior uncertainty and the second-stage classification score to model the final detection score. Finally, we propose a new hard example mining method named boosting reweighting. Specifically, when the region proposal network miscalculates the object prior probability for a sample, boosting reweighting will increase the classification loss of the sample in the R-CNN head during training, while reducing the loss of easy samples with accurately estimated priors. Thus, a robust detection head in the second stage can be obtained. During the inference stage, the R-CNN has the capability to rectify the error of the first stage to improve the performance. Comprehensive experiments on two underwater datasets and two generic object detection datasets demonstrate the effectiveness and robustness of our method. The link of code: https://github.com/mousecpn/Boosting-R-CNN Pinhao Song, Pengteng Li et al.: Preprint submitted to Elsevier</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Oceans account for 71% of the earth's total area and contain rich biological and mineral resources. Humans cast their eyes on ocean exploitation, for the resources on the land have been fully exploited, which means the research on the oceans is meaningful. Over the past few years, more and more researchers have considered applying underwater object detection (UOD) to autonomous underwater vehicles (AUVs) with visual systems to fulfill a series of underwater tasks such as marine organism capturing.</p><p>Generic Object Detection (GOD) has been researched for a long time and obtained abundant achievements. However, GOD is not perfectly suitable for underwater environments which bring new challenges to object detection (see <ref type="figure" target="#fig_1">Figure 1</ref>): (i) The images captured by the underwater visual system suffer from unbalanced light conditions and low contrast, which make the object boundary hard to be distinguished from the background. (ii) The aquatic organisms tend to live together, which cause severe occlusion. (iii) The aquatic organisms are good at hiding themselves, which have the similar color with the background and make it hard for people to recognize them. Facing these new challenges, the boundaries between the objects and background and the boundaries between different objects will be vague, leading to the existence of the vague objects in underwater environments.</p><p>Pinhaosong@pku.edu.cn (P. Song); 2110276192@email.szu.edu.cn (P. Li); dailinhui@pku.edu.cn (L. Dai); taowang@pku.edu.cn (T. Wang); zhanchen\_cz@pku.edu.cn (Z. <ref type="bibr">Chen)</ref> ORCID(s):  Existing works on UOD typically apply data augmentation methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref> and use a strong feature extractor <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b56">57]</ref> to improve the performance. However, these methods suffer from problems listed as follows. (i) Previous underwater detectors receive the same supervision signal for all objects regardless of their vagueness. Thus, the classification score trained with simple cross entropy loss does not accurate reflect the vagueness of the objects, which would cause false over-confident predictions. However, accurately ranking the detection results is crucial for object detectors to achieve high performance. It is expected that the detectors assign low scores to the detection results containing vague objects and assign high scores to the results with clear objects. (ii) Previous underwater detectors are vulnerable to vague objects with blurring boundaries and similar color to the background. That is because the gradient of the easy samples will dominate the training of underwater detectors, which makes detectors difficult to learn the subtle differences between vague objects and underwater background.</p><p>Different from existing UOD methods, we address the above problems through uncertainty modeling and hard example mining. We propose a two-stage detector named Boosting R-CNN (see <ref type="figure" target="#fig_2">Figure 2</ref>), which consists of three key components: RetinaRPN, probabilistic inference pipeline, and boosting reweighting. Specifically, RetinaRPN generates proposals from backbone features with heavier heads to perform three tasks: objectness prediction, IoU prediction, and box localization. It includes the IoU prediction and objectness as two indicators to model the prior uncertainty in order to accurately measure the vagueness of the objects. With a proposed fast IoU loss, high-quality proposals can be obtained. Second, the probabilistic inference pipeline combines the RetinaRPN's object prior and the R-CNN classification score to make a prediction, which uses the uncertainty from the first stage to improve the robustness of the detector. Third, boosting reweighting attaches more attention to hard examples whose priors are miscalculated by amplifying the loss according to the RPN's error. Since the final classification score of the object combines the RPN's prior and the R-CNN's scores, the R-CNN trained with reweighted samples has a strong robustness to hard examples, modifying its score to correct the false positive and false negative of the RetinaRPN.</p><p>With these three components, our Boosting R-CNN can handle complicated underwater challenges and be robust to vague objects. Our method is evaluated on two underwater object detection datasets: UTDAC2020 1 and Brackish <ref type="bibr" target="#b34">[35]</ref>, not only achieving state-of-the-art performance but also maintaining a relatively high inference speed. Moreover, the experiments on the Pascal VOC <ref type="bibr" target="#b12">[13]</ref> and the MS COCO <ref type="bibr" target="#b28">[29]</ref> dataset show Boosting R-CNN obtains favorable performance on general object detection. Our code will be released at https://github.com/mousecpn/Boosting-R-CNN-Reweighting-R-CNN-Samples-by-RPN-s-Error-for-Underwater-Object-Detection.git</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Detection</head><p>Existing object detection can be categorized into two mainstreams: two-stage and one-stage detectors. For twostage detectors, the basic idea is to reduce the detection task to the classification problem <ref type="bibr" target="#b39">[40]</ref>. In the first stage, the region proposal network (RPN) aims to propose candidate object bounding boxes, and RoI Pooling and RoI 1 http://uodac.pcl.ac.cn/ Align are leveraged to crop the features from backbone and resize them to the same size. In the second stage, the R-CNN head realizes classification and regression tasks of all objects. One-stage detectors abandon the usage of the RPN and RoI Align, directly obtaining the coordinates of bounding boxes and classes of the objects. Nowadays, onestage detectors can achieve the same level of performance as two-stage detectors. There are two branches of one-stage detectors: anchor-based methods and anchor-free methods. Early works on one-stage detectors are mostly anchor-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28]</ref>. Recently, some works rethink whether the anchor is necessary, and propose their designs to abandon the use of anchors <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>As the research on object detection goes deeper, the researchers find that the concepts of one-stage and two-stage detectors are not entirely different. Some research works aim to leverage the advantages of two-stage to enhance the performance of one-stage detectors. RefineDet <ref type="bibr" target="#b53">[54]</ref> separates the one-stage detection into two sub-module: the anchor refinement module and the object detection module. AlignDet <ref type="bibr" target="#b7">[8]</ref> uses deformable convolution (DCN) to imitate RoIAlign to obtain aligned features in the second stage. RepPoints <ref type="bibr" target="#b49">[50]</ref> leverages the idea of refinement and feature alignment and applies it to the proposed anchor-free detectors based on keypoint detection. Two-stage detectors are also nurtured by the achievements of one-stage detectors. CenterNet2 <ref type="bibr" target="#b57">[58]</ref> finds that a strong anchor-free one-stage detector as the RPN can predict an accurate object likelihood that informs the overall detection score. Combining the object likelihood of RPN and the conditional classification score of the R-CNN will achieve higher performance with fewer proposals, which reduces the inference cost. Our Boosting R-CNN is a probabilistic two-stage detector like CenterNet2. The difference is that we build a strong anchor-based RPN, and apply a hard example mining mechanism based on the RPN's errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Hard Example Mining.</head><p>Hard example mining methods aim to attach more attention to hard examples, relying on the hypothesis that training on hard examples leads to better performance. The first deep detector to use hard example mining is Single Shot Detector <ref type="bibr" target="#b31">[32]</ref>, which chooses only the negative examples with the highest loss values. Online Hard Exampling Mining (OHEM) <ref type="bibr" target="#b41">[42]</ref> considers both hard positive and negative examples for training. Considering the efficiency and memory problems of OHEM, IoU-based sampling <ref type="bibr" target="#b33">[34]</ref> is proposed, associating the hardness of the negative examples with their IoUs, and sampling averagely across all IoU ranges. Focal Loss <ref type="bibr" target="#b27">[28]</ref> is a soft hard-example mining method, dynamically assigning more weight to the hard examples based on the classification score. Prime Sample Attention (PISA) <ref type="bibr" target="#b2">[3]</ref> proposes an IoU Hierarchical Local Rank for all samples, assigning higher weight for positive examples with higher IoUs. Different from the methods mentioned above, our two-stage Boosting R-CNN defines the hardness of the examples based on their prior probability from the proposed RetinaRPN. A soft reweighting mechanism is proposed to amplify the loss of the hard examples and shrink the loss of the easy examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Underwater Object Detection.</head><p>As an indispensable technology for AUVs to perform multiple tasks under the water, underwater object detection has attracted a large amount of attention from researchers all around the world. For instance, Huang et al. <ref type="bibr" target="#b18">[19]</ref> introduce perspective transformation, turbulence simulation, and Illumination synthesis into data augmentation. Chen et al. <ref type="bibr" target="#b8">[9]</ref> design a novel underwater salient detection model that is established by mathematically stimulating the biological vision mechanism of aquatic animals RoIMix <ref type="bibr" target="#b29">[30]</ref> is a data augmentation method that applies mixup on the RoI level to imitate occlusion conditions. SWIPENET <ref type="bibr" target="#b6">[7]</ref> takes full advantage of both high resolution and semanticrich hyper feature maps to increase the performance of small objects. Besides, a novel sample-reweighted loss and a new training paradigm CMA are proposed which are noiseimmune. Poisson GAN <ref type="bibr" target="#b46">[47]</ref> is also a data augmentation method, which pastes the object on the underwater background by poisson blending and uses GAN to correct the artifact. FERNet <ref type="bibr" target="#b13">[14]</ref> consists of three modules: composite connected backbone, receptive field augmentation module, and prediction refinement scheme. Composited FisherNet <ref type="bibr" target="#b56">[57]</ref> is based on underwater video object detection, leveraging the differences between the foreground and background to extract salient features and proposing an enhanced path aggregation network to solve the insufficient utilization of semantic information caused by linear up-sampling. RoIAttn <ref type="bibr" target="#b26">[27]</ref> considers RoI patches as tokens and applies the external attention module on the RoIs to improve the performance of underwater object detection. Compared with the methods mentioned above, to the best of our knowledge, our idea of considering using RPN's error for hard example mining has not been investigated by any existing underwater object detection approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Boosting R-CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Different from the vanilla two-stage detector Faster R-CNN, the proposed two-stage detector Boosting R-CNN has three key components: RetinaRPN, the probabilistic inference pipeline, and boosting reweighting. The pipeline of our Boosting R-CNN is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. In detail, the backbone and the feature fusion neck (e.g., ResNet+PAFPN) first extract features from images. Second, RetinaRPN provides a series of high-quality proposals with corresponding prior probability. Third, boosting reweighting amplifies the classification loss of the hard examples whose priors are miscalculated, while decreasing the weight of the easy examples with accurately estimated priors. Fourth, the R-CNN head which contains two fully-connected layers is trained on reweighted RoI samples. In the inference stage, the final score is the square root of the multiplication of the prior and the classification score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone and Feature Fusion Neck</head><p>Given an image ? ? 3? 0 ? 0 (with RGB channels), a backbone (e.g. ResNet50) generates multi-scale feature maps { } 5 =3 at 3 -5 ( has resolution 2 smaller than the input). The multi-scale feature maps will be sent into the feature fusion neck.</p><p>PAFPN <ref type="bibr" target="#b30">[31]</ref> is employed as the feature fusion neck. PAFPN contains two parts: the top-down path and the bottom-up path. In the top-down path, the high-level feature is used to enhance the low-level feature. Given the multiscale feature maps { } 5 =3 from backbone, the output feature { } 5 =3 as:</p><formula xml:id="formula_0">5 = ( 5 ),<label>(1)</label></formula><formula xml:id="formula_1">4 = ( 4 ) + ( 5 ),<label>(2)</label></formula><formula xml:id="formula_2">3 = ( 3 ) + ( 4 ),<label>(3)</label></formula><p>where (?) denotes the convolution layer, and (?) denotes the 2x upsampling layer. In the bottom-up path, the lowlevel feature is leveraged to augment the high-level feature to obtain feature maps { } 7 =3 , as:</p><formula xml:id="formula_3">3 = ( 3 ),<label>(4)</label></formula><formula xml:id="formula_4">4 = ( 4 ) + ( 3 ),<label>(5)</label></formula><formula xml:id="formula_5">5 = ( 5 ) + ( 4 ),<label>(6)</label></formula><formula xml:id="formula_6">6 = ( 5 ),<label>(7)</label></formula><formula xml:id="formula_7">7 = ( 6 ),<label>(8)</label></formula><p>where (?) denotes the convolution layer with stride 2, (?) denotes the 2x downsampling layer. The output multiscale features { } 7 =3 are fed into the detection head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RetinaRPN</head><p>The RPN is responsible for providing proposals that have potential objects. Underwater images are blurring, lowcontrast and distorted, which make it difficult to distinguish the objects from the background. Besides, in the occlusion condition, the objectness trained with simple cross entropy loss in the vanilla RPN is not a good estimation of the proposal box localization accuracy. As a result, the highquality proposals may be filtered by the poorly regressed proposals with higher objectness. To obtain high-quality proposals with accurate prior probabilities, we aim to build a strong RPN inspired by the designs of the current one-stage detector, which is named retina region proposal network (RetinaRPN). Heavier Head. Instead of using one simple convolution layer in the vanilla RPN, we use four convolution layers with group normalization. More convolution layers have a more powerful capability to detect vague objects in blurring, lowcontrast, and distorted underwater images. Multi-Ratio Anchors. For each FPN level, we use anchors at three aspect ratios {1:2, 1:1, 2:1} with sizes {2 0 , 2 1?3 , 2 2?3 } of 32 2 to 512 2 for FPN levels 3 -7 . In total, there are =9 anchors per pixel. Anchor is an important prior  for regressing and classifying aquatic organisms with vague boundaries. Loss Function. RetinaRPN performs three tasks: objectness prediction, box localization, and IoU prediction. The objectness branch is trained to predict whether there is an object in an anchor. We leverage the focal loss as objectness loss:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet Backbone PAFPN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-CNN</head><formula xml:id="formula_8">(?) = { ? (1 ??) log(?), = 1, ? (1 ? )?log(1 ??), = 0,<label>(9)</label></formula><formula xml:id="formula_9">? = 1 ? =1 (?),<label>(10)</label></formula><p>where?is the objectness score of the anchor output by the RetinaRPN, is the balance parameter, and is the focus parameter. is the number of anchors. ? {0, 1} is the label of anchor . It is set to 1 if anchor is a positive sample, otherwise, it is set to 0. As for the positive and negative samples assignment, the anchors with IoU over 0.5 with ground-truth boxes are regarded as positive samples, while the anchors with IoU below 0.5 are regarded as negative samples.</p><p>The localization branch aims to output the proposals which are refined on the anchors. Usually, IoU loss is leveraged in the regression loss:</p><formula xml:id="formula_10">= (?, * ),<label>(11)</label></formula><formula xml:id="formula_11">(?) = 1 ? ,<label>(12)</label></formula><p>where?and * are the predicted box and corresponding ground-truth box, and is the IoU between them. IoU loss has some good properties, such as non-negative, symmetry, triangle inequality, and scale insensitivity. And it is the metric of object detection. However, the convergence speed of the IoU loss is slow. In order to increase the convergence speed, L2 loss is added to IoU loss. The improved IoU loss can be rewritten as:</p><formula xml:id="formula_12">? (?) = 1 ? + ? ?{ , , ,?} ||?, ? * , || 2 2 ,<label>(13)</label></formula><formula xml:id="formula_13">, =?? ,?, =?? ? ,<label>(14)</label></formula><formula xml:id="formula_14">, = log(?),?, ? = log(? ? ),<label>(15)</label></formula><formula xml:id="formula_15">* , = * ? , * , = * ? ? ,<label>(16)</label></formula><formula xml:id="formula_16">* , = log( * ), * ,? = log( ? * ? ).<label>(17)</label></formula><p>where { , , , ? } are the coordinates of anchor , {?,?,?,? } and { * , * , * , ? * } are the coordinates of the predicted box and its corresponding ground truth, and {?, ,?, ,?, ,?, ? } and { * , , * , , * , , * ,? } denote the encoding of the 4 coordinates of the predicted box and ground truth respectively. This encoding method is the same as <ref type="bibr" target="#b39">[40]</ref>. However, L2 loss is very vulnerable to outliers, which will harm the regression accuracy. To solve this problem, we design the fast IoU loss (FIoU), which is inspired by <ref type="bibr" target="#b55">[56]</ref>, as:</p><formula xml:id="formula_17">(?) = (1 ? + ? ?{ , , ,?} ||?, ? * , || 2 2 ), (18) ? = 1 ? =1 (?),<label>(19)</label></formula><p>where is a parameter to control the degree of inhibition of outliers, and is the number of positive samples. We add an IoU weighted term to alleviate the problem of vulnerability to outliers. With the IoU weighted term, lowquality samples with high regression loss will be filtered, for the weighted term will become small. And the RetinaRPN will focus on the prime samples with moderate regression accuracy, which will enhance the robustness to the outliers and remain the fast convergence.</p><p>The IoU prediction branch is trained to predict the IoUs between regressed boxes and their corresponding ground truths. And the cross entropy is used as the loss function:</p><formula xml:id="formula_18">? = 1 ? =1 ?[ log(?) + (1 ? ) log(1 ??)],<label>(20)</label></formula><p>where?is the predicted IoU of the anchor . The object prior is the square root of the multiplication of the objectness score and IoU prediction, namely:</p><formula xml:id="formula_19">= ?? * ?.<label>(21)</label></formula><p>With the IoU prediction branch, the detector can provide the uncertainty into the prior when the objects are occluded in the underwater environment. In detail, objectness denotes the likelihood of the object in an anchor. Although objectness trained with focal loss can effectively filter the negative samples, it will also assign a high value to the proposal in which the object is severely covered by other objects. IoU prediction predicts the IoU between the proposal and its ground truth and assigns a value to the object according to its level of occlusion. Combining two indicators includes uncertainties from different perspectives, and comprehensively models the prior probabilities of the proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Probabilistic Inference Pipeline</head><p>For the two-stage detector, in the first stage, the RPN outputs K proposal boxes 1 , ..., . And for the proposal ? {1, ..., }, RPN predicts a class-agnostic foreground prior probability ( ), where =1 denotes the proposal is an object and =0 suggests the background. This is realized by a binary classifier trained with a log-likelihood objective. In the second stage, high-scoring proposals are sampled to train the R-CNN head, a softmax classifier. The R-CNN learns to classify each proposal into one of the foreground classes or background. The output classification score of the proposal for the class can be seen as a conditional categorical probability ( | =1) ( ?{?, }, ? is the set of classes and denotes background). However, in the inference stage, the final detection score directly uses the classification score in the R-CNN head, ignoring the prior probability from the RPN. During the training stage in the R-CNN head, since the supervision signals of all proposals are the equivalent with a softmax classifier regardless of the localization accuracy, the R-CNN head easily outputs false over-confident predictions. Thus, compared with using the conditional categorical probability ( = | =1), it is more reasonable to use the marginal probability ( = ), ? ? as the final detection score. We set ( = | =0)=1 and ( = | = 0) = 0, which means that it is impossible for the R-CNN head to reconsider a proposal as a positive sample if the RPN regards the proposal as a negative sample. The marginal probability ( = ) can be written as:</p><formula xml:id="formula_20">( = ) = ? ?{0,1} ( = | ) ( = ) = ( = | = 1) ( = 1) + ( = | = 0) ( = 0) = ( = | = 1) ( = 1).<label>(22)</label></formula><p>From the equation above, the marginal probability is the multiplication of the first-stage prior probability and the second-stage conditional distribution. Using the marginal probability, the first-stage prior uncertainty can be taken into consideration in the final prediction. In the implementation, the final detection score of the proposal is the square root of the multiplication of RetinaRPN's prior and the R-CNN's  classification score, namely:</p><formula xml:id="formula_21">( ) = ? * ( ),<label>(23)</label></formula><p>where ( ) is the classification score of the sample for class in R-CNN, and ( ) is the final score.</p><p>With the probabilistic inference pipeline, the detector can take the first-stage uncertainty into consideration to make the final predictions. Thus, compared with using the conditional probability, the marginal probability is a better estimation of the detection box localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Boosting Reweighting</head><p>There is a deficiency in the previous probabilistic inference pipeline. In the original two-stage detector, the second stage makes predictions that is independent of the first stage. As a result, a low score for a high-quality sample in the first stage will not influence the final detection result as long as the sample is selected as a proposal. However, in the probabilistic two-stage pipeline, when the RPN mistakenly generates a low prior for a high-quality positive proposal, it is hard to re-consider it as a high-confidence prediction, for the final score is the square root of the multiplication of prior and classification score. In underwater environments, vague objects as hard examples often happen, and the RPN will severely suffer from those.</p><p>To solve this problem, we hope that when the RPN miscalculates the prior of the proposal, the R-CNN can rectify the error. Thus, we propose a soft sampling strategy named boosting reweighting (BR, shown in <ref type="figure" target="#fig_3">Fig. 3)</ref>, which borrows the idea of reweighting from boosting algorithm and well fits the existing frameworks. Different from vanilla Faster R-CNN, where the weights of all proposals are set to 1, BR tends to attach more attention to hard examples whose priors are miscalculated. In detail, for the sample , its classification weight is:</p><formula xml:id="formula_22">= { (1 ? ) , ? ? ,<label>(24a)</label></formula><p>,</p><formula xml:id="formula_23">? ?,<label>(24b)</label></formula><p>where ?0 is the boosting parameter. ? denotes the set of foreground samples, and ? denotes the set of background samples. With the reweighting scheme, the classification loss of the R-CNN can be written as:</p><formula xml:id="formula_24">= 1 ? =1 ? ? =1 (? ? log(?)),<label>(25)</label></formula><p>where?and denote the predicted classification score and label of sample for class , ? {0, 1}. and are the number of proposals in second stage and the number of classes respectively. Note that the weighted terms are all smaller than 1, the total value of the classification loss will shrink, which will cause the shrink of the gradient. In order to keep the norm of the total loss unchanged, we normalize to ? :</p><formula xml:id="formula_25">? = ? ? =1 ? =1 (? ? log(?)) ? =1 ? ? =1 (? ? log(?)) ,<label>(26)</label></formula><formula xml:id="formula_26">? = 1 ? =1 ? ? ? =1 (? ? log(?)).<label>(27)</label></formula><p>When the detector encounters hard positive/negative samples, obviously the priors of RPN will be small/large. As a result, the weighted term (1? ( )) / ( ) will increase and amplify the loss of the hard examples, while the loss of the easy samples will be decreased.</p><p>BR can be seen as hard example mining. There are two similar works to our BR: OHEM and focal loss. OHEM is a bootstrapping method, which is originally designed for Fast R-CNN (without RPN), it performs a feedforward for all RoIs on the R-CNN, and selects the hardest samples for training on the second feedforward. Our BR leverages the prior information from the RPN from only one feedforward, saving lots of memory cost and training time. Focal loss is designed for RetinaNet to solve the extreme imbalance between foreground and background. However, the NMS in the RPN and bootstrapping mechanism in the second stage alleviate the imbalance problem, which overlaps the function of focal loss. Our BR is used in combination with NMS and bootstrapping. To avoid the shrinking of the loss, normalization is leveraged to re-distribute the weight of each sample. Thus, BR aims to handle the problem of the hard samples in the underwater environment instead of foreground-background imbalance. Both OHEM and focal loss reweight the loss by the R-CNN's error, while our BR reweights the loss by the RPN's error. The R-CNN trained with BR excavates the subtle differences between aquatic organisms and background and is robust to the samples to which the RPN is vulnerable. Thus, the R-CNN can rectify the RPN's error in the inference stage. The experiments show that BR is more compatible with the probabilistic inference pipeline compared with other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Loss Function</head><p>In the R-CNN head, we apply L1 loss on the positive proposals in the second stage for regression:</p><formula xml:id="formula_27">= 1 ? =1 ? ?{ , , ,?} |?, ? * , |,<label>(28)</label></formula><formula xml:id="formula_28">, =?? ,?, =?? ? ,<label>(29)</label></formula><formula xml:id="formula_29">, = log(?),?, ? = log(? ? ),<label>(30)</label></formula><formula xml:id="formula_30">* , = * ? , * , = * ? ? ,<label>(31)</label></formula><formula xml:id="formula_31">* , = log( * ), * ,? = log( ? * ? ),<label>(32)</label></formula><p>where { , , , ? } are the coordinates of proposals , {?,?,?,? } and { * , * , * , ? * } are the coordinates of predicted box and its corresponding ground truths, {?, ,?, ,?, ,?, ? } and { * , , * , , * , , * ,? } denote the encoding of the 4 coordinates of the predicted box and ground truth respectively. The total loss in the Boosting R-CNN is:</p><formula xml:id="formula_32">= ? ? + ? ? + ? ? + + ,<label>(33)</label></formula><p>where ? , ? , ? , , are the balanced parameters for each loss respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on four challenging object detection datasets to validate the generalization performance of our method.</p><p>(i) UTDAC2020 is an underwater dataset from the underwater target detection algorithm competition 2020. There are 5,168 training images and 1,293 validation images. It contains four classes: echinus, holothurian, starfish, and scallop. The images contain four resolutions: 3840?2160, 1920?1080, 720?405, and 586?480. We follow the COCOstyle evaluation metric.</p><p>(ii) Brackish is an early proposed underwater image dataset collected in temperate brackish waters. It contains six classes: bigfish, crab, jellyfish, shrimp, small fish, and starfish. There are 9,967, 1,467, 1,468 images in training, validation, and test set, containing 25,613 annotations. The image size is 960?540. We follow the MS COCO-style AP[0.5:0.95:0.05] metric and Pascal VOC-style AP50 metric as the original paper.</p><p>(iii) Pascal VOC is a generic object detection dataset, which contains 20 object categories. The dataset includes VOC2007 part and VOC2012 part. In VOC2007 part, there are 9963 annotated images, consisting of trainval (5011 images) and test set (4952 images). In VOC2012 set, there are 11540 annotated images in trainval set. We train our detector on 07+12 trainval dataset, and evaluate it on 07 test set.</p><p>(iiii) MS COCO is a generic object detection dataset, which contains 80 object categories. It contains 118k images for training (trainval), 5k images for validation (val) and 20k images for testing without provided annotations (test-dev). The final results are reported on test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our method is implemented on MMdetection <ref type="bibr" target="#b5">[6]</ref>. There are two training recipes in the experiments. The first one is the default training recipes, which adopts the classic 1x training scheme (12 epochs). SGD is adopted as an optimizer, where the weight decay is 0.0001 and the momentum is 0.9. The initial learning rate is 0.005. The learning rate is divided by a factor of 10 at epoch <ref type="bibr">8 and 11. No</ref>  AdamW is leveraged as the optimizer with an initial learning rate of 0.0001 and a weight decay of 0.05. The learning rate is divided by a factor of 10 at epoch 24 and 33. The method is trained on a single NVIDIA GTX 1080Ti GPU. During the inference, we use a maximum of 256 proposal boxes in the second stage, which improves the inference speed. As for the balanced parameters of loss, ? , ? , ? , , are set to 1, 2, 1, 2, 2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with Other State-of-the-art Methods</head><p>We compare Boosting R-CNN against some state-of-theart methods in four object detection datasets in <ref type="table" target="#tab_2">Table 1</ref>, 2, 3, and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Results on UTDAC2020</head><p>The experiment results on the UTDAC2020 dataset are shown in <ref type="table" target="#tab_2">Table 1</ref>. CenterNet2* and Boosting R-CNN* denote that the models use multi-scale training and 3? training time. Besides, Deformable DETR is trained for 50 epochs with multi-scale training. All the detectors are implemented by MMdetection <ref type="bibr" target="#b5">[6]</ref> except CenterNet2 which is officially implemented in Detectron2 <ref type="bibr" target="#b48">[49]</ref>.</p><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, in single-scale training setting, Boosting R-CNN achieves 48.5% AP, which is higher than DetectoRS (47.6% AP), PAA (47.5% AP), and CenterNet2 (47.2% AP). In multi-scale training setting, Boosting R-CNN still surpasses CenterNet2 (51.4% AP vs 48.9% AP). As a result, our Boosting R-CNN defeats all the detectors and builds new state-of-the-art performance. As for the inference speed, Boosting R-CNN achieves 13.5 FPS, which is higher than most of the two-stage detectors including Faster R-CNN (11.6 FPS) but lower than CenterNet2 (14.2 FPS).  Moreover, among the one-stage detectors, we can conclude that the anchor-based methods (SSD, RetinaNet, ATSS, FreeAnchor, PAA, and GFL) can obtain relatively high performances than the anchor-free methods (FSAF, FCOS, RepPoints, VFNet, and AutoAssign). That is because the boundaries of the aquatic organisms are vague in the lowcontrast and distorted underwater images, and anchors help the model to obtain boundaries prior, which boosts the convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Results on Brackish</head><p>The experiment results on the Brackish dataset are shown in <ref type="table" target="#tab_3">Table 2</ref>. Boosting R-CNN achieves 82.0% AP and 97.4% AP50, achieving the highest performance. Compared with DetectoRS (81.6% AP and 97.4% AP50), Boosting R-CNN is 0.4% higher in both AP and AP50 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Results on Pascal VOC</head><p>The experiment results on Pascal VOC are shown in <ref type="table" target="#tab_4">Table 3</ref>. In single-scale training setting, Boosting R-CNN achieves 81.9% mAP performance, which is higher than Faster R-CNN (79.5%), Cascade R-CNN (80.0%), DSSD513 (81.5% AP) and CenterNet2 (76.8%). In multi-scale training setting, Boosting R-CNN achieves the highest performance of 83.0% mAP. <ref type="table" target="#tab_5">Table 4</ref> compares our method to the state-of-the-art detectors with a large backbone on MS COCO test-dev. All the models presented are using single-scale testing. Boosting R-CNN achieves 44.4% AP only with ResNet50 backbone, outperforming some two-stage detectors (e.g. Cascade R-CNN, Grid R-CNN, and Libra R-CNN), and some one-stage detectors (e.g. FCOS, CornerNet, and FSAF). And Boosting R-CNN further achieves 50.7% AP with Res2Net101-DCN backbone, outperforming GFLV2 with the same backbone (50.6% AP) and achieving the highest performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Results on MS COCO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We also conduct extensive ablation studies on UT-DAC2020 to validate each module in our proposed Boosting R-CNN. <ref type="table">Table 5</ref> shows the detailed road map from the default Faster R-CNN to the proposed Boosting R-CNN. Our proposed RetinaRPN improves the performance from 44.5% AP (row 1) to 46.9% AP (row 13), which indicates that RetinaRPN provides higher-quality proposals. With an accurate estimation of the object prior in the RetinaRPN, the probabilistic inference pipeline considers the uncertainty of the first stage to make a prediction, increasing the performance from 46.9% AP to 47.9% AP (row 13 vs row 14). Using the boosting reweighting alone achieves a favorable improvement (from 44.5% AP to 45.3% AP), which is higher than OHEM (45.1% AP) in <ref type="table" target="#tab_2">Table 1</ref>, and BR boosts the performance from 47.9% AP to 48.3% AP (row 14 vs row 15). With PAFPN, the final performance of 48.5% AP can be obtained (row 16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Ablation Study of RetinaRPN</head><p>The first 5 columns in <ref type="table">Table 5</ref> shows the detailed ablation studies of the proposed RetinaRPN. (i) Using 4 layer convolution layers with GN (row 1 vs row 2) increases the capability of feature extraction, improving performance (44.5% to 45.1% in AP). (ii) Using focal loss can take all the samples into consideration (row 4), and further improve the performance (45.4 % AP). (iii) Compared with 3 anchors, 9 anchors provide a great performance increase (row 4 vs row 6, 45.4% to 46.7% AP), which further illustrates the importance of anchors in underwater environments. (vi) Adding an IoU prediction provides an accurate prior, which further improves the robustness to the vague objects (row 8 vs row 9, 47.2% AP to 47.5% AP). (vi) Our proposed fast IoU loss (row 14, 47.9% AP) is superior to L1 loss (row 9, 47.5% AP), GIoU loss (row 10, 47.6% AP), CIoU loss (row 11, 47.6% AP), and focal EIoU loss (row12, 47.7% AP).  <ref type="figure" target="#fig_4">Figure 4</ref> shows the choice of hyper-parameter in fast IoU loss. If is too large, the gradient will be dominated by the easy samples with high IoUs. If is too small, it will lack the ability to filter the outliers. When is set to 2, the highest performance of 48.5% AP can be obtained. When is set to 0, which is equivalent to the fast IoU loss without the IoU weighted term, the performance is lower. Thus, the training of the model will suffer from the outliers. <ref type="table">Table 6</ref> shows the experiments of different hard example mining methods. Since our BR is a kind of hard example mining, it is necessary to compare it to other hard example mining methods, i.e., OHEM, PISA, and focal loss. We replace BR with these methods to evaluate the effectiveness and the compatibility with the probabilistic inference pipeline. "Cls. Loss" denotes the classification loss in the R-CNN head, "Random" means randomly sampling positive and negative RoIs during training. The first row (47.9% AP) corresponds to the next-to-last row in <ref type="table">Table 5</ref>. BR achieves the highest performance (48.3% AP). Using OHEM instead gives a relatively lower performance (47.5% AP). PISA severely does harm to the performance (46.9% AP). Focal loss also causes severe performance decrease. When the is set lower, which means that focal loss gets closer to crossentropy loss, the performance is restored. The experiments in 6 show that in the probabilistic pipeline, BR helps R-CNN to correct the mistakes of RPN, which is more compatible than OHEM, PISA, and focal loss. <ref type="figure" target="#fig_5">Figure 5</ref> is the experiment of the choice of in BR. In this experiment, PAFPN is not used. From the figure, it can be concluded that normalization improves the performance and shifts the optimal value of . The highest performance of 48.3% AP is obtained when is set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Hard Example Mining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Anchor Assignment</head><p>In <ref type="table">Table 7</ref>, we adopt other positive and negative anchor assignment strategies in our RetinaRPN. PAFPN and boosting reweighting is not leveraged in the experiments. Although ATSS <ref type="bibr" target="#b52">[53]</ref>, PAA <ref type="bibr" target="#b19">[20]</ref> and OTA <ref type="bibr" target="#b15">[16]</ref> achieve astonishing performances in one-stage detectors, they decreases the performance when they play the role of the RPN. The <ref type="table">Table 5</ref> A detailed abalation study of Boosting R-CNN. The first five columns denotes the ablation studies of RetinaRPN. "4 L." denotes using 4 convolution layers with GN. "NA." denotes the number of anchors. "Reg Loss" denotes the regression loss in RPN. "FL" denotes using focal loss and abandoning the bootstrapping in RPN. "IoUp." means adding IoU prediction in RPN with cross entropy loss. "Prob" denotes using probabilistic inference pipeline. "BR" means boosting reweighting.  <ref type="table">Table 6</ref> The ablation studies of hard example mining. "FL( , )" denotes using focal loss with hyper-parameter and . reason may be that the adaptive assignment provides overconfident priors, which decreases the recall. Our setting "(0.5, 0.5)" achieves the best performance in underwater object detection. <ref type="figure">Figure 6</ref> shows the qualitative comparison between Boosting R-CNN and other state-of-the-art methods on UTDAC2020 dataset. We apply the detectors to some challenging cases, and the prediction score threshold set to 0.05. For clarity, in each image, we visualize the prediction boxes with the top-k scores, and k is the number of the ground-truth boxes in the images. The orange boxes in the images denote a prediction box whose IoU with a certain ground truth is over 0.5 and higher than other predictions. The blue boxes denote the unmatched predictions. Besides, we also print the missed ground-truth boxes in red. Thus, more blue boxes in the images suggest lower precision, and more red boxes in the images suggest lower recall. <ref type="table">Table 7</ref> Positive and negative assignment. "(0.5, 0.5)" denotes that the samples with IoUs over 0.5 are regarded as positive, while the samples with IoUs below 0.5 are regarded as negative. BR is not used in the experiment. The first two rows denote the blurring and low contrast conditions. Boosting R-CNN can detect all the ground truths (no red box) in the images with the highest precision (only one blue box). The third row denotes the unbalanced light condition. ATSS, PAA, and DetectoRS all miss the echinus in the center. Our Boosting R-CNN does not miss any ground truths. The Fourth row denotes the occlusion condition. Boosting R-CNN can detect the condition that a starfish cover a scallop, on which DetectoRS makes a mistake. The last two rows denote the mimicry condition. In the fifth case, the stone is very similar to the scallop. Boosting R-CNN can precisely distinguish the scallop from the stone. Moreover, for the small echinus on the left-top corner, other detectors miss it for the lack of regression capacity. The proposed Boosting R-CNN accurately detects this echinus, which proves that RetinaRPN with FIoU loss can provide high-quality proposals. In the last case, although the starfish hides in the waterweeds, Boosting R-CNN still successfully detects the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cls. Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assignment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FCOS</head><p>ATSS PAA DetectoRS Boosting R-CNN <ref type="figure">Figure 6</ref>: Qualitative comparison results on the UTDAC2020 dataset. The orange boxes denote the matched predictions. The blue boxes denote the unmatched predictions. The red boxes denote the undetected ground truths. The first two rows denote the blurring and low contrast conditions. The third row denotes the unbalanced light condition. The Fourth row denotes the occlusion condition. The last two rows denote the mimicry condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2828">2979 2513</head><p>prior wo/ Prob. full <ref type="figure">Figure 7</ref>: Visualization of the detection results of each variant of Boosting R-CNN. "prior" denotes replacing the secondstage classification score with the first-stage priors. "wo/ Prob." means dropping the probabilistic inference pipeline. "full" denotes the proposed Boosting R-CNN.</p><p>To further investigate the mechanism of Boosting R-CNN, we visualize the detection results of the variants.</p><p>"prior" denotes replacing the second-stage classification score with the first-stage priors, and the labels are from the classes with the highest score in the R-CNN head. "wo/ Prob." means droping the probabilistic inference pipeline and using the R-CNN head results directly. "full" denotes the proposed Boosting R-CNN. The detection scores in "full" are smaller than "wo/ Prob.", which suggests that the RetinaRPN can provide the uncertainty for the R-CNN head to avoid overfitting. For example, in the first column, the detection scores of the blue box in the top left corner ("wo/ Prob.") are decreased by the RetinaRPN. Thus, this false over-confident prediction is filtered in the final results ("full"). Besides, with the RetinaRPN and the probabilistic inference pipeline, the detection confidences are more reasonable, for Boosting R-CNN considers various uncertainty. In the second column, compared with "wo/ Prob.", no ground truth is missed in the result of Boosting R-CNN. What's more, the R-CNN trained with BR can rectify the error of the RetinaRPN. In the third column, the ground truth (the red box) in the bottom of the "prior" is missed for the RetinaRPN assigns a too small prior for the predictions. With the correction of the R-CNN, the missed ground truth can be detected by increasing the second-stage score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Underwater object detection is facing new challenges such as blur, low contrast, occlusion, and mimicry conditions compared with generic object detection. In this paper, we propose a brand new two-stage underwater detector Boosting R-CNN to solve the problems mentioned above. First, the proposed RetinaRPN has a strong capacity to detect objects in blurring, low-contrast, distorted images, and provide highquality proposals with accurate estimations of object prior probability in the occlusion condition. Second, the proposed probabilistic inference pipeline helps the detector make a prediction based on the uncertainties of the vague objects, resulting in a reasonable ranking of the prediction scores. Third, boosting reweighting is proposed to learn the second stage by the error of the first stage, which is a kind of hard example mining and helps the second stage to rectify the error at the probabilistic pipeline. The experiments on two public underwater datasets demonstrate that Boosting R-CNN outperforms other state-of-the-art detectors in underwater object detection. The competitive performances on two public generic object detection datasets show the generalization of Boosting R-CNN. Comprehensive ablation studies show the effectiveness of the proposed modules.</p><p>Tao Wang received the B.E. degree in Automation in 2020. He is currently pursuing the M.S. degree at the School of Electronic and Computer Engineering, Peking University. His research interests include person re-identification, object detection, and deep metric learning. Zhan Chen received the B.S. degree from Hunan University(HNU), China. He is a research graduate student studying at Peking University (PKU), China. His research interest lies in machine learning and computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The challenges of underwater environments. (a) Complicated underwater terrains cause unbalanced light condition. (b) The low-contrast image makes the boundaries of two holothurian blurred. (c) The aquatic organisms tend to live together, causing occlusion. (d) The starfish has the similar color with the environment, which makes them difficult to spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The overview of the proposed Boosting R-CNN. The backbone and the feature fusion neck first extract features from images. RetinaRPN provides a series of high-quality proposals with corresponding prior probability. Boosting reweighting amplifies the classification loss of the hard examples whose priors are miscalculated while decreasing the weight of the easy examples with accurately estimated priors. The R-CNN head which contains two fully-connected layers is trained on reweighted RoI samples. In the inference stage, the final score is the square root of the multiplication of the prior and the classification score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The overview of the proposed boosting reweighting. The patch size denotes the weight of RoI samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FPSFigure 4 :</head><label>4</label><figDesc>The choice of in FIoU loss. = 0 means droping the IoU weighted term. is utilized. The second training recipe adopts the 3x training scheme (36 epochs) with crop and multi-scale augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The choice of in boosting reweighting. Blue line denotes the BR with normalization, while green dash line denotes the BR without normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Comparisons with other object detection methods on UTDAC2020 dataset. The FPS is tested on a single Nvidia GTX 1080Ti GPU. '*' means that the model uses the second training recipe.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell cols="6">AP50 AP75 APS APM APL FPS</cell></row><row><cell>Two-Stage Detector:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN w/ FPN [40]</cell><cell>ResNet50</cell><cell>44.5</cell><cell>80.9</cell><cell>44.1</cell><cell>20.0</cell><cell>39.0</cell><cell cols="2">50.8 11.6</cell></row><row><cell cols="2">OHEM+Faster R-CNN w/ FPN [42] ResNet50</cell><cell>45.1</cell><cell>82.0</cell><cell>45.1</cell><cell>21.6</cell><cell>39.1</cell><cell cols="2">51.4 11.6</cell></row><row><cell>Cascade R-CNN [2]</cell><cell>ResNet50</cell><cell>46.6</cell><cell>81.5</cell><cell>49.3</cell><cell>21.0</cell><cell>40.9</cell><cell>53.3</cell><cell>8.8</cell></row><row><cell>Libra R-CNN [34]</cell><cell>ResNet50</cell><cell>45.8</cell><cell>82.0</cell><cell>46.4</cell><cell>20.1</cell><cell>40.2</cell><cell cols="2">52.3 11.0</cell></row><row><cell>Cascade RPN [45]</cell><cell>ResNet50</cell><cell>46.5</cell><cell>79.5</cell><cell>41.2</cell><cell>20.4</cell><cell>38.6</cell><cell>47.7</cell><cell>8.3</cell></row><row><cell>Faster R-CNN w/ PAFPN [31]</cell><cell>ResNet50</cell><cell>45.5</cell><cell>82.1</cell><cell>45.9</cell><cell>18.8</cell><cell>39.7</cell><cell cols="2">51.9 10.9</cell></row><row><cell>Double-Head [48]</cell><cell>ResNet50</cell><cell>45.3</cell><cell>81.5</cell><cell>45.7</cell><cell>20.2</cell><cell>40.0</cell><cell>51.4</cell><cell>5.7</cell></row><row><cell>Dynamic R-CNN [51]</cell><cell>ResNet50</cell><cell>45.6</cell><cell>80.1</cell><cell>47.3</cell><cell>19.0</cell><cell>39.7</cell><cell cols="2">52.1 12.1</cell></row><row><cell>Faster R-CNN w/ FPG [5]</cell><cell>ResNet50</cell><cell>45.4</cell><cell>81.6</cell><cell>46.0</cell><cell>19.8</cell><cell>39.7</cell><cell cols="2">51.4 13.1</cell></row><row><cell>GRoIE [41]</cell><cell>ResNet50</cell><cell>45.7</cell><cell>82.4</cell><cell>45.6</cell><cell>19.9</cell><cell>40.1</cell><cell>52.0</cell><cell>6.0</cell></row><row><cell>SABL+Faster R-CNN [46]</cell><cell>ResNet50</cell><cell>46.6</cell><cell>81.6</cell><cell>48.2</cell><cell>19.6</cell><cell>40.4</cell><cell>53.4</cell><cell>9.9</cell></row><row><cell>PISA [3]</cell><cell>ResNet50</cell><cell>46.3</cell><cell>82.1</cell><cell>47.4</cell><cell>20.8</cell><cell>40.8</cell><cell cols="2">52.6 10.3</cell></row><row><cell>Sparse R-CNN [43]</cell><cell>ResNet50</cell><cell>37.4</cell><cell>70.4</cell><cell>35.9</cell><cell>17.7</cell><cell>33.3</cell><cell>43</cell><cell>10.8</cell></row><row><cell>DetectoRS [36]</cell><cell>ResNet50</cell><cell>47.6</cell><cell>82.8</cell><cell>49.9</cell><cell>23.1</cell><cell>41.8</cell><cell>54.2</cell><cell>4.0</cell></row><row><cell>RoIAttn [27]</cell><cell>ResNet50</cell><cell>46.0</cell><cell>82.0</cell><cell>47.5</cell><cell>22.9</cell><cell>40.5</cell><cell>52.2</cell><cell>8.8</cell></row><row><cell>CenterNet2 [58]</cell><cell>ResNet50</cell><cell>47.2</cell><cell>81.6</cell><cell>49.8</cell><cell>18.2</cell><cell>41.3</cell><cell cols="2">53.4 14.2</cell></row><row><cell>CenterNet2* [58]</cell><cell>ResNet50</cell><cell>48.9</cell><cell>83.0</cell><cell>52.6</cell><cell>21.7</cell><cell>43.5</cell><cell cols="2">55.2 14.2</cell></row><row><cell>One-Stage Detector:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD512 [32]</cell><cell>VGG16</cell><cell>40.0</cell><cell>77.5</cell><cell>36.5</cell><cell>14.7</cell><cell>36.1</cell><cell cols="2">45.1 25.0</cell></row><row><cell>RetinaNet [28]</cell><cell>ResNet50</cell><cell>43.9</cell><cell>80.4</cell><cell>42.9</cell><cell>18.1</cell><cell>38.2</cell><cell cols="2">50.1 11.4</cell></row><row><cell>FSAF [61]</cell><cell>ResNet50</cell><cell>43.9</cell><cell>81.0</cell><cell>42.9</cell><cell>18.5</cell><cell>38.9</cell><cell cols="2">50.9 12.8</cell></row><row><cell>CenterNet [59]</cell><cell>ResNet18</cell><cell>31.3</cell><cell>61.1</cell><cell>27.6</cell><cell>11.9</cell><cell>32.5</cell><cell>33.4</cell><cell>6.2</cell></row><row><cell>FCOS [44]</cell><cell>ResNet50</cell><cell>43.9</cell><cell>81.1</cell><cell>43.0</cell><cell>19.9</cell><cell>38.2</cell><cell cols="2">50.4 12.7</cell></row><row><cell>RepPoints [50]</cell><cell>ResNet50</cell><cell>44.0</cell><cell>80.5</cell><cell>43.0</cell><cell>18.7</cell><cell>38.5</cell><cell cols="2">50.3 11.1</cell></row><row><cell>FreeAnchor [55]</cell><cell>ResNet50</cell><cell>46.3</cell><cell>82.3</cell><cell>46.9</cell><cell>21.0</cell><cell>40.5</cell><cell cols="2">52.6 11.4</cell></row><row><cell>RetinaNet w/ NASFPN [17]</cell><cell>ResNet50</cell><cell>37.4</cell><cell>70.3</cell><cell>35.8</cell><cell>12.4</cell><cell>36.4</cell><cell cols="2">40.4 13.8</cell></row><row><cell>ATSS [53]</cell><cell>ResNet50</cell><cell>46.2</cell><cell>82.5</cell><cell>46.9</cell><cell>19.7</cell><cell>41.4</cell><cell cols="2">52.4 11.8</cell></row><row><cell>PAA [20]</cell><cell>ResNet50</cell><cell>47.5</cell><cell>83.1</cell><cell>49.7</cell><cell>19.5</cell><cell>42.4</cell><cell>53.6</cell><cell>6.6</cell></row><row><cell>AutoAssign [60]</cell><cell>ResNet50</cell><cell>46.3</cell><cell>83.0</cell><cell>47.6</cell><cell>18.0</cell><cell>41.3</cell><cell cols="2">52.2 12.3</cell></row><row><cell>GFL [25]</cell><cell>ResNet50</cell><cell>46.4</cell><cell>81.9</cell><cell>47.8</cell><cell>19.3</cell><cell>40.9</cell><cell cols="2">52.5 12.7</cell></row><row><cell>VFNet [52]</cell><cell>ResNet50</cell><cell>44.0</cell><cell>79.3</cell><cell>44.1</cell><cell>18.8</cell><cell>38.1</cell><cell cols="2">50.4 10.5</cell></row><row><cell>Transfromer:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Deformable DETR [62]</cell><cell>ResNet50</cell><cell>46.6</cell><cell>84.1</cell><cell>47.0</cell><cell>24.1</cell><cell>42.4</cell><cell>51.9</cell><cell>7.6</cell></row><row><cell>Ours:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Boosting R-CNN</cell><cell>ResNet50</cell><cell>48.5</cell><cell>82.4</cell><cell>52.5</cell><cell>21.1</cell><cell>42.4</cell><cell cols="2">55.0 13.5</cell></row><row><cell>Boosting R-CNN*</cell><cell cols="2">ResNet50 51.4</cell><cell>85.5</cell><cell>56.8</cell><cell>23.8</cell><cell>45.8</cell><cell cols="2">57.8 13.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Comparisons with other object detection methods on Brackish dataset. "Baseline" is the performance reported in the original paper of Brackish.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell>AP50</cell></row><row><cell>Baseline (YOLOv3) [35]</cell><cell cols="2">DarkNet53 38.9</cell><cell>83.7</cell></row><row><cell>Faster R-CNN w/ FPN [21]</cell><cell>ResNet50</cell><cell>79.3</cell><cell>97.4</cell></row><row><cell>Cascade R-CNN [2]</cell><cell>ResNet50</cell><cell>80.7</cell><cell>96.9</cell></row><row><cell>RetinaNet [28]</cell><cell>ResNet50</cell><cell>78.0</cell><cell>96.5</cell></row><row><cell>DetectoRS [36]</cell><cell>ResNet50</cell><cell>81.6</cell><cell>97.0</cell></row><row><cell>CenterNet2 [58]</cell><cell>ResNet50</cell><cell>79.3</cell><cell>97.4</cell></row><row><cell>Boosting R-CNN</cell><cell>ResNet50</cell><cell>82.0</cell><cell>97.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Comparisons with other object detection methods on PASCAL VOC dataset. '*' means that our model uses the second training recipe. The performances of Faster R-CNN and RetinaNet are published by MMdetection. " ?" means our reimplementation. The performances of other methods are from their original paper.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input Size</cell><cell>mAP</cell></row><row><cell>Two-Stage Detector:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN w/ FPN [21]</cell><cell>ResNet50</cell><cell cols="2">1000 ? 600 79.5</cell></row><row><cell>MR-CNN [18]</cell><cell>VGG16</cell><cell cols="2">1000 ? 600 78.2</cell></row><row><cell>R-FCN [10]</cell><cell>ResNet101</cell><cell cols="2">1000 ? 600 80.5</cell></row><row><cell>RON384++ [22]</cell><cell>VGG16</cell><cell>384 ? 384</cell><cell>77.6</cell></row><row><cell>Cascade R-CNN ? [2]</cell><cell>ResNet50</cell><cell cols="2">1000 ? 600 80.0</cell></row><row><cell>CenterNet2 ? [58]</cell><cell>ResNet50</cell><cell cols="2">1000 ? 600 76.8</cell></row><row><cell>One-Stage Detector:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD300 [32]</cell><cell>VGG16</cell><cell>300 ? 300</cell><cell>74.3</cell></row><row><cell>SSD512 [32]</cell><cell>VGG16</cell><cell>512 ? 512</cell><cell>76.8</cell></row><row><cell>YOLO [38]</cell><cell>GoogleNet</cell><cell>448 ? 448</cell><cell>63.4</cell></row><row><cell>YOLOv2 [39]</cell><cell>DarkNet19</cell><cell>544 ? 544</cell><cell>78.6</cell></row><row><cell>RefineDet512 [54]</cell><cell>VGG16</cell><cell>512 ? 512</cell><cell>81.8</cell></row><row><cell>DSSD513 [15]</cell><cell>ResNet101</cell><cell>513 ? 513</cell><cell>81.5</cell></row><row><cell>RetinaNet [28]</cell><cell>ResNet50</cell><cell cols="2">1000 ? 600 77.3</cell></row><row><cell>FERNet [14]</cell><cell>VGG16+ResNet50</cell><cell>512 ? 512</cell><cell>81.0</cell></row><row><cell>Boosting R-CNN</cell><cell>ResNet50</cell><cell cols="2">1000 ? 600 81.9</cell></row><row><cell>Boosting R-CNN*</cell><cell>ResNet50</cell><cell cols="2">1000 ? 600 83.0</cell></row><row><cell></cell><cell cols="2">%5 %5ZRQRUP</cell><cell></cell></row><row><cell>$3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Comparisons with other object detection methods on MS COCO dataset with the large backbone in single-scale testing. '*' means that our model uses the second training recipe. The performances of other methods are published in their papers, and they all use 3x training time.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>Two-Stage Detector:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Faster R-CNN w/ FPN[40] ResNet101</cell><cell>36.2</cell><cell>59.1</cell><cell>39.0</cell><cell>18.2</cell><cell>39.0</cell><cell>48.2</cell></row><row><cell>Cascade R-CNN [2]</cell><cell>ResNet101</cell><cell>42.8</cell><cell>62.1</cell><cell>46.3</cell><cell>23.7</cell><cell>45.5</cell><cell>55.2</cell></row><row><cell>Grid R-CNN [33]</cell><cell>ResNet101</cell><cell>41.5</cell><cell>60.9</cell><cell>44.5</cell><cell>23.3</cell><cell>44.9</cell><cell>53.1</cell></row><row><cell>Libra R-CNN [34]</cell><cell>ResNeXt101-64x4d</cell><cell>43.0</cell><cell>64.0</cell><cell>47.0</cell><cell>25.3</cell><cell>45.6</cell><cell>54.6</cell></row><row><cell>Double-Head [48]</cell><cell>ResNet101</cell><cell>42.3</cell><cell>62.8</cell><cell>46.3</cell><cell>23.9</cell><cell>44.9</cell><cell>54.3</cell></row><row><cell>Dynamic R-CNN [51]</cell><cell>ResNet101-DCN</cell><cell>46.9</cell><cell>65.9</cell><cell>51.3</cell><cell>28.1</cell><cell>49.6</cell><cell>60.0</cell></row><row><cell>BorderDet [37]</cell><cell cols="2">ResNeXt101-64x4d-DCN 48.0</cell><cell>67.1</cell><cell>52.1</cell><cell>29.4</cell><cell>50.7</cell><cell>60.5</cell></row><row><cell>TridentNet [26]</cell><cell>ResNet101-DCN</cell><cell>46.8</cell><cell>67.6</cell><cell>51.5</cell><cell>28.0</cell><cell>51.2</cell><cell>60.5</cell></row><row><cell>CPN [12]</cell><cell>HG104</cell><cell>47.0</cell><cell>65.0</cell><cell>51.0</cell><cell>26.5</cell><cell>50.2</cell><cell>60.7</cell></row><row><cell>One-Stage Detector:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCOS [44]</cell><cell cols="2">ResNeXt101-64x4d-DCN 46.6</cell><cell>65.9</cell><cell>50.8</cell><cell>28.6</cell><cell>49.1</cell><cell>58.6</cell></row><row><cell>CornerNet [23]</cell><cell>HG104</cell><cell>40.6</cell><cell>56.4</cell><cell>43.2</cell><cell>19.1</cell><cell>42.8</cell><cell>54.3</cell></row><row><cell>CenterNet [59]</cell><cell>HG104</cell><cell>42.1</cell><cell>61.1</cell><cell>45.9</cell><cell>24.1</cell><cell>45.5</cell><cell>52.8</cell></row><row><cell>CentripetalNet [11]</cell><cell>HG104</cell><cell>46.1</cell><cell>63.1</cell><cell>49.7</cell><cell>25.3</cell><cell>48.7</cell><cell>59.2</cell></row><row><cell>RetinaNet [28]</cell><cell>ResNet101</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>FSAF [61]</cell><cell>ResNeXt101-64x4d</cell><cell>42.9</cell><cell>63.8</cell><cell>46.3</cell><cell>26.6</cell><cell>46.2</cell><cell>52.7</cell></row><row><cell>RepPoints [50]</cell><cell>ResNet101-DCN</cell><cell>45.0</cell><cell>66.1</cell><cell>49.0</cell><cell>26.6</cell><cell>48.6</cell><cell>57.5</cell></row><row><cell>RepPointsV2</cell><cell>ResNet101-DCN</cell><cell>48.1</cell><cell>67.5</cell><cell>51.8</cell><cell>28.7</cell><cell>50.9</cell><cell>60.8</cell></row><row><cell>FreeAnchor [55]</cell><cell>ResNeXt101-32x8d</cell><cell>46.0</cell><cell>65.6</cell><cell>49.8</cell><cell>27.8</cell><cell>49.5</cell><cell>57.7</cell></row><row><cell>ATSS [53]</cell><cell cols="2">ResNeXt101-32x8d-DCN 47.7</cell><cell>66.5</cell><cell>51.9</cell><cell>29.7</cell><cell>50.8</cell><cell>59.4</cell></row><row><cell>PAA [20]</cell><cell cols="2">ResNeXt101-64x4d-DCN 49.0</cell><cell>67.8</cell><cell>53.3</cell><cell>30.2</cell><cell>52.8</cell><cell>62.2</cell></row><row><cell>AutoAssign [60]</cell><cell cols="2">ResNeXt101-64x4d-DCN 49.5</cell><cell>68.7</cell><cell>54.0</cell><cell>29.9</cell><cell>52.6</cell><cell>62.0</cell></row><row><cell>GFL [25]</cell><cell cols="2">ResNeXt101-32x4d-DCN 48.2</cell><cell>67.4</cell><cell>52.6</cell><cell>29.2</cell><cell>51.7</cell><cell>60.2</cell></row><row><cell>GFLV2 [24]</cell><cell>Res2Net101-DCN</cell><cell>50.6</cell><cell>69.0</cell><cell>55.3</cell><cell>31.3</cell><cell>54.3</cell><cell>63.5</cell></row><row><cell>YOLOv4 [1]</cell><cell>CSPDarkNet-53</cell><cell>43.5</cell><cell>65.7</cell><cell>47.3</cell><cell>26.7</cell><cell>46.7</cell><cell>53.3</cell></row><row><cell>Transfromer:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DETR [4]</cell><cell>ResNet101</cell><cell>43.5</cell><cell>63.8</cell><cell>46.4</cell><cell>21.9</cell><cell>48.0</cell><cell>61.8</cell></row><row><cell>Deformable DETR [62]</cell><cell cols="2">ResNeXt101-64x4d-DCN 50.1</cell><cell>69.7</cell><cell>54.6</cell><cell>30.6</cell><cell>52.8</cell><cell>65.6</cell></row><row><cell>Ours:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Boosting R-CNN*</cell><cell>ResNet50</cell><cell>44.4</cell><cell>63.9</cell><cell>48.2</cell><cell>26.9</cell><cell>47.0</cell><cell>54.8</cell></row><row><cell>Boosting R-CNN*</cell><cell>Res2Net101-DCN</cell><cell>50.7</cell><cell>69.2</cell><cell>55.8</cell><cell>31.7</cell><cell>54.1</cell><cell>63.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<title level="m">Optimal speed and accuracy of object detection</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prime sample attention in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03580</idno>
		<title level="m">2020a. Feature pyramid grids</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10006</idno>
		<title level="m">Swipenet: Object detection in noisy underwater images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Revisiting feature alignment for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01570</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Underwater salient object detection by combining 2d and 3d visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10519" to="10528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Corner proposal network for anchor-free, two-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge. International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual refinement underwater object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dssd: Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn for marine organisms detection and recognition using data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">337</biblScope>
			<biblScope unit="page" from="372" to="384" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parallel feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Kook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5936" to="5944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6054" to="6063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Excavating roi attention for underwater object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Roimix: Proposalfusion among multiple images for underwater object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics,Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detection of marine animals in a new underwater dataset with varying visibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruslund Haurum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="549" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A novel region of interest extraction layer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13665</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-toend object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cascade rpn: Delving into high-quality region proposal network with adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Side-aware boundary localization for more precise object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Udd: an underwater open-sea farm object detection dataset for underwater robot picking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01446</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking classification and localization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Detec-tron2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic r-cnn: Towards high quality object detection via dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Varifocalnet: An iou-aware dense object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sunderhauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08158</idno>
		<title level="m">2021b. Focal and efficient iou loss for accurate bounding box regression</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Composited fishnet: Fish detection and species recognition from low-quality underwater videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4719" to="4734" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Probabilistic two-stage detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03496</idno>
		<title level="m">Autoassign: Differentiable label assignment for dense object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Feature selective anchorfree module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
