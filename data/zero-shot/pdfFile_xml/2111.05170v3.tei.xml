<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Robust Unsupervised Video Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-12">12 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghao</forename><surname>Zang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Shu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518034</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IET Research Journals</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">IET Research Journals</orgName>
								<address>
									<addrLine>pp. 1-10</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Robust Unsupervised Video Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-12">12 Feb 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised video person re-identification (reID) methods usually depend on global-level features. And many supervised reID methods employed local-level features and achieved significant performance improvements. However, applying local-level features to unsupervised methods may introduce an unstable performance. To improve the performance stability for unsupervised video reID, this paper introduces a general scheme fusing part models and unsupervised learning. In this scheme, the global-level feature is divided into equal local-level feature. A local-aware module is employed to explore the poentials of local-level feature for unsupervised learning. A global-aware module is proposed to overcome the disadvantages of local-level features. Features from these two modules are fused to form a robust feature representation for each input image. This feature representation has the advantages of local-level feature without suffering from its disadvantages. Comprehensive experiments are conducted on three benchmarks, including PRID2011, iLIDS-VID, and DukeMTMC-VideoReID, and the results demonstrate that the proposed approach achieves state-of-the-art performance. Extensive ablation studies demonstrate the effectiveness and robustness of proposed scheme, local-aware module and global-aware module. The code and generated features are available at https://github.com/deropty/uPMnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (reID) aims to find the correct person from gallery for the person of interest in query within non-overlapping cameras, which is an important practical application in surveillance camera network <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b2">[3]</ref>. In recent years, video-based reID has gained increasing attention because video sequences can provide rich temporal and spatial information for a specific person identity <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b5">[6]</ref>. Meanwhile, with development of convolution neural networks (CNN), supervised video reID has been improved gradually. However, as the number of cameras increases, practical applications are largely limited due to the growing cost of labeling work. Consequently, this situation motivates researchers to develop unsupervised methods.</p><p>In the context of unsupervised learning, approaches for reID can be roughly divided into four categories: 1) transfer learningbased methods <ref type="bibr" target="#b6">[7]</ref> [8] <ref type="bibr" target="#b8">[9]</ref>; 2) one-shot learning-based methods <ref type="bibr" target="#b9">[10]</ref> [11] <ref type="bibr" target="#b11">[12]</ref>; 3) clustering-based methods <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b13">[14]</ref> [15]; 4) trackletbased association learning <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr" target="#b18">[19]</ref>. These methods above employed global-level features to achieve unsupervised learning. However, more detailed feature representation often introduces remarkable performance improvement. For example, supervised methods for reID task employed local-level features <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b20">[21]</ref>, texture semantics <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b22">[23]</ref>, skeleton point information <ref type="bibr" target="#b23">[24]</ref> [25] <ref type="bibr" target="#b25">[26]</ref>, etc., and achieve a better performance.</p><p>It is well known that part models-based methods are much common for the reID task in a supervised setting, even overstudied <ref type="bibr" target="#b19">[20]</ref> [27] <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b23">[24]</ref> [29] <ref type="bibr" target="#b20">[21]</ref>. These methods improved performance significantly in the context of supervised learning. For unsupervised learning, the performance of part models remains to be explored. Thus, the unsupervised method DAL <ref type="bibr" target="#b29">[30]</ref> is employed directly to train each local-level feature individually. MobileNet <ref type="bibr" target="#b30">[31]</ref> is used as the backbone for rapid deployment. For different benchmarks, the performances are uncertain, as illustrated in <ref type="figure">Fig. 1</ref>. Significantly, there is an apparent performance drop for the benchmark iLIDS-VID <ref type="bibr" target="#b31">[32]</ref>.</p><p>The possible reason for these uncertain performances is below. For the reID task, the same body parts of different persons usually have a more similar appearance than their holistic image <ref type="bibr" target="#b32">[33]</ref>, as  <ref type="figure">Fig. 1</ref>: Directly applying part models to unsupervised video reID has an unstable performance (k is the number of parts). The Baseline method only employs a global-level feature, i.e., k=1. '+/-' means the percentage increase or decrease relative to the baseline method. illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. These similar appearances make it hard to distinguish the same body parts of different persons, which introduces a negative effect. On the other hand, the dimension of concatenated local-level features is several times the global one, which presents a richer feature representation and has a positive effect. These contrary effects of part models on unsupervised learning lead to uncertain performances.</p><p>To improve the performance stability, this paper introduces a robust part models-based scheme for unsupervised video reID, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Global-level feature is divided into equal locallevel features. The global-level feature and local-level features are collected and delivered to local/global-aware module. The localaware module is used to explore the potentials of local-level features for unsupervised learning. The global-aware module is introduced to overcome the disadvantages of local-level features. The globalaware module increases the robustness of proposed scheme in various challenging situations and has a complementary performance with the local-aware module. The proposed scheme is trained twice where the local-aware module and the global-aware module are incorporated, respectively. The features from these two modules are fused to form a rich and robust feature representation. This feature representation has the superiority of part models and discards its shortcomings. Through extensive experiments, proposed scheme achieves state-of-the-art performance on three benchmarks, i.e., PRID2011 <ref type="bibr" target="#b33">[34]</ref>, iLIDS-VID <ref type="bibr" target="#b31">[32]</ref>, and DukeMTMC-VideoReID <ref type="bibr" target="#b34">[35]</ref>  <ref type="bibr" target="#b35">[36]</ref>. The ablation study also demonstrates the effectiveness of proposed scheme and modules.</p><p>The main contributions of this paper can be summarized as follows:</p><p>? This paper introduces a robust part models-based scheme for unsupervised video reID, which makes it easier to explore the different effects of part models on unsupervised learning. ? A global-aware module is proposed to improve the robustness of part models. The features from the local-aware module and globalaware module are fused to form a robust feature representation for a better unsupervised video reID.</p><p>? Experimental results demonstrate that proposed method achieves significant performance gains. Specifically, the proposed method achieves Rank-1 improvements of 6.7% on PRID2011, 5.8% on iLIDS-VID, and 0.8% on DukeMTMC-VideoReID, compared to other state-of-the-art unsupervised methods.</p><p>The rest of this paper is organized as follows. The related works are reviewed and analyzed in Section 2, and then proposed method is introduced in Section 3. Experimental results and analysis are presented in Section 4, and Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most video reID methods are formulated in a supervised manner, which hinders their deployment in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Video Person Re-identification</head><p>Unsupervised video reID has attracted increasing research interest recently <ref type="bibr" target="#b36">[37]</ref> [38] [39] <ref type="bibr" target="#b39">[40]</ref>. These methods can be roughly divided into four categories as explained in Section 1.</p><p>Transfer Learning: Transfer learning-based methods, also called domain adaption-based methods, is an important branch of unsupervised approaches, aiming to transfer the source domain-trained model to the target domain <ref type="bibr" target="#b40">[41]</ref> [42]. Huang et al. <ref type="bibr" target="#b42">[43]</ref> introduced a domain adaptive attention model (DAAM) to separate the feature map into domain-shared feature map and domain-specific feature map. These two features were used to improve the performance in target domain and alleviate the negative effects introduced by domain divergence, respectively. However, these transfer learningbased methods require one labeled source dataset, which still needs a lot of labeling work.</p><p>One-shot Learning: One-shot learning-based methods <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b11">[12]</ref> often assume the prior spatio-temporal topology knowledge could be used to achieve person identities (labels). Specifically, Wu et al. <ref type="bibr" target="#b35">[36]</ref> proposed a dynamic sampling strategy, which started with easy and reliable unlabeled samples and incorporated diverse tracklets, to update their model. Ye et al. <ref type="bibr" target="#b10">[11]</ref> designed an anchor embedding method with regularized affine hull and a manifold smoothing term for this task. Ye et al. <ref type="bibr" target="#b11">[12]</ref> introduced a dynamic graph matching framework to estimate cross-camera labels and used learned metrics to update camera graphs dynamically. However, these methods still require one labeled tracklet per identity for model initialization, which only partially reduces the annotation workload.</p><p>Clustering Analysis: The clustering-based method is a longstanding paradigm for unsupervised learning. With the surge of deep CNN, recent studies have attempted to optimize clustering analysis and representation learning jointly <ref type="bibr" target="#b13">[14]</ref> [15]. Lin et al. <ref type="bibr" target="#b13">[14]</ref> proposed a bottom-up clustering approach that jointly optimized a CNN model and the relationship among the individual samples. Ding et al. <ref type="bibr" target="#b14">[15]</ref> introduced a dispersion-based criterion to evaluate the quality of the automatically generated clusters, which showed the importance of cluster validity quantification. However, only focusing on cluster-level repelling and merging neglects the latent variational information among the individual samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracklet Association Learning:</head><p>Recently, some tracklet-based methods <ref type="bibr" target="#b16">[17]</ref> [18] <ref type="bibr" target="#b18">[19]</ref> have been proposed to achieve unsupervised video reID. Chen et al. <ref type="bibr" target="#b16">[17]</ref> and Li et al. <ref type="bibr" target="#b17">[18]</ref> explored end-to-end learning architectures to associate within-camera and cross-camera tracklets by optimizing specifically tailored objective functions. Wu et al. <ref type="bibr" target="#b18">[19]</ref> designed multiple unsupervised learning objectives including tracklet frame coherence, tracklet neighborhood compactness, and tracklet cluster structure in a unified formulation for both image-based and video-based unsupervised reID. However, these methods extract global-level features as compact representation, which neglects the fine-grained part information (e.g., head, torso) and constrains the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part-based Person Re-identification</head><p>Employing local-level features for pedestrian image description offers fine-grained information and has been verified as beneficial for person retrieval in recent studies. These part-based methods fall into three categories: 1) using pose estimator to extract a pose map <ref type="bibr" target="#b43">[44]</ref>; 2) leveraging saliency map to implicitly estimate body part <ref type="bibr" target="#b44">[45]</ref>;  <ref type="bibr" target="#b25">[26]</ref> introduced pose-guided feature alignment (PGFA) to disentangle the useful information from the occlusion noise. The pose-guided global-level feature and partial features were used to measure the distances of possible image pairs. Gao et al. <ref type="bibr" target="#b49">[50]</ref> employed a pose estimator to perform spatial and temporal alignment and selected image sets with the same pose and high quality for video reID. However, these methods above often depend on a pre-trained pose estimator which introduces inevitable bias due to the differences between the pre-trained dataset and reID dataset.</p><p>Saliency Learning: Salient regions of person images provide valuable information for reID task <ref type="bibr" target="#b50">[51]</ref> [52]. Zhao et al. <ref type="bibr" target="#b52">[53]</ref> employed human salience to extract salient patches and utilized patch matching with adjacency constraint to build dense correspondence for different image pairs in an unsupervised manner. He et al. <ref type="bibr" target="#b53">[54]</ref> utilized the saliency heatmap to generate the adaptive weights for different image parts and achieved guided adaptive spatial matching (GASM) for reID task. However, these methods above depend on the off-the-shelf saliency detector which also introduces inevitable bias. strategy and assembled the part-informed features into a convolutional descriptor, which significantly improved the performance for supervised reID. Considering the relationships between one body part and other body parts, Park and Ham <ref type="bibr" target="#b47">[48]</ref> presented a relation network that makes each local-level feature more discriminative. These methods above employed the person identity to supervise the model to learn each part individually. Then these methods assembled local-level features for the test. This strategy gave each feature discriminative information and introduced a remarkable performance improvement. However, applying part models to unsupervised video reID may introduce an unstable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The proposed scheme is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, which is an unsupervised Part Models-based network (uPMnet) for the video reID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Given a large quantity of video data captured by disjoint surveillance cameras {C 1 , C 2 , ? ? ? }, each camera C contains a varying number of tracklets T , as C = {T 1 , T 2 , ? ? ? }. Each tracklet T consists of multiple person images I, as T = {I 1 , I 2 , ? ? ? }. The tracklet information (tracklet ID) and camera information (camera ID) of each image are automatically labeled during annotation. Thus it is assumed that these pieces of information are available for the unsupervised video reID. The function of each part in <ref type="figure" target="#fig_2">Fig. 3</ref> is explained below. In the first part of proposed scheme, i.e., feature extraction, CNN is employed to extract an initial feature map f with a size of h ? w ? c (h, w, c are height, width, and the number of channels, respectively) from a person image. In the second part, i.e., local/global-aware feature generation, feature map f is divided equally into k horizontal stripes, which follows the operations in <ref type="bibr" target="#b19">[20]</ref> [48] <ref type="bibr" target="#b28">[29]</ref>. Each stripe has a smaller receptive field than the global-level feature f . These stripes are local-level features and are called part models. These stripes and original feature map are followed by global average pooling (GAP) and normalization to generate the local-level features {x i } k i=1 and global-level feature x 0 . Each of these features is regarded as a compact feature with a size of 1 ? 1 ? c. All the above features are collected and delivered to aware modules which are designed to deal with the local-level and global-level features and output local/globalaware features for the following part. The local-aware module and global-aware module are introduced in Section 3.2 and Section 3.3.</p><p>In the third part of this scheme, i.e., unsupervised learning, k modules are utilized to achieve unsupervised learning. These modules have the same operation but different network parameters. Each module independently handles the corresponding feature. Section 3.4 introduces the third part of this scheme. And Section 3.5 presents the details of model training and testing.</p><p>In supervised methods <ref type="bibr" target="#b45">[46]</ref> [47], the global max pooling (GMP) is employed to generate the compact feature. The GMP operation can aggregate the feature from the most discriminative parts in person image. In this paper, global average pooling (GAP) is empirically used for unsupervised learning, and the GAP operation introduces an undiscriminating feature representation covering the whole image. The experimental details will be discussed in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local-aware Module</head><p>In the second part of proposed scheme, the aware module can be local-aware module or global-aware module. The local-aware module is introduced first as,</p><formula xml:id="formula_0">x l i = x i , i = (1, ? ? ? , k),<label>(1)</label></formula><p>where only the local-level features {x i } k i=1 are used. These locallevel features can introduce significant performance improvement for supervised reID <ref type="bibr" target="#b19">[20]</ref>. Here the potentials of local-level features on unsupervised learning are explored. Therefore, only the locallevel features are employed. The Eq. 1 is denoted as local-aware module. The compact local-level features</p><formula xml:id="formula_1">{x i } k i=1 are regarded as local-aware features {x l i } k i=1</formula><p>. For unsupervised problems, the performance of local-aware feature is uncertain. First, the concatenated local-aware feature is k times the original global-level feature because each local-aware feature and global-level feature have the same dimension. The feature with a larger dimension often introduces better performance <ref type="bibr" target="#b19">[20]</ref>. Second, each part has a smaller receptive field. The difference between the same body part of different persons is much smaller than the difference between the different persons. Distinguishing these local-level features is much harder than distinguishing the global-level features <ref type="bibr" target="#b32">[33]</ref>. Employing part models may be harmful to unsupervised learning. Therefore, compared to global-level features, the part models introduce positive and negative effects. The local-aware module is proposed to explore the pros and cons of part models for the unsupervised video reID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global-aware Module</head><p>The global-aware module is introduced in this section, and the motivation is explained first. When many background clutter and occlusions appear on an image, the same part of different images may give a more similar feature representation than their holistic images. In this situation, the negative effect of part models is amplified. A global-aware module is proposed to optimize this issue, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. Concretely, a 1 ? 1 convolutional layer is added to each local-level feature x i (i = 1, ? ? ? , k) and global one x 0 to generate feature vectors {x i } k i=0 with a size of 1 ? 1 ? c ? . Thenx 0 is concatenated to the end of each local-level featur? x i (i = 1, ? ? ? , k), which outputs new feature vectors with a size of 1 ? 1 ? 2c ? . These new feature vectors are followed by a subnetwork consisting of a 1 ? 1 convolution, batch normalization, and ReLU layers. The function F(x i ,x 0 , {W i }) is utilized to represent the concatenation operation and the following sub-network. The output of each F is a residual feature r i with a size of 1 ? 1 ? c ? . A shortcut connection <ref type="bibr" target="#b54">[55]</ref> is used to combine the residual feature r i and global featurex 0 to generate the global-aware feature x g i ,</p><formula xml:id="formula_2">x g i = F(x i ,x 0 , {W i }) +x 0 , i = (1, ? ? ? , k).<label>(2)</label></formula><p>sub-network The operation F +x 0 is achieved by an element-wise addition. The size of global-aware feature x g i is 1 ? 1 ? c ? . The weights of k global-aware modules are not shared to ensure independent training for each aware module.</p><p>The principle of the global-aware module is explained below. When dealing with the image with many background clutter and occlusions, the global-aware module explores the relationship between the global-level and local-level features using the function</p><formula xml:id="formula_3">F(x i ,x 0 , {W i }).</formula><p>Then a shortcut connection between the residual feature r i and global featurex 0 is used to make the global-aware feature x g i overcome the disadvantages of part models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unsupervised Learning</head><p>In this paper, a specific unsupervised method is not the emphasis. Therefore, the existing unsupervised method, i.e., Deep Association Learning (DAL) <ref type="bibr" target="#b16">[17]</ref>, is employed and incorporated into proposed scheme. Here, the formulation of this unsupervised module is presented using new notations.</p><p>Anchor Update: An intra-camera anchor I is introduced to represent the feature center of each tracklet T , and the part intra-camera anchor I i is employed to represent the part tracklet T i . The part intra-camera anchor I i is incrementally updated using Exponential</p><p>Moving Average (EMA) strategy as follows,</p><formula xml:id="formula_4">I t+1 i = I t i ? ?(I t i ? x ? i ), i = (1, ? ? ? , k),<label>(3)</label></formula><p>where i represent the i th part, ? refers to the update rate, and t is the mini-batch learning iteration. x ? i represents the local/global-aware feature. Since the camera and tracklet IDs are known, the features of the same body part from the person image within one tracklet</p><formula xml:id="formula_5">{x ? i } k i=1</formula><p>are used to form the part inter-camera anchor I i . The global Cyclic Ranking Consistency (CRC) <ref type="bibr" target="#b29">[30]</ref> is employed to explore the relationship between images from different cameras. For the specific I i and? i , they are the same part of different tracklets and are from different camera views. The CRC means that the Euclidean distance between them is the smallest. Therefore, I i and I i is likely to be the same person from different cameras. Since I i and? i are highly related, a set of part cross-camera anchors C i is introduced to denote their average feature representation,</p><formula xml:id="formula_6">C t+1 i = ? ? ? 1 2 (I t+1 i +? t i ), s.t. CRC I t+1 i , others,<label>(4)</label></formula><p>where s.t. is the abbreviation of subject to.</p><p>Distance Metric: In the training process, each mini-batch contains M person images from different camera views. Each feature x ? i represents a local/global-aware feature of one person image. This image is from one particular tracklet, i.e., its source tracklet. Therefore, each feature x ? i has one source part intra-camera anchor I i . The Euclidean distances between each feature x ? i and all part intracamera anchors I i from the same camera view are computed. The smallest distance of these Euclidean distances is denoted as D min i , which means feature x ? i belongs to this tracklet with a high possibility. Since there are many part intra-camera anchors I i , the distance between feature x ? i and its source part intra-camera anchor I i is denoted as D I i , which may not be the smallest. The distance D C i can be obtained via the same operation by feature x ? i and its source part cross-camera anchor C i . For the images from the same camera view in a mini-batch, the average value of the smallest distances D min i is denoted asD i . This average valueD i is used to ensure each feature has the same distance with its feature center, i.e., its source part intra-camera anchor I i . The k unsupervised modules are trained individually. Therefore, the distances above are calculated using the i th feature and its corresponding part intra/cross-camera anchors.</p><p>Association Loss: Two top-push margin-based association losses are used. The first one is intra-camera association loss,</p><formula xml:id="formula_7">L I i = [D I i ? D min i + m] + , D I i = D min i [D I i ?D i + m] + , D I i = D min i ,<label>(5)</label></formula><p>where [?] + = max(0, ?), and m is the margin enforcing the deep model to assign the source part tracklet as the top-rank. The second one is the cross-camera association loss,</p><formula xml:id="formula_8">L C i = [D C i ? D min i + m] + , D I i = D min i [D C i ?D i + m] + , D I i = D min i .<label>(6)</label></formula><p>The goal of L I i is to associate feature x ? i with its source part intracamera anchor I i at a proper distance. Meanwhile, L C i is to pull the part intra-camera anchor I i close to its CRC part intra-camera anchor? i . The final learning objective for this unsupervised module is to optimize two association losses jointly,</p><formula xml:id="formula_9">L u i = L I i + ?L C i ,<label>(7)</label></formula><p>where ? is a trade-off parameter balancing these two losses. Besides, other unsupervised methods can be employed to deal with the different features x ? i in the same manner, which makes proposed scheme a general one.</p><p>Then k learning objectives L u i are employed to calculate the final loss L,</p><formula xml:id="formula_10">L = 1 k k i=1 L u i .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Training and Testing</head><p>In the training process, there are n warmup epochs, where the part cross-camera anchor C i is always set as its corresponding I i as following,</p><formula xml:id="formula_11">C t i = I t i , t in n epoches.<label>(9)</label></formula><p>After n warmup epochs, the part cross-camera anchors are updated using Eq. 4 for the rest of the epochs. This trick ensures that the model learns intra-camera association in warmup epochs and then learns intra-camera and cross-camera association jointly. The proposed scheme is trained twice where local-aware module and global-aware module are incorporated, respectively. In each training stage, proposed scheme learns different perceptive abilities. The scheme using local-aware module receives a smaller receptive field for each part and has an unstable performance. The scheme using global-aware module explores the relationship between the local-level feature and global-level feature, which provides a general performance improvement. The local-aware module is based on local-level features, and the global-aware module is based on global-level features.</p><p>After the training process, the features from these two modules are fused to form a robust feature representation for each input image, as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>. Concretely, each image is represented by k local-aware features with a size of 1 ? 1 ? c and k globalaware features with a size of 1 ? 1 ? c ? . Then the corresponding ones, x l i and x g i , are concatenated to form k features</p><formula xml:id="formula_12">{x i } k i=1 with a size of 1 ? 1 ? (c + c ? ). After normalization, k features {x i } k i=1</formula><p>are concatenated to form the robust feature representation with a size of 1 ? 1 ? k(c + c ? ) for each image. Each tracklet consists of N person images. Thus the size of the feature for each tracklet is N ? k(c + c ? ). A max operation is applied to this feature to select the most discriminative value along the N images to generate the feature representation with a size of 1 ? k(c + c ? ). After normalization, this generated feature is regarded as the feature of each tracklet for the test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Comprehensive experiments are conducted to validate the effectiveness of proposed uPMnet. Experimental settings, experimental results, ablation study are given in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets: The proposed uPMnet is evaluated on three video-based reID benchmarks. ? iLIDS-VID <ref type="bibr" target="#b31">[32]</ref> has been collected from two disjoint cameras at an airport hall. It consists of 600 tracklets of 300 persons. There are two sequences from different camera views for each person, where each tracklet has an average duration of 73 frames. It is a quite challenging benchmark due to significant lighting, viewpoint variations, severe background clutter, and mutual occlusions.</p><p>? DukeMTMC-VideoReID is derived from the image-based benchmark, DukeMTMC <ref type="bibr" target="#b34">[35]</ref>, and re-organized by Wu et al. <ref type="bibr" target="#b35">[36]</ref>. DukeMTMC-VideoReID contains 4832 tracklets and 1812 identities in total, and each tracklet has 168 frames on average.</p><p>Evaluation Protocol: For PRID2011 and iLIDS-VID benchmarks, the whole set of tracklet pairs are randomly divided into two halves for training and testing in multiple trials. These tracklets of the same person from two camera views compose the probe set and gallery set, respectively. The trials are repeated ten times to ensure a statistically stable result. For DukeMTMC-VideoReID benchmarks, the training and testing split manners are followed the experimental setting of <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b61">[62]</ref>.</p><p>Metrics: The performances are measured by average Cumulated Matching Characteristics (CMC) curves. The Rank-1, Rank-5, Rank-20 scores are employed to represent the CMC curve. For DukeMTMC-VideoReID benchmarks, the mean Average Precision (mAP) are also used to measure the performance. For PRID2011 and iLIDS-VID benchmarks, each person ID has only one correct tracklet in the gallery. In other words, each person ID has only one precision value. It is not necessary to calculate the "average precision (AP)". And the CMC metric is enough to measure the ranked gallery list. Therefore, the mAP metric is not used for these two benchmarks.</p><p>Implementation Details: The proposed uPMnet is implemented using Tensorflow <ref type="bibr" target="#b62">[63]</ref>. For PRID2011 and iLID-VID, RMSProp with an initial learning rate of 4.5 ? 10 ?2 is used to train the models 2 ? 10 4 and 1.5 ? 10 4 iterations, respectively. For DukeMTMC-VideoReID, standard stochastic gradient descent (SGD) with learning rate initialized to 1 ? 10 ?2 with momentum is used to train the models 2.5 ? 10 4 iterations. For all datasets, the batch size M is set to 64, and person images are resized to 256 ? 128. The pretrained ResNet50 <ref type="bibr" target="#b54">[55]</ref> and MobileNet <ref type="bibr" target="#b30">[31]</ref>   <ref type="bibr" target="#b49">[50]</ref> proposed to assign different weights for each person image from the same tracklet, studying the adaptive weight is not the emphasis of this paper.  <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" target="#b2">3</ref> The best results are in bold. 4 ' ?' : supervised method, AGRL, is listed as an upper bound of performance on each benchmark. <ref type="bibr" target="#b4">5</ref> On each benchmark, the reported best results of each method are listed.  <ref type="table" target="#tab_3">Table 1</ref> illustrates the comparison results between proposed uPMnet with other state-of-the-art methods. These methods are OIM <ref type="bibr" target="#b57">[58]</ref>, TAUDL <ref type="bibr" target="#b17">[18]</ref>, DAL <ref type="bibr" target="#b16">[17]</ref>, BUC <ref type="bibr" target="#b13">[14]</ref>, UGA <ref type="bibr" target="#b37">[38]</ref>, DBC <ref type="bibr" target="#b14">[15]</ref>, UTAL <ref type="bibr" target="#b15">[16]</ref>, TSSL <ref type="bibr" target="#b18">[19]</ref>, SSL <ref type="bibr" target="#b58">[59]</ref>, NHAC <ref type="bibr" target="#b59">[60]</ref>. A supervised method, AGRL <ref type="bibr" target="#b60">[61]</ref>, is also listed as an upper bound of performance on each benchmark. Among all these existing methods, proposed uPMnet is the only one using part models-based CNN model. On each benchmark, the reported best results of each method are listed. As illustrated in <ref type="table" target="#tab_3">Table 1</ref>, proposed uPMnet with different backbones achieve the first and second place alternatively on these three benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on RPID2011:</head><p>Compared to the other state-of-the-art approaches on PRID2011, proposed uPMnet achieves the best performance. There is a large Rank-1 improvement of 6.7% (92.0-85.3) between proposed uPMnet and the best competitor, DAL (ResNet50) <ref type="bibr" target="#b16">[17]</ref>. In this paper, DAL is the baseline method, which uses two margin-based association losses to constrain each person image feature to its source intra-camera and cross-camera feature center. The DAL uses the global-level feature only. The significant performance improvement demonstrates the effectiveness of proposed uPMnet. Besides, proposed uPMnet is the first model that achieves above 90.0% on Rank-1, which is very close to the result of supervised method AGRL <ref type="bibr" target="#b60">[61]</ref>. <ref type="table" target="#tab_3">Table 1</ref> illustrates that proposed uPMnet achieves the best performance on iLIDS-VID benchmark. Compared to PRID and DukeMTMC-VideoReID benchmarks, iLIDS-VID is a challenging one due to its surveillance environment. As illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref>, background clutter and mutual occlusions are pervasive in iLIDS-VID benchmark <ref type="bibr" target="#b31">[32]</ref>. However, compared to the other stateof-the-art approaches, proposed uPMnet still achieves improvement by a large Rank-1 improvement of 5.8% (63.1-57.3) over the best competitor, UGA <ref type="bibr" target="#b37">[38]</ref>. The UGA employed a two-stage training manner to learn a view-invariant feature representation from the possible images. While proposed uPMnet introduces a robust part models-based scheme and train the model in an end-to-end fashion. This comparison demonstrates that part models with elaborate design can achieve a robust performance for the unsupervised video reID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on iLIDS-VID:</head><p>Results on DukeMTMC-VideoReID: On DukeMTMC-VideoReID, proposed uPMnet also achieves state-of-the-art performance. Compared to PRID2011 and iLIDS-VID, DukeMTMC-VideoReID is a large-scale benchmark with 4832 tracklets in total, which is eight times of iLIDS-VID (600 tracklets) and thirteen times of PRID2011 (356 tracklets). The proposed uPMnet achieves performance improvement by a Rank-1 improvement of 0.8% (83. <ref type="bibr">6-82.8)</ref> and an mAP improvement of 0.9% (76.9-76.0) over the best competitor, NHAC <ref type="bibr" target="#b59">[60]</ref>. The NHAC method proposed a graph trimming module and a node re-sampling module to improve the clusteringbased unsupervised learning method. Proposed uPMnet depends on the camera ID to explore the intra-camera and cross-camera association learning and achieves better performance in this large-scale benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>The ablation study for proposed scheme with different backbones is illustrated in <ref type="table" target="#tab_4">Table 2</ref>. The baseline method is a standard DAL <ref type="bibr" target="#b29">[30]</ref> and is a special case of proposed scheme. For the baseline method, only the global-level feature x 0 is used, and there is only one unsupervised module to operate this feature x 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Proposed Local-aware Module:</head><p>In this section, proposed scheme with the local-aware module is analyzed. When using MobileNet as the backbone, proposed scheme achieves an average Rank-1 improvement of 5.7% and mAP improvement of 1.3% on the PRID2011 and DukeMTMC-VideoReID benchmarks compared to the baseline. The performance gap can be up to 9.2% (91.5-82.3, the Rank-1 gap on PRID2011 benchmark compared to the baseline). When using ResNet50 as the backbone, proposed scheme achieves an average Rank-1 improvement of 3.8% and mAP improvement of 4.3% on three benchmarks compared to the baseline method. Since the local-aware module only employs the local-level information, these performance improvements demonstrate that part models can improve the performance for unsupervised learning to some extent.</p><p>For the iLIDS-VID benchmark, proposed scheme using MobileNet as backbone performs worse than the baseline method. Although proposed scheme using ResNet50 as backbone achieves performance improvement, the performance improvement is much  <ref type="bibr">8. 2</ref> For different backbone, the best results are in bold. <ref type="bibr" target="#b2">3</ref> The performance of proposed uPMnet are listed at the last row for each backbone. less than the performance improvement on PRID and DukeMTMC-VideoReID, i.e., Rank-1 improvement of 0.7% on iLIDS-VID vs. average Rank-1 improvement of 5.4% on PRID and DukeMTMC-VideoReID. The main reason mainly comes from the nature of the iLIDS-VID benchmark, where background clutters and mutual occlusions are pervasive <ref type="bibr" target="#b31">[32]</ref>  <ref type="bibr" target="#b36">[37]</ref>, as illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref>. In this situation, the part models produce more similar feature representations than the global one, which makes the unsupervised method hard to distinguish the same body part of different persons. In this situation, the negative effects of part models are amplified remarkably.</p><p>Effectiveness of Proposed Global-aware Module: In this section, proposed scheme with the global-aware module is analyzed. For all the benchmarks, proposed scheme using MobileNet as backbone achieves an average Rank-1 improvement of 2.0% compared to the baseline method. Proposed scheme using ResNet50 as backbone achieves an average Rank-1 improvement of 2.7%. Proposed globalaware module explores the relationship between the local-level features and global-level features and overcomes the disadvantages of part models. These comparisons demonstrate the effectiveness of proposed global-aware module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Proposed Feature Fusion:</head><p>The performance differences on three benchmarks are analyzed here. Either using MobileNet as backbone or using ResNet50 as backbone, proposed scheme using the global-aware module achieves more obvious improvement on the iLIDS-VID benchmark, i.e., average Rank-1 improvement of 4.7% on iLIDS-VID vs. average Rank-1 improvement of 1.2% on PRID and DukeMTMC-VideoReID. Meanwhile, proposed scheme using local-aware module achieves more obvious improvement on PRID and DukeMTMC-VideoReID than on iLIDS-VID. The local-aware module and global-aware module have complimentary performances. <ref type="table" target="#tab_4">Table 2</ref> illustrates the results of feature representation fused from the local-aware feature and the global-aware feature. The fused feature obtains first place for almost all metrics. Through the feature fusion, the complementary performances are combined decently, and the performance is improved further. The fused feature absorbs the superiority of part models and discards its shortcomings for various challenging situations. These comparisons demonstrate the effectiveness of the feature fusion operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Global Average Pooling vs. Global Max Pooling</head><p>Many supervised methods <ref type="bibr" target="#b45">[46]</ref> [47] <ref type="bibr" target="#b47">[48]</ref> proved that global max pooling (GMP) is more effective than global average pooling (GAP).</p><p>In the second part of proposed scheme, i.e., part/global-aware feature generation, GAP, not GMP, is used to generate the compact feature. The performance comparisons are listed in <ref type="table" target="#tab_5">Table 3</ref>. Compared to the scheme with GAP, the scheme with GMP is almost inferior on each metric. These comparisons are explained below. Supervised methods supervise the model to learn discriminative features to distinguish different persons, while GMP can aggregate the feature from the most discriminative part. Unsupervised methods focus on the undiscriminating feature because they mostly rely on the similarities to distinguish possible pair features, while GAP introduces an undiscriminating feature representation covering the whole person image. Thus employing GAP is a better choice for unsupervised learning. <ref type="table" target="#tab_6">Table 4</ref> shows the performances of proposed uPMnet using different partition scales. The MobileNet is selected as the backbone for rapid deployment. Since the height of feature map is 8, the partition scale k is set to 1,2,4,8 to take the experiments. With the increase of partition scale k, e.g., from 1 to 8, proposed uPMnet improves apparently. The average gain between adjacent scales, e.g., 2 vs. 1, 4 vs. 2 and 8 vs. 4, is 2.2% for Rank-1 on these three benchmarks. These comparisons demonstrate that a more fine-grained feature can optimize the performance of the unsupervised method. Since the Rank-1 metric represents the model ability to recognize the easiest person images, proposed uPMnet can effectively improve this ability. For the metric of mAP, the average gain between adjacent scales is 0.9% on the DukeMTMC-VideoReID benchmark. These comparisons demonstrate the effectiveness of proposed scheme. <ref type="figure" target="#fig_6">Fig. 7</ref> shows visual comparisons of person retrieval results on large-scale benchmarks, DukeMTMC-VideoReID. After spotting all correct ones, the incorrect ones are deliberately kept as negative ones. The images with green, red, and blue boxes are correct, incorrect, and negative ones. The difference between the baseline and proposed uPMnet can be observed. For the red ones at the first row in <ref type="figure" target="#fig_6">Fig. 7</ref>, the baseline approach spots the assigned person by similar contour, which results in obvious mistakes. Meanwhile, proposed method recognizes all the correct person images. These comparisons demonstrate that proposed uPMnet can achieve more discriminative features than the baseline method. <ref type="figure" target="#fig_8">Fig. 8</ref> visualizes the activation maps and illustrate the different effects of using local-aware module and global-aware module.   An unsupervised method-oriented activation map is implemented by learning from Class Activation Mapping (CAM) <ref type="bibr" target="#b64">[65]</ref>. Since no image labels can be used to compute the weight of different channels in the feature map for unsupervised learning, the weight of each channel is set to 1. It is reasonable because each channel has an equal contribution to the test process. In <ref type="figure" target="#fig_8">Fig. 8(a)</ref>, proposed scheme with local-aware module focuses on more accurate and fine-grained image regions than the baseline approach. At the same time, the baseline method has a large response on the head part of the person images. In <ref type="figure" target="#fig_8">Fig. 8(b)</ref>, proposed scheme with global-aware module gives broader perspectives than baseline and retains more accurate feature response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Computational Complexity</head><p>This section first compares the FLOPs and trainable parameters between proposed uPMnet and other state-of-the-art methods. For different benchmarks, unsupervised methods usually have different trainable parameters. In this paper, the largest benchmark, DukeMTMC-VideoReID, is employed to make this comparison between proposed uPMnet and other state-of-the-art methods, as illustrated in Tabel 6. The proposed uPMnet using MobileNet as backbone only requires 1.8GFLOPs to run a single instance and achieves the best Rank-1 score. Besides, the FLOPs metric is highly related to the backbone. Benefiting from the effective MobileNet, the proposed uPMnet achieves the best performance with low computational complexity.</p><p>An ablation study for computational complexity is also represented, as illustrated in Tabel 5. The benchmark PRID2011, iLIDS-VID, and DukeMTMC-VideoReID have 178, 300, and 2196 tracklets, respectively. For different benchmarks, the proposed uPMnet needs to initial intra-camera anchors I and cross-camera anchors   C of different numbers. In other words, the proposed scheme needs different trainable parameters for different benchmarks. Besides, the computational complexity is also affected by the network architecture of different aware modules. <ref type="table" target="#tab_7">Table 5</ref> illustrates the computational complexity changes when using different aware modules on different benchmarks. The FLOPs and trainable parameters of the proposed uPMnet are the sum value from the local-aware network and globalaware network. Compared to the baseline, the proposed scheme using a local-aware or global-aware module achieves an average 2.6% FLOPs increase along with an average 4.7% Rank-1 increase for these three benchmarks. Although the proposed uPMnet, which fuses the local-aware and global-aware feature, increases FLOPs by an average value of 105.1%, it improves the Rank-1 score with an average value of 11.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, a robust part models-based scheme for unsupervised video reID is proposed. The proposed scheme is general and flexible and fuses part models and unsupervised learning decently. A local-aware module is employed to explore the potentials of part models, and a global-aware module is designed to overcome the disadvantages of part models. Features from local-aware module and global-aware module are fused to generate a rich and robust feature representation, which absorbs the advantages of part models without suffering from its disadvantages. Compared to the baseline method, proposed scheme achieves performance improvements significantly on each metric. Extensive evaluations show proposed approach achieves state-of-the-art performance on three video-based reID benchmarks. Concretely, compared to other state-of-the-art methods, proposed uPMnet achieves Rank-1 improvement of 6.7%, 5.8%, and 0.8% on PRID2011, iLIDS-VID, and DukeMTMC-VideoReID benchmarks, respectively. Ablation study, parameter analysis, and qualitative analysis also verify the effectiveness of proposed scheme, local-aware module and global-aware module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The same body parts of different persons usually have a more similar appearance than their holistic image. For example, each image pair above has similar foot appearances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Stripe/Part-related Learning: Stripe/Part-related learning employs a simple strategy and divides the feature map into several equal parts<ref type="bibr" target="#b19">[20]</ref> <ref type="bibr" target="#b47">[48]</ref>. Sun et al.<ref type="bibr" target="#b19">[20]</ref> employed this uniform partition Overview of proposed scheme, which mainly consists of three parts: 1) extracting feature map from the input person image; 2) generating the local/global-aware features from the feature map; 3) achieving sophisticated unsupervised learning by employing unsupervised modules. The partition scale k is set to 4 in this diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of i th (i = 1, ? ? ? , k) global-aware module. There are k global-aware modules in our framework. 'C' and '+' represent the concatenation operation and element-wise addition operation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of the fusion process. After the training process, the proposed scheme incorporating the local-aware module is denoted as the local-aware network. The global-aware network incorporates the global-aware module. The partition scale k is set to 4 in this diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Background clutter and mutual occlusions are pervasive in iLIDS-VIS benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>The top-7 retrieval tracklets of baseline and proposed uPMnet are shown here. Each tracklet is represented by one image randomly sampled from this tracklet for convenience.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Activation maps of the baseline and proposed scheme using different aware modules. The first row is the randomly sampled images. The class activation maps on PRID2011 and DukeMTMC-VideoReID benchmarks are illustrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>PRID2011 [34] is captured by two non-overlapping surveillance cameras on campus. It includes 385 tracklets in camera A and 749 tracklets in camera B. In this dataset, 200 persons appear in both camera views, which outputs 400 tracklets with an average length of 100 frames. The experimental setting of [34] [17] [56] [57] is followed, and only 178 identities are used, where each tracklet has more than 27 frames.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are employed as the backbone. The dimensions (height ? width ? channel) of feature map outputted from ResNet50 and MobileNet are 8 ? 4 ? 2048 and 8 ? 4 ? 1024, respectively. The update rate ? and the margin m are empirically set to 0.5, and the trade-off parameter ? is set to 1. The reduced feature channel c ? for the global-aware module is set to 256. The warmup epoch n is set to 2 and 1 for small-</figDesc><table /><note>scale datasets (PRID2011 and iLIDS-VID) and large-scale dataset (DukeMTMC-VideoReID), respectively. The average feature repre- sentation of all person images within the same tracklet is used as the tracklet feature. Although previous existing literature [64]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Performance comparisons with other state-of-the-art unsupervised video reID methods on PRID2011, iLIDS-VID and DukeMTMC-VideoReID.</figDesc><table><row><cell>Methods</cell><cell>Venue</cell><cell>Backbone</cell><cell></cell><cell>PRID2011</cell><cell></cell><cell cols="2">iLIDS-VID</cell><cell></cell><cell cols="4">DukeMTMC-VideoReID</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R1</cell><cell cols="3">R5 R20 R1</cell><cell cols="3">R5 R20 R1</cell><cell cols="3">R5 R20 mAP</cell></row><row><cell>OIM* [58]</cell><cell cols="2">CVPR2017 ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">51.1 70.5 76.2 43.8</cell></row><row><cell>TAUDL [18]</cell><cell cols="8">ECCV2018 ResNet50 49.4 78.7 98.9 26.7 51.3 82.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DAL [17]</cell><cell cols="8">BMVC2018 MobileNet 84.6 96.3 99.1 52.8 76.7 91.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DAL [17]</cell><cell cols="8">BMVC2018 ResNet50 85.3 97.0 99.6 56.9 80.6 91.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BUC [14]</cell><cell cols="2">AAAI2019 ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">69.2 81.1 85.8 61.9</cell></row><row><cell>UGA [38]</cell><cell>ICCV2019</cell><cell cols="7">ResNet50 80.9 94.4 100 57.3 72.0 87.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DBC [15]</cell><cell cols="2">BMVC2019 ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">75.2 87.0</cell><cell>-</cell><cell>66.1</cell></row><row><cell>UTAL [16]</cell><cell cols="8">TPAMI2019 ResNet50 54.7 83.1 96.2 35.1 59.0 83.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TSSL [19]</cell><cell cols="2">AAAI2020 ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.9</cell><cell>-</cell><cell>-</cell><cell>64.6</cell></row><row><cell>SSL [59]</cell><cell cols="2">CVPR2020 ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">76.4 88.7</cell><cell>-</cell><cell>69.3</cell></row><row><cell>NHAC [60]</cell><cell>arXiv2021</cell><cell>ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">82.8 92.7</cell><cell>-</cell><cell>76.0</cell></row><row><cell>Proposed uPMnet</cell><cell></cell><cell cols="11">ResNet50 92.0 97.7 100 63.1 81.9 92.5 81.3 91.7 97.2 74.6</cell></row><row><cell>Proposed uPMnet</cell><cell></cell><cell cols="11">MobileNet 90.2 97.8 100 62.6 80.9 91.6 83.6 93.1 97.2 76.9</cell></row><row><cell>AGRL ?[61] (supervised)</cell><cell>TIP2020</cell><cell></cell><cell cols="10">94.6 99.1 100 84.5 96.7 99.5 97.0 99.3 99.9 95.4</cell></row><row><cell cols="8">1 R1, R5 and R20 are the abbreviations of Rank-1, Rank-5 and Rank-20, respectively. 2 '-': no reported results. '*': reported in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Performance comparisons for proposed uPMnet with different backbones on PRID2011, iLIDS-VID, and DukeMTMC-VideoReID.</figDesc><table><row><cell>Backbone Local-aware Global-aware</cell><cell></cell><cell>PRID2011</cell><cell></cell><cell></cell><cell>iLIDS-VID</cell><cell></cell><cell cols="3">DukeMTMC-VideoReID</cell><cell></cell></row><row><cell></cell><cell cols="10">Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 mAP</cell></row><row><cell></cell><cell>82.3</cell><cell>96.9</cell><cell>99.6</cell><cell>54.1</cell><cell>76.9</cell><cell>90.7</cell><cell>79.3</cell><cell>92.5</cell><cell>96.7</cell><cell>74.2</cell></row><row><cell>MobileNet</cell><cell>91.5 83.3</cell><cell>98.3 96.0</cell><cell>99.7 99.6</cell><cell>52.4 58.2</cell><cell>74.1 80.0</cell><cell>88.4 91.3</cell><cell>81.4 80.3</cell><cell>92.7 92.3</cell><cell>97.0 96.0</cell><cell>75.5 73.9</cell></row><row><cell></cell><cell>90.2</cell><cell>97.8</cell><cell>100</cell><cell>62.6</cell><cell>80.9</cell><cell>91.6</cell><cell>83.6</cell><cell>93.1</cell><cell>97.2</cell><cell>76.9</cell></row><row><cell></cell><cell>83.9</cell><cell>95.9</cell><cell>99.7</cell><cell>54.8</cell><cell>76.6</cell><cell>89.9</cell><cell>74.2</cell><cell>88.1</cell><cell>94.8</cell><cell>66.0</cell></row><row><cell>ResNet50</cell><cell>90.4 84.3</cell><cell>97.5 95.9</cell><cell>99.7 99.6</cell><cell>55.5 60.1</cell><cell>78.3 81.1</cell><cell>92.0 91.6</cell><cell>78.4 76.7</cell><cell>90.1 88.6</cell><cell>96.0 94.5</cell><cell>70.3 68.5</cell></row><row><cell></cell><cell>92.0</cell><cell>97.7</cell><cell>100</cell><cell>63.1</cell><cell>81.9</cell><cell>92.5</cell><cell>81.3</cell><cell>91.7</cell><cell>97.2</cell><cell>74.6</cell></row><row><cell>1 The partition scale k is set to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Performance comparisons of uPMnet with GAP and GMP on PRID2011, iLIDS-VID, and DukeMTMC-VideoReID.</figDesc><table><row><cell>Proposed uPMnet</cell><cell></cell><cell>PRID2011</cell><cell></cell><cell></cell><cell>iLIDS-VID</cell><cell></cell><cell cols="3">DukeMTMC-VideoReID</cell><cell></cell></row><row><cell>(MobileNet, k=2)</cell><cell cols="10">Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 mAP</cell></row><row><cell>w/ GAP</cell><cell>85.1</cell><cell>97.4</cell><cell>99.8</cell><cell>58.7</cell><cell>78.5</cell><cell>90.8</cell><cell>80.7</cell><cell>92.7</cell><cell>96.8</cell><cell>74.9</cell></row><row><cell>w/ GMP</cell><cell>85.8</cell><cell>96.7</cell><cell>99.6</cell><cell>51.9</cell><cell>75.3</cell><cell>89.2</cell><cell>75.7</cell><cell>89.1</cell><cell>94.8</cell><cell>69.0</cell></row><row><cell cols="7">1 For rapid deployment, MobileNet is employed as backbone and k is set to 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Performance comparisons of uPMnet with different partition scales on PRID2011, iLIDS-VID, and DukeMTMC-VideoReID. MobileNet is employed as backbone for rapid deployment.</figDesc><table><row><cell>MobileNet</cell><cell></cell><cell></cell><cell>PRID2011</cell><cell></cell><cell></cell><cell>iLIDS-VID</cell><cell></cell><cell cols="3">DukeMTMC-VideoReID</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="10">Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 mAP</cell></row><row><cell>Baseline</cell><cell>k = 1</cell><cell>82.3</cell><cell>96.9</cell><cell>99.6</cell><cell>54.1</cell><cell>76.9</cell><cell>90.7</cell><cell>79.3</cell><cell>92.5</cell><cell>96.7</cell><cell>74.2</cell></row><row><cell></cell><cell>k = 2</cell><cell>85.1</cell><cell>97.4</cell><cell>99.8</cell><cell>58.7</cell><cell>78.5</cell><cell>90.8</cell><cell>80.7</cell><cell>92.7</cell><cell>96.8</cell><cell>74.9</cell></row><row><cell>Proposed uPMnet</cell><cell>k = 4</cell><cell>88.8</cell><cell>97.8</cell><cell>100</cell><cell>58.6</cell><cell>79.6</cell><cell>91.1</cell><cell>82.6</cell><cell>93.3</cell><cell>97.4</cell><cell>75.6</cell></row><row><cell></cell><cell>k = 8</cell><cell>90.2</cell><cell>97.8</cell><cell>100.0</cell><cell>62.6</cell><cell>80.9</cell><cell>91.6</cell><cell>83.6</cell><cell>93.1</cell><cell>97.2</cell><cell>76.9</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Computational Complexity Comparisons on PRID2011, iLIDS-VID, and DukeMTMC-VideoReID. Means the percentage increase relative to the baseline method. 2 ResNet50 is employed as backbone in this table.</figDesc><table><row><cell>Method</cell><cell></cell><cell>PRID2011</cell><cell></cell><cell></cell><cell>iLIDS-VID</cell><cell></cell><cell cols="2">DukeMTMC-VideoReID</cell><cell></cell></row><row><cell></cell><cell>FLOPs</cell><cell>Params</cell><cell>Rank-1</cell><cell>FLOPs</cell><cell>Params</cell><cell>Rank-1</cell><cell>FLOPs</cell><cell>Params</cell><cell>Rank-1</cell></row><row><cell>Baseline</cell><cell>4.6G</cell><cell>23.8M</cell><cell>83.9</cell><cell>4.6G</cell><cell>24.1M</cell><cell>54.8</cell><cell>4.7G</cell><cell>28.0M</cell><cell>74.2</cell></row><row><cell>Local-aware</cell><cell>4.7G(?0.8%)</cell><cell>26.4M(?10.7%)</cell><cell cols="2">90.4(?7.7%) 4.7G(?1.4%)</cell><cell>28.4M(?17.9%)</cell><cell>55.5(?1.2%)</cell><cell>5.2G(?10.6%)</cell><cell cols="2">59.5M(?112.5%) 78.4(?5.6%)</cell></row><row><cell>Global-aware</cell><cell>4.7G(?0.8%)</cell><cell>29.6M(?24.1%)</cell><cell cols="2">84.3(?0.4%) 4.7G(?0.8%)</cell><cell>29.9M(?23.9%)</cell><cell>60.1(?9.6%)</cell><cell>4.7G(?0.8%)</cell><cell>33.7M(?20.6%)</cell><cell>76.7(?3.3%)</cell></row><row><cell cols="10">Proposed uPMnet 9.4G(?101.7%) 56.0M(?134.9%) 92.0(?9.6%) 9.4G(?102.3%) 58.3M(?141.8%) 63.1(?15.1%) 10.0G(?111.4%) 93.3M(?233.1%) 81.3(?9.5%)</cell></row><row><cell>1 '?':</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Computational Complexity Comparisons on DukeMTMC-VideoReID. Proposed uPMnet ResNet50 10.0G 93.3M 81.3 Proposed uPMnet MobileNet 1.8G 32.3M 83.6 1 '*' : the results are produced by us.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">DukeMTMC-VideoReID</cell></row><row><cell></cell><cell></cell><cell cols="2">FLOPs Params Rank-1</cell></row><row><cell>TAUDL [18]</cell><cell cols="2">ResNet50 5.3G</cell><cell>34.2M 61.5*</cell></row><row><cell>DAL [17]</cell><cell cols="2">ResNet50 4.7G</cell><cell>28.0M 74.4*</cell></row><row><cell>DAL [17]</cell><cell cols="2">MobileNet 0.7G</cell><cell>5.45M 79.3*</cell></row><row><cell>BUC [14]</cell><cell cols="2">ResNet50 5.3G</cell><cell>28.3M 69.2</cell></row><row><cell>DBC [15]</cell><cell cols="2">ResNet50 5.3G</cell><cell>28.3M 75.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">IET Research Journals, pp. 1-10</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? The Institution of Engineering and Technology 2015</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">IET Research Journals, pp. 1-10 ? The Institution of Engineering and Technology 2015</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information fusion from multiple cameras for gait-based re-identification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IET Image Processing</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="969" to="976" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hpiln: a feature learning framework for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2897" to="2904" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training approach using the shallow model and hard triplet mining for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning multigranular hypergraphs for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2899" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatial-temporal graph convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3289" to="3299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-granularity reference-aided attentive feature aggregation for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10407" to="10416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-modality person reidentification based on dual-path multi-branch network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saddik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="11706" to="11713" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive transfer network for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7202" to="7211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel unsupervised camera-aware domain adaptation framework for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8080" to="8089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person reidentification by unsupervised video matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust anchor embedding for unsupervised video person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic graph co-matching for unsupervised video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2976" to="2990" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Energy clustering for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<idno>arXiv:190900112</idno>
	</analytic>
	<monogr>
		<title level="j">Journal Article</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8738" to="8745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards better validity: Dispersion based clustering for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Journal Article]arXiv preprint arXiv:190601308</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised tracklet person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep association learning for unsupervised video person re-identification&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno>arXiv:180807301</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Journal Article]arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="737" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tracklet self-supervised learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identity-guided human semantic parsing for person re-identification&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Texture semantically aligned with visibility-aware for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3771" to="3779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pose-guided visible part matching for occluded person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11744" to="11752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fd-gan: Pose-guided feature distilling gan for robust person re-identification&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1222" to="1233" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3642" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scpnet: Spatialchannel parallelism network for joint holistic and partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Deep mutual learning&apos;. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">170404861</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<title level="m">Person re-identification by descriptive and discriminative classification&apos;. Scandinavian conference on Image analysis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Patch-based discriminative feature learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3633" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video-based person re-identification using unsupervised tracklet matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riachy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khelifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouridane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="20596" to="20606" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised graph association for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8321" to="8330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning to align multi-camera domain for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Journal Article]arXiv preprint arXiv:190913248</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Consistent crossview matching for unsupervised person re-identification&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Journal Article]arXiv preprint arXiv:190810486</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to discover crossdomain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Domain adaptive attention model for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Journal Article]arXiv preprint arXiv:190510529</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2860" to="2871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8295" to="8302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Relation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<idno>arXiv:191109318</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Journal Article]arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly and semi supervised human body part parsing via pose-guided knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno>arXiv:180504310</idno>
	</analytic>
	<monogr>
		<title level="j">Journal Article</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pose-guided spatiotemporal alignment for video-based person re-identification&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]Information Sciences</title>
		<imprint>
			<biblScope unit="volume">527</biblScope>
			<biblScope unit="page" from="176" to="190" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Person re-identification by saliency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Oyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Guided saliency feature learning for person re-identification in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="357" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stepwise metric promotion for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5142" to="5150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via softened similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3390" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Unsupervised video person reidentification via noise and hard frame aware clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<idno>arXiv:210605441</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Journal Article]arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adaptive graph representation learning for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E F</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Article]IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Tensorflow: A system for large-scale machine learning&apos;. 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Video-based person re-identification via self paced weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
