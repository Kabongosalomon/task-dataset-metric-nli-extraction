<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OBJECT-CENTRIC AND MEMORY-GUIDED NORMALITY RECONSTRUCTION FOR VIDEO ANOMALY DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Bergaoui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA, List</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Naji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA, List</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LISN</orgName>
								<address>
									<postCode>91400</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Setkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA, List</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang?lique</forename><surname>Loesch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA, List</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mich?le</forename><surname>Gouiff?s</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LISN</orgName>
								<address>
									<postCode>91400</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA, List</orgName>
								<address>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OBJECT-CENTRIC AND MEMORY-GUIDED NORMALITY RECONSTRUCTION FOR VIDEO ANOMALY DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-deep learning</term>
					<term>abnormal event detection</term>
					<term>video anomaly detection</term>
					<term>object-centric normality modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the anomaly detection problem for videosurveillance. Due to the inherent rarity and heterogeneity of abnormal events, this problem is tackled from a normality modeling perspective, where our model learns object-centric normal patterns without seeing anomalous samples during training. Our main contributions consist in coupling objectlevel action features with a cosine distance-based anomaly estimation function. We therefore extend previous methods by introducing explicit geometric constraints to the mainstream reconstruction-based strategy. Our framework leverages both appearance and motion information to learn object-level behavior and captures prototypical patterns within a memory module. Experiments on several well-known datasets demonstrate the effectiveness of our method as it outperforms current state-of-the-art on most relevant spatio-temporal evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Video Anomaly Detection (VAD) is an open research problem which consists in detecting rare occurrences of abnormal events. This is a challenging problem due to two main reasons. Although anomalous events are generally defined as rare occurrences that deviate from normal patterns observed in familiar events <ref type="bibr">[1]</ref>, this definition does not differentiate anomalous events from rare normal ones. Secondly, abnormal events are inherently more difficult to collect and to learn, due to their few occurrences and the multiplicity of their nature. For these reasons, the VAD problem is often viewed within the one-class paradigm <ref type="bibr" target="#b0">[2]</ref>.</p><p>In a pioneering work <ref type="bibr" target="#b1">[3]</ref>, a model is trained to predict future "normal" frame, and anomalies are viewed as inaccurate predictions. The recent method <ref type="bibr" target="#b2">[4]</ref> combines multiple proxy tasks (e.g. arrow of time prediction) to characterize anomalous events. Other approaches quantify the deviation from learned normal patterns including distance-based <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b7">9]</ref> and * Equal contributions reconstruction-based methods <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b11">13]</ref>. Although they have been empirically shown to attain impressive performance levels on current standard benchmark datasets, the used strategy is not always in line with the nature of anomalous event detection. In fact, as pointed out in <ref type="bibr" target="#b2">[4]</ref> , a car stopped in a pedestrian area should be labeled as an anomaly, yet the car is trivial to reconstruct (at a pixel-wise level) in a future frame, since it is still standing. This example shows that pixel-wise reconstruction error is suboptimal for anomaly detection. The recent work <ref type="bibr" target="#b11">[13]</ref> addressed this challenge by learning objectlevel patterns of normal appearance and motion by training a discriminator network to classify (normal vs. abnormal samples) given pairs of reconstruction error maps. Despite being the current state of the art, this method includes out-ofdomain observations during training, introducing a bias to the normality modeling. Instead, we propose to tackle these challenges by extending the mainstream reconstruction assumption on which most state-of-the-art methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b11">13]</ref> are implicitly based: Given a normality model, normal observations are easier to reconstruct from a low-dimensional representation than abnormal observations. We propose to add geometric constraints in the reconstruction space in order to further narrow down this assumption to be more in line with the anomaly detection task. Different from prior works, we combine a cosine distance-based anomaly estimation function with pretrained object-level features. Additionally, we propose to constrain our model to reconstruct independent motion and appearance features from a single embedding space. This way, our network has fewer degrees of freedom to perform the training task, which is in line with the aforementioned reconstruction assumption. Following <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b11">13]</ref> we apply an object detector allowing to localize anomalies at the object level, which is semantically more relevant than at the pixellevel. Similarly to <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12]</ref>, we incorporate a memory block in our framework in order to model diverse normality patterns. In summary, our contributions are:</p><p>? Imposing geometric constraints in the reconstruction space using cosine distance.</p><p>? Introducing object-level action prototypical features.</p><p>? State-of-the-art results on the most relevant metrics. Their linear combination concatenated with h is sent to the decoder to obtain object appearance and motion reconstructions. The anomaly score is defined based on the dissimilarity between the input features and their reconstructions as well the dissimilarity between the query h and its neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>The architecture of the proposed method: OMAE which stands for object centric memory-guided auto-encoder is displayed in <ref type="figure" target="#fig_0">Fig. 1</ref>.First, we detect objects and compute optical flow for each frame. Next, appearance x app and motion x mo features are extracted for each object. The former are obtained using a pretrained CNN, whereas the latter consist of motion magnitude and angle maps. We denote the input features</p><formula xml:id="formula_0">X = {(x app , x mo ) j ; 1 ? j ? O},</formula><p>where O is the total number of objects in the training set. Then, the object representations are encoded and fused into a single embedding h. The three encoders have the same structure: two successive blocks, each block is a shallow fully connected network of two layers. The fusion block consists of a single shallow fully connected network of two layers that learns new embeddings from the concatenation of the three encoders' bottlenecks. Thus, we obtain a single hidden representation combining motion and appearance, which can be interpreted as an object-level action feature vector. We will use H to denote the set of these hidden features corresponding to input vectors in X . This action feature is then used as a query to the memory of normal patterns to extract similar existing prototypes in the memory module M = {m i , 1 ? i ? N }, where N is the total number of memory items. Similar to <ref type="bibr" target="#b10">[12]</ref> the most similar memory item m k to the query h is defined as the soft nearest neighbor:</p><formula xml:id="formula_1">k = argmax 1?i?N (w i ) ; w i = exp(h T m i ) N j=1 exp(h T m j ) ; 1 ? i ? N.</formula><p>After the memory readout step, and following an attentionmechanism strategy, a linear combination of the memory items is computed as c = N j=1 w j .m j and then concatenated to the query h to obtain an augmented hidden representation z = (c||h) that will be used as input to the decoder network. Finally, a single fully connected decoder network learns to reconstruct x given z. This way, our auto-encoder model is trained to reconstruct object-centric featuresx under two major constraints:</p><p>1. The auto-encoder learns normality patterns (memory items) that allow the reconstruction of both appearance and motion features from a single embedding space.</p><p>2. The decoder's reconstructive capacity is limited by the set of memory items thus its generalization ability is reduced, which is useful for detecting anomalies (via poor outlier reconstructions).</p><p>During inference, an anomaly score is attributed to each object (cf. Section 2.3) based on the dissimilarity between x andx as well as the dissimilarity between h and its neighbors in the space of prototypical patterns ((m i ) 1?i?N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss functions</head><p>To take into account the above-mentioned learning constraints, we combine different loss terms in our objective function. We incorporate a reconstruction term L rec to minimize the discrepancy between the input x and its reconstruction and a memory term L mem to capture normal prototypical patterns observed in the training set. Hence, the total loss is given by:</p><formula xml:id="formula_2">L = L rec + L mem .</formula><p>Reconstruction Loss: We constrain the reconstruction to not only be in the Euclidean neighborhood of the input but also to lie on the same spatial direction. The geometrical constraint is applied via a cosine distance loss and controlled via the hyperparameter ? cos such that:</p><formula xml:id="formula_3">L rec = ( x app ?x app 2 + x mo ?x mo 2 ) + ? cos ? 1 ? &lt; x app ,x app &gt; x app 2 x app 2</formula><p>Memory Loss: This loss is obtained as a combination of three terms:</p><formula xml:id="formula_4">L mem = ? comp .L comp + ? tr .L tr + ? OLE .L OLE</formula><p>Similarly to <ref type="bibr" target="#b10">[12]</ref>, discriminative normality action features prototypes are learnt based on nearest neighbor distances within the memory space via a loss that favors compactness of data samples around prototypes:</p><formula xml:id="formula_5">L comp = N k=1 j?U k ; U k =? h j ? m k 2 where U k ? {1, .</formula><p>.., O} is the subset of training object indices which have the memory item m k as their first nearest neighbor.</p><p>We also use the same triplet loss L tr introduced in <ref type="bibr" target="#b10">[12]</ref>. Contrarily to <ref type="bibr" target="#b10">[12]</ref>, we incorporate a third term L OLE that adds orthogonality constraints within the memory space. This is achieved through the geometric loss formulation proposed in <ref type="bibr" target="#b13">[15]</ref> for supervised classification. We adapt the OLE (Orthogonal Low-rank Embedding) loss to our setting by formulating the memory query step as a classification problem:</p><formula xml:id="formula_6">L OLE = N c=1 max(?, H c * ) ? H *</formula><p>where ? is a positive real number that we set to 1 in all our experiments, H c is the sub-matrix of object hidden representations within the batch that are attributed to memory item m c .</p><p>. * denotes the nuclear norm i.e the sum of matrix singular values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Inference: Abnormality score</head><p>At test time, given the t th frame, a set of abnormality scores is denoted as</p><formula xml:id="formula_7">S t = {s j t , 1 ? j ? O t },</formula><p>where O t is the number of detected objects in frame t.</p><p>Each score s j t is computed as follows:</p><formula xml:id="formula_8">s j t = s = 1 3 s rec L2 + s rec cos + s mem where: s rec L2 = g ( x app ?x app 2 + x mo ?x mo 2 ) s rec cos = g (1 ? &lt;x app ,x app &gt; x app 2 x app 2 ) + (1 ? &lt;x mo ,x mo &gt; x mo 2 x mo 2 ) s mem = g 1 ? &lt;h,m k &gt; h 2 m k 2</formula><p>with g(.) a normalization function:</p><formula xml:id="formula_9">g( ) = ? min max ? min</formula><p>where min and max are the lowest and the highest object level scores respectively across the entire video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head><p>Datasets. Several benchmarks had been proposed for evaluating anomaly detection methods <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b20">21]</ref>, In order to compare our methods with existing approaches, we performed experiments on the most common datasets : UCSD ped2 <ref type="bibr" target="#b14">[16]</ref> includes 16 training and 12 testing videos of resolution 240x360. Anomalous events include riding a bike and driving a vehicle on a sidewalk. CUHK Avenue <ref type="bibr" target="#b15">[17]</ref> consists of 16 training and 21 test videos of resolution 360x640 with abnormal events such as running, walking towards the camera, or throwing papers. We use the annotations provided by <ref type="bibr" target="#b5">[7]</ref>. ShanghaiTech <ref type="bibr" target="#b16">[18]</ref> contains 330 training and 107 testing videos of resolution 480x856 with 13 different scenes. Each scene has a different background or camera angle. Abnormal events include jumping, running, or stalking on a sidewalk.</p><p>Evaluation metrics. Since we focus on spatio-temporal anomaly detection, we adopt the Region-Based Detection Criterion (RBDC) and the Track-Based Detection Criterion (TBDC) metrics introduced in <ref type="bibr" target="#b5">[7]</ref> as an alternative to the flawed pixel-level AUC metric. We also report the Area Under of the ROC Curve (AUC) obtained with respect to the frame-level ground-truth annotations. Yet, it gives only a global frame score and, therefore, doesn't reflect the model capacity to localize anomalies. Emphasis is given to the parts of the ROC curve where false positive rate is too high for a practical use <ref type="bibr" target="#b21">[22]</ref>. Hence, AUC is the least relevant metric in our study.</p><p>Parameters and implementation details. The first step of our framework is to perform object detection using Yolov3 <ref type="bibr" target="#b22">[23]</ref> pretrained on COCO dataset as in <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b3">5]</ref>. We set the detection confidence to 0.7 on ShanghaiTech and Avenue for both the training and testing sets. Since the image resolution of UCSD ped2 is lower, we reduced the threshold to 0.5. We used ResNet101 to precompute appearance features of detected objects and Farneback's algorithm to compute optical flows <ref type="bibr" target="#b23">[24]</ref>. Similarly to <ref type="bibr" target="#b2">[4]</ref>, we trained the network for 30 epochs on each dataset using Adam optimizer with a learning rate of 10 ?3 . We use a batch size of 256 object-level action features for the smallest dataset (UCSD ped2) and 512 for ShanghaiTech and Avenue. For all experiments, we set ? cos = 0.1 so that the cosine term has a similar order of magnitude as the L 2 reconstruction term; as well as fixed empirical values for the number of memory items and the memory loss weights: N = 40, ? comp = 1.6 , ? tr = 0.2, ? OLE = 0.3. The epoch achieving the lowest loss value in training is used for inference. As a post-processing step and similarly to <ref type="bibr" target="#b11">[13]</ref>, we apply a spatio-temporal mean filtering to smooth object level scores and a Gaussian filter at the frame level. In the case of Avenue dataset, scale change is taken into account by an anomaly score adjustment: the anomaly score of an object is multiplied by its bounding box width. This post-processing allows an increase of 26% in RBDC and 2% in TBDC without degrading AUC.  Ablation study. We conduct an ablation study on Shang-haiTech dataset to assess the importance of each component in our framework. The corresponding results are shown in <ref type="table" target="#tab_1">Table  2</ref>. We can see that both appearance and motion features are necessary to model usual actions to better detect anomalies. Indeed, the baseline model takes only appearance features as input and performs lower than the current state of the art on all metrics. Including the motion information through optical flow improves the frame-AUC by 8.97% and significantly increases RBDC and TBDC by 15.83% and 32.37% respectively allowing OMAE to outperform the current state of the art on these metrics. We also note that including the cosine similarity together with MSE or MAE in the reconstruction loss as well as in the abnormality scores improves all metrics significantly, showing the importance of the orthogonality constraints.</p><p>Comparison with state of the art. In <ref type="table" target="#tab_0">Table 1</ref>, we present our results in comparison with the state-of-the-art methods on the 3 benchmark datasets. Our framework significantly outperforms current state-of-the-art methods on the most relevant evaluation metrics RBDC and TBDC which quantifies the model's ability to localize anomalies spatially and to track them temporally. It's important to highlight that, given a single dataset, there is not a single best method that outperforms the other approaches on the three metrics. On ShanghaiTech dataset, we outperform best previous work by a margin of 10.08% on RBDC and 3.4% on TBDC while remaining competitive with respect to other object-centric approaches in terms of frame-level AUC. Our model also reaches new stateof-the-art performances in terms of RBDC and TBDC on UCSD ped2 with significant margins of 6.07% and 2.24% respectively. On Avenue, our model improves the current state of the art by 10,78% on the RBDC metric and outperforms the recent object-centric approach <ref type="bibr" target="#b11">[13]</ref> in terms of TBDC by a margin of 3.17% while remaining competitive on the AUC metric. Advantageously, inference time takes only 20ms for a batch of 256 precomputed object action features. The preprocessing time required for computing those features is the following: optical flow extraction (170ms), object detection (50ms) and feature extraction (40ms). In addition, training 3 is much faster than in <ref type="bibr" target="#b10">[12]</ref>: only 70 min on ShanghaiTech, 9 min on Avenue and 4 min on UCSD ped2 with a single NVIDIA TITAN X (PASCAL) GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this work we introduced OMAE, an object-centric VAD framework that uses a memory module for object-level appearance and motion features with a new abnormality scoring strategy based on cosine distance. In our experiments, OMAE reaches superior results on localization and tracking metrics while remaining competitive on the frame-level AUC. This shows the effectiveness of our approach to localize anomalies better than the current state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">REFERENCES</head><p>[1] Bharathkumar Ramachandra, Michael Jones, and Ranga Raju Vatsavai, "A survey of single-scene video anomaly detection," IEEE transactions on pattern analysis and machine intelligence, 2020.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of OMAE. At the preprocessing step, object bounding boxes and Optical Flow (OF) are computed. Object appearance features, extracted using a pretrained CNN, motion magnitude and angle maps are fed to the corresponding autoencoders. The encoded representations are fused (h) and sent as a query to the memory module which fetches similar memory items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>.46 80.07 95.39 79.18 51.51 82.19 93.56 75.83 70.02 Comparison with the state-of-the-art methods (%). Best results in bold and second best results are underlined</figDesc><table><row><cell cols="2">Approach</cell><cell></cell><cell cols="2">Method</cell><cell>AUC</cell><cell cols="2">UCSD Ped2 RBDC TBDC</cell><cell cols="3">ShanghaiTech AUC RBDC TBDC</cell><cell>AUC</cell><cell>Avenue RBDC TBDC</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MemAE [11]</cell><cell>94.1</cell><cell>-</cell><cell>-</cell><cell>71.2</cell><cell>-</cell><cell>-</cell><cell>83.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">frame-level</cell><cell cols="2">MNAD [12]</cell><cell>97.0</cell><cell>-</cell><cell>-</cell><cell>72.5</cell><cell>-</cell><cell>-</cell><cell>88.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SSMT [4]</cell><cell>92.4</cell><cell>-</cell><cell>-</cell><cell>83.5</cell><cell>-</cell><cell>-</cell><cell>86.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>video</cell><cell></cell><cell cols="3">StreetScene [7] 88.3</cell><cell cols="2">62.50 80.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.0</cell><cell>35.80 80.90</cell></row><row><cell cols="2">patch-level</cell><cell></cell><cell cols="2">Siamese [8]</cell><cell>94.0</cell><cell>74.0</cell><cell>89.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.2</cell><cell>41.20 78.60</cell></row><row><cell cols="3">frame &amp; object level</cell><cell cols="2">SSMT [4]</cell><cell>99.8</cell><cell>-</cell><cell>-</cell><cell>90.2</cell><cell>-</cell><cell>-</cell><cell>92.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">dummyAE [5]</cell><cell>82.2</cell><cell>-</cell><cell>-</cell><cell>78.6</cell><cell cols="2">20.65 44.54</cell><cell>88.9</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">object-level</cell><cell cols="2">SSMT [4]</cell><cell>99.8</cell><cell>72.8</cell><cell>91.2</cell><cell>89.3</cell><cell>-</cell><cell>-</cell><cell>91.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">BAF [13]</cell><cell>98.7</cell><cell cols="2">69.23 93.15</cell><cell>82.7</cell><cell cols="2">41.43 78.79</cell><cell>92.3</cell><cell>65.05 66.85</cell></row><row><cell cols="6">Reconstruction OMAE (ours) 96Features Anomaly Evaluation loss (AE) score metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MSE</cell><cell>COS</cell><cell>MAE</cell><cell>COS</cell><cell>AUC</cell><cell cols="2">RBDC TBDC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>appearance</cell><cell></cell><cell></cell><cell>(AE+Mem)</cell><cell cols="3">70.21 35.68 49.82</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="3">70.48 35.15 69.16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>(Mem)</cell><cell cols="3">71.61 27.72 57.70</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>appearance</cell><cell>-</cell><cell>-</cell><cell>(AE)</cell><cell cols="3">76.66 43.02 68.13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell>-</cell><cell></cell><cell>(AE+Mem)</cell><cell cols="3">75.98 39.53 67.49</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>motion</cell><cell></cell><cell></cell><cell>-</cell><cell cols="3">70.71 35.96 71.27</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>(Mem)</cell><cell cols="3">69.50 31.18 74.47</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>(AE)</cell><cell cols="3">76.68 43.99 69.72</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(AE+Mem)</cell><cell cols="3">77.81 49.37 83.61</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ 3D smoothing</cell><cell></cell><cell></cell><cell>(AE+Mem)</cell><cell cols="3">79.18 51.51 82.19</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Frame-AUC, RBDC and TBDC scores (in %) obtained on ShanghaiTech by making gradual design changes to the baseline method, until the final framework. (AE stands for the auto-encoder component; Mem for the nearest-neighbor memory item.)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This publication was made possible by the use of the FactoryIA supercomputer, financially supported by the Ile-de-France Regional Council</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for anomaly detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Longbing Cao, and Anton Van Den Hengel</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection -a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection in video via selfsupervised and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Barbalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object-centric autoencoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting abnormal events in video using narrowed normality clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Street scene: A new dataset and evaluation protocol for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning a distance function with a siamese network to localize anomalies in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga Raju</forename><surname>Vatsavai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Any-shot sequential anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="934" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong-Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning memory-guided normality for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoun</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14372" to="14381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A background-agnostic framework with adversarial training for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><forename type="middle">Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continual learning for anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ol?: Orthogonal low-rank embedding, a plug and play geometric loss for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Mus?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
	<note>Anomaly detection in crowded scenes</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Realworld anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring background-bias for anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1490" to="1499" />
		</imprint>
	</monogr>
	<note>MM &apos;19</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A day on campus -an anomaly detection dataset for events in a single camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantini</forename><surname>Pranav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhenggang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shah</forename><surname>Shishir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auc: a misleading measure of the performance of predictive distribution models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimundo</forename><surname>Jim?nez-Valverde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global ecology and Biogeography</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
