<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-29">29 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Kwon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongseop</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunseo</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kwon</forename><surname>Choi</surname></persName>
						</author>
						<title level="a" type="main">ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-29">29 Jun 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-ofthe-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scaleinvariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generalization of deep neural networks has recently been studied with great importance to address the shortfalls of pure optimization, yielding models with no guarantee on generalization ability. To understand the generalization phenomenon of neural networks, many studies have attempted to clarify the relationship between the geometry of the loss surface and the generalization performance <ref type="bibr" target="#b10">(Hochreiter et al., 1995;</ref><ref type="bibr" target="#b23">McAllester, 1999;</ref><ref type="bibr" target="#b16">Keskar et al., 2017;</ref><ref type="bibr" target="#b27">Neyshabur et al., 2017;</ref><ref type="bibr" target="#b13">Jiang et al., 2019)</ref>. Among many proposed measures used to derive generalization bounds, loss surface sharpness and minimization of the derived generalization bound have proven to be effective in attaining state-of-the-art performances in various tasks <ref type="bibr" target="#b9">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b24">Mobahi, 2016;</ref><ref type="bibr" target="#b2">Chaudhari et al., 2019;</ref><ref type="bibr" target="#b30">Sun et al., 2020;</ref><ref type="bibr" target="#b37">Yue et al., 2020)</ref>.</p><p>Proceedings of the 38 th International Conference on Machine <ref type="bibr">Learning, PMLR 139, 2021</ref>. Copyright 2021 by the author(s).</p><p>Especially, Sharpness-Aware Minimization (SAM) <ref type="bibr" target="#b6">(Foret et al., 2021)</ref> as a learning algorithm based on PAC-Bayesian generalization bound, achieves a stateof-the-art generalization performance for various image classification tasks benefiting from minimizing sharpness of loss landscape, which is correlated with generalization gap. Also, they suggest a new sharpness calculation strategy, which is computationally efficient, since it requires only a single gradient ascent step in contrast to other complex generalization measures such as sample-based or Hessian-based approach.</p><p>However, even sharpness-based learning methods including SAM and some of sharpness measures suffer from sensitivity to model parameter re-scaling. <ref type="bibr" target="#b5">Dinh et al. (2017)</ref> point out that parameter re-scaling which does not change loss functions can cause a difference in sharpness values so this property may weaken correlation between sharpness and generalization gap. We call this phenomenon scaledependency problem.</p><p>To remedy the scale-dependency problem of sharpness, many studies have been conducted recently <ref type="bibr" target="#b21">(Liang et al., 2019;</ref><ref type="bibr" target="#b36">Yi et al., 2019;</ref><ref type="bibr" target="#b14">Karakida et al., 2019;</ref><ref type="bibr" target="#b31">Tsuzuku et al., 2020)</ref>. However, those previous works are limited to proposing only generalization measures which do not suffer from the scale-dependency problem and do not provide sufficient investigation on combining learning algorithm with the measures.</p><p>To this end, we introduce the concept of normalization operator which is not affected by any scaling operator that does not change the loss function. The operator varies depending on the way of normalizing, e.g., element-wise and filter-wise. We then define adaptive sharpness of the loss function, sharpness whose maximization region is determined by the normalization operator. We prove that adaptive sharpness remains the same under parameter re-scaling, i.e., scale-invariant. Due to the scale-invariant property, adaptive sharpness shows stronger correlation with generalization gap than sharpness does.</p><p>Motivated by the connection between generalization metrics and loss minimization, we propose a novel learning method, adaptive sharpness-aware minimization (ASAM), which adaptively adjusts maximization regions thus acting uniformly under parameter re-scaling. ASAM minimizes the corresponding generalization bound using adaptive sharpness to generalize on unseen data, avoiding the scale-dependency issue SAM suffers from.</p><p>The main contributions of this paper are summarized as follows:</p><p>? We introduce adaptive sharpness of loss surface which is invariant to parameter re-scaling. In terms of rank statistics, adaptive sharpness shows stronger correlation with generalization than sharpness does, which means that adaptive sharpness is more effective measure of generalization gap.</p><p>? We propose a new learning algorithm using adaptive sharpness which helps alleviate the side-effect in training procedure caused by scale-dependency by adjusting their maximization region with respect to weight scale.</p><p>? We empirically show its consistent improvement of generalization performance on image classification and machine translation tasks using various neural network architectures.</p><p>The rest of this paper is organized as follows. Section 2 briefly describes previous sharpness-based learning algorithm. In Section 3, we introduce adaptive sharpness which is a scale-invariant measure of generalization gap after scale-dependent property of sharpness is explained. In Section 4, ASAM algorithm is introduced in detail using the definition of adaptive sharpness. In Section 5, we evaluate the generalization performance of ASAM for various models and datasets. We provide the conclusion and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head><p>Let us consider a model f : X ? Y parametrized by a weight vector w and a loss function l :</p><formula xml:id="formula_0">Y ? Y ? R + .</formula><p>Given a sample S = {(x 1 , y 1 ), . . . , (x n , y n )} drawn from data distribution D with i.i.d condition, the training loss can be defined as L S (w) = n i=1 l(y i , f (x i ; w))/n. Then, the generalization gap between the expected loss L D (w) = E (x,y)?D [l(y, f (x; w))] and the training loss L S (w) represents the ability of the model to generalize on unseen data. Sharpness-Aware Minimization (SAM) <ref type="bibr" target="#b6">(Foret et al., 2021)</ref> aims to minimize the following PAC-Bayesian generalization error upper bound</p><formula xml:id="formula_1">L D (w) ? max ? p ?? L S (w + ?) + h w 2 2 ? 2<label>(1)</label></formula><p>for some strictly increasing function h. The domain of max operator, called maximization region, is an ? p ball with radius ? for p ? 1. Here, sharpness of the loss function L is defined as</p><formula xml:id="formula_2">max ? 2 ?? L S (w + ?) ? L S (w).<label>(2)</label></formula><p>Because of the monotonicity of h in Equation 1, it can be substituted by ? 2 weight decaying regularizer, so the sharpness-aware minimization problem can be defined as the following minimax optimization</p><formula xml:id="formula_3">min w max ? p ?? L S (w + ?) + ? 2 w 2 2</formula><p>where ? is a weight decay coefficient.</p><p>SAM solves the minimax problem by iteratively applying the following two-step procedure for t = 0, 1, 2, . . . as</p><formula xml:id="formula_4">? t = ? ?L S (w t ) ?L S (w t ) 2 w t+1 = w t ? ? t (?L S (w t + ? t ) + ?w t ) (3)</formula><p>where ? t is an appropriately scheduled learning rate. This procedure can be obtained by a first order approximation of L S and dual norm formulation as</p><formula xml:id="formula_5">? t = arg max ? p ?? L S (w t + ?) ? arg max ? p ?? ? ? ?L S (w t ) = ? sign(?L S (w t )) |?L S (w t )| q?1 ?L S (w t ) q?1 q and w t+1 = argmin w L S (w + ? t ) + ? 2 w 2 2 ? argmin w (w ? w t ) ? ?L S (w t + ? t ) + ? 2 w 2 2 ? w t ? ? t (?L S (w t + ? t ) + ?w t )</formula><p>where 1/p + 1/q = 1 and | ? | denotes element-wise absolute value function, and sign(?) also denotes element-wise signum function. It is experimentally confirmed that the above two-step procedure produces the best performance when p = 2, which results in Equation 3.</p><p>As can be seen from Equation 3, SAM estimates the point w t + ? t at which the loss is approximately maximized around w t in a rigid region with a fixed radius by performing gradient ascent, and performs gradient descent at w t using the gradient at the maximum point w t + ? t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaptive Sharpness: Scale-Invariant Measure of Generalization Gap</head><p>In <ref type="bibr" target="#b6">Foret et al. (2021)</ref>, it is experimentally confirmed that the sharpness defined in Equation 2 is strongly correlated with the generalization gap. Also they show that SAM helps to find minima which show lower sharpness than other learning strategies and contributes to effectively lowering generalization error.</p><p>However, <ref type="bibr" target="#b5">Dinh et al. (2017)</ref> show that sharpness defined in the rigid spherical region with a fixed radius can have a weak correlation with the generalization gap due to nonidentifiability of rectifier neural networks, whose parameters can be freely re-scaled without affecting its output.</p><p>If we assume that A is a scaling operator on the weight space that does not change the loss function, as shown in <ref type="figure">Figure 1</ref>(a), the interval of the loss contours around Aw becomes narrower than that around w but the size of the region remains the same, i.e., max ? 2??</p><formula xml:id="formula_6">L S (w + ?) = max ? 2?? L S (Aw + ?).</formula><p>Thus, neural networks with w and Aw can have arbitrarily different values of sharpness defined in Equation 2, although they have the same generalization gaps. This property of sharpness is a main cause of weak correlation between generalization gap and sharpness and we call this scale-dependency in this paper.</p><p>To solve the scale-dependency of sharpness, we introduce the concept of adaptive sharpness. Prior to explaining adaptive sharpness, we first define normalization operator. The normalization operator that cancels out the effect of A can be defined as follows.</p><p>Definition 1 (Normalization operator). Let {T w , w ? R k } be a family of invertible linear operators on R k . Given a weight w, if T ?1 Aw A = T ?1 w for any invertible scaling operator A on R k which does not change the loss function, we say T ?1 w is a normalization operator of w.</p><p>Using the normalization operator, we define adaptive sharpness as follows.</p><p>Definition 2 (Adaptive sharpness). If T ?1 w is the normalization operator of w in Definition 1, adaptive sharpness of w is defined by</p><formula xml:id="formula_7">max T ?1 w ? p ?? L S (w + ?) ? L S (w)<label>(4)</label></formula><p>where 1 ? p ? ?.</p><p>Adaptive sharpness in Equation 4 has the following properties.</p><p>(a) ? 2 ? ? and ? ? 2 ? ? <ref type="bibr" target="#b6">(Foret et al., 2021)</ref>  <ref type="bibr" target="#b16">Keskar et al., 2017)</ref> (c) T ?1 w ? 2 ? ? and T ?1 w ? ? ? 2 ? ? (In this paper) <ref type="figure">Figure 1</ref>. Loss contours and three types of maximization regions: (a) sphere, (b) cuboid and (c) ellipsoid. w = (1, 1) and w ? = (3, 1/3) are parameter points before and after multiplying a scaling operator A = diag(3, 1/3) and are expressed as dots and triangles, respectively. The blue contour line has the same loss at w, and the red contour line has a loss equal to the maximum value of the loss in each type of region centered on w. ? * and ? ? * are the ? and ? ? which maximize the loss perturbed from w and w ? , respectively.</p><formula xml:id="formula_8">(b) T ?1 w ? ? ? ? and T ?1 w ? ? ? ? ? ? (</formula><p>Theorem 1. For any invertible scaling operator A which does not change the loss function, values of adaptive sharpness at w and Aw are the same as</p><formula xml:id="formula_9">max T ?1 w ? p ?? L S (w + ?) ? L S (w) = max T ?1 Aw ? p ?? L S (Aw + ?) ? L S (Aw)</formula><p>where T ?1 w and T ?1 Aw are the normalization operators of w and Aw in Definition 1, respectively.</p><p>Proof. From the assumption, it suffices to show that the first terms of both sides are equal. By the definition of the normalization operator, we have T ?1</p><formula xml:id="formula_10">Aw A = T ?1 w . Therefore, max T ?1 Aw ? p ?? L S (Aw + ?) = max T ?1 Aw ? p ?? L S (w + A ?1 ?) = max T ?1 Aw A? ? p ?? L S (w + ? ? ) = max T ?1 w ? ? p?? L S (w + ? ? ) where ? ? = A ?1 ?.</formula><p>By Theorem 1, adaptive sharpness defined in Equation 4 is scale-invariant as with training loss and generalization loss. This property makes the correlation of adaptive sharpness with the generalization gap stronger than that of sharpness in Equation 2.</p><p>Figure 1(b) and 1(c) show how a re-scaled weight vector can have the same adaptive sharpness value as that of the original weight vector. It can be observed that the boundary line of each region centered on w ? is in contact with the red line. This implies that the maximum loss within each region centered on w ? is maintained when T ?1 w ? ? ? p ? ? is used for the maximization region. Thus, in this example, it can be seen that adaptive sharpness in 1(b) and 1(c) has scale-invariant property in contrast to sharpness of the spherical region shown in <ref type="figure">Figure 1</ref> The question that can be asked here is what kind of operators T w can be considered as normalization operators which satisfy T ?1 Aw A = T ?1 w for any A which does not change the loss function. One of the conditions for the scaling operator A that does not change the loss function is that it should be node-wise scaling, which corresponds to rowwise or column-wise scaling in fully-connected layers and channel-wise scaling in convolutional layers. The effect of such node-wise scaling can be canceled using the inverses of the following operators:</p><formula xml:id="formula_11">? element-wise T w = diag(|w 1 |, . . . , |w k |) where w = [w 1 , w 2 , . . . , w k ],</formula><p>? filter-wise</p><formula xml:id="formula_12">T w = diag(concat( f 1 2 1 n(f1) , . . . , f m 2 1 n(fm) , |w 1 |, . . . , |w q |))<label>(5)</label></formula><p>where  Here, f i is the i-th flattened weight vector of a convolution filter and w j is the j-th weight parameter which is not included in any filters. And m is the number of filters and q is the number of other weight parameters in the model. If there is no convolutional layer in a model (i.e., m = 0), then q = k and both normalization operators are identical to each other. Note that we use T w + ?I k rather than T w for sufficiently small ? &gt; 0 for stability. ? is a hyper-parameter controlling trade-off between adaptivity and stability.</p><formula xml:id="formula_13">w = concat(f 1 , f 2 , . . . , f m , w 1 , w 2 , . . . , w q ).</formula><p>To confirm that adaptive sharpness actually has a stronger correlation with generalization gap than sharpness, we compare rank statistics which demonstrate the change of adaptive sharpness and sharpness with respect to generalization gap. For correlation analysis, we use 4 hyperparameters: mini-batch size, initial learning rate, weight decay coefficient and dropout rate. As can be seen in Table 1, Kendall rank correlation coefficient <ref type="bibr" target="#b15">(Kendall, 1938)</ref> of adaptive sharpness is greater than that of sharpness regardless of the value of p. Furthermore, we compare granulated coefficients <ref type="bibr" target="#b13">(Jiang et al., 2019)</ref> with respect to different hyper-parameters to measure the effect of each hyperparameter separately. In <ref type="table" target="#tab_0">Table 1</ref>, the coefficients of adaptive sharpness are higher in most hyper-parameters and the average as well. Scatter plots illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> also show stronger correlation of adaptive sharpness. The dif- ference in correlation behaviors of adaptive sharpness and sharpness provides an evidence that scale-invariant property helps strengthen the correlation with generalization gap. The experimental details are described in Appendix B.</p><p>Although there are various normalization methods other than the normalization operators introduced above, this paper covers only element-wise and filter-wise normalization operators. Node-wise normalization can also be viewed as a normalization operator. <ref type="bibr" target="#b31">Tsuzuku et al. (2020)</ref> suggest node-wise normalization method for obtaining normalized flatness. However, the method requires that the parameter should be at a critical point. Also, in the case of node-wise normalization using unit-invariant SVD <ref type="bibr" target="#b32">(Uhlmann, 2018)</ref>, there is a concern that the speed of the optimizer can be degraded due to the significant additional cost for scaledirection decomposition of weight tensors. Therefore the node-wise normalization is not covered in this paper. In the case of layer-wise normalization using spectral norm or Frobenius norm of weight matrices <ref type="bibr" target="#b27">(Neyshabur et al., 2017)</ref>, the condition T ?1 Aw A = T ?1 w is not satisfied. Therefore, it cannot be used for adaptive sharpness so we do not cover it in this paper.</p><p>Meanwhile, even though all weight parameters including biases can have scale-dependency, there remains more to consider when applying normalization to the biases. In terms of bias parameters of rectifier neural networks, there also exists translation-dependency in sharpness, which weakens the correlation with the generalization gap as well. Using the similar arguments as in the proof of Theorem 1, it can be derived that diagonal elements of T w corresponding to biases must be replaced by constants to guarantee translation-invariance, which induces adaptive sharpness that corresponds to the case of not applying bias normalization. We compare the generalization performance based on adaptive sharpness with and without bias normalization, in Section 5.</p><p>There are several previous works which are closely related to adaptive sharpness. <ref type="bibr" target="#b20">Li et al. (2018)</ref>, which suggest a methodology for visualizing loss landscape, is related to adaptive sharpness. In that study, filter-wise normalization which is equivalent to the definition in Equation 5 is used to remove scale-dependency from loss landscape and make comparisons between loss functions meaningful. In spite of their empirical success, <ref type="bibr" target="#b20">Li et al. (2018)</ref> do not provide a theoretical evidence for explaining how filter-wise scaling contributes the scale-invariance and correlation with generalization. In this paper, we clarify how the filter-wise normalization relates to generalization by proving the scaleinvariant property of adaptive sharpness in Theorem 1.</p><p>Also, sharpness suggested in <ref type="bibr" target="#b16">Keskar et al. (2017)</ref> can be regarded as a special case of adaptive sharpness which uses p = ? and the element-wise normalization operator. <ref type="bibr" target="#b13">Jiang et al. (2019)</ref> confirm experimentally that the adaptive sharpness suggested in <ref type="bibr" target="#b16">Keskar et al. (2017)</ref> shows a higher correlation with the generalization gap than sharpness which does not use element-wise normalization operator. This experimental result implies that Theorem 1 is also practically validated.</p><p>Therefore, it seems that sharpness with p = ? suggested by <ref type="bibr" target="#b16">Keskar et al. (2017)</ref> also can be used directly for learning as it is, but a problem arises in terms of generalization performance in learning. <ref type="bibr" target="#b6">Foret et al. (2021)</ref> confirm experimentally that the generalization performance with sharpness defined in square region ? ? ? ? result is worse than when SAM is performed with sharpness defined in spherical region ? 2 ? ?.</p><p>We conduct performance comparison tests for p = 2 and p = ?, and experimentally reveal that p = 2 is more suitable for learning as in <ref type="bibr" target="#b6">Foret et al. (2021)</ref>. The experimental results are shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adaptive Sharpness-Aware Minimization</head><p>In the previous section, we introduce a scale-invariant measure called adaptive sharpness to overcome the limitation of sharpness. As in sharpness, we can obtain a generalization bound using adaptive sharpness, which is presented in the following theorem.</p><formula xml:id="formula_14">Theorem 2. Let T ?1 w be the normalization operator on R k . If L D (w) ? E ?i?N (0,? 2 ) [L D (w+?)] for some ? &gt; 0, then with probability 1 ? ?, L D (w) ? max T ?1 w ? 2 ?? L S (w + ?) + h w 2 2 ? 2 ? 2<label>(6)</label></formula><p>where h : R + ? R + is a strictly increasing function, n = |S| and ? = ? k?(1 + log n/k)/?.</p><p>Note that Theorem 2 still holds for p &gt; 2 due to the monotonicity of p-norm, i.e., if 0 &lt; r &lt; p, x p ? x r for any x ? R n . If T w is an identity operator, Equation 6 is reduced equivalently to Equation 1. The proof of Equation <ref type="formula" target="#formula_14">6</ref> is described in detail in Appendix A.1.</p><p>Algorithm 1 ASAM algorithm (p = 2)</p><p>Input: Loss function l, training dataset S := ? n i=1 {(x i , y i )}, mini-batch size b, radius of maximization region ?, weight decay coefficient ?, scheduled learning rate ?, initial weight w 0 . Output: Trained weight w Initialize weight w :</p><formula xml:id="formula_15">= w 0 while not converged do Sample a mini-batch B of size b from S ? := ? T 2 w ?L B (w) T w ?L B (w) 2 w := w ? ? (?L B (w + ?) + ?w) ? end while return w</formula><p>The right hand side of Equation 6, i.e., generalization bound, can be expressed using adaptive sharpness as</p><formula xml:id="formula_16">max T ?1 w ? p ?? L S (w + ?) ? L S (w) + L S (w) + h w 2 2 ? 2 ? 2 .</formula><p>Since h w 2 2 /? 2 ? 2 is a strictly increasing function with respect to w 2 2 , it can be substituted with ? 2 weight decaying regularizer. Therefore, we can define adaptive sharpness-aware minimization problem as</p><formula xml:id="formula_17">min w max T ?1 w ? p?? L S (w + ?) + ? 2 w 2 2 .<label>(7)</label></formula><p>To solve the minimax problem in Equation 7, it is necessary to find optimal ? first. Analogous to SAM, we can approximate the optimal ? to maximize L S (w + ?) using a first-order approximation as</p><formula xml:id="formula_18">? t = arg max ? p ?? L S (w t + T wt? ) ? arg max ? p ??? ? T wt ?L S (w t ) = ? sign(?L S (w t )) |T wt ?L S (w t )| q?1 T wt ?L S (w t ) q?1 q where? = T ?1 w ?.</formula><p>Then, the two-step procedure for adaptive sharpness-aware minimization (ASAM) is expressed as</p><formula xml:id="formula_19">? ? ? ? t = ?T wt sign(?L S (w t )) |T wt ?L S (w t )| q?1 T wt ?L S (w t ) q?1 q w t+1 = w t ? ? t (?L S (w t + ? t ) + ?w t )</formula><p>for t = 0, 1, 2, ? ? ? . Especially, if p = 2, and if p = ?, ? t = ?T wt sign(?L S (w t )).</p><formula xml:id="formula_20">? t = ? T 2 wt ?L S (w t ) T wt ?L S (w t ) 2</formula><p>In this study, experiments are conducted on ASAM in cases of p = ? and p = 2. The ASAM algorithm with p = 2 is described in detail on Algorithm 1. Note that the SGD <ref type="bibr" target="#b26">(Nesterov, 1983)</ref> update marked with ? in Algorithm 1 can be combined with momentum or be replaced by update of another optimization scheme such as Adam <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we evaluate the performance of ASAM. We first show how SAM and ASAM operate differently in a toy example. We then compare the generalization performance of ASAM with other learning algorithms for various model architectures and various datasets: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b18">(Krizhevsky et al., 2009</ref>), ImageNet <ref type="bibr" target="#b4">(Deng et al., 2009)</ref> and IWSLT'14 DE-EN <ref type="bibr" target="#b0">(Cettolo et al., 2014)</ref>. Finally, we show how robust to label noise ASAM is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Toy Example</head><p>As mentioned in Section 4, sharpness varies by parameter re-scaling even if its loss function remains the same, while adaptive sharpness does not. To elaborate this, we consider a simple loss function L(w) = |w 1 ReLU(w 2 )?0.04| where w = (w 1 , w 2 ) ? R 2 . <ref type="figure" target="#fig_3">Figure 3</ref> presents the trajectories of SAM and ASAM with two different initial weights w 0 = (0.2, 0.05) and w 0 = (0.3, 0.033). The red line represents the set of minimizers of the loss function L, i.e., {(w 1 , w 2 ); w 1 w 2 = 0.04, w 2 &gt; 0}. As seen in <ref type="figure">Figure 1(a)</ref>, sharpness is maximized when w 1 = w 2 within the same loss contour line, and therefore SAM tries to converge to (0.2, 0.2). Here, we use ? = 0.05 as in <ref type="bibr" target="#b6">Foret et al. (2021)</ref>. On the other hand, adaptive sharpness remains the same <ref type="figure">Figure 4</ref>. Test accuracy curves obtained from ASAM algorithm using a range of ? with different factors: element-wise normalization with p = ?, element-wise normalization with p = 2 with and without bias normalization (BN) and filter-wise normalization with p = 2.</p><p>along the same contour line which implies that ASAM converges to the point in the red line near the initial point as can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Since SAM uses a fixed radius in a spherical region for minimizing sharpness, it may cause undesirable results depending on the loss surface and the current weight. If w 0 = (0.3, 0.033), while SAM even fails to converge to the valley with ? = 0.05, ASAM converges no matter which w 0 is used if ? &lt; ? 2. In other words, appropriate ? for SAM is dependent on the scales of w on the training trajectory, whereas ? of ASAM is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image Classification: CIFAR-10/100 and ImageNet</head><p>To confirm the effectiveness of ASAM, we conduct comparison experiments with SAM using CIFAR-10 and CIFAR-100 datasets. We use the same data split as the original paper <ref type="bibr" target="#b18">(Krizhevsky et al., 2009</ref>). The hyper-parameters used in this test is described in <ref type="table" target="#tab_2">Table 3</ref>. Before the comparison tests with SAM, there are three factors to be chosen in ASAM algorithm:</p><p>? normalization schemes: element-wise vs. filter-wise ? p-norm: p = ? vs. p = 2</p><p>? bias normalization: with vs. without First, we perform the comparison test for filter-wise and element-wise normalization using WideResNet-16-8  model <ref type="bibr" target="#b39">(Zagoruyko &amp; Komodakis, 2016)</ref> and illustrate the results in <ref type="figure">Figure 4</ref>. As can be seen, both test accuracies are comparable across ?, and element-wise normalization provides a slightly better accuracy at ? = 1.0.</p><p>Similarly, <ref type="figure">Figure 4</ref> shows how much test accuracy varies with the maximization region for adaptive sharpness. It can be seen that p = 2 shows better test accuracies than p = ?, which is consistent with <ref type="bibr" target="#b6">Foret et al. (2021)</ref>. We could also observe that bias normalization does not contribute to the improvement of test accuracy in <ref type="figure">Figure 4</ref>. Therefore, we decide to use element-wise normalization operator and p = 2, and not to employ bias normalization in the remaining tests.</p><p>As ASAM has a hyper-parameter ? to be tuned, we first conduct a grid search over {0.00005, 0.0001, 0.0002, . . . , 0.5, 1.0, 2.0} for finding appropriate values of ?. We use ? = 0.5 for CIFAR-10 and ? = 1.0 for CIFAR-100, because it gives moderately good performance across various models. We set ? for SAM as 0.05 for CIFAR-10 and 0.1 for CIFAR-100 as in <ref type="bibr" target="#b6">Foret et al. (2021)</ref>. ? for ASAM is set to 0.01. We set mini-batch size to 128, and m-sharpness suggested by <ref type="bibr" target="#b6">Foret et al. (2021)</ref> is not employed. The number of epochs is set to 200 for SAM and ASAM and 400 for SGD. Momentum and weight decay coefficient are set to 0.9 and 0.0005, respectively. Cosine learning rate decay <ref type="bibr" target="#b22">(Loshchilov &amp; Hutter, 2016</ref>) is adopted with an initial learning rate 0.1. Also, random resize, padding by four pixels, normalization and random horizontal flip are applied for data augmentation and label smoothing <ref type="bibr" target="#b25">(M?ller et al., 2019</ref>) is adopted with its factor of 0.1.</p><p>Using the hyper-parameters, we compare the best test accuracies obtained by SGD, SAM and ASAM for various rectifier neural network models: VGG <ref type="bibr" target="#b29">(Simonyan &amp; Zisserman, 2015)</ref>, ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref>, DenseNet <ref type="bibr" target="#b11">(Huang et al., 2017)</ref>, WideResNet <ref type="bibr" target="#b39">(Zagoruyko &amp; Komodakis, 2016)</ref>, and ResNeXt <ref type="bibr" target="#b34">(Xie et al., 2017)</ref>.</p><p>For PyramidNet-272 <ref type="bibr" target="#b7">(Han et al., 2017)</ref>, we additionally apply some latest techniques: AutoAugment <ref type="bibr" target="#b3">(Cubuk et al., 2019)</ref>, CutMix <ref type="bibr" target="#b38">(Yun et al., 2019)</ref> and ShakeDrop <ref type="bibr" target="#b35">(Yamada et al., 2019)</ref>. We employ the m-sharpness strategy with m = 32. Initial learning rate and mini-batch size are set to 0.05 and 256, respectively. The number of epochs is set to 900 for SAM and ASAM and 1800 for SGD. We choose ? for SAM as 0.05, as in <ref type="bibr" target="#b6">Foret et al. (2021)</ref>, and ? for ASAM as 1.0 for both CIFAR-10 and CIFAR-100. Every entry in the tables represents mean and standard deviation of 5 independent runs. In both CIFAR-10 and CIFAR-100 cases, ASAM generally surpasses SGD and SAM, as can be seen in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>.</p><p>For the sake of evaluations at larger scale, we compare the performance of SGD, SAM and ASAM on ImageNet. We apply each method with ResNet-50 and use ? = 0.05 for SAM and ? = 1.0 for ASAM. The number of training epochs is 200 for SGD and 100 for SAM and ASAM. We use mini-batch size 512, initial learning rate 0.2, and SGD optimizer with weight decay coefficient 0.0001. Other hyper-parameters are the same as those of CIFAR-10/100 tests. We also employ m-sharpness with m = 128 for both SAM and ASAM. <ref type="table" target="#tab_3">Table 4</ref> shows mean and standard deviation of maximum test accuracies over 3 independent runs for each method. As can be seen in the table, ASAM achieves higher accuracies than SGD and SAM. These results imply that ASAM can enhance generalization performance of rectifier neural network architectures in image classification task beyond CIFAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Machine Translation: IWSLT'14 DE-EN</head><p>To validate effectiveness of ASAM in tasks other than image classification, we apply SAM and ASAM to IWSLT'14 DE-EN, a dataset on machine translation task.</p><p>In this test, we adopt Transformer architecture <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> and Adam optimizer as a base optimizer of SAM and ASAM instead of SGD. Learning rate, ? 1 and ? 2 for Adam are set to 0.0005, 0.9 and 0.98, respectively. Dropout rate and weight decay coefficient are set to 0.3 and 0.0001, respectively. Label smoothing is adopted with its factor 0.1. We choose ? = 0.1 for SAM and ? = 0.2 for ASAM as a result of a grid search over {0.005, 0.01, 0.02, . . . , 0.5, 1.0, 2.0} using validation dataset. The results of the experiments are obtained from 3 independent runs.</p><p>As can be seen in <ref type="table" target="#tab_4">Table 5</ref>, we could observe improvement even on IWSLT'14 in BLEU score when using Adam+ASAM instead of Adam or Adam+SAM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Robustness to Label Noise</head><p>As shown in <ref type="bibr" target="#b6">Foret et al. (2021)</ref>, SAM is as robust to label noise in the training data as MentorMix <ref type="bibr" target="#b12">(Jiang et al., 2020)</ref>, which is a state-of-the-art method. We expect that ASAM would share the robustness to label noise. To confirm this, we compare the test accuracies of SGD, SAM and ASAM for ResNet-32 model and CIFAR-10 dataset whose labels in the training data are corrupted by symmetric label noise <ref type="bibr" target="#b28">(Rooyen et al., 2015)</ref> with noise levels of 20%, 40%, 60% and 80%, and the test data is not touched. Hyperparameter settings are the same as that of previous CIFAR experiments. <ref type="table" target="#tab_5">Table 6</ref> shows test accuracies for SGD, SAM and ASAM obtained from 3 independent runs with respect to label noise levels. Compared to SGD and SAM, ASAM generally enhances the test accuracy across various noise level by retaining the robustness to label noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have introduced adaptive sharpness with scale-invariant property that improves training path in weight space by adjusting their maximization region with respect to weight scale. Also, we have confirmed that this property, which ASAM shares, contributes in improvement of generalization performance. The superior performance of ASAM is notable from the comparison tests conducted against SAM, which is currently state-of-theart learning algorithm in many image classification benchmarks. In addition to the contribution as a learning algorithm, adaptive sharpness can serve as a generalization measure with stronger correlation with generalization gap benefiting from their scale-invariant property. Therefore adaptive sharpness has a potential to be a metric for assessment of neural networks. We have also suggested the condition of normalization operator for adaptive sharpness but we did not cover all the normalization schemes which satisfy the condition. So this area could be further investigated for better generalization performance in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head><p>A.1. Proof of Theorem 2</p><p>We first introduce the following concentration inequality. Lemma 1. Let {? i , i = 1, . . . , k} be independent normal variables with mean 0 and variance ? 2 i . Then,</p><formula xml:id="formula_21">P ? ? k i=1 ? 2 i ? k? 2 max 1 + log n k 2 ? ? ? 1 ? n where ? max = max{? i }.</formula><p>Proof. From Lemma 1 in <ref type="bibr" target="#b19">Laurent &amp; Massart (2000)</ref>, for any x &gt; 0,</p><formula xml:id="formula_22">P ? ? k i=1 ? 2 i ? k i=1 ? 2 i + 2 k i=1 ? 4 i x + 2? 2 max x ? ? ? exp(?x). Since k i=1 ? 2 i + 2 k i=1 ? 4 i x + 2? 2 max x ? ? 2 max (k + 2 ? kx + 2x) ? ? 2 max ( ? k + ? 2x) 2 ,</formula><p>plugging x = 1 2 log n proves the lemma.</p><formula xml:id="formula_23">Theorem 3. Let T ?1 w be a normalization operator of w on R k . If L D (w) ? E ?i?N (0,? 2 ) [L D (w + ?)] for some ? &gt; 0, then with probability 1 ? ?, L D (w) ? max T ?1 w ? 2?? L S (w + ?) + 1 n ? 1 ? ? k log ? ? 1 + w 2 2 ? 2 ? 2 1 + log n k 2 ? ? + 4 log n ? + O(1) ? ?</formula><p>where n = |S| and ? = ? k?(1 + log n/k)/?.</p><p>Proof. The idea of the proof is given in <ref type="bibr" target="#b6">Foret et al. (2021)</ref>. From the assumption, adding Gaussian perturbation on the weight space does not improve the test error. Moreover, from Theorem 3.2 in <ref type="bibr" target="#b1">Chatterji et al. (2019)</ref>, the following generalization bound holds under the perturbation:</p><formula xml:id="formula_24">E ?i?N (0,? 2 ) [L D (w + ?)] ? E ?i?N (0,? 2 ) [L S (w + ?)] + 1 n ? 1 1 4 k log 1 + w 2 2 k? 2 + log n ? + C(n, ?, k) .</formula><p>Therefore, the left hand side of the statement can be bounded as</p><formula xml:id="formula_25">L D (w) ? E ?i?N (0,? 2 ) [L S (w + ?)] + 1 n ? 1 1 4 k log 1 + w 2 2 k? 2 + log n ? + C ? 1 ? 1 ? n max T ?1 w ? 2?? L S (w + ?) + 1 ? n + 1 n ? 1 1 4 k log 1 + w 2 2 k? 2 + log n ? + C ? max T ?1 w ? 2?? L S (w + ?) + 1 n ? 1 k log 1 + w 2 2 k? 2 + 4 log n ? + 4C</formula><p>where the second inequality follows from Lemma 1 and T ?1 w 2 ? 1 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Correlation Analysis</head><p>To capture the correlation between generalization measures, i.e., sharpness and adaptive sharpness, and actual generalization gap, we utilize Kendall rank correlation coefficient <ref type="bibr" target="#b15">(Kendall, 1938)</ref>. Formally, given the set of pairs of a measure and generalization gap observed S = {(m 1 , g 1 ), . . . , (m n , g n )}, Kendall rank correlation coefficient ? is given by ? (S) = 2 n(n ? 1) i&lt;j sign(m i ? m j )sign(g i ? g j ).</p><p>Since ? represents the difference between the proportion of concordant pairs, i.e., either both m i &lt; m j and g i &lt; g j or both m i &gt; m j and g i &gt; g j among the whole n 2 point pairs, and the proportion of discordant pairs, i.e., not concordant, the value of ? is in the range of [?1, 1].</p><p>While the rank correlation coefficient aggregates the effects of all the hyper-parameters, granulated coefficient <ref type="bibr" target="#b13">(Jiang et al., 2019)</ref> can consider the correlation with respect to the each hyper-parameter separately. If ? = N i=1 ? i is the Cartesian product of each hyper-parameter space ? i , granulated coefficient with respect to ? i is given by</p><formula xml:id="formula_26">? i = 1 |? ?i | ?1??1 ? ? ? ?i?1??i?1 ?i+1??i+1 ? ? ? ?N ??N ? ?i??i {(m(?), g(?)} where ? ?i = ? 1 ? ? ? ? ? i?1 ? ? i+1 ? ? N .</formula><p>Then the average ? = N i=1 ? i /N of ? i indicates whether the correlation exists across all hyper-parameters.</p><p>We vary 4 hyper-parameters, mini-batch size, initial learning rate, weight decay coefficient and dropout rate, to produce different models. It is worth mentioning that changing one or two hyper-parameters for correlation analysis may cause spurious correlation <ref type="bibr" target="#b13">(Jiang et al., 2019)</ref>. For each hyper-parameter, we use 5 different values in <ref type="table" target="#tab_6">Table 7</ref> which implies that 5 4 = 625 configurations in total. By using the above hyper-parameter configurations, we train WideResNet-28-2 model on CIFAR-10 dataset. We use SGD as an optimizer and set momentum to 0.9. We set the number of epochs to 200 and cosine learning rate decay <ref type="bibr" target="#b22">(Loshchilov &amp; Hutter, 2016</ref>) is adopted. Also, random resize, padding by four pixels, normalization and random horizontal flip are applied for data augmentation and label smoothing <ref type="bibr" target="#b25">(M?ller et al., 2019</ref>) is adopted with its factor of 0.1. Using model parameters with training accuracy higher than 99.0% among the generated models, we calculate sharpness and adaptive sharpness with respect to generalization gap.</p><p>To calculate adaptive sharpness, we fix normalization scheme to element-wise normalization. We calculate adaptive sharpness and sharpness with both p = 2 and p = ?. We conduct a grid search over {5e?6, 1e?5, 5e?5, . . . , 5e?1, 1.0} to obtain each ? for sharpness and adaptive sharpness which maximizes correlation with generalization gap. As results of the grid search, we select 1e?5 and 5e?4 as ?s for sharpness of p = 2 and p = ?, respectively, and select 5e?1 and 5e?3 as ?s for adaptive sharpness of p = 2 and p = ?, respectively. To calculate maximizers of each loss function for calculation of sharpness and adaptive sharpness, we follow m-sharpness strategy suggested by <ref type="bibr" target="#b6">Foret et al. (2021)</ref> and m is set to 8.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Adaptive sharpness (p = ?), ? = 0.616.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Scatter plots which show correlation of sharpness and adaptive sharpness with respect to generalization gap and their rank correlation coefficients ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Trajectories of SAM and ASAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Rank statistics for sharpness and adaptive sharpness.</figDesc><table><row><cell></cell><cell cols="2">p = 2</cell><cell cols="2">p = ?</cell></row><row><cell></cell><cell>sharpness</cell><cell>adaptive sharpness</cell><cell>sharpness</cell><cell>adaptive sharpness</cell></row><row><cell>? (rank corr.)</cell><cell>0.174</cell><cell>0.636</cell><cell>0.257</cell><cell>0.616</cell></row><row><cell>mini-batch size</cell><cell>0.667</cell><cell>0.696</cell><cell>0.777</cell><cell>0.817</cell></row><row><cell>learning rate</cell><cell>0.563</cell><cell>0.577</cell><cell>0.797</cell><cell>0.806</cell></row><row><cell>weight decay</cell><cell>?0.297</cell><cell>0.534</cell><cell>?0.469</cell><cell>0.656</cell></row><row><cell>dropout rate</cell><cell>0.102</cell><cell>?0.092</cell><cell>0.161</cell><cell>0.225</cell></row><row><cell>? (avg.)</cell><cell>0.259</cell><cell>0.429</cell><cell>0.316</cell><cell>0.626</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Maximum test accuracies for SGD, SAM and ASAM on CIFAR-10 dataset.</figDesc><table><row><cell>Model</cell><cell>SGD</cell><cell>SAM</cell><cell>ASAM</cell></row><row><cell>DenseNet-121</cell><cell cols="3">91.00?0.13 92.00?0.17 93.33?0.04</cell></row><row><cell>ResNet-20</cell><cell cols="3">93.18?0.21 93.56?0.15 93.82?0.17</cell></row><row><cell>ResNet-56 VGG19-BN  *</cell><cell cols="3">94.58?0.20 95.18?0.15 95.42?0.16 93.87?0.09 94.60 95.07?0.05</cell></row><row><cell>ResNeXt29-32x4d</cell><cell cols="3">95.84?0.24 96.34?0.30 96.80?0.06</cell></row><row><cell>WRN-28-2</cell><cell cols="3">95.13?0.16 95.74?0.08 95.94?0.05</cell></row><row><cell>WRN-28-10</cell><cell cols="3">96.34?0.12 96.98?0.04 97.28?0.07</cell></row><row><cell>PyramidNet-272  ?</cell><cell cols="3">98.44?0.08 98.55?0.05 98.68?0.08</cell></row><row><cell cols="4">Some runs completely failed, thus giving 10% of accuracy (suc-</cell></row><row><cell cols="4">cess rate: SGD: 3/5, SAM 1/5, ASAM 3/5)  ? PyramidNet-272 architecture is tested 3 times for each learning</cell></row><row><cell>algorithm.</cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Maximum test accuracies for SGD, SAM and ASAM on</cell></row><row><cell>CIFAR-100 dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>SGD</cell><cell>SAM</cell><cell>ASAM</cell></row><row><cell>DenseNet-121</cell><cell cols="3">68.70?0.31 69.84?0.12 70.60?0.20</cell></row><row><cell>ResNet-20</cell><cell cols="3">69.76?0.44 71.06?0.31 71.40?0.30</cell></row><row><cell>ResNet-56 VGG19-BN  *</cell><cell cols="3">73.12?0.19 75.16?0.05 75.86?0.22 71.80?1.35 73.52?1.74 75.80?0.27</cell></row><row><cell>ResNeXt29-32x4d</cell><cell cols="3">79.76?0.23 81.48?0.17 82.30?0.11</cell></row><row><cell>WRN-28-2</cell><cell cols="3">75.28?0.17 77.25?0.35 77.54?0.14</cell></row><row><cell>WRN-28-10</cell><cell cols="3">81.56?0.13 83.42?0.04 83.68?0.12</cell></row><row><cell>PyramidNet-272  ?</cell><cell cols="3">88.91?0.12 89.36?0.20 89.90?0.13</cell></row><row><cell cols="4">Some runs completely failed, thus giving 10% of accuracy (suc-</cell></row><row><cell cols="4">cess rate: SGD: 5/5, SAM 4/5, ASAM 4/5)  ? PyramidNet-272 architecture is tested 3 times for each learning</cell></row><row><cell>algorithm.</cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Top1 and Top5 maximum test accuracies for SGD, SAM and ASAM on ImageNet dataset using ResNet-50. SGD SAM ASAM Top1 75.79?0.22 76.39?0.03 76.63?0.18 Top5 92.62?0.04 92.97?0.07 93.16?0.18</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>BLEU scores for Adam, Adam+SAM and Adam+ASAM on IWSLT'14 DE-EN dataset using Transformer.</figDesc><table><row><cell>BLEU score</cell><cell>Adam</cell><cell cols="2">Adam+SAM Adam+ASAM</cell></row><row><cell>Validation</cell><cell cols="2">35.34?&lt;0.01 35.52?0.01</cell><cell>35.66?&lt;0.01</cell></row><row><cell>Test</cell><cell cols="2">34.86?&lt;0.01 34.78?0.01</cell><cell>35.02?&lt;0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Maximum test accuracies of ResNet-32 models trained on CIFAR-10 with label noise.</figDesc><table><row><cell>Noise rate</cell><cell>SGD</cell><cell>SAM</cell><cell>ASAM</cell></row><row><cell>0%</cell><cell>94.50?0.11</cell><cell>94.80?0.12</cell><cell>94.88?0.12</cell></row><row><cell>20%</cell><cell>91.32?0.23</cell><cell>92.94?0.12</cell><cell>93.21?0.10</cell></row><row><cell>40%</cell><cell>87.68?0.05</cell><cell>90.62?0.18</cell><cell>90.89?0.13</cell></row><row><cell>60%</cell><cell>82.50?0.30</cell><cell>86.58?0.30</cell><cell>87.41?0.16</cell></row><row><cell>80%</cell><cell cols="2">68.35?0.85 69.92?0.98</cell><cell>67.69?1.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Hyper-parameter configurations.</figDesc><table><row><cell>mini-batch size</cell><cell>32, 64, 128, 256, 512</cell></row><row><cell>learning rate</cell><cell>0.0033, 0.01, 0.033, 0.1, 0.33</cell></row><row><cell>weight decay</cell><cell>5e?7, 5e?6, 5e?5, 5e?4, 5e?3</cell></row><row><cell>dropout rate</cell><cell>0, 0.125, 0.25, 0.375, 0.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Samsung Research, Seoul, Republic of Korea. Correspondence to: Jeongseop Kim &lt;jisean.kim@samsung.com&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>We would like to thank Kangwook Lee, Jaedeok Kim and Yonghyun Ryu for supports on our machine translation experiments. We also thank our other colleagues at Samsung Research -Joohyung Lee, Chiyoun Park and Hyun-Joo Jung -for their insightful discussions and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT evaluation campaign, IWSLT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam</title>
		<meeting>the International Workshop on Spoken Language Translation, Hanoi, Vietnam</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The intriguing role of module criticality in the generalization of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124018</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5927" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Flat minima. Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simplifying neural nets by discovering flat minima. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4804" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fantastic generalization measures and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The normalization method for alleviating pathological sharpness in wide neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Karakida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CIFAR-10 and CIFAR-100 datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adaptive estimation of a quadratic functional by model selection. Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Massart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1302" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;18: Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6391" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fisher-rao metric, geometry, and complexity of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stokes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="888" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PAC-Bayesian model averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twelfth annual conference on Computational learning theory</title>
		<meeting>the twelfth annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="164" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks by diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04114</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E. ; H</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Dokl. akad. nauk Sssr</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">;</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: the importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring the vulnerability of deep neural networks: A study of parameter corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2006.05620</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using PAC-Bayesian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuzuku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9636" to="9647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A generalized matrix inverse that is consistent with respect to diagonal transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="781" to="800" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shakedrop regularization for deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="186126" to="186136" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02237</idno>
		<title level="m">Positively scale-invariant flatness of relu neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nouiehed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Kontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.05348</idno>
		<title level="m">Sharpnessaware learning rates for improved generalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
