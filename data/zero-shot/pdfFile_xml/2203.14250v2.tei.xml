<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Active Speaker Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Le?n Alc?zar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Cordes</surname></persName>
							<email>moritz.cordes@stud.leuphana.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Leuphana University of L?neburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<email>chen.zhao@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Active Speaker Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in the Active Speaker Detection (ASD) problem build upon a two-stage process: feature extraction and spatio-temporal context aggregation. In this paper, we propose an end-to-end ASD workflow where feature learning and contextual predictions are jointly learned. Our end-to-end trainable network simultaneously learns multimodal embeddings and aggregates spatio-temporal context. This results in more suitable feature representations and improved performance in the ASD task. We also introduce interleaved graph neural network (iGNN) blocks, which split the message passing according to the main sources of context in the ASD problem. Experiments show that the aggregated features from the iGNN blocks are more suitable for ASD, resulting in state-of-the art performance. Finally, we design a weakly-supervised strategy, which demonstrates that the ASD problem can also be approached by utilizing audiovisual data but relying exclusively on audio annotations. We achieve this by modelling the direct relationship between the audio signal and the possible sound sources (speakers), as well as introducing a contrastive loss. All the resources of this project will be made available at: https://github.com/fuankarion/end-to-end-asd.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In active speaker detection (ASD), the current speaker must be identified from a set of available candidates, which are usually defined by face tracklets assembled from temporally linked face detections <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33]</ref>. Initial approaches to the ASD problem focused on the analysis of individual visual tracklets and the associated audio track, aiming to maximize the agreement between the audio signal and the visual patterns <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b56">57]</ref>. Such an approach is suitable for scenarios where a single visual track is available. However, in the general (multi-speaker) scenario, this naive correspondence will suffer from false positive detections, leading to incorrect speech-to-speaker assignments.</p><p>Current approaches for ASD rely on two-stage models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>. First, they associate the facial motion pat-terns and its concurrent audio stream by optimizing a multimodal encoder <ref type="bibr" target="#b39">[40]</ref>. This multi-modal encoder serves as a feature extractor for a second stage, in which multimodal embeddings from multiple speakers are fused <ref type="bibr" target="#b0">[1]</ref>.</p><p>These two-stage approaches are currently preferred given the technical challenges of end-to-end training with video data. Despite the computational efficiency of these approaches, their two-stage nature precludes them from fully leveraging the learning capabilities of modern neural architectures, namely directly optimizing the features for the multi-speaker ASD task.</p><p>In this paper, we present a novel alternative to the traditional two-stage ASD methods, called End-to-end Active Speaker dEtEction (EASEE), which is the first end-to-end pipeline for active speaker detection. Unlike conventional methods, EASEE is able to learn multi-modal features from multiple visual tracklets, while simultaneously modeling their spatio-temporal relations in an end-to-end manner. As a consequence, EASEE feature embeddings are optimized to capture information from multiple speakers and enable effective speech-to-speaker assignments in a fully supervised manner. To generate its final predictions, our end-toend architecture relies on a spatio-temporal module for context aggregation. We propose an interleaved Graph Neural Network (iGNN) block to model the relationships between speakers in adjacent timestamps. Instead of greedily fusing all available feature representations from multiple timestamps, the iGNN block provides a more principled way of modeling spatial and temporal interactions. iGNN performs two message passing steps: first a spatial message passing that models local interactions between speakers visible at the same timestamp, and then a temporal message passing that effectively aggregates long-term temporal information.</p><p>Finally, EASEE's end-to-end nature allows the use of alternative supervision targets. In this paper, we propose a weakly-supervised strategy for ASD, named EASEE-W (shown in <ref type="figure">Figure 1</ref>). EASEE-W relies exclusively on audio labels, which are easier to obtain, to train the whole architecture. To optimize our network without the visual labels, we model the inherent structure in the ASD task, namely the direct relationship between the audio signal and its possible  <ref type="figure">Figure 1</ref>. Fully and weakly-supervised audiovisual embeddings. In the fully supervised scenario (left), we use the face crops as visual data and the Mel-frequency cepstral coefficients as audio data, we rely on visual and audio labels to directly optimize a shared feature embedding. In contrast, in the weakly supervised scenario, we omit the visual labels and optimize using only audio supervision. By modeling the visual-temporal consistency and speech-to-speaker assignments, we are able to optimize a shared embedding that can detect the active speakers without any visual supervision.</p><p>sound sources, i.e., the speakers.</p><p>Contributions. This paper proposes EASEE, a novel strategy for active speaker detection. Its end-to-end nature enables direct optimization of audio-visual embeddings and leverages novel training strategies, namely weak supervision. Our work brings the following contributions: <ref type="bibr" target="#b0">(1)</ref> We devise the first end-to-end trainable neural architecture EASEE for the active speaker problem (Section 3.1), which learns effective feature representations. <ref type="bibr" target="#b1">(2)</ref> In EASEE, we propose a novel iGNN block to aggregate spatial and temporal context based on a composition of spatial and temporal message passing. We show this reformulation of the graph structure is key to achieve state-of-the-art results (Section 4.1). (3) Based on EASEE, we propose the first weakly-supervised ASD approach that enables the use of only audio labels to generate predictions on visual data (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early approaches to the ASD problem <ref type="bibr" target="#b11">[12]</ref> attempted to correlate audiovisual patterns using time-delayed neural networks <ref type="bibr" target="#b47">[48]</ref>. Follow up works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16]</ref> approached the ASD task by limiting the analysis only to visual patterns. These approaches rely only on visual data given the biases of the single speaker scenario (i.e. speech can only be attributed to the single visible speaker). A parallel corpus of work focused on the complementary task of voice activity detection (VAD), which aims at finding speech activities among other acoustic events <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b5">6]</ref>. Similar to visual data, audio-only information was also proven to be useful in single speaker scenarios <ref type="bibr" target="#b13">[14]</ref>.</p><p>The recent interest in deep neural architectures <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref> shifted the focus in the ASD problem from hand-crafted feature design to multi-modal representation learning <ref type="bibr" target="#b36">[37]</ref>. As a consequence, ASD has become dominated by CNNbased approaches, which rely on convolutional encoders originally devised for image analysis tasks <ref type="bibr" target="#b39">[40]</ref>. Recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref> approached the more general multi-speaker scenario, relying on the fusion of multi-modal information from individual speakers. Concurrent works have also focused on audiovisual feature alignment. This resulted in methods that rely on audio as the primary source of supervision <ref type="bibr" target="#b3">[4]</ref>, or focused on the design of multi-modal embeddings <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>The recent availability of large-scale data for the ASD task <ref type="bibr" target="#b39">[40]</ref> has enabled the use of state-of-the-art deep convolutional encoders <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. In addition to these deep encoders, current approaches have shifted focus to directly modeling the temporal features over short temporal windows, typically by optimizing a Siamese Network with modality specific streams. The work of Chung et al. <ref type="bibr" target="#b8">[9]</ref> explored the use of a hybrid 3D-2D encoder pretained on Vox-Celeb <ref type="bibr" target="#b9">[10]</ref> to analyze these temporal windows, while Zhang et al. <ref type="bibr" target="#b56">[57]</ref> focused on improving the feature representation by using a contrastive loss <ref type="bibr" target="#b18">[19]</ref> between the modalities.</p><p>To complement this short-term analysis, many methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref> have aimed to incorporate contextual information from overlapping visual tracklets. The work of Alcazar et al. <ref type="bibr" target="#b0">[1]</ref> introduced a data structure to represent an active speaker scene, and the features in this structure are improved by using self-attention <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b46">47]</ref> and recurrent networks <ref type="bibr" target="#b21">[22]</ref>.  <ref type="figure">Figure 2</ref>. Overview of the EASEE architecture. We fuse information from multiple visual tracklets, and their associated audio track. We rely on a 3D CNN to encode individual face tracklets, and a 2D CNN to encode the audio stream (Gray Encoders). These embeddings are assembled into an initial multi-modal embedding (?) containing audiovisual information from multiple persons in a scene. We map this embedding into a graph structure that performs message passing steps over spatial (light orange) and temporal dimensions (light green). Our layer arrangement favors independent massage passing steps along the temporal and spatial dimensions.</p><p>Current state-of-the-art techniques incorporate contextual representation and rely on deep 3D encoders for the initial feature encoding and recurrent networks or selfattention to analyze the scene's contextual information <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b55">56]</ref>. We depart from this standard approach and devise a strategy to train end-to-end networks that simultaneously optimize features from a shared multi-modal encoder. This enables the direct optimization of temporal and spatial features for the ASD problem in a multi-speaker setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Convolutional Networks</head><p>The current interest in non-Euclidean data <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref> has focused the attention of the research community on Graph Convolutional Networks (GCNs) as an efficient variant of CNNs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b51">52]</ref>. GCNs have achieved state-of-the-art results in zero-shot recognition <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b25">26]</ref>, 3D understanding <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53]</ref>, and action recognition in video <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> by harnessing the flexibility of graphs representations. Recently, GCNs have been widely used in the field of action recognition, focusing on skeleton-based approaches that rely only on visual data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>. For applications in audiovisual contexts, GCNs have been utilized to study inter-correlations in videos for automatic recognition of emotions in conversations <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. In the ASD domain, Alcazar et al. <ref type="bibr" target="#b32">[33]</ref> introduced the use of GCNs, developing a two-stage approach where a GCN network would module interactions between audio and video across multiple frames. We present an alternative to this approach where we focus on the end-to-end modeling, and perform independent steps of message passing along the spatial and temporal dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">End-to-End Active Speaker Detection</head><p>Our approach relies on the initial generation of independent audio and visual embeddings at specific timestamps. These embeddings are fused and jointly optimized by means of a graph convolutional network <ref type="bibr" target="#b28">[29]</ref>. To this end, we devise a neural architecture with three main components: (i) audio Encoder, (ii) visual Encoder, and a (iii) spatio-temporal Module. The visual encoder (f v ) performs multiple forward passes (one for each available tracklet), and the audio encoder (f a ) performs a single forward pass on the shared audio clip. These features are arranged according to their temporal order and (potential) spatial overlap, creating an intermediate feature embedding (?) that enables spatio-temporal reasoning. Unlike other methods, we construct ? such that it can be optimized end-to-end. Thus ? captures multi-modal and multi-speaker information, enables information flow across modalities, and ultimately improves network predictions. <ref type="figure">Figure 2</ref> contains an overview of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">EASEE Network Architecture</head><p>The main goal of EASEE is to aggregate related temporal and spatial information from different modalities over a video segment. To enable efficient end-to-end computation, we do not densely sample all the available tracklets in a temporal window, but rather define a strategy to sub-sample audiovisual segments inside a video. We define a set of temporal endpoints where the original video data (visual and audio) is densely sampled. At every temporal endpoint, we collect visual information from the available face tracklets and sample the associated audio signal (See <ref type="figure">Figure 3</ref>). To further limit the memory usage, we define a fixed number of tracklets (i) to sample at every endpoint. Since the vi-...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Encoder Encoder Encoder</head><p>Encoder Encoder <ref type="figure">Figure 3</ref>. EASEE Sub-Sampling. For every temporal endpoint, we sample i face tracklets and the corresponding audio signal. This sampling is repeated over l consecutive temporal endpoints separated by stride k. The i + 1 feature embeddings obtained at each timestamp are forwarded through the audio (yellow) and visual (light green) encoders fused into the spatio-temporal embedding ? i,k,l,t .</p><formula xml:id="formula_0">1 2 i 1 2 i 1 2 i t t+k i,k,l,t t+lk</formula><p>sual stream might contain an arbitrary number of tracklets, we follow <ref type="bibr" target="#b0">[1]</ref> at training time and sample i tracklets with replacement. Hence, from every temporal endpoint, we create i + 1 feature embeddings associated with it (i visual embeddings from f v and the audio embedding from f a ). We create temporal endpoints over a video segment following a simple strategy, we select a timestamp t and create l temporal endpoints over the video at a fixed stride of k frames. The location of every endpoint is then given by L = {t, t + k, ..., t + lk}. This reduces the total number of samples from the video data by a factor of k and allows us to sample longer sections of video for training and inference.</p><p>Spatio-Temporal Embedding. We build the embedding ? over the endpoint set L.</p><p>We define the spatiotemporal embedding e at time t for speaker s as e t,s = {f a (t), f v (s, t)}. Since there may be multiple visible persons at this endpoint (i.e. |s| ? 1), we define the embedding for an endpoint at time t with up to i speakers as E t,i = {e t,0 , e t,1 , e t,2 , ..., e t,i }. The full spatiotemporal embedding ? i,k,l,t is created by sampling audio and visual features over the endpoint set L, thus ? i,k,l,t = {E t,i , ..., E t+k,i , ..., E t+lk,i }. As ? i,k,l,t is assembled from independent forward passes of the f a and f v encoders, we share weights for forward passes in the same modality, thus each forward/backward pass accumulates gradients over the same weights. This shared weight scheme largely simplifies the complexity of the proposed network, and keeps the total number of parameters stable regardless of the values for l and i.</p><p>Upon computing the initial modality embeddings, we map ? i,k,l,t into a spatio-temporal graph representation. Following <ref type="bibr" target="#b32">[33]</ref>, we map each feature in ? i,k,l,t into an individual node, resulting in a total of (i + 1) * l nodes. Every feature embedding goes through a linear layer for dimensionality reduction before being assigned to a node. Unlike <ref type="bibr" target="#b32">[33]</ref>, we are not interested in building a unique graph struc-ture that performs message passing over all the possible relationships in the node set. Instead, we choose to independently model the two types of information flow in the graph, namely spatial information and temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph Neural Network Architecture</head><p>In EASEE, the GCN component fuses spatio-temporal information from video segments. This module implements a novel composition pattern where the spatial and temporal information message passing are performed in subsequent layers. We devise a building block (iGNN) where the spatial message passing is performed first, then temporal message passing occurs. After these two forward passes, we fuse the feature representation with the previously estimated feature embedding (residual connection). We define the iGNN block at layer J as:</p><formula xml:id="formula_1">? s = M s (A s ? J ; ? s ), ? t = M t (A t ? J ; ? t ) iGN N (? J ) = (M t ? M s )(? J ) + ? J</formula><p>Here, M s is a GCN layer that performs spatial message passing using the spatial adjacency matrix A s over an initial feature embedding (? J ), thus producing an intermediate representation with aggregated local features (? J+1 ). Afterwards, the GCN layer M t performs a temporal message passing using the temporal adjacency matrix A t . ? s and ? t are the parameter set of their respective layers. The final output is complemented with a residual connection, thus favoring gradient propagation.</p><p>In EASEE, the assignment of elements from the embedding ? i,k,l,s to graph nodes remains stable throughout the entire GCN structure (i.e. we do not perform any pooling). This allows us to create a final prediction for every tracklet and audio clip contained in ? i,k,l,t by applying a single linear layer. This arrangement creates two types of nodes: Audio Nodes, which generate predictions for the audio embeddings (i.e. speech detected or silent scene), and Video  Losses &amp; Intermediate Supervision. Audio nodes are supervised in training, but their forward phase output is not suitable for the ASD task. The training loss is defined as:</p><formula xml:id="formula_2">L = L a + L v .</formula><p>where L a is the loss over all audio nodes and L v is the loss over all the video nodes. L a and L v are implemented as cross-entropy (CE) losses. iGNN is also supervised with CE loss, which is calculated individually at every node in the last layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weakly Supervised Active Speaker Detection</head><p>State-of-the-art methods rely on fully supervised approaches to generate consistent predictions in the ASD problem. Typically, they work in a fully supervised manner in both learning stages, using audiovisual labels to train the initial feature encoder and also to supervise the second stage learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b32">33]</ref>. The end-to-end nature of EASEE enables us to approach the active speaker problem from a novel perspective, where the multi-speaker scenario can be analyzed relying on a weak supervision signal, namely only audio labels. In comparison to visual labels, audio groundtruth is less expensive to acquire, as it only establishes the start and end point of a speech event. Meanwhile, labels for visual data must establish the fine-grained association between every temporal interval in the speech event and its visual source.</p><p>Directly training EASEE with audio labels only, would optimize the predictions for the audio nodes (speech events). As outlined before, such predictions are suitable for the voice activity detection task, but the more fine grained ASD task will have poor performance as the visual nodes lack any supervision and yield random outputs. To generate meaningful predictions for the visual nodes while relying only on audio supervision, we reformulate our end-to-end training to enforce information flow between modalities by adding two extra loss functions on the graph structure. This reformulation enables meaningful predictions over the visual data despite the lack of visual ground-truth. We name this version of our approach EASEE-W, a novel architecture that is capable of making active speaker predictions that rely only on weak binary supervision labels from the audio stream. An overview of the key differences between EASEE and EASEE-W is shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Local assignment loss. We design a loss function that models local dependencies in the ASD problem: if there is a speech event, we must attribute the speech to one of the locally associated video nodes. Let V t be the output of video nodes at time t (|V t | ? 2), and y at the ground truth for the audio signal at time t:</p><formula xml:id="formula_3">L s = y at (y at ? max(V t )) + (1 ? y at ) max(V t )</formula><p>The first term y at (y at ? max(V t )) will force EASEE-W to generate at least one positive prediction in V t if y at = 1 (i.e. select a speaker if speech is detected). Likewise, the second term (1 ? y at ) max(V t ) will force EASEE-W to generate only negative predictions in V t in the absence of speech. While this loss forces the network to generate video labels that are locally consistent with the audio supervision, we show that these predictions only improve the performance over a fully random baseline and do not represent an improvement over trivial audiovisual assignments.</p><p>Visual contrastive loss. We complement L s with a contrastive loss (L c ) applied over the video data. As shown in <ref type="figure">Figure 5</ref>, the goal of this loss is to enforce feature similarity between video nodes that belong to the same person, and promote feature differences for non-matching identities. Considering that the AVA-ActiveSpeaker dataset <ref type="bibr" target="#b39">[40]</ref> does not include identity meta-data, we approximate the sampling of different identities by selecting visual data from concurrent tracklets 1 . To simplify the contrastive learning, we modify the sampling scheme for EASEE-W, and force i = 2 regardless of the real number of simultaneous tracklets. If there are more than 2 visible persons in the scene, we just sample without replacement.</p><p>In practice, we follow <ref type="bibr" target="#b6">[7]</ref> and apply this loss on the second to last layer of the iGNN block. Let L a be the loss for the audio nodes in the last iGNN block (see <ref type="bibr">Figures 4 and 5)</ref>, then the loss used for EASEE-W is:</p><formula xml:id="formula_4">L w = L a +L s +L c .</formula><p>No video labels are required, i.e. the speaker-to-speech assignments are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>We implement the audio encoder f a with the Resnet18 convolutional encoder <ref type="bibr" target="#b20">[21]</ref> pre-trained on ImageNet <ref type="bibr" target="#b12">[13]</ref>. We adapt the raw 1D audio signal to fit the input of a 2D encoder by generating Mel-frequency cepstral coefficients (MFCCs) of the original audio clip, and then averaging the filters of the network's first convolutional layer to adapt for a single channel input <ref type="bibr" target="#b39">[40]</ref>. We create the MFCCs with a sampling rate of 16 kHz and an analysis window of 0.025 ms. Our filter bank consists of 26 filters and a fast Fourier transform of size 256 is applied, resulting in 13 cepstrums.</p><p>The visual encoder f v is based on the R3D architecture, pre-trained on Kinetics-400 dataset <ref type="bibr" target="#b26">[27]</ref>. For fair comparison with other methods, we also implement f v as a 2D encoder by stacking the temporal and channel dimensions into a single one, then we replicate the filters on the encoder's first layer to accommodate for the input of dimension (B, CT, H, W ) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40]</ref>. We also rely on ImageNet pre-training <ref type="bibr" target="#b12">[13]</ref> for this encoder.</p><p>We assemble ? on-the-fly with multiple forward passes of f a , f v , and then map ? into nodes of the Graph Convolutional Network and continue with the GCN in a single forward pass. We design the GCN module using the pytorchgeometric library <ref type="bibr" target="#b16">[17]</ref> and use the EdgeConvolution opera- <ref type="bibr" target="#b0">1</ref> Since tracklets include a single face and were manually curated, it is guaranteed that two tracklets that overlap in time belong to different identities. If there is a single person in the scene, we sample additional visual data from another tracklet in a different movie where no speech event is detected.</p><p>tor <ref type="bibr" target="#b50">[51]</ref> with filters of size 128. Each GCN layer contains a single iGNN block. EdgeConvolution allows to build a subnetwork that performs the message passing between nodes, where every layer (spatial or temporal) in the iGNN is built by a sub-network of two linear layers with ReLu <ref type="bibr" target="#b30">[31]</ref> and batch normalization <ref type="bibr" target="#b22">[23]</ref>. Therefore, a single iGNN block contains 4 linear layers in total.</p><p>Training EASEE. We Train EASEE for a total of 12 epochs 2 using the ADAM optimizer <ref type="bibr" target="#b27">[28]</ref>, and supervise every node in the final layer with the Cross-Entropy Loss. We also apply intermediate supervision at the end of f a and f v encoders <ref type="bibr" target="#b39">[40]</ref>. We empirically observe that this favors faster learning and provides a small performance boost. The learning rate is set to 3?10 ?4 and is decreased with annealing ? = 0.1 at epochs 6 and 8. This very same procedure is applied regardless of the backbone. For every experiment we use a crop size of 160 ? 160. On average training EASEE-50 requires 28 hours on a single NVIDIA-V100 GPU.</p><p>Training EASEE-W. The training procedure for EASEE-W is similar to that of EASEE. However, we drop all the visual supervision (intermediate supervision of f v included) and reduce the learning rate to 7 ? 10 ?6 . We apply the assignment loss after the final layer. We calculate this loss individually for every temporal endpoint and accumulate for the full graph. We train for 20 epochs and take learning rate steps at epochs 12 and 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we provide extensive experimental evaluation of our proposed method. We mainly evaluate EASEE on the AVA-ActiveSpeaker dataset <ref type="bibr" target="#b39">[40]</ref> and also present additional results on Talkies <ref type="bibr" target="#b32">[33]</ref>. We begin with a direct comparison to state-of-the-art methods. Then, we perform an ablation analysis, assessing our main design decisions and their individual contributions to EASEE's final performance. We conclude by presenting the empirical evaluation of EASSE-W in the weakly supervised setup.</p><p>The AVA-ActiveSpeaker dataset <ref type="bibr" target="#b39">[40]</ref> is the first largescale test-bed for ASD. AVA-ActiveSpeaker contains 262 Hollywood movies: 120 in the training set, 33 in validation, and the remaining 109 in testing. The dataset provides bounding boxes for a total of 5.3 million faces. These face detections are manually linked to produce face tracks that contain a single identity. All AVA-ActiveSpeaker results reported in this paper were obtained using the official  <ref type="table">Table 1</ref>. State-of-the-art Comparison on AVA-ActiveSpeaker. Our best network (EASEE-50) outperforms any other method by at least 0.6 mAP even approaches that build upon much deeper networks. Our smaller network (EASEE-18) remains competitive with the previous state-of-the-art. In the 2D scenario EASEE-2D only lags behind UniCon <ref type="bibr" target="#b55">[56]</ref>, improving the closest method by at least 0.9 mAP. Finally, our weakly supervised configuration (EASEE-W) has a comparable performance with the fully supervised baselines of <ref type="bibr" target="#b39">[40]</ref>. evaluation tool provided by the dataset creators, which uses average precision (mAP) as the main metric for evaluation.</p><p>Talkies is a manually labeled dataset for the ASD task <ref type="bibr" target="#b32">[33]</ref>. This dataset was collected from social media videos and contains 23, 507 face tracks extracted from a total of 799,446 individual face detections. Unlike AVA-ActiveSpeaker, it is based on short clips and about 20% of the speech events are off-screen speech, i.e. the event cannot be attributed to a visible person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison to state-of-the-art</head><p>We compare EASEE against state-of-the-art ASD methods. The results for EASEE are obtained with l = 7 temporal endpoints, i = 2 tracklets per endpoint, and a stride of k = 5. This configuration allows for a sampling window of about 2.41 seconds regardless of the selected backbone. For fair comparison with other methods, we report results of three EASEE variants: 'EASEE-50' that uses a 3D backbone based on the ResNet50 architecture, 'EASEE-18' that uses a 3D model based on the much smaller Resnet18 architecture, and 'EASEE-2D' that uses a 2D Resnet18 backbone. Results are summarized in <ref type="table">Table 1</ref>.</p><p>We find that the optimal number of IGNN blocks changes according to the baseline architecture. For the ResNet18 encoder, 6 blocks (24 layers total in the GCN) are required to achieve the best performance, whereas for ResNet50, only 4 blocks (16 layers total in the GCN) are required. Since we find the best results with i = 2, and there are scenes with 3 or more simultaneous tracklets, we follow <ref type="bibr" target="#b32">[33]</ref>. At inference time, we split the speakers in non-overlapping groups of 2, and perform multiple forward passes until every tracklet has been labeled.</p><p>We observe that our method outperforms all the other approaches in the validation subset. EASEE-50 is 0.6 mAP higher than the previous state-of-the-art (ASDNet <ref type="bibr" target="#b29">[30]</ref>). We highlight that ASDNet relies on the deep ResNext101 encoder, whereas EASEE-50 is built on the much smaller ResNet50. Our smaller version (EASEE-18) only lags behind ASDNet by 0.2, and outperforms every other model by at least 1.0 mAP. We also implement a version of EASEE-50 that models only spatial relations (i.e. l = 1). This model reaches 89.6 mAP, outperforming every other network that generates predictions without long-term temporal modeling by at least 4.5 mAP. Finally EASEE-2D outperforms every other 2D approach except UniCon, we explain this result as <ref type="bibr" target="#b55">[56]</ref> presents a far more complex approach that includes multiple 2D backbones to analyze audiovisual data, scene layout and speaker suppression, along with bi-directional GRUs <ref type="bibr" target="#b7">[8]</ref> for temporal aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We ablate our best model (EASEE-50) to assess the individual contributions of our design choices, namely end-toend training, the iGNN block, and the residual connections between the iGNN blocks. tectural design is the end-to-end training, which contributes 1.6 mAP. The proposed iGNN brings about 0.4 mAP when compared against a baseline network where spatial and temporal message passing is performed in the same layer. Finally, residual connections between iGNN blocks contribute with an improved performance of 0.3 mAP.</p><p>Intermediate Embedding Configuration. We compare the performance of EASEE-50 with different configurations of the intermediate embedding ?. In <ref type="table">Table 3</ref>, we assess the performance of EASEE-50 when changing the number of temporal endpoints l and the number of tracklets i. We observe that the best performance arises when i = 2, which is in stark contrast to other methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b55">56]</ref> that often rely on aggregating information from 4 or more visual tracklets. We attribute this to the end-to-end nature of EASEE, where contextual cues are directly optimized for the ASD problem, thus requiring less spatial data for effective predictions. Nonetheless, for small values of l, we find that EASEE actually benefits from a larger number of visual tracklets (i = 3). This suggests that in the absence of strong temporal cues, EASEE will focus on extracting meaningful information from the spatially adjacent tracklets.</p><p>We also observe that the temporal dimension of the problem (number of endpoints l) is more relevant than the spatial component (number of concurrent tracklets i). When increasing l from 1 to 7, performance improves significantly, by 4.8 mAP on average. In contrast, increasing visual tracklets from i = 1 to i = 4 only yields 1.1 mAP improvement on average. This is consistent with related works, which show a performance boost when incorporating recurrent Units and long temporal samplings <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In <ref type="table">Table 4</ref>, we analyze the effect of the input clip size to the encoder f v in EASEE. We find that as the clip size increases, performance also improves but saturates around 15 frames (about 0.62 seconds). For every clip size, longer temporal sampling (more endpoints) provides better results. The best result is achieved at l = 7 with clips of 15 frames.  <ref type="table">Table 3</ref>. End-Points vs speaker. In EASEE longer temporal windows allow to improve the performance, achieving the best result at l = 7. A large number of speakers favors performance in shorter windows but i = 2 is the best parameter for long windows (l ? 3).  <ref type="table">Table 4</ref>. End-Points vs Input Clip. Long temporal sampling enables better predictions in most scenarios. In the EASEE architecture the input size for the 3D encoder also provides improved performance, the optimal is 15 frames, which equals 0.62 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End Points (l)</head><p>Design of iGNN blocks. We begin the empirical assessment of the iGNN module by analyzing two architectural decisions in EASEE: i) The effect of the number of iGNN modules, and ii) the size (number of neurons) in the linear layers in the iGNN blocks. We first analyze the effect of the number of iGNN blocks. We control this hyper-parameter for the Resnet50 Backbone and the Resnet18 Backbone, and evaluate from 2 to 7 iGNN modules. <ref type="table">Table 5</ref> summarizes the results. Deeper GNN networks lead to higher performance, but this improvement stalls at 4 iGNN blocks for the Resnet50 backbone and 6 iGNN blocks for the Resnet18.</p><p>We conclude by analyzing the effect of the size of the linear layers used in iGNN. Our best models (EASEE-50 &amp; EASEE-18) use linear layers of size 128. In table 6 we ablate the size of this layer in the EASE50 architecture. We see a smaller impact on this hyper-parameter, where a smaller net only loses 0.3 mAP, and iGNN blocks with double the number of neurons only lose 0.2 mAP.</p><p>After testing the size configurations of the iGNN block, we assess the effectiveness of the proposed iGNN block by comparing it against the following fusion alternatives: (a) Temporal-Spatial (iGNN-TS), an immediate alternative to iGNN where temporal message passing is performed before any spatial message passing is done; (b) Two Stream, where two independent GCN streams perform spatial and temporal message passing respectively, and these streams are fused at the end of the network; (c) Parallel, where the block performs spatial and temporal message passing in parallel and fuses the features using a fully connected layer; (d) Spatio-  <ref type="table">Table 7</ref>. iGNN Layering Strategies. We compare multiple strategies to assemble our iGNN block, we find that interleaving the temporal and spatial messages brings the best results. In comparison a joint massage passing will reduce the performance by 0.4 mAP, a naive join of these steps with a linear layer reports the same performance reduction.</p><p>Temporal, where a single graph structure performs temporal and spatial message passing at the same time <ref type="bibr" target="#b32">[33]</ref>. <ref type="table">Table 7</ref> summarizes the results.</p><p>Overall, we find that the best block design is the one in which spatial message passing occurs first. Reversing the order of message passing results in a very similar alternative with only minor performance degradation. In comparison, the two-stream approach performs significantly worse than all other alternatives, suggesting that the fusion of temporal and spatial information must occur earlier to be effective in an end-to-end scenario. Joint spatio-temporal messaging also has high performance, but still lags behind iGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Talkies Dataset</head><p>We conclude this subsection with the evaluation of EASEE on the Talkies dataset <ref type="bibr" target="#b32">[33]</ref>. Here, we test: (i) a direct transfer of EASEE-50 into the validation set of Talkies, (ii) directly training EASEE on Talkies, and (iii) using Talkies as downstream task after pre-training on AVA-ActiveSpeaker. <ref type="table" target="#tab_6">Table 8</ref> summarizes the results. EASEE outperforms <ref type="bibr" target="#b32">[33]</ref> for the direct transfer on the Talkies dataset. Moreover, training on Talkies results in a high performance comparable to that of the AVA-ActiveSpeaker dataset, this is particularly interesting as Talkies is a dataset that contains a large portion of scenes with out-of screen speech, a situation that is extremely rare in the AVA-Active Speaker. Finally, using talkies as a downstream task results in 1.0 mAP improvement, which is about 15% relative error improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Weak Supervision</head><p>We conclude this section by evaluating the weakly supervised version of EASEE, i.e. EASEE-W. To the best of our knowledge, there are no comparable methods that strictly rely on weak (audio only) supervision in the ASD task. Therefore, we establish multiple baselines, from random predictions to direct speech-to-speaker assignment.</p><p>We first consider baselines that ignore audio labels and the structure of the ASD problem: i) random baseline where every speaker gets a random score sampled from a uniform distribution between [0, 1]. ii) Naive Recall where we trivially predict every tracklet as an active speaker and iii) Naive Precision that trivially predicts every tracklet as silent. We also build baselines that rely on audio supervision. We use our trained audio encoder f a to detect speech intervals and generate random speech-to-speaker assignments within that time window. We explore two approaches: iv) Naive Audio assignment where we choose a random visible speaker whenever a speech event is detected. v) Largest Face Audio assignment since AVA-Active Speaker is a collection of Hollywood movies, we follow a common bias in commercial movies, and assign the speech event to the tracklet that occupies the largest area in the screen. <ref type="table">Table 9</ref> summarizes the results of these experiments in the AVA-ActiveSpeaker dataset.</p><p>Overall, we see that random baselines largely underperform. Even when the predictions have a bias towards the largest class (silent) results are just 27.1 mAP. A relevant increment in performance (about 20 mAP) appears when the audio supervision is used to generate the naive visual assignments. This improvement is a direct result of the structure in the ASD problem, where speech events are attributed to a defined set of sources.  <ref type="table">Table 9</ref>. Weak Supervision. We show that EASEE-W largely improves over baseline approaches for ASD, it outperforms a naive baseline by 28.5 mAP, and remains competitive with fully supervised 2D encoder.</p><p>When we apply EASEE-W, we see the complementary behavior of the proposed loss functions. The baseline with audio supervision (L a only) exhibits no meaningful improvement over the random base, despite the GCN structure. A similar situation can be observed if we use the audio supervision and enforce temporal consistency on the visual features (L c ). This indicates that information flow across modalities can not be trivially enforced by the GCN module or temporal visual consistency. Including the assignment loss (L s ) results in a scenario that already improves over the naive assignments suggesting that local attributions already favors some meaningful audiovisual patterns. Finally, the best result is achieved when assignments and temporal consistency for the visual data are considered. This result improves over any baseline by at least 27 mAP. We conclude this section highlighting that this result is competitive with baseline approaches that rely on encoding short-temporal information from a single speaker as outlined in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EASEE-W Ablation Analysis</head><p>We conclude this section by ablating EASEE-W according to the face size and number of simultaneous tracklets in the AVA-ActiveSpeaker validation set. We compare EASEE-W performance against the baseline approach of <ref type="bibr" target="#b39">[40]</ref>. Despite the large difference in the training and supervision signals, EASEE-W follows a similar error pattern to the fully supervised method of <ref type="bibr" target="#b39">[40]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional Experimental Results</head><p>We complement the analysis of EASEE, and assess its performance in known challenging scenarios. We follow the procedure of <ref type="bibr" target="#b39">[40]</ref>, and evaluate EASEE in the AVA-ActiveSpeakers dataset according to: i) number of visible faces, and ii) the size of the face. <ref type="table">Table 10</ref> shows the ablation of the performance of EASEE according to the face size. Overall, EASEE shows a similar behavior to state-of-the-art methods, where smaller faces (less than 64 ? 64) are harder to classify (79.3 mAP). Medium images (between 64 ? 64 and 128 ? 128) show an improvement in performance over small images, and large faces report the highest mAP at 97.7 mAP.  <ref type="table">Table 10</ref>. AVA-ActiveSpeaker Face Size. We evaluate EASEE in the AVA-ActiveSpeaker dataset according to the size of the faces. As observed in previous works smaller faces are harder to classify. EASEE outperforms the state-of-the art in every scenario <ref type="table">Table 11</ref> evaluates the performance of EASEE according to the number of simultaneous faces. Just like other ensemble methods, EASEE shows an improved performance in the multi-speaker scenario when compared to the single speaker baseline <ref type="bibr" target="#b39">[40]</ref> (20.8 mAP improvement for two speakers, 29.5 mAP improvement for 3 speakers).  <ref type="table">Table 11</ref>. Performance evaluation by number of faces. We evaluate EASEE in the AVA-ActiveSpeaker according to the number of visible faces (tracklets) in the scene. Multi-speaker scenes are far more challenging, our method outperforms the current state-of-the-art in any scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Faces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduced EASEE, a multi-modal end-to-end trainable network for the ASD task. EASEE outperforms state-of-the-art approaches in the large scale AVA-ActiveSpeaker <ref type="bibr" target="#b39">[40]</ref> dataset, and transfers effectively to smaller sets that contain out-of-screen speech. EASEE allows for fully supervised and weakly supervised training by leveraging the inherent structure of the ASD problem and the natural consistency in video data. Future explorations on the ASD problem might rely on our label efficient training setup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>EASEE Weakly Supervised. We drop all the visual supervision (Lv) in EASEE (intermediate supervision included) and enforce positive predictions amongst the video nodes (light green) in the presence of a speech event (Ls), along with consistent visual feature representations if the nodes contain the same identity Nodes which generate predictions for the visual tracklets (i.e. active speaker or silent). EASEE's final predictions are made only from the output of visual nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Figure 5 .</head><label>35</label><figDesc>Weakly Supervised Losses. We enforce an individual speaker assignment if there is a detected speech event (left). Temporal consistency pulls together features for faces of the same person and creates differences for faces of different persons (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Interleaving GNN Spatial Message Passing Prediction + ... 3D CNN Temporal Message Passing ... ... Weight Sharing 2D CNN Residual Block ... Spatial Message Passing Temporal Message Passing Spatial Message Passing Temporal Message Passing I-GNN End-to-End ... LCE LCE LCE I-GNN</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>contains the individual assessment of each component. The most important archi-</figDesc><table><row><cell></cell><cell></cell><cell>Residual</cell></row><row><cell>Network</cell><cell>End-to-End iGNN</cell><cell>Connections mAP</cell></row><row><cell>EASEE-50</cell><cell></cell><cell>91.9</cell></row><row><cell>EASEE-50</cell><cell></cell><cell>93.5</cell></row><row><cell>EASEE-50</cell><cell></cell><cell>93.7</cell></row><row><cell>EASEE-50</cell><cell></cell><cell>93.8</cell></row><row><cell>EASEE-50</cell><cell></cell><cell>94.1</cell></row><row><cell cols="3">Table 2. AVA-ActiveSpeaker Ablation. We assess the empirical</cell></row><row><cell cols="3">contribution of the most relevant components in EASEE. Resid-</cell></row><row><cell cols="3">ual connections contribute about 0.3 mAP and the proposed iGNN</cell></row><row><cell cols="3">block 0.4 mAP. Overall the most relevant design choice is the end-</cell></row><row><cell cols="3">to-end trainable nature of EASEE contributing 1.6 mAP.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>EASEE Performance By iGNN Depth. We analyze the effect of the number of iGNN blocks in EASEE. Stacking blocks improves the performance util 4 blocks are stacked (Resnet50) or 6 blocks are stacked (Resnet18) Linear layer size. We assess the effect of the layer size in the iGNN module. We find slightly reduced performance by altering the size of the iGNN module.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Backbone 2 iGNN 3 iGNN 4 iGNN 5 iGNN 6 iGNN 7 iGNN</cell></row><row><cell></cell><cell></cell><cell cols="2">EASEE-18</cell><cell>92.8</cell><cell>93.0</cell><cell>93.2</cell><cell>93.2</cell><cell>93.3</cell><cell>93.2</cell></row><row><cell></cell><cell></cell><cell cols="2">EASEE-50</cell><cell>93.6</cell><cell>93.8</cell><cell>94.1</cell><cell>94.0</cell><cell>93.8</cell><cell>93.8</cell></row><row><cell></cell><cell>Layer Size</cell><cell>64</cell><cell cols="2">128 224</cell><cell>256</cell><cell></cell></row><row><cell></cell><cell cols="5">EASEE-50 93.8 94.1 93.9 93.9</cell><cell></cell></row><row><cell cols="6">iGNN iGNN-TS Two Stream Parallel Spatio-Temp. [33]</cell><cell></cell></row><row><cell>94.1</cell><cell>94.0</cell><cell>92.8</cell><cell>93.7</cell><cell></cell><cell>93.7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Evaluation on Talkies Dataset. We evaluate EASEE on the Talkies dataset. It outperforms the existing baseline on the direct transfer from AVA-ActiveSpeaker, and show the results of training EASEE end-to-end in Talkies. Finally we test the effectiveness of AVA-ActiveSpeakers as pre-training for Talkies.</figDesc><table><row><cell>Talkies</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Similar to<ref type="bibr" target="#b0">[1]</ref>, we find that sampling every element in the tracklet leads to overfit. For every training epoch, we randomly sample only 4 training examples inside every tracklet.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active speakers in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Juan Le?n Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jolo-gcn: mining joint-centered light-weight information for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinmiao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2735" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Favoa: Face-voice association favours ambiguous speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="439" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who&apos;s speaking? audio-supervised classification of active speakers in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayeh</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Van Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active speaker detection with audiovisual co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Zegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Van Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="312" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Voice activity detection based on multiple statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Hyuk</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjit K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1965" to="1976" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 6</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Naver at activitynet challenge 2019-task b active speaker detection (ava)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Joon Son</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10555</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<title level="m">Voxceleb2: Deep speaker recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: Speaker detection using video and audio correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo-Yiin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04284</idno>
		<title level="m">Personal vad: Speaker-conditioned voice activity detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Duhme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12946</idno>
		<title level="m">Fusion-gcn: Multimodal action recognition using graph convolutional networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Taking the bite out of automated naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="545" to="559" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02739</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Mesh r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking knowledge graph propagation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11487" to="11496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">How to design a three-stage architecture for audio-visual active speaker detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>K?p?kl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Taseska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03932</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a backpropagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Maas: Multi-modal assignation for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Le?n-Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03682</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">C-gcn: Correlation based graph convolutional network for audiovideo emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ava active speaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kevin Wilson, James Glass, and Trevor Darrell. Visual speech recognition with loosely synchronized feature streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Siracusa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Voice activity detection in nonstationary noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>S G?khun Tanyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="478" to="482" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bimodal recurrent neural network for audiovisual voice activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1938" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Rohan Kumar Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3927" to="3935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR, 2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Point clouds learning with attention-based graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13445</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unicon: Unified context network for robust active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3964" to="3972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multi-task learning for audio-visual active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
