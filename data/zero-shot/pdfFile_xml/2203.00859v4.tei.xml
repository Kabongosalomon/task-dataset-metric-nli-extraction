<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
							<email>tuzhigang@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Yang</surname></persName>
							<email>jyyang@suda.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Chen</surname></persName>
							<email>yujin.chen@tum.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent transformer-based solutions have been introduced to estimate 3D human pose from 2D keypoint sequence by considering body joints among all frames globally to learn spatio-temporal correlation. We observe that the motions of different joints differ significantly. However, the previous methods cannot efficiently model the solid inter-frame correspondence of each joint, leading to insufficient learning of spatial-temporal correlation. We propose MixSTE (Mixed Spatio-Temporal Encoder), which has a temporal transformer block to separately model the temporal motion of each joint and a spatial transformer block to learn inter-joint spatial correlation. These two blocks are utilized alternately to obtain better spatio-temporal feature encoding. In addition, the network output is extended from the central frame to entire frames of the input video, thereby improving the coherence between the input and output sequences. Extensive experiments are conducted on three benchmarks (i.e. Human3.6M, MPI-INF-3DHP, and HumanEva). The results show that our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and 7.6% MPJPE. The code is available at https://github. com/JinluZhang1126/MixSTE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation from monocular observations is a fundamental vision task that reconstructs 3D body joint locations from the input images or video. Since this task can obtain meaningful expressions of body geometry and motion, it has a wide range of applications, such as action recognition <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b55">55]</ref>, virtual human <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b52">52]</ref>, and humanrobot interaction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b50">50]</ref>. Most recent works are based on the 2D-to-3D lifting pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b57">57]</ref>, which detects 2D keypoints firstly and then lift them to 3D. Due to the depth ambiguity of monocular data, multiple potential 3D poses may be mapped from the same 2D pose, so FPS(frame/s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE(mm)</head><p>PoseFormer (T=81) <ref type="bibr" target="#b57">[57]</ref> Anatomy-aware (T=81) <ref type="bibr" target="#b3">[4]</ref> Anatomy-aware (T=243) <ref type="bibr" target="#b3">[4]</ref> STE (T=243) <ref type="bibr" target="#b22">[23]</ref> AM (T=243) <ref type="bibr" target="#b27">[28]</ref> VideoPose3D (T=243) <ref type="bibr" target="#b37">[37]</ref> Spatial Correlation of each frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning of S-T Correlation time</head><p>Each joint has a different motion Separate Temporal Correlation of each joint <ref type="figure">Figure 1</ref>. Top: Overview of spatio-temporal correlation modeling. Each 2D keypoint is separated in the temporal domain to learn different motion trajectories of body joints, and the spatial and temporal correlation are alternately stacked to improve the sequence coherence modeling ability. Bottom: Accuracy (MPJPE) and efficiency (FPS) comparison with different methods on Human3.6M dataset, the blue and orange colors indicate that the input sequence length T is equal to 81 and 243, respectively.</p><p>it is difficult to recover an accurate 3D pose merely based on the information of a single frame 2D keypoints. Notable progress has been made by exploiting temporal information contained in the input video to address the above issues in a single frame <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b46">46]</ref>. Recently, driven by the success of transformer <ref type="bibr" target="#b45">[45]</ref> for its ability to model sequence data, Zheng et al. <ref type="bibr" target="#b57">[57]</ref> introduces a transformer-based 3D human pose estimation network. It takes advantage of spatio-temporal information for estimating the more accurate central-frame pose in video. By modeling spatial correlations between all joints and temporal correlations among consecutive frames, PoseFormer <ref type="bibr" target="#b57">[57]</ref> achieves performance improvement. However, it ignores the motion differences among body joints, which causes the insufficient learning of spatio-temporal correlation. Moreover, it increases the dimension of the temporal transformer module, which limits the usage of longer input sequence.</p><p>Poseformer <ref type="bibr" target="#b57">[57]</ref> takes a video as input and only estimates the human pose of the central frame, which we summarize this pipeline as the seq2frame approach. Many recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b57">57]</ref> follow it and they utilize adjacent frames to improve the accuracy of estimating the pose of a certain moment, but the sequence coherence is ignored due to the single frame output. Additionally, during the inference, these seq2frame solutions need to input a 2D keypoint sequence repeatedly with large overlap to obtain 3D poses of all frames, which brings redundant calculation. In contrast to the seq2frame approach, there is also the seq2seq approach, which regresses the 3D pose sequence from the input 2D keypoints. These methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">46]</ref> mainly depend on long short-term memory (LSTM) <ref type="bibr" target="#b14">[15]</ref> cell or graph convolution network (GCN) <ref type="bibr" target="#b20">[21]</ref>, and perform well in learning temporal information among continuous estimation results. However, current seq2seq networks lack the global modeling ability between input and output sequences, which tend to be excessively smooth <ref type="bibr" target="#b37">[37]</ref> in the output poses of a long sequence. The low efficiency of LSTM <ref type="bibr" target="#b14">[15]</ref> is also a severe issue for estimating human pose from video.</p><p>While previous work has focused on associating all joints in the spatial and temporal domains, we observe that the motion trajectories of the different body joints vary from frame to frame and should be learned separately. Additionally, the input 2D keypoint sequence and the output 3D pose sequence have solid global coherence, and they should be tightly coupled to promote accurate and smooth 3D poses.</p><p>Motivated by the above observations, in this work, we propose MixSTE to learn the separate temporal motion of each body joint and imbue sequential coherent human pose sequence in a seq2seq approach. In contrast to the prior method <ref type="bibr" target="#b57">[57]</ref> which reconstructs the central frame and ignores the single joint motion, the MixSTE lifts 2D keypoint sequence to 3D pose sequence via a novel seq2seq architecture and a set of motion-aware constraints. Specifically, as shown at the top of <ref type="figure">Figure 1</ref>, we propose the joint separation to consider temporal motion information of each joint. It takes each 2D joint as an individual feature (which is referred to as a token in transformer) to sufficiently learn spatio-temporal correlation and helps to reduce the dimension of the joint features in temporal domain. Moreover, we propose an alternating design with seq2seq to flexibly obtain better sequence coherence within a long sequence, which decreases redundant calculation and excessive smoothness. In this way, temporal motion trajectories of different body joints could be adequately con-sidered to predict accurate 3D pose sequence. To the best of our knowledge, the proposed method is the first to utilize the transformer encoder in the seq2seq pipeline, which enhances learning spatio-temperal correlation for accurate pose estimation and significantly improves the inference speed from seq2frame methods (see the bottom of <ref type="figure">Fig.1</ref>) Besides, our approach can easily adapt to any length of the input sequence.</p><p>Our contributions to 3D human pose estimation can be summarized in three folds:</p><p>? The MixSTE is proposed to effectively capture the temporal motion of different body joints over the long sequence, which helps to model sufficient spatiotemporal correlation.</p><p>? We propose a novel alternating design with transformer-based seq2seq model to learn the global coherence between sequences to improve the accuracy of reconstruction poses.</p><p>? Our approach achieves state-of-the-art performance on three benchmarks and has outstanding generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Human Pose Estimation. Estimating 3D human pose from monocular data was started by relying on the kinematics feature or the skeleton structure prior <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref>. With the development of deep learning, more datadriven methods have been proposed, and these methods can be divided into end-to-end manner and 2D-to-3D lifting manner. The end-to-end manner directly estimates the 3D coordinates from the input without the intermediate 2D pose representation. Some methods <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b44">44]</ref> followed this manner but required a high computation cost due to regressing directly from the image space. Different from the end-to-end manner, 2D-to-3D lifting pipeline first estimates 2D keypoints in the RGB data and then leverages the correspondences between 2D and 3D human structures to lift the 2D keypoints to 3D pose. Benefiting from the reliable effort of 2D keypoint detection works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b41">41]</ref>, recent 2Dto-3D lifting methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref> outperformed end-to-end approaches. Therefore, we follow the 2D-to-3D lifting manner to obtain robust 2D intermediate supervision.</p><p>Seq2frame and Seq2seq under 2D-to-3D Lifting. Recently, temporal information from video has been exploited to produce more robust predictions by many methods. With the video input, many influential works (seq2frame) pay attention to predicting the central frame of the input video to produce a more robust prediction and less sensitivity to noise. Pavllo et al. <ref type="bibr" target="#b37">[37]</ref> proposed the dilated temporal convolutions based on the temporal convolution network (TCN) to extract temporal features. Some following works improved the performance of TCN by utilizing the attention mechanism <ref type="bibr" target="#b27">[28]</ref>, or decomposing the pose estimation task into bone length and bone direction prediction <ref type="bibr" target="#b3">[4]</ref>, but they have to fix the receptive field of the input sequence. In contrast to them, our approach is no need to preset the length of each input with respect to the convolution kernel or the sliding window size. Besides, GCN <ref type="bibr" target="#b20">[21]</ref> was also applied to the task by <ref type="bibr" target="#b0">[1]</ref> to learn multi-scale features of human and hand poses. These works achieved good performance; however, calculation redundancy is a common flaw of these methods.</p><p>On the other hand, some works (seq2seq) improve the coherence and efficiency of 3D pose estimation and reconstruct all frames of input sequence at once. LSTM <ref type="bibr" target="#b14">[15]</ref> was introduced to estimate 3D poses in video from a set of 2D keypoints <ref type="bibr" target="#b25">[26]</ref>. Hossain et al. <ref type="bibr" target="#b15">[16]</ref> presented a temporal derivative loss function to ensure the temporal consistency over a sequence, but it faces the low computing efficiency issue. Wang et al. <ref type="bibr" target="#b46">[46]</ref> exploited a GCN-based approach and designed a corresponding loss to model motion in both short temporal intervals and long temporal ranges, but it lacks global modeling ability of input sequence. In contrast to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">46]</ref>, our method has the advantage of global modeling ability of each joint in the spatial and temporal domains. Besides, it enables parallel processes for frames and joints to address the low-efficiency issue of LSTM <ref type="bibr" target="#b14">[15]</ref>.</p><p>Self-attention and Transformer The transformer architecture with self-attention was firstly proposed by <ref type="bibr" target="#b45">[45]</ref>, and then was applied to various visual tasks, e.g. classification with visual transformer (ViT) <ref type="bibr" target="#b9">[10]</ref>, and detection with DETR <ref type="bibr" target="#b1">[2]</ref>. For the human pose estimation task, <ref type="bibr" target="#b49">[49]</ref> proposed the Transpose to estimate 2D pose from images. <ref type="bibr" target="#b24">[25]</ref> presented a transformer framework for both human mesh recovery and pose estimation from a single image but ignored the temporal information in the video. Some researchers also explored the multi-view 3D human pose estimation scheme <ref type="bibr" target="#b13">[14]</ref>. The stride transformer encoder <ref type="bibr" target="#b22">[23]</ref> was introduced to incorporate local contexts. Furthermore, PoseFormer <ref type="bibr" target="#b57">[57]</ref> constructed a model based on ViT <ref type="bibr" target="#b9">[10]</ref> to capture the spatial and temporal dependency sequentially. Both <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b57">[57]</ref> have to fix the order of spatial and temporal encoders, and only the central frame of video is reconstructed. Our approach is similar to them in applying transformer architecture. But we consider motion trajectories of different body joints and apply the seq2seq to better model sequence coherence.</p><p>From the above analysis and comparison of related works, further exploration for transformer-based methods in 3D human pose estimation is necessary and feasible, but there is no method combining the transformer with seq2seq framework in the 3D human pose task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our network takes a concatenated 2D coordinates C N,T ? R N ?T ?2 with N joints and T frames as input, where the channel size of the input is 2. Firstly, we project the input keypoint sequence C N,T to high-dimensional feature P N,T ? R N ?T ?dm with feature dimension d m for each joint representation. Then we utilize the position embedding matrix for retaining the position information of the spatial and temporal domains. The proposed MixSTE takes the P N,T as input and aims to alternately learn the spatial correlation and separate temporal motion. Finally, we use a regression head to concatenate the outputs X ? R N ?T ?dm of encoder, and take the dimension d m to 3 to get the 3D human pose sequence Out ? R N ?T ?3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mixed Spatio-Temporal Encoder</head><p>We utilize the MixSTE to model spatial dependency and temporal motion for a given 2D input keypoint sequence, respectively. MixSTE consists of a Spatial Transformer Block (STB) and a Temporal Transformer Block (TTB). Here, the STB computes the self-attention between joints and aims to learn the body joint relations of each frame, while the TTB computes the self-attention between frames and focuses on learning the global temporal correlation of each joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Separate Temporal Correlation Learning</head><p>To imbue effective motion trajectories into the learned representations, we consider the temporal correspondence of each joint in order to explicitly model correlations on the same joint over the dynamic sequence. Different from the previous method <ref type="bibr" target="#b57">[57]</ref>, we do not treat all body joints as a token in the temporal transformer block. We separate different joints in time dimension, so that the trajectory of each joint is an individual token p ? R 1?T ?dm , and different joints of body are modeled paralleled. From the perspective of the time dimension, different motion trajectories of body joints are modeled separately to represent temporal correlations better. The joint separation is operated as follows:</p><formula xml:id="formula_0">X t l = Concat(F(p i,1 , p i,2 , ...p i,T )), i ? N,<label>(1)</label></formula><p>where p i,j ? P N,T denotes the i-th joint in the j-th frame, F indicates the temporal encoder function and the output of the l-th TTB encoder is X l ? R N ?T ?dm . Furthermore, treating each body joint as an individual token can decrease dimension of the model to d m from N ? d m of PoseFormer <ref type="bibr" target="#b57">[57]</ref>, and it also enables the longer sequence processed in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Spatial Correlation Learning</head><p>We employ the spatial transformer block (STB) to learn spatial correlations among joints in each frame. Given 2D keypoints with N joints, we consider each joint as a token in spatial attention. Firstly, we take 2D keypoints as input and project each keypoint to a high-dimensional feature with the linear embedding layer. The feature is referred to as a spatial token in STB. We then embed the spatial position information with a positional matrix E s?pos ? R N ?dm . After that, spatial tokens P i ? R N ?dm of the i-th frame is fed into spatial self-attention mechanism of STB to model dependencies across all joints and output the high-dimensional tokens X s l ? R N ?T ?dm in l-th STB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Alternating design with Seq2seq</head><p>Alternating design in spatio-temporal correlation. The STB and TTB are designed in an alternating way to encode different high-dimensional tokens. The process of alternating design is like recurrent neural network (RNN), but we can parallel over joint and time dimensions. We stack STB and TTB for d l loops, and the dimension of the feature is preserved as a fixed size d m to promise that spatial-temporal correlation learning focuses on the same joint. Specifically, the spatial and temporal position embedding is applied only in the first encoder to retain two kinds of position information. Moreover, there is the independence of the spatial and temporal domains, where previous methods often only learn partial sequence coherence due to the single process of spatio-temporal modeling. The proposed alternating design with stacking architecture can obtain better coherence and spatio-temporal feature encoding. Seq2seq framework. Furthermore, to better utilize the global sequence coherence between the input sequence of 2D keypoints and the output sequence of 3D poses, we leverage the seq2seq pipeline in our model. It can predict all 3D poses of input 2D keypoints at once, which helps to preserve sequence coherence between the input and output sequences. Besides, for a sequence containing T frames, we need fewer times of inference, which means higher efficiency. Assuming that the sequence length of each input t &lt; T , the inference time gap G between our model and the seq2frame methods will become higher with the increase of t:</p><formula xml:id="formula_1">G = T (1 + 2?) ( T +2? t ) = T (1 + 2?) T + 2 ? ? ? t ? (1 + 2?) ? t,<label>(2)</label></formula><p>where ? indicates the padding length of the input sequence.</p><p>In summary, due to these advanced components, our model can capture various temporal motions and global sequence coherence with less calculation redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer Block in MixSTE</head><p>The transformer blocks in MixSTE follow the scaled dotproduct attention <ref type="bibr" target="#b45">[45]</ref>. The attention computing of query, key, and value matrix Q, K, V in each head are formulated by:</p><formula xml:id="formula_2">Attention(Q, K, V ) = Sof tmax( QK T ? d m )V,<label>(3)</label></formula><p>where {Q, K, V } ? R N ?dm , N indicates the number of tokens, and d m is the dimension of each token. The concatenated attention of h heads is defined as follows:</p><formula xml:id="formula_3">M SA = Concat(head1, ..., head h )W O ,<label>(4)</label></formula><formula xml:id="formula_4">head i = Attention(Q i , K i , V i ), i ? h,<label>(5)</label></formula><p>where the linear projection weight is W O ? R dm?dm . In the transformer encoder of our approach, each joint token p ? P N is projected from joint c i of the 2D coordinates C N ? R N ?2 . Joint token p is embedded with the position information by a matrix E pos ? R N ?dm :</p><formula xml:id="formula_5">X = N orm(L e (c i ) + E pos ), X ? R N ?dm ,<label>(6)</label></formula><p>where N orm denotes the layer normalization, and L e indicates the linear embedding layer. The spatial-temporal dependencies among joints are then computed by the STB and TTB as follows:</p><formula xml:id="formula_6">R s = M SA(U Q , U K , U V ) + X,<label>(7)</label></formula><formula xml:id="formula_7">U i = XW m , m ? {Q, K, V },<label>(8)</label></formula><p>where R s denotes the attention output of the joint token X, U i is the matrix mapped from X by linear transformation, and W m is the corresponding linear transformation weight matrix of query, key and value in joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>The network is trained in an end-to-end manner, the final loss function L is defined as:</p><formula xml:id="formula_8">L = L w + ? t L t + ? m L m ,<label>(9)</label></formula><p>where L w is the WMPJPE loss, L t is the TCLoss, and L m denotes the MPJVE loss. During the training stage, different coefficients ? t and ? m are employed to L t and L m to avoid excessive smoothness in sequence.</p><p>In detail, we firstly explored a weighted mean per-joint position error (WMPJPE), which pays different attention to different joints of the human body when computing the MPJPE. The WMPJPE L w with weight W is computed as follows:</p><formula xml:id="formula_9">L w = 1 N s N s i=1 (W ? 1 T T j=1 ? p i,j ? gt i,j ? 2 2 )),<label>(10)</label></formula><p>where N s indicates N joints of human skeleton s in three datasets, T denotes the number of frames in sequence, p i,j and gt i,j are the prediction and the ground truth 3D pose of i-th joint in j-th frame. Moreover, the temporal consistency loss (TCLoss) in <ref type="bibr" target="#b15">[16]</ref> is introduced to produce the smooth poses. The MPJVE <ref type="bibr" target="#b37">[37]</ref> is also a loss in our model to improve the temporal coherence between the predicted pose sequence and the ground truth sequence. We merge the TCLoss and MPJVE as the temporal loss function (T-Loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Protocols</head><p>We evaluate our model on three 3D human pose estimation datasets: Human3.6M <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b31">[32]</ref> and HumanEva <ref type="bibr" target="#b40">[40]</ref> individually.</p><p>Human3.6M is the most commonly used indoor dataset for the 3D human pose estimation tasks. Following the same policy of previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b57">57]</ref>, the 3D human pose in Human3.6M is adopted as a 17-joint skeleton, and the subjects S1, S5, S6, S7, S8 from the dataset are applied during training, the subjects S9 and S11 are used for testing. The two commonly used evaluation metrics (MPJPE and P-MPJPE) are involved in this dataset. In addition, mean per-joint velocity error (MPJVE) <ref type="bibr" target="#b37">[37]</ref> is applied to measure the smoothness of the prediction sequence. We also compute the variance (VAR.) of MPJPE between action categories to evaluate the stability.</p><p>MPI-INF-3DHP is also a recently popular large-scale 3D human pose dataset. Our setting follows previous works <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b57">57]</ref>. The area under the curve (AUC), percentage of correct keypoints (PCK), and MPJPE are reported as evaluation metrics.</p><p>HumanEva is a smaller dataset than above datasets. As the same setting of <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b57">57]</ref>, actions (Walk, Jog) in subjects S1, S2, S3 are evaluation data. The metrics MPJPE and P-MPJPE are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The proposed model is implemented with Pytorch. We use 2D keypoints from 2D pose detector <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">41]</ref> or 2D ground truth to analyze the performance of our framework. Although the proposed model can easily adapt to any length of input sequence, to be fair, we select some specific sequence lengths T for three datasets to compare our method with other methods which must have a certain 2D input length <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">37]</ref>: Human3.6M (T =81,243), MPI-INF-3DHP (T =1,27), HumanEva (T =43). Analysis about the frame length setting is discussed in the ablation study Section 4.4. The W in WMPJPE is set based on different joint groups (torso, head, middle limb, and terminal limb) with different values (1.0, 1.5, 2.5, and 4.0, respectively). The Adam optimizer <ref type="bibr" target="#b19">[20]</ref> is employed for the training model. The batch size, dropout rate, and activation function for datasets are set to 1024, 0.1, and GELU. We utilize the stride data sample strategy with interval is as same as the input length to make there no overlapping frames between sequences(more details in the supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art Methods</head><p>Results on Human3.6M. Two types of 2D joint detection data are applied in the experiment: CPN <ref type="bibr" target="#b7">[8]</ref>, which is the most typical 2D estimator used in previous approaches, and HRNet <ref type="bibr" target="#b41">[41]</ref> which is used to further investigate the upper bound of our method. The results compared with other methods, including the error of all 15 actions and the average error, are reported in <ref type="table">Table 1</ref>. For CPN <ref type="bibr" target="#b7">[8]</ref> detector, our model obtains the best result of average MPJPE of 40.9mm under Protocol 1 and 32.6mm P-MPJPE under Protocol 2, which outperforms PoseFormer [57] by 3.4mm MPJPE (7.6%). Furthermore, our method achieves the best under T = 243 setting and second-best under T = 81 setting in all actions.</p><p>Utilizing more powerful 2D detector HRNet <ref type="bibr" target="#b41">[41]</ref>, our model further improves roughly 4.5mm (10.2%) under Protocol 1. We also compare our method with <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b57">57]</ref> using 2D ground truth, and the results are illustrated in the <ref type="table">Table 2</ref>. Our method significantly outperforms all other methods and achieves approximately 31.0% improvement of average MPJPE compared with PoseFormer <ref type="bibr" target="#b57">[57]</ref>.</p><p>Furthermore, we compare the MPJPE distribution in the testset S9 and S11 with other methods <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b57">57]</ref> to evaluate the ability of estimating difficult poses. It can be observed in <ref type="figure" target="#fig_1">Figure 3</ref> that there are much fewer poses with high errors in our method. Moreover, the proportion of poses with over 40mm MPJPE, which causes loss of accuracy, is consis-  In <ref type="figure" target="#fig_3">Figure 4</ref>, we compare the MPJPE for individual joints on all frames of Human3.6M testset to evaluate the estima-tion accuracy of different joints. The joints of limbs have higher errors due to flexible movements, while the trunk joints have lower errors because of stable motion. Our accuracy of each joint category achieves the best, and the variance (V AR.) comparison shows that our method has a more stable performance.</p><p>Results on MPI-INF-3DHP. <ref type="table" target="#tab_2">Table 3</ref> reports the detailed comparison with other methods on the MPI-INF-3DHP testset. In addition, the 1-frame setting is employed to evaluate the single-frame performance. The input is ground truth 2D keypoints. As shown in the table, the method (T =27) performs the best in three evaluation metrics, and the single-frame setting (T =1) also achieves the second-best accuracy. These results demonstrate the strong performance of our model in single-frame and multi-frame scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reults on HumanEva.</head><p>We utilize HumanEva to evaluate the generalization abil-    ity of the proposed method and the impact of finetuning from large datasets. The MPJPE results on HumanEva finetuning from Human3.6M are reported in the <ref type="table" target="#tab_3">Table 4</ref>. Due to seq2seq setting and limitation of transformer in small dataset, our method without fine-tuning is slightly worse than our baseline. But the performance can be improved by using smaller data sample strides (interval=1). The experiment shows that our model has a better generalization ability than previous methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To evaluate the impact and performance of each component in our model, we evaluate their effectiveness in this section. The Human3.6M dataset and the CPN <ref type="bibr" target="#b7">[8]</ref> detector are employed to provide 2D keypoints.</p><p>Effect of Each Component. As shown in <ref type="table">Table 5</ref>, we first modify the central frame 3D pose output to the sequence output without any other optimization to get the seq2seq baseline model. For a fair comparison, the parameter setting of the seq2seq baseline is directly applied to the proposed method, and the MPJPE loss is utilized in the baseline model. After applying the alternating design, the result shows that our method decreases 6.2mm MPJPE (from 51.7mm to 45.5mm). Then joint separation is utilized to demonstrate its advantage in both improving the performance (from 45.5 to 41.7) and reducing computing cost (FLOPs for each frame decreases to 645 from 186405). By applying our loss function to replace MPJPE loss, our result achieves the best (40.9mm MPJPE with 645 FLOPs). The MixSTE with our loss function improves 20.9% (from 51.7 to 40.9) compared to the seq2seq baseline, and it proves the rationality of our network design.</p><p>Effect of Loss Function. We have explored the contribution of our loss function in detail. As shown in Table 6, the MPJPE metric decreases from 41.7 to 41.3 after applying the WMPJPE loss. The result demonstrates that the WMPJPE is an essential loss to improve accuracy. Then the temporal consistency loss (TCLoss) following <ref type="bibr" target="#b15">[16]</ref> is employed to improve the temporal smoothness performance (MPJVE) by 1.0 (decreases from 4.6 to 3.6), and the coherence gets better after using the MPJVE loss (decreases from 4.6 to 2.6). The motion loss <ref type="bibr" target="#b46">[46]</ref> has less contribution to the coherence than TCLoss and MPJVE loss. Finally, after applying the T-Loss and WMPJPE loss to our method, the result achieves the best on the MPJPE and MPJVE metrics  <ref type="table">)</ref>, the dimension of model (d m ), and the input sequence length (T ). We divide the configurations into 3 groups row-wise, and different values are assigned for one hyper-parameters while keeping the other two hyperparameters fixed to evaluate the impact and choice of each configuration. Based on the results in the table, we choose the combination of Depth=8, Channel=512, and Input Length=243. Note that we choose the Depth = 8 rather than Depth = 10 because the latter setting introduces a more significant number of parameters (33.7M vs. 42.2M).  <ref type="table">Table 7</ref>. Ablation study for hyper-parameter setting in depth (d l ), dimension (dm) and input length (T ). The evaluation is performed on Human3.6M with MPJPE (mm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>As shown in <ref type="figure">Figure 5</ref>, we further conduct visualization on spatial and temporal attention. The selected action (Sit-tingDown of testset S11) is applied for visualization. Moreover, attention outputs of different heads are averaged to observe the overall correlations of joints and frames, and the attention outputs are normalized to [0, 1]. It can be easily observed from spatial attention map (left of <ref type="figure">Figure 5</ref>) that our model learns different dependencies between joints. Furthermore, we also visualize the temporal attention map (right of <ref type="figure">Figure 5</ref>) from the last temporal attention layer. The two parts with light color have similar poses with adjacent frames, while the dark color corresponded frame (the middle image in the frame sequence) has a more different pose with adjacent frames. We also evaluate the visual result of estimated poses and 3D ground truth of Human3.6M in <ref type="figure">Figure 6</ref> to show that we can estimate more accurate poses compared to PoseFormer <ref type="bibr" target="#b57">[57]</ref>.  <ref type="figure">Figure 5</ref>. Visualization of self-attentions among body joints and frames. The x-axis and y-axis correspond to the queries and the predicted outputs, respectively. Each row shows the attention weight wi,j of the j-th query for the i-th output.</p><p>PoseFormer Ours Ground Truth <ref type="figure">Figure 6</ref>. Qualitative comparison between our method (MixSTE) and <ref type="bibr" target="#b57">[57]</ref> with the Photo and SittingDown actions on on Hu-man3.6M. The green circle highlights locations where our method has better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented MixSTE, a novel transformer-based seq2seq approach for 3D pose estimation from monocular video. The model can better capture global sequence coherence and temporal motion trajectories of different body joints. Moreover, the efficiency of 3D human pose estimation is much improved. Comprehensive evaluation results show that our model obtains the best performance. As a new universal baseline, the proposed method also opens up many possible directions for future works. Nonethless, our method is still limited by inaccurate 2D detection results e.g. missing and noisy keypoints. It may be alleviated by applying better 2D detector, but modeling distribution of input noise is also a feasible and valuable exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed framework. The MixSTE is stacked for d l loops, and each MixSTE models spatio-temporal dependencies independently. The WMPJPE Loss denotes the weighted per-joint position error loss. The T-Loss indicates the loss function of temporal coherence in Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The MPJPE distribution on Human3.6M testset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The average joint error comparison across all frames of the testset in the Human3.6M. The V AR. indicates the variance among joint errors divided by a factor (10.0), and the joints of the same part (e.g. right knee and left knee) are divided into the same category for the sake of display.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Dir. Disc. Eat Greet Phone Photo Pose Pur. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg. Detailed quantitative comparison results of MPJPE in millimeters (mm) on Human3.6M under Protocol 1 using 2D ground truth keypoints as input. The best results are highlighted in bold.tently lower, and the proportion of less than 30mm MPJPE is much higher than other methods. The results demonstrate our method performs better on difficult actions.</figDesc><table><row><cell cols="2">Protocol #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Pavlakos et al. [35]</cell><cell></cell><cell cols="3">CVPR2018</cell><cell cols="4">48.5 54.4 54.4 52.0</cell><cell>59.4</cell><cell cols="2">65.3</cell><cell cols="4">49.9 52.9 65.8 71.1</cell><cell>56.6</cell><cell>52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell cols="7">Pavllo et al. [37](CPN, T =243)( ?) CVPR2019</cell><cell cols="4">45.2 46.7 43.3 45.6</cell><cell>48.1</cell><cell cols="2">55.1</cell><cell cols="4">44.6 44.3 57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell cols="3">Cai et al. [1](CPN, T =7)( ?)</cell><cell></cell><cell cols="3">ICCV2019</cell><cell cols="4">44.6 47.4 45.6 48.8</cell><cell>50.8</cell><cell cols="2">59.0</cell><cell cols="4">47.2 43.9 57.9 61.9</cell><cell>49.7</cell><cell>46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4</cell><cell>48.8</cell></row><row><cell cols="3">Yeh et al. [51]( ?)</cell><cell></cell><cell cols="3">NIPS2019</cell><cell cols="4">44.8 46.1 43.3 46.4</cell><cell>49.0</cell><cell cols="2">55.2</cell><cell cols="4">44.6 44.0 58.3 62.7</cell><cell>47.1</cell><cell>43.9</cell><cell>48.6</cell><cell>32.7</cell><cell>33.3</cell><cell>46.7</cell></row><row><cell cols="4">Liu et al. [28](CPN, T =243)( ?)</cell><cell cols="3">CVPR2020</cell><cell cols="4">41.8 44.8 41.1 44.9</cell><cell>47.4</cell><cell cols="2">54.1</cell><cell cols="4">43.4 42.2 56.2 63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell cols="4">Wang et al. [46](CPN, T =96)( ?)</cell><cell cols="3">ECCV2020</cell><cell cols="4">40.2 42.5 42.6 41.1</cell><cell>46.7</cell><cell cols="2">56.7</cell><cell cols="4">41.4 42.3 56.2 60.4</cell><cell>46.3</cell><cell>42.2</cell><cell>46.2</cell><cell>31.7</cell><cell>31.0</cell><cell>44.5</cell></row><row><cell cols="4">Chen et al. [4](CPN, T =243)( ?)</cell><cell cols="7">TCSVT2021 41.4 43.5 40.1 42.9</cell><cell>46.6</cell><cell cols="2">51.9</cell><cell cols="4">41.7 42.3 53.9 60.2</cell><cell>45.4</cell><cell>41.7</cell><cell>46.0</cell><cell>31.5</cell><cell>32.7</cell><cell>44.1</cell></row><row><cell cols="3">Xu et al. [48](T =1)</cell><cell></cell><cell cols="3">CVPR2021</cell><cell cols="4">45.2 49.9 47.5 50.9</cell><cell>54.9</cell><cell cols="2">66.1</cell><cell cols="4">48.5 46.3 59.7 71.5</cell><cell>51.4</cell><cell>48.6</cell><cell>53.9</cell><cell>39.9</cell><cell>44.1</cell><cell>51.9</cell></row><row><cell cols="3">Lin et al. [25](T =1)(*)</cell><cell></cell><cell cols="3">CVPR2021</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.0</cell></row><row><cell cols="3">Zeng et al. [53]( ?)</cell><cell></cell><cell cols="3">ICCV2021</cell><cell cols="4">43.1 50.4 43.9 45.3</cell><cell>46.1</cell><cell cols="2">57.0</cell><cell cols="4">46.3 47.6 56.3 61.5</cell><cell>47.7</cell><cell>47.4</cell><cell>53.5</cell><cell>35.4</cell><cell>37.3</cell><cell>47.9</cell></row><row><cell cols="7">Zheng et al. [57](CPN, T =81)( ?)(*) ICCV2021</cell><cell cols="4">41.5 44.8 39.8 42.5</cell><cell>46.5</cell><cell cols="2">51.6</cell><cell cols="4">42.1 42.0 53.3 60.7</cell><cell>45.5</cell><cell>43.3</cell><cell>46.1</cell><cell>31.8</cell><cell>32.2</cell><cell>44.3</cell></row><row><cell cols="3">Ours(CPN, T =81)( ?)(*)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">39.8 43.0 38.6 40.1</cell><cell>43.4</cell><cell cols="2">50.6</cell><cell cols="4">40.6 41.4 52.2 56.7</cell><cell>43.8</cell><cell>40.8</cell><cell>43.9</cell><cell>29.4</cell><cell>30.3</cell><cell>42.4</cell></row><row><cell cols="3">Ours(CPN, T =243)( ?)(*)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">37.6 40.9 37.3 39.7</cell><cell>42.3</cell><cell cols="2">49.9</cell><cell cols="4">40.1 39.8 51.7 55.0</cell><cell>42.1</cell><cell>39.8</cell><cell>41.0</cell><cell>27.9</cell><cell>27.9</cell><cell>40.9</cell></row><row><cell cols="7">Wang et al. [46](HRNet, T =96)( ?) ECCV2020</cell><cell cols="4">38.2 41.0 45.9 39.7</cell><cell>41.4</cell><cell cols="2">51.4</cell><cell cols="4">41.6 41.4 52.0 57.4</cell><cell>41.8</cell><cell>44.4</cell><cell>41.6</cell><cell>33.1</cell><cell>30.0</cell><cell>42.6</cell></row><row><cell cols="7">Wehrbein et al. [47](HRNet, T =200) ICCV2021</cell><cell cols="4">38.5 42.5 39.9 41.7</cell><cell>46.5</cell><cell cols="2">51.6</cell><cell cols="4">39.9 40.8 49.5 56.8</cell><cell>45.3</cell><cell>46.4</cell><cell>46.8</cell><cell>37.8</cell><cell>40.4</cell><cell>44.3</cell></row><row><cell cols="3">Ours(HRNet, T =243)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">36.7 39.0 36.5 39.4</cell><cell>40.2</cell><cell cols="2">44.9</cell><cell cols="4">39.8 36.9 47.9 54.8</cell><cell>39.6</cell><cell>37.8</cell><cell>39.3</cell><cell>29.7</cell><cell>30.6</cell><cell>39.8</cell></row><row><cell cols="2">Protocol #2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Dir. Disc. Eat Greet Phone Photo Pose Pur.</cell><cell>Sit</cell><cell cols="2">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell cols="4">Wang et al. [46](CPN, T =96)( ?)</cell><cell cols="3">ECCV2020</cell><cell cols="4">31.8 34.3 35.4 33.5</cell><cell>35.4</cell><cell cols="2">41.7</cell><cell cols="4">31.1 31.6 44.4 49.0</cell><cell>36.4</cell><cell>32.2</cell><cell>35.0</cell><cell>24.9</cell><cell>23.0</cell><cell>34.5</cell></row><row><cell cols="4">Liu et al. [28](CPN, T =243)( ?)</cell><cell cols="3">CVPR2020</cell><cell cols="4">32.3 35.2 33.3 35.8</cell><cell>35.9</cell><cell cols="2">41.5</cell><cell cols="4">33.2 32.7 44.6 50.9</cell><cell>37.0</cell><cell>32.4</cell><cell>37.0</cell><cell>25.2</cell><cell>27.2</cell><cell>35.6</cell></row><row><cell cols="7">Zheng et al. [57](CPN, T =81)( ?)(*) ICCV2021</cell><cell cols="4">34.1 36.1 34.4 37.2</cell><cell>36.4</cell><cell cols="2">42.2</cell><cell cols="4">34.4 33.6 45.0 52.5</cell><cell>37.4</cell><cell>33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell cols="3">Ours(CPN, T =81)( ?)(*)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">32.0 34.2 31.7 33.7</cell><cell>34.4</cell><cell cols="2">39.2</cell><cell cols="4">32.0 31.8 42.9 46.9</cell><cell>35.5</cell><cell>32.0</cell><cell>34.4</cell><cell>23.6</cell><cell>25.2</cell><cell>33.9</cell></row><row><cell cols="3">Ours(CPN, T =243)( ?)(*)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">30.8 33.1 30.3 31.8</cell><cell>33.1</cell><cell cols="2">39.1</cell><cell cols="4">31.1 30.5 42.5 44.5</cell><cell>34.0</cell><cell>30.8</cell><cell>32.7</cell><cell>22.1</cell><cell>22.9</cell><cell>32.6</cell></row><row><cell cols="3">Wang et al. [46](HRNet)( ?)</cell><cell></cell><cell cols="3">ECCV2020</cell><cell cols="4">28.4 32.5 34.4 32.3</cell><cell>32.5</cell><cell cols="2">40.9</cell><cell cols="4">30.4 29.3 42.6 45.2</cell><cell>33.0</cell><cell>32.0</cell><cell>33.2</cell><cell>24.2</cell><cell>22.9</cell><cell>32.7</cell></row><row><cell cols="7">Wehrbein et al. [47](HRNet, T =200) ICCV2021</cell><cell cols="4">27.9 31.4 29.7 30.2</cell><cell>34.9</cell><cell cols="2">37.1</cell><cell cols="4">27.3 28.2 39.0 46.1</cell><cell>34.2</cell><cell>32.3</cell><cell>33.6</cell><cell>26.1</cell><cell>27.5</cell><cell>32.4</cell></row><row><cell cols="3">Ours(HRNet, T =243)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">28.0 30.9 28.6 30.7</cell><cell>30.4</cell><cell cols="2">34.6</cell><cell cols="4">28.6 28.1 37.1 47.3</cell><cell>30.5</cell><cell>29.7</cell><cell>30.5</cell><cell>21.6</cell><cell>20.0</cell><cell>30.6</cell></row><row><cell cols="2">MPJVE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Dir. Disc. Eat Greet Phone Photo Pose Pur.</cell><cell>Sit</cell><cell cols="2">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell cols="3">Pavllo et al. [37]( ?)</cell><cell></cell><cell cols="3">CVPR2019</cell><cell>3.0</cell><cell>3.1</cell><cell>2.2</cell><cell>3.4</cell><cell>2.3</cell><cell>2.7</cell><cell></cell><cell>2.7</cell><cell>3.1</cell><cell>2.1</cell><cell>2.9</cell><cell>2.3</cell><cell>2.4</cell><cell>3.7</cell><cell>3.1</cell><cell>2.8</cell><cell>2.8</cell></row><row><cell cols="3">Chen et al. [4]( ?)</cell><cell></cell><cell cols="4">TCSVT2021 2.7</cell><cell>2.8</cell><cell>2.0</cell><cell>3.1</cell><cell>2.0</cell><cell>2.4</cell><cell></cell><cell>2.4</cell><cell>2.8</cell><cell>1.8</cell><cell>2.4</cell><cell>2.0</cell><cell>2.1</cell><cell>3.4</cell><cell>2.7</cell><cell>2.4</cell><cell>2.5</cell></row><row><cell cols="3">Zheng et al. [57]( ?)(*)</cell><cell></cell><cell cols="3">ICCV2021</cell><cell>3.2</cell><cell>3.4</cell><cell>2.6</cell><cell>3.6</cell><cell>2.6</cell><cell>3.0</cell><cell></cell><cell>2.9</cell><cell>3.2</cell><cell>2.6</cell><cell>3.3</cell><cell>2.7</cell><cell>2.7</cell><cell>3.8</cell><cell>3.2</cell><cell>2.9</cell><cell>3.1</cell></row><row><cell cols="3">Ours(CPN, T =243)( ?)(*)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.5</cell><cell>2.7</cell><cell>1.9</cell><cell>2.8</cell><cell>1.9</cell><cell>2.2</cell><cell></cell><cell>2.3</cell><cell>2.6</cell><cell>1.6</cell><cell>2.2</cell><cell>1.9</cell><cell>2.0</cell><cell>3.1</cell><cell>2.6</cell><cell>2.2</cell><cell>2.3</cell></row><row><cell cols="3">Protocol #1</cell><cell></cell><cell></cell><cell></cell><cell cols="10">Dir. Disc. Eat Greet Phone Photo Pose Pur.</cell><cell>Sit</cell><cell cols="2">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell cols="3">Liu et al. [28](T =243)( ?)</cell><cell cols="7">CVPR2020 34.5 37.1 33.6 34.2</cell><cell>32.9</cell><cell></cell><cell>37.1</cell><cell cols="5">39.6 35.8 40.7 41.4</cell><cell>33.0</cell><cell>33.8</cell><cell>33.0</cell><cell>26.6</cell><cell>26.9</cell><cell>34.7</cell></row><row><cell cols="10">Wang et al. [46](GT, T =96) ECCV2020 23.0 25.7 22.8 22.6</cell><cell>24.1</cell><cell></cell><cell>30.6</cell><cell cols="5">24.9 24.5 31.1 35.0</cell><cell>25.6</cell><cell>24.3</cell><cell>25.1</cell><cell>19.8</cell><cell>18.4</cell><cell>25.6</cell></row><row><cell cols="6">Zheng et al. [57](T = 81)( ?)(*)ICCV2021</cell><cell cols="4">30.0 33.6 29.9 31.0</cell><cell>30.2</cell><cell></cell><cell>33.3</cell><cell cols="5">34.8 31.4 37.8 38.6</cell><cell>31.7</cell><cell>31.5</cell><cell>29.0</cell><cell>23.3</cell><cell>23.1</cell><cell>31.3</cell></row><row><cell cols="3">Ours(T =81)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">25.6 27.8 24.5 25.7</cell><cell>24.9</cell><cell></cell><cell>29.9</cell><cell cols="5">28.6 27.4 29.9 29.0</cell><cell>26.1</cell><cell>25.0</cell><cell>25.2</cell><cell>18.7</cell><cell>19.9</cell><cell>25.9</cell></row><row><cell cols="3">Ours(T =243)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">21.6 22.0 20.4 21.0</cell><cell>20.8</cell><cell></cell><cell>24.3</cell><cell cols="5">24.7 21.9 26.9 24.9</cell><cell>21.2</cell><cell>21.5</cell><cell>20.8</cell><cell>14.7</cell><cell>15.7</cell><cell>21.6</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell cols="3">PoseFormer [57]</cell><cell></cell><cell cols="2">VideoPose3D [37]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>19.00%</cell><cell>17.74%</cell><cell cols="2">18.26%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>17.00%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15.00%</cell><cell>12.07%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Propotion</cell><cell>7.00% 9.00% 11.00% 13.00%</cell><cell>6.12%</cell><cell cols="2">14.83% 10.83%</cell><cell>7.82%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.90% 9.16%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.00% 3.00% 5.00%</cell><cell>4.12%</cell><cell></cell><cell></cell><cell></cell><cell>5.68%</cell><cell>3.89%</cell><cell>2.75%</cell><cell cols="2">1.91% 4.22%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="9">&lt;25 25-30 30-35 35-40 40-45 45-50 50-55 55-60 60-65 65-70 &gt;70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">The District of MPJPE (mm)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Detailed quantitative comparison results of MPJPE in millimeters (mm) on Human3.6M under Protocol 1 (no rigid alignment applied) and Protocol 2 (rigid alignment). Top table: results under Protocol 1 (MPJPE); Middle table: results under Protocol 2 (P- MPJPE); Bottom table: results of MPJVE. T denotes the number of input frames estimated by the respective approaches, ( ?) indicates using temporal information, and (*) indicates the transformer-based methods. The best and second-best results are highlighted in bold and underlined formats, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Detailed quantitative comparison results on MPI-INF-3DHP with three metrics. The ? indicates the higher, the better, the ? indicates the lower, the better. The best and second-best results are highlighted in bold and underlined formats, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>#Protocol1</cell><cell></cell><cell>Walk</cell><cell></cell><cell></cell><cell>Jog</cell><cell>Avg.</cell></row><row><cell>Pavllo et al. [37](T =81)</cell><cell cols="6">13.1 10.1 39.8 20.7 13.9 15.6 18.9</cell></row><row><cell cols="7">Pavllo et al. [37](T =81, FT) 14.0 12.5 27.1 20.3 17.9 17.5 18.2</cell></row><row><cell>Zheng et al. [57](T =43)</cell><cell>16.3</cell><cell>11</cell><cell>47.1</cell><cell>25</cell><cell cols="2">15.2 15.1 21.6</cell></row><row><cell cols="7">Zheng et al. [57](T =43, FT) 14.4 10.2 46.6 22.7 13.4 13.4 20.1</cell></row><row><cell>Ours(T =43)</cell><cell cols="6">20.3 22.4 34.8 27.3 32.1 34.3 28.5</cell></row><row><cell>Ours(T =43, interval=1)</cell><cell cols="6">16.2 14.2 21.6 24.6 23.2 25.8 20.9</cell></row><row><cell>Ours(T =43, FT)</cell><cell cols="6">12.7 10.9 17.6 22.6 15.8 17.0 16.1</cell></row></table><note>. The MPJPE on HumanEva testset under Protocol 1. FT indicates using the pretrained model on Human3.6M for finetun- ing. The best result is highlighted in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation study for loss function in our method with MPJPE and MPJVE.(40.9mm MPJPE, 2.3 MPJVE). The ablation study demonstrates that our loss function is comprehensive for the proposed model regarding accuracy and smoothness.Parameter Setting Analysis.Table 7 shows how the setting of different hyper-parameters in our method impacts the performance under Protocol 1 with MPJPE. There are three main hyper-parameters for the network: the depth of MixSTE (d l</figDesc><table><row><cell>Seq2seq</cell><cell>Alternating Design</cell><cell>Joint Separation</cell><cell>Our Loss</cell><cell cols="2">MPJPE FLOPs (M)</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell>51.7</cell><cell>186405</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.5</cell><cell>186405</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>41.7</cell><cell>645</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>40.9</cell><cell>645</cell></row><row><cell cols="6">Table 5. Ablation study for each component used in our method.</cell></row><row><cell cols="6">The evaluation is performed on Human3.6M with MPJPE (mm)</cell></row><row><cell>and FLOPs.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MPJPE MPJVE</cell></row><row><cell>MPJPE Loss</cell><cell></cell><cell></cell><cell>41.7</cell><cell>5.0</cell></row><row><cell cols="2">WMPJPE Loss</cell><cell></cell><cell>41.3</cell><cell>4.6</cell></row><row><cell cols="3">WMPJPE Loss + Motion Loss [46]</cell><cell>41.3</cell><cell>4.3</cell></row><row><cell cols="3">WMPJPE Loss + TCLoss [16]</cell><cell>41.2</cell><cell>3.6</cell></row><row><cell cols="3">WMPJPE Loss + MPJVE Loss</cell><cell>41.2</cell><cell>2.6</cell></row><row><cell cols="3">Ours (WMPJPE Loss + T-Loss)</cell><cell>40.9</cell><cell>2.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the National Natural Science Foundation of China under Grant 62106177 and 61773272.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu Catalin Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Anatomy-aware 3d human pose estimation with bone-based pose decomposition. IEEE Transactions on Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">So-handnet: Self-organizing network for 3d hand pose estimation with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6961" to="6970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelbased 3d hand reconstruction via self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefei</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10451" to="10460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint hand-object 3d reconstruction from a single image with cross-branch feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4008" to="4021" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta agent teaming active learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8575" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting temporal contexts with strided transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
	<note>Sen-ching Cheung, and Vijayan Asari</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Remote: Reinforced motion transformation network for semi-supervised 2d pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context modeling in 3d human pose estimation: A unified perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6238" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Fifth International Conference on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reconstructing 3D Human Pose from 2D Image Landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<editor>Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pose estimation and adaptive robot behaviour for human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Svenstrup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soren</forename><surname>Tranberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hans Jorgen Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3571" to="3576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="991" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Probabilistic monocular 3d human pose estimation with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Wehrbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11199" to="11208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph stacked hourglass networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Takano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transpose: Towards explainable human pose estimation by transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14214,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collaborative refining for person reidentification with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="379" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8163" to="8173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pose-guided human animation from a single image in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kripasindhu</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="15039" to="15048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11436" to="11445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training for temporal action localization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zou</forename><surname>Yuexian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A spatial attentive and temporal dilated (satd) gcn for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoxiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixu</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
