<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TinyViT: Fast Pretraining Distillation for Small Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinnian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TinyViT: Fast Pretraining Distillation for Small Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pretraining</term>
					<term>Knowledge Distillation</term>
					<term>Small Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformer (ViT) recently has drawn great attention in computer vision due to its remarkable model capability. However, most prevailing ViT models suffer from huge number of parameters, restricting their applicability on devices with limited resources. To alleviate this issue, we propose TinyViT, a new family of tiny and efficient small vision transformers pretrained on large-scale datasets with our proposed fast distillation framework. The central idea is to transfer knowledge from large pretrained models to small ones, while enabling small models to get the dividends of massive pretraining data. More specifically, we apply distillation during pretraining for knowledge transfer. The logits of large teacher models are sparsified and stored in disk in advance to save the memory cost and computation overheads. The tiny student transformers are automatically scaled down from a large pretrained model with computation and parameter constraints. Comprehensive experiments demonstrate the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k while using 4.2 times fewer parameters. Moreover, increasing image resolutions, TinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using only 11% parameters. Last but not the least, we demonstrate a good transfer ability of TinyViT on various downstream tasks. Code and models are available at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer <ref type="bibr" target="#b40">[65]</ref> has taken computer vision domain by storm and are becoming increasingly popular in both research and practice <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b53">78]</ref>. One of the recent trends for vision transforms (ViT) is to continue to grow in model size while yielding improved performance on standard benchmarks <ref type="bibr" target="#b53">[78,</ref><ref type="bibr">42,</ref><ref type="bibr" target="#b31">56]</ref>. For example, V-MoE <ref type="bibr" target="#b31">[56]</ref> uses 305 million images to train an extremely large model with 14.7 billion parameters, achieving state-of-the-art performance on image classification. Meanwhile, the Swin transformer uses 3 billion parameters with 70</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1:</head><p>Comparison of our TinyViT with other small vision transformer models <ref type="bibr" target="#b39">[64,</ref><ref type="bibr">43]</ref> on ImageNet-1k in terms of w/ and w/o ImageNet-21k pretraining and distillation. Pretraining with distillation can effectively improve the performance of all these small transformer models, further unveiling their capacities. Best viewed in color. million pretraining images, to attain promising results on downstream detection and segmentation tasks <ref type="bibr">[43,</ref><ref type="bibr">42]</ref>. Such large model sizes and the accompanying heavy pretraining costs make these models unsuitable for applications involving limited computational budgets, such as mobile and IoT edge devices <ref type="bibr" target="#b55">[80]</ref>.</p><p>In contrast to scaling up models to large scales, this work turns attention to downsizing vision transformers, aiming to generate a new family of tiny models and elevate their transfer capacities in downstream tasks. In particular, we explore the following key issue: how to effectively transfer the knowledge of existing large-scale transformers to small ones, as well as unleash the power of large-scale data to elevate the representability of small models? In computer vision, it has long been recognized that large models pretrained on large datasets often achieve better results, while small models easily become saturated (or underfitting) as the growth of data <ref type="bibr" target="#b53">[78,</ref><ref type="bibr">42]</ref>. Is there any possible way for small models to absorb knowledge from massive data and further unveil their capacities?</p><p>To answer this question, we introduce a fast knowledge distillation method to pretrain small models, and show that small models can also get the dividends of massive pretraining data with the guidance of large models. More specifically, we observe that direct pretraining of small models suffers from performance saturation, especially when the data scale increases. But if we impose distillation during pretraining, using a powerful model as the teacher, the potentials of large-scale pretraining data can be unlocked for small models, as demonstrated in <ref type="figure">Fig. 1</ref>. Meanwhile, the distilled small models can be transferred well to downstream tasks, since they have learned a great deal of knowledge about how to generalize from the large model as well as the large-scale pretraining data. We give a detailed discussion in Sec. 4 exploring the underlying reasons why pretraining distillation is able to further unveil the capacities of small models.</p><p>Pretraining models with distillation is inefficient and costly, because a considerable proportion of computing resources is consumed on passing training data through the large teacher model in each iteration, rather than training the target small student. Also, a giant teacher may occupy the most GPU memory, significantly slowing down the training speed of the students (due to limited batch size). To address this issue, we propose a fast and scalable distillation strategy. More concretely, we propose to generate a sparse probability vector as the soft label of each input image in advance, and store it into label files together with the corresponding data augmentation information like random cropping, Ran-dAugment <ref type="bibr" target="#b15">[17]</ref>, CutMix <ref type="bibr" target="#b51">[76]</ref>, etc. During training, we reuse the stored sparse soft labels and augmentations to precisely replicate the distillation procedure, successfully omitting the forward computation and storage of large teacher models. Such strategy has two advantages: 1) Fast. It largely saves the memory cost and computation overheads of generating teachers' soft labels during training. Thus, the distillation of small models can be largely speed up because it is able to use much larger batch size. Besides, since the teacher logits per epoch are independent, they can be saved in parallel, instead of epoch-by-epoch in conventional methods. 2) Scalable. It can mimic any kind of data augmentation and generate the corresponding soft labels. We just need to forward the large teacher model for only once, and reuse the soft labels for arbitrary student models.</p><p>We verify the efficacy of our fast pretraining distillation framework not only on existing small vision transformers, such as DeiT-T <ref type="bibr" target="#b39">[64]</ref> and Swin-T [43], but also over our new designed tiny architectures. Specifically, following <ref type="bibr" target="#b19">[21]</ref>, we adopt a progressive model contraction approach to scale down a large model and generate a family of tiny vision transformers (TinyViT). With our fast pretraining distillation on ImageNet-21k <ref type="bibr" target="#b16">[18]</ref>, TinyViT with 21M parameters achieves 84.8% top-1 accuracy on ImageNet-1k, being 4.2 times smaller than the pretrained Swin-B (85.2% accuracy with 88M parameters). With higher resolution, our model can reach 86.5% top-1 accuracy, establishing new state-of-the-art performance on ImageNet-1k under aligned settings. Moreover, TinyViT models demonstrate good transfer capacities on downstream tasks. For instance, TinyViT-21M gets an AP of 50.2 on COCO object detection benchmark, being 2.1 points superior to Swin-T using 28M parameters.</p><p>In summary, the main cotributions of this work are twofold.</p><p>-We propose a fast pretraining distillation framework to unleash the capacity of small models by fully leveraging the large-scale pretraining data. To our best knowledge, this is the first work exploring small model pretraining. -We release a new family of tiny vision transformer models, which strike a good trade-off between computation and accuracy. With pretraining distillation, such models demonstrate good transfer ability on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the related work on large-scale pretraining, small vision transformers, and knowledge distillation. It is notable that our work is orthogonal to existing literature on model compression techniques such as quantization <ref type="bibr">[44,</ref><ref type="bibr">36,</ref><ref type="bibr">26,</ref><ref type="bibr">44]</ref> and pruning <ref type="bibr" target="#b60">[85,</ref><ref type="bibr" target="#b47">72,</ref><ref type="bibr" target="#b48">73,</ref><ref type="bibr">39]</ref>. These techniques can be used as a post-processing for our TinyViT to further improve model efficiency.</p><p>Large-scale pretraining. Bommasani et al. <ref type="bibr" target="#b4">[6]</ref> first coined the concept of foundation models that are pretrained from large-scale data and have outstanding performance in various downstream tasks. For example, BERT <ref type="bibr" target="#b17">[19]</ref> and GPT-3 <ref type="bibr" target="#b28">[53]</ref> have been demonstrated to be effective foundation models in natural language processing. Recently, there are some research efforts in developing foundation models in computer vision, including CLIP <ref type="bibr" target="#b27">[52]</ref>, Align <ref type="bibr">[37]</ref> and Florence <ref type="bibr" target="#b50">[75]</ref>. They have shown impressive transfer and zero-shot capabilities. However, these large models are unsuitable for downstream applications with limited computational budgets. By contrast, our work investigates the pretraining method for small models and improves their transferability to various downstream tasks.</p><p>Small vision transformers. Lightweight CNNs have powered many mobile vision tasks <ref type="bibr">[33,</ref><ref type="bibr" target="#b37">62]</ref> [24] optimized the inference time of small and medium-sized ViTs and generated a family of throughput-efficient ViTs. Different from these manually designed or automatically searched small models, our work explores model contraction to generate small models by progressively slimming a large seed model, which can be considered as a complementary work to existing literature on scaling-up large vision transformers <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b53">78,</ref><ref type="bibr" target="#b31">56,</ref><ref type="bibr">42]</ref>.</p><p>Knowledge distillation. Distillation in a teacher-student framework [31] is widely used to leverage knowledge from large teacher models. It has been extensively studied in convolutional networks <ref type="bibr" target="#b21">[23]</ref>. Recently, there are several research works in developing distillation techniques for ViTs <ref type="bibr" target="#b48">[73,</ref><ref type="bibr" target="#b59">84]</ref>. For example, Touvron et al. <ref type="bibr" target="#b39">[64]</ref> introduced a distillation token to allow the transformer to learn from a ConvNet teacher, while Jia et al. <ref type="bibr">[38]</ref> proposed to excavate knowledge from the teacher transformer via the connection between images and patches. Distillation for ViTs is still under-explored, especially for pretraining distillation.</p><p>In knowledge distillation, the mostly related work to ours is the recent FKD <ref type="bibr" target="#b34">[59]</ref>. Both methods share a similar spirit on saving teacher logits to promote training efficiency, but our framework has two advantages. 1) More efficient. Instead of saving the explicit information of each transformation in data augmentation using hundreds of bytes, such as crop coordinates and rotation degree, our framework only needs 4 bytes to store a random seed. The seed will be used as the initial state of the random number generator to reproduce the number sequence that controls the transformations in data augmentation to generate crop coordinates and rotation degree, etc. 2) More general. Our framework supports all existing types of data augmentation including the complex Mixup <ref type="bibr" target="#b54">[79]</ref> and Cutmix <ref type="bibr" target="#b51">[76]</ref>, which are not explored in FKD. Moreover, the studied problem in <ref type="bibr" target="#b34">[59]</ref> is different to ours. We focus on pretraining-stage distillation for transformers, while FKD explores finetune-stage distillation for CNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TinyViT</head><p>This section proposes TinyViT, a new family of tiny and efficient models with fast pretraining distillation on large-scale data. We first introduce the fast knowledge distillation framework for small model pretraining in Sec. <ref type="bibr" target="#b1">3</ref>  <ref type="figure">Fig. 2</ref>: Our fast pretraining distillation framework. Top: the branch for saving teacher logits. Encoded data augmentation and sparsified teacher logits are saved. Middle: the disk for storing information. Bottom: the branch for training the student. The decoder reconstructs the data augmentation, and distillation is conducted between the teacher logits and student outputs. Note that the two branches are independent and asynchronous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fast Pretraining Distillation</head><p>We observe that direct pretraining of small models on massive data does not bring much gains, especially when transferring them to downstream tasks, as presented in <ref type="figure">Fig. 1</ref>. To address this issue, we resort to knowledge distillation to further unveil the power of pretraining for small models. Different from prior work that pays most attention to finetune-stage distillation <ref type="bibr" target="#b39">[64]</ref>, we focus on pretraining distillation, which not only allows small models to learn from largescale model, but also elevates their transfer capacities for downstream tasks.</p><p>Pretraining with distillation is inefficient and costly, because a considerable proportion of computing resources is consumed on passing training data through the large teacher model in each iteration, rather than training the target small student. Also, a giant teacher may occupy the most GPU memory, slowing down the training speed of the target students (due to limited batch size). To solve this problem, we propose a fast pretraining distillation framework. As depicted in <ref type="figure">Fig. 2</ref>, we store the information of data augmentation and teacher predictions in advance. During training, we reuse the stored information to precisely replicate the distillation procedure, successfully omitting the forward computation and memory occupation of the large teacher model.</p><p>Mathematically, for an input image x with strong data augmentation A, such as RandAugment <ref type="bibr" target="#b15">[17]</ref> and CutMix <ref type="bibr" target="#b51">[76]</ref>, we store both A and teacher prediction y = T (A(x)), where T (?) and A(x) are the teacher model and the augmented image. It is notable that passing the same image through the same data augmentation pipeline multiple times will generate different augmented images due to the inherent randomness in data augmentation. Therefore, the pair (A,?) needs to be saved for each image in each iteration, as illustrated in <ref type="figure">Fig. 2</ref>.</p><p>In the training process, we only need to recover the pairs (A,?) from stored files, and optimize the following objective function for student model distillation:</p><formula xml:id="formula_0">L = CE (?, S(A(x))) ,<label>(1)</label></formula><p>where S(?) and CE(?) are the student model and cross entropy loss, respectively. Note that our framework is label-free, i.e., with no need for ground-truth labels, because we only use the soft labels generated by teacher models for training. Therefore, it can utilize numerous off-the-shelf web data without labels for largescale pretraining. Such a label-free strategy is workable in practice because the soft labels are accurate enough while carrying a lot of discriminative information for classification such as category relations. We also observe that distillation with ground-truth would cause slight performance drops. The reason may be that not all the labels in ImageNet-21k <ref type="bibr" target="#b16">[18]</ref> are mutually exclusive <ref type="bibr" target="#b30">[55]</ref>, including correlative pairs like "chair" and "furniture", "horse" and "animal". Therefore, the one-hot ground-truth label could not describe an object precisely, and in some cases it suppresses either child classes or parent classes during training. Moreover, our distillation framework is as fast as training models without distillation since the cumbersome teacher T (?) is removed during training in Eq. (1). Besides, our distillation framework is fast due to two key components: sparse soft labels and data augmentation encoding. They can largely reduce the storage consumption while improving memory efficiency during training.</p><p>Sparse soft labels. Let's consider the teacher model outputs C logits for the prediction. It often consumes much storage space to save the whole dense logits of all augmented images if C is large, e.g., C = 21, 841 for ImageNet-21k. Therefore, we just save the most important part of the logits, i.e., sparse soft labels. Formally, we select the top-K values in?, i.e., {? I(k) } K k=1 ??, and store them along with their indices {I(k)} K k=1 into our label files. During training, we only reuse the stored sparse labels for distillation with label smoothing <ref type="bibr" target="#b36">[61,</ref><ref type="bibr" target="#b33">58]</ref>, which is defined as?</p><formula xml:id="formula_1">c = ? I(k) if c = I(k), 1? K k=1?I(k) C?K otherwise,<label>(2)</label></formula><p>where? c is the recovered teacher logits for student model distillation, i.e.,? = [? 1 , . . . ,? c , . . . ,? C ]. When the sparsity factor K is small, i.e. K ? C, it can reduce logits' storage by orders of magnitude. Moreover, we empirically show that such sparse labels can achieve comparable performance to the dense labels for knowledge distillation, as presented in Sec. 5.2. Data augmentation encoding. Data augmentation involves a set of parameters d, such as the rotation degree and crop coordinates, to transform the input image. Since d is different for each image in each iteration, saving it directly becomes memory-inefficient. To solve this problem, we encode d by a single parameter d 0 = E(d), where E(?) is the encoder in <ref type="figure">Fig. 2</ref>. Then in the training process, we recover d = E ?1 (d 0 ) after loading d 0 in the storage files, where E ?1 (?) is viewed as the decoder. Therefore, the data augmentation can be accurately reconstructed. In practice, a common choice for the decoder is the pseudo-random number generator (i.e. PCG <ref type="bibr" target="#b24">[49]</ref>). It takes a single parameter as the input and generates a sequence of parameters. As for the encoder, we simply implement it by a generator for d 0 and reusing the decoder E ?1 (?). It outputs d = E ?1 (d 0 ) for the teacher model. d 0 is saved for the decoder to reproduce d when training the student. Thus, the implementation becomes more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architectures</head><p>In this subsection, we present a new family of tiny vision transformers by scaling down a large model seed with a progressive model contraction approach <ref type="bibr" target="#b19">[21]</ref>. Specifically, we start with a large model and define a basic set of contraction factors. Then in each step, smaller candidate models are generated around the current model by adjusting the contraction factors. We select models that satisfy both constraints on the number of parameters and throughput. The model with the best validation accuracy will be utilized for further reduction in the next step until the target is achieved. This is a form of constrained local search <ref type="bibr">[32]</ref> in the model space spanned by the contraction factors.</p><p>We adopt a hierarchical vision transformer as the basic architecture, for the convenience of dense prediction downstream tasks like detection that require multi-scale features. More concretely, our base model consists of four stages with a gradual reduction in resolution similar to Swin [43] and LeViT <ref type="bibr">[24]</ref>. The patch embedding block consists of two convolutions with kernel size 3, stride 2 and padding 1. We apply lightweight and efficient MBConvs [33] in Stage 1 and down sampling blocks, since convolutions at earlier layers are capable of learning low-level representation efficiently due to their strong inductive biases <ref type="bibr">[24,</ref><ref type="bibr" target="#b45">70]</ref>. The last three stages are constructed by transformer blocks, with window attention to reduce computational cost. The attention biases [24] and a 3 ? 3 depthwise convolution between attention and MLP are introduced to capture local information <ref type="bibr" target="#b44">[69,</ref><ref type="bibr" target="#b13">15]</ref> We scale down the above factors with a progressive model contraction approach <ref type="bibr" target="#b19">[21]</ref> and generate a new family of tiny vision transformers: All models share the same factors: </p><formula xml:id="formula_2">{? N1 , ? N2 , ? N3 , ? N4 } = {2, 2, 6, 2}, {? W2 , ? W3 , ? W4 } = {7, 14, 7} and {? R , ? M , ? E } = {4, 4, 32}. For the embeded dimensions {? D1 , ? D2 , ? D3 , ? D4 }, TinyViT-21M: {96,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis and Discussions</head><p>In this section, we provide analysis and discussions on two key questions: 1) What are the underlying factors limiting small models to fit large data? 2) Why distillation can unlock the power of large data for small models? To answer the above questions, we conduct experiments on the widely used large-scale benchmark ImageNet-21k <ref type="bibr" target="#b16">[18]</ref>, which contains 14M images with 21,841 categories</p><p>What are the underlying factors limiting small models to fit large data? We observe that there are many hard samples existing in IN-21k, e.g., images with wrong labels and similar images with different labels due to the existence of multiple equally prominent objects in the images. This is also recognized by existing literature <ref type="bibr" target="#b52">[77,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b30">55]</ref> and approximately 10% images in ImageNet are considered as hard samples. Small models struggle to fit these hard samples, leading to low training accuracy compared to large models (TinyViT-21M: 53.2% vs. Swin-L-197M [43]: 57.1%) and limited transferability on ImageNet-1k (TinyViT-21M w/ pretraining: 83.8% vs. w/o pretraining: 83.1%).</p><p>To verify the impact of hard samples, we resort to two techniques. 1) Inspired by <ref type="bibr" target="#b3">[5]</ref>, we exploit the powerful pretrained model Florence <ref type="bibr" target="#b50">[75]</ref> finetuned on ImageNet-21k to identify the images whose labels lie outside the top-5 predictions of Florence. Through this procedure, we remove 2M images from ImageNet-21k, approximately 14%, and then pretrain TinyViT-21M and Swin-T on the cleaned dataset. 2) We perform distillation to pretrain TinyViT-21M/Swin-T using Florence as the teacher model, which generates soft labels to replace the polluted groundtruth labels in ImageNet-21k. The results of the pretrained models with finetuning on ImageNet-1k are reported in Tab. 1.</p><p>We obtain several insights from the results. 1) Pretraining small models on the original ImageNet-21k dataset brings limited performance gains on ImageNet-1k (0.7% for both Swin-T and TinyViT-21M). 2) After removing parts of the hard samples in ImageNet-21k, both models can better leverage the large data and achieve higher performance gains (1.0%/1.1% for Swin-T/TinyViT-21M).</p><p>3) Distillation is able to avoid the defects of hard samples, because it does not use the groundtruth labels that are the main cause of hard samples. Thus, it gets higher improvements (2.2%/1.7% for Swin-T and TinyViT-21M).</p><p>Why can distillation improve the performance of small models on large datasets? The answer is that the student models can directly learn domain knowledge from teachers. Namely, the teacher injects class relationship prior when training the student, while filtering noisy labels (hard samples) for small student models.</p><p>To analyze the class relationships of teacher predictions, we select 8 images per class from IN-21k with totally 21,841 classes. These images are then fed into  Florence <ref type="bibr" target="#b50">[75]</ref> to extract prediction logits. Following <ref type="bibr" target="#b38">[63]</ref>, we can generate the heatmap of Pearson correlation coefficients between classes on the prediction logits. In <ref type="figure" target="#fig_1">Fig. 3(a)</ref>, simialr or related classes clearly have a high correlations with each other (red), illustrated by the block diagonal structure. In addition, the teacher model can also capture uncorrelated classes (shown in blue). This observation verifies that teacher predictions indeed reveal class relationships. We compare the Pearson correlations on the predictions of TinyViT-21M w/o and w/ distillation, as shown in <ref type="figure" target="#fig_1">Fig. 3(b)</ref> and <ref type="figure" target="#fig_1">Fig. 3</ref>(c) respectively. The block diagonal structure is less obvious without distillation, indicating that the small model is difficult to capture more class relations. However, distillation can guide the student model to imitate the teacher behaviors, leading to better excavating knowledge from large datasets. As shown in <ref type="figure" target="#fig_1">Fig. 3(c)</ref>, the Pearson correlations of TinyViT with distillation are closer to the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first provide ablation studies on our proposed fast pretraining distillation framework. Next, we compare our TinyViT with other state-of-theart models. At last, we demonstrate the transferability on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>ImageNet-21k pretraining. We pretrain TinyViT for 90 epochs on ImageNet-21k <ref type="bibr" target="#b16">[18]</ref> with an AdamW [45] optimizer, a weight decay of 0.01, initial learning rate of 0.002 with a cosine scheduler, 5 epochs warm-up, batch size of 4,096 and  <ref type="bibr" target="#b58">[83]</ref>, RandAugment <ref type="bibr" target="#b15">[17]</ref>, Mixup <ref type="bibr" target="#b54">[79]</ref> and Cutmix <ref type="bibr" target="#b51">[76]</ref>. ImageNet-1k finetuning from the pretrained model. We finetune the pretrained models for 30 epochs on ImageNet-1k, using a batch size of 1,024, a cosine learning rate scheduler with 5-epoch warm-up. The initial learning rate is 5 ? 10 ?4 and weight decay is 10 ?8 . The learning rate of each layer is decayed by the rate 0.8 from the output layer to the input layer. The running statistics of BatchNorm are frozen. We disable Mixup and Cutmix.</p><p>ImageNet-1k finetuning on higher resolution. When finetuning TinyViT on higher resolution, the windows of each self-attention layer are enlarged as the increasing of input resolution. The attention biases are bilinear-interpolated to adapt the new window size. For example, the window sizes of the four stages are {7, 7, 14, 7} on 224 2 resolution, {12, 12, 24, 12} on 384 2 resolution and {16, 16, 32, 16} on 512 2 resolution. We finetune the model for 30 epochs, using an accumulated batch size of 1024, a cosine learning rate scheduler with 5-epoch warm up. The initial learning rate is 4 ? 10 ?5 and weight decay is 10 ?8 . The running statistic of BatchNorm are frozen. Mixup and Cutmix are disabled.</p><p>ImageNet-1k training from scratch. We train our models for 300 epochs on ImageNet-1k with an AdamW optimizer, a weight decay of 0.05, initial learning rate of 0.001 with a cosine scheduler, 20 warm-up epochs, batch size of 1,024 and gradient clipping with a max norm of 5.0. The stochastic depth rate is set to 0.0/0.1/0.2 for TinyViT-5/11M/21M, respectively.</p><p>Knowledge distillation. We pre-store the top-100 logits of teacher models for IN-21k, including Swin-L [43], BEiT-L <ref type="bibr" target="#b2">[4]</ref>, CLIP-ViT-L/14 <ref type="bibr" target="#b27">[52,</ref><ref type="bibr" target="#b18">20]</ref> and Florence <ref type="bibr" target="#b50">[75]</ref> for all 90 epochs. Note that CLIP-ViT-L/14 and Florence are finetuned on IN-21k for 30 epochs to serve as teachers. Then, we distill the student models using the stored teacher logits with the same hyper-parameters as the distillation involving the teacher model. The distillation temperature is set to 1.0. We disable Mixup <ref type="bibr" target="#b54">[79]</ref> and Cutmix <ref type="bibr" target="#b51">[76]</ref> for pretraining distillation on TinyViT. All models are implemented using PyTorch <ref type="bibr" target="#b26">[51]</ref> with timm library <ref type="bibr" target="#b43">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>Impact of pretraining distillation on existing small ViTs. We study the effectiveness of our proposed fast pretraining distillation framework on two popular vision transformers: DeiT <ref type="bibr" target="#b39">[64]</ref> and Swin  However, our proposed fast pretraining distillation framework increases the accuracy by 2.2%/2.1%/2.2% respectively. It indicates that pretraining distillation allows small models to benefit more from large-scale datasets. Impact of pretraining data scale. We investigate the representation quality of TinyViT-5M/21M with respect to the total number of images "seen" (batch size times number of steps) during pretraining on IN-21k, following the strategies in <ref type="bibr" target="#b53">[78]</ref>. We use CLIP-ViT-L/14 <ref type="bibr" target="#b27">[52,</ref><ref type="bibr" target="#b18">20]</ref> as the teacher. The results on IN-1k after finetuning are shown in <ref type="figure">Fig. 4</ref>. We have the following observations. 1) For both models, pretraining distillation can consistently brings performance gains over different data size. 2) All models tends to saturate as the number of epochs increase, which may be bottlenecked by the model capacities.</p><p>Impact of the number of saved logits. The effects of sparse logits on distilling TinyViT-21M by using Swin-L [43] as the teacher model are shown in <ref type="figure">Fig. 5</ref>. On both IN-1k and IN-21k, we observe that the accuracy increases as the number of sparse logits K grows until saturation, meanwhile the storage cost grows linearly.</p><p>This observation is aligned with existing work on knowledge distillation <ref type="bibr" target="#b38">[63,</ref><ref type="bibr" target="#b34">59]</ref>, where teacher logtis capture class relationships but also contain noise. This makes it possible to sparsify teacher logits such that the class relationships are reserved while reducing noise. Moreover, memory consumption also impose constraints on the choice of K. To obtain comparable accuracy under limited storage space, we select the slightly larger K, where K=10 (1.0% logits) on IN-1k for 300 epochs and K=100 (0.46% logits) on IN-21k for 90 epochs using 16 GB/481 GB storage cost, respectively. Impact of teacher models. We evaluate the impact of teacher models for pretraining distillation. As shown in Tab. 3, a better teacher can yield better student models (#1 vs. #2 vs. #3 and #4). TinyViT-21M distilled by Florence on IN-21k is 1.0%/0.6%/1.0% higher in top-1 accuracy on three benchmark datasets than trained from scratch on IN-21k (#0 vs. #4). However, better teacher models are often large in model size, resulting in high GPU memory consumption and long <ref type="table">Table 3</ref>: Ablation study on different teacher models for pretraining distillation. Teacher performance are listed in the brackets: (the number of parameters, linear probe performance on IN-1k). We report the training time cost and memory consumption of teacher models on NVIDIA V100 GPUs without using our proposed fast pretraining distillation.  training time, e.g., Florence (#4) with 682M parameters occupies 11GB GPU memory and leads to 2.4 times longer training time.</p><p>Note that our fast pretraining distillation framework simply loads the teacher logits from a hard disk during training. Therefore, it does not require additional GPU memory and has the same training time as #0. Moreover, the framework is compatible with all types of teacher models. Therefore, the performance of TinyViT can be further improved by introducing more powerful teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on ImageNet</head><p>In this section, we compare our scaled TinyViT models with state-of-the-art methods on IN-1k <ref type="bibr" target="#b16">[18]</ref>. The performance is reported in <ref type="figure" target="#fig_4">Fig. 6</ref> and Tab. 4. The models with ? indicates pretraining on IN-21k with the proposed fast distillation framework using CLIP-ViT-L/14 <ref type="bibr" target="#b27">[52,</ref><ref type="bibr" target="#b18">20]</ref> as the teacher. It shows that, without distillation, our TinyViT models achieve comparable performance to current prevailing methods, such as Swin transformer <ref type="bibr">[43]</ref> and LeViT [24], with similar parameters. This indicates the effectiveness of the proposed new architectures and the model scaling techniques. Moreover, with the fast pretraining distillation, the performance of TinyViT can be largely improved, outperforming the stateof-the-art CNN, transformer and hybrid models. In particular, using only 21M parameters, TinyViT trained from scratch on IN-1k gets 1.9%/3.2% higher top-1 accuracy than Swin-T [43] and DeiT-S <ref type="bibr" target="#b39">[64]</ref> respectively, while after pretraining with distillation on IN-21k, the improvements arise to 3.6% and 4.9%. With higher resolution, TinyViT-21M reaches a top-1 accuracy of 86.5%, establishing <ref type="table">Table 4</ref>: TinyViT performance on IN-1k <ref type="bibr" target="#b16">[18]</ref> with comparisons to state-of-the-art models. MACs (multiply-accumulate operations) and Throughput are measured using the GitHub repository of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">24]</ref> and a V100 GPU. ?: pretrain on IN-21k with the proposed fast distillation; ?: finetune with higher resolution.  <ref type="bibr" target="#b9">[11]</ref> and GLiT <ref type="bibr" target="#b7">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Transfer Learning Results</head><p>Linear Probe. For linear probe, we follow the same setting as in MOCO v3 <ref type="bibr" target="#b11">[13]</ref>, i.e., replacing the head of TinyViT models with a linear layer, while only finetuning the linear layer on downstream datasets and frozing other weights. We consider five classification benchmarks: CIFAR-10 [40], CIFAR-100 [40], Flowers <ref type="bibr" target="#b23">[48]</ref>, Cars [2] and Pets <ref type="bibr" target="#b25">[50]</ref>. The results are reported in Tab. 5.</p><p>We compare the performance of TinyViT-21M with 4 different training settings. It is clear that distillation can improve the linear probe performance of TinyViT (#0 vs. #1, #2 vs. #3). Besides, when trained on larger datasets (i.e., IN-21k), TinyViT gets more than 10% gains over CIFAR-100, Flowers and Cars (#0,#1 vs. #2, #3), indicating better representability. Thus, pretraining with distillation on large-scale datasets achieves the best representability (#3).</p><p>Few-shot Learning. We also evaluate the transferability of TinyViT with different training settings on few-shot learning benchmark <ref type="bibr">[25]</ref>. The benchmark datasets include: CropDisease <ref type="bibr" target="#b22">[47]</ref> (plant leaf images, 38 disease stages over 14 plant species), EuroSAT [29] (RGB satellite images, 10 categories), ISIC 2018 <ref type="bibr" target="#b14">[16]</ref> (dermoscopic images of skin lesions, 7 disease states) and ChestX <ref type="bibr" target="#b42">[67]</ref> (Chest Xrays, 16 conditions). The learning and inference settings are the same as in  [25]. The evaluation protocol involves 5-way classification across 5-, 20-and 50shot. The classes and shots are randomly sampled for each episode, for 600 episodes per way and shot. Average accuracy over all episodes is reported. We add a single linear layer in replace of the original classification layer in TinyViT-21M. As shown in Tab. 5, we obtain same observations as the linear probe results, except of ChestX, where gray-scale medical images are the least similar to natural images, as well as few in the training dataset for the teacher models and the student models. In combination of these results, we can conclude that pretraining distillation is significant in improving the representability of small models, and thus our proposed fast pretraining distillation framework is effective. Object Detection. We also investigate the transfer ability of our TinyViT on object detection task <ref type="bibr">[41]</ref>. We use Cascade R-CNN <ref type="bibr" target="#b5">[7]</ref> with Swin-T [43] as our baseline. We follow the same training settings used in Swin transformer <ref type="bibr">[43]</ref>. The results on COCO 2017 validation set are reported in Tab. 6. Under the same training recipe, our TinyViT architecture achieves better performance than Swin-T, getting 1.5% AP improvements. Furthermore, after applying pretraining distillation, TinyViT gets another 0.6% AP improvements, being 2.1% higher than Swin-T. This clearly demonstrates our fast pretraining distillation framework is effective and capable of improving the transfer ability of small models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed a new family of tiny and efficient vision transformers pretrained on large-scale datasets with our proposed fast distillation framework, named TinyViT. Extensive experiments demonstrate the efficacy of TinyViT on ImageNet-1k, and its superior transferability on various downstream benchmarks. In future work, we will consider using more data to further unlock the representability of small models with the assistance of more powerful teacher models. Designing a more effective scaling down method to generate small models with better computation/accuracy is another interesting research direction.  Besides, we provide some interesting observations about model contraction. It may help both the manual design and the search space design for efficient small vision transformers. 1) For small vision transformers, it improves the accuracy when replacing the transformer block in the first stage with MBConv [33] blocks. We conjecture that early convolution introduces inductive bias like locality <ref type="bibr" target="#b45">[70,</ref><ref type="bibr">24]</ref>. It provides more prior knowledge to help small models converge well.</p><p>2) It reduces the number of parameters significantly when decreasing the embeded dimension ? D1?4 , so it is the first step to scale the model down. When the model becomes narrower, its depth (especially in the depth of the third stage ? N3 ) is increased to satisfy the constraint of the number of parameters. 3) The MLP expansion ratio ? M 4 is better than 3 in our models. 4) Window sizes ? W2?4 do not affect the model size, but larger windows imrpove the accuracy with more computational cost. Especially for Stage 3, 14 ? 14 window size improves the accuracy with little extra computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B How fast the distillation is?</head><p>The proposed fast pretraining distillation is faster than the conventional distillation method by 29.8% when using Florence model as the teacher (682M Params and 97.9 GFLOPs). More concretely, our method takes 92.4 GPU days to store the top-100 logits of Florence and 140.0 GPU days to pretrain TinyViT-21M (4.4 GFLOPs) with the saved logits for 90 epochs on ImageNet-21k, while the conventional distillation uses 330.9 GPU days due to limited batch size. Since the teacher logits per epoch are different and independent, they can be saved in parallel, instead of epoch-by-epoch in the conventional method. Besides, the saved logits can be reused for arbitrary student models, and avoid re-forwarding cost of the large teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Distillation with ground-truth</head><p>We compare the performance under the distillation with and without groundtruth. The student model is a variant of TinyViT-21M, equipped with talking head <ref type="bibr" target="#b32">[57]</ref> and shared blocks in Stage 4. As shown in Tab 2, the distillation with ground-truth would cause slight performance drops. This is probably because that not all the labels in ImageNet-21k <ref type="bibr" target="#b16">[18]</ref> are mutually exclusive. For example, it contains labels like "chair" and "furniture", "horse" and "animal" <ref type="bibr" target="#b30">[55]</ref>, which are correlative pairs. Therefore, the one-hot ground-truth label could not describe an object precisely, and in some cases it suppresses either child classes <ref type="table" target="#tab_5">Table 2</ref>: Comparison for pretraining distillation w/ and w/o ground truth (GT) labels. The student model is a variant of TinyViT-21M, pretrained for 90 epochs on ImageNet-21k and then finetuned for 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IN-21k</head><p>Distillation or parent classes during training. By contrast, the soft labels generated by pretrained foundation models carry a lot of category relation information, that is helpful for distilling a small model, as presented in <ref type="figure" target="#fig_1">Fig. 3</ref> of the main paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Pearson correlations of output predictions on ImageNet-21k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[43]. As shown in Tab. 2, comparing to training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Comparison on pretrained TinyViT-21M/5M over training data size. The accuracy on IN-1k and storage cost of TinyViT-21M along different saved logits K. Left: distill TinyViT-21M on IN-1k Right: distill TinyViT-21M on IN-21k then finetune it on IN-1k. from scratch on IN-1k, pretraining without distillation on IN-21k can only bring limited gains, i.e. 0.8%/0.6%/0.7% for DeiT-Ti/DeiT-S/Swin-T, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison with state-of-the-art methods on IN-1k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>24. Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J?gou, H., Douze, M.: Levit: a vision transformer in convnet's clothing for faster inference. In: ICCV (2021) 25. Guo, Y., Codella, N.C., Karlinsky, L., Codella, J.V., Smith, J.R., Saenko, K., Rosing, T., Feris, R.: A broader study of cross-domain few-shot learning. In: ECCV (2020) 26. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv (2015) 27. He, K., Gkioxari, G., Doll?r, P., Girshick, R.: Mask r-cnn. In: ICCV (2017) 28. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.In: CVPR (2016) 29. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (2019) 30. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv (2016) 31. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv (2015) 32. Hoos, H.H., St?tzle, T.: Stochastic local search: Foundations and applications. Elsevier (2004) 33. Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: ICCV (2019) 34. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth. In: ECCV (2016) 35. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: ICML (2015) 36. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and training of neural networks for efficient integerarithmetic-only inference. In: CVPR (2018) 37. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: ICML (2021) 38. Jia, D., Han, K., Wang, Y., Tang, Y., Guo, J., Zhang, C., Tao, D.: Efficient vision transformers via fine-grained manifold distillation. arXiv (2021) 39. Kong, Z., Dong, P., Ma, X., Meng, X., Niu, W., Sun, M., Ren, B., Qin, M., Tang, H., Wang, Y.: Spvit: Enabling faster vision transformers via soft token pruning. arXiv (2021) 40. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009) 41. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 42. Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et al.: Swin transformer v2: Scaling up capacity and resolution. In: CVPR (2022) 43. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021) 44. Liu, Z., Wang, Y., Han, K., Zhang, W., Ma, S., Gao, W.: Post-training quantization for vision transformer. NeurIPS (2021) 45. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2018) 46. Mehta, S., Rastegari, M.: Mobilevit: Light-weight, general-purpose, and mobilefriendly vision transformer. In: ICLR (2021)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Recently, there are several attempts developing light vision transformers (ViTs). Mehta et al. [46] combined standard convolutions and transformers to develop MobileViT, which outperforms the prevailing Mo-bileNets [33] and ShuffleNet [82]. Gong et al. [22] employed NAS and identified a family of efficient ViTs with MACs ranging from 200M to 800M, surpassing the state-of-the-art. Graham et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Residual connection [28] is applied on each block in Stage 1, as well as attention blocks and MLP blocks. All activation functions are GELU [30]. The normalization layers of convolution and linear are BatchNorm [35] and LayerNorm<ref type="bibr" target="#b1">[3]</ref>, respectively.Contraction factors. We consider the following factors to form a model: ? D1?4 : embeded dimension of four stages respectively. Decreasing them results in a thinner network with fewer heads in multi-head self-attention. ? N1?4 : the number of blocks in four stages respectively. The depth of the model is decreased by reducing these values. ? W2?4 : window size in the last three stages respectively. As these values become smaller, the model has fewer parameters and higher throughput.? R : channel expansion ratio of the MBConv block. We can obtain a smaller model size by reducing this factor. ? M : expansion ratio of MLP for all transformer blocks. The hidden dimension of MLP will be smaller if scaling down this value. ? E : the dimension of each head in multi-head attention. The number of heads will be increased when scaling it down, bringing lower computation cost.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>192, 384, 576}, TinyViT-11M: {64, 128, 256, 448} and TinyViT-5M: {64, 128, 160, 320}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Impact of hard samples. Models are pretrained on IN-21k and then finetuned on IN-1k.</figDesc><table><row><cell>#</cell><cell>Model</cell><cell>Pretraining Dataset</cell><cell>IN-1k Top-1(%)</cell><cell cols="2">IN-Real [5] IN-V2 [54] Top-1(%) Top-1(%)</cell></row><row><cell>0</cell><cell></cell><cell>Train from scratch on IN-1k</cell><cell>81.2</cell><cell>86.7</cell><cell>69.7</cell></row><row><cell>1 2</cell><cell>Swin-T [43]</cell><cell>Original IN-21k Cleaned IN-21k</cell><cell>81.9(+0.7) 82.2(+1.0)</cell><cell>87.0(+0.3) 87.3(+0.6)</cell><cell>70.6(+0.9) 71.1(+1.4)</cell></row><row><cell>3</cell><cell></cell><cell cols="2">Original IN-21k w/ distillation 83.4(+2.2)</cell><cell>88.0(+1.3)</cell><cell>72.6(+2.9)</cell></row><row><cell>4</cell><cell></cell><cell>Train from scratch on IN-1k</cell><cell>83.1</cell><cell>88.1</cell><cell>73.1</cell></row><row><cell>5 6</cell><cell>TinyViT-21M (ours)</cell><cell>Original IN-21k Cleaned IN-21k</cell><cell>83.8(+0.7) 84.2(+1.1)</cell><cell>88.4(+0.3) 88.5(+0.4)</cell><cell>73.8(+0.7) 73.8(+0.7)</cell></row><row><cell>7</cell><cell></cell><cell cols="2">Original IN-21k w/ distillation 84.8(+1.7)</cell><cell>88.9(+0.8)</cell><cell>75.1(+2.0)</cell></row><row><cell></cell><cell>(a) Teacher</cell><cell>(b) TinyViT w/o distill.</cell><cell></cell><cell></cell><cell></cell></row></table><note>(c) TinyViT w/ distill.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on different pretraining strategies for Swin[43]  and DeiT<ref type="bibr" target="#b39">[64]</ref>. The performance on IN-1k is reported.</figDesc><table><row><cell>Model</cell><cell>#Params (M)</cell><cell>Train on IN-1k</cell><cell cols="2">Pretrain on IN-21k w/o distill. w/ distill.</cell></row><row><cell>DeiT-Ti [64]</cell><cell>5</cell><cell>72.2</cell><cell>73.0(+0.8)</cell><cell>74.4(+2.2)</cell></row><row><cell>DeiT-S [64]</cell><cell>22</cell><cell>79.9</cell><cell>80.5(+0.6)</cell><cell>82.0(+2.1)</cell></row><row><cell>Swin-T [43]</cell><cell>28</cell><cell>81.2</cell><cell>81.9(+0.7)</cell><cell>83.4(+2.2)</cell></row><row><cell cols="5">gradient clipping with a max norm of 5. The stochastic depth [34] rate is set</cell></row><row><cell cols="5">to 0 for TinyViT-5/11M and 0.1 for 21M, respectively. The data augmentation</cell></row><row><cell cols="5">techniques include random resize and crop, horizontal flip, color jittering, random</cell></row><row><cell>erasing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance of TinyViT-21M w/ and w/o pretraining for linear probe and few-shot image classification.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Linear probe</cell><cell></cell><cell></cell><cell cols="2">5-shot</cell><cell></cell><cell></cell><cell cols="2">20-shot</cell><cell></cell><cell></cell><cell cols="2">50-shot</cell></row><row><cell cols="2"># Training dataset</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>Flowers</cell><cell>Cars</cell><cell>Pets</cell><cell>ISIC</cell><cell>EuroSAT</cell><cell>CropD</cell><cell>ChestX</cell><cell>ISIC</cell><cell>EuroSAT</cell><cell>CropD</cell><cell>ChestX</cell><cell>ISIC</cell><cell>EuroSAT</cell><cell>CropD</cell><cell>ChestX</cell></row><row><cell>0</cell><cell>IN-1k</cell><cell cols="17">91.7 75.2 80.9 56.3 86.5 42.9 82.4 92.2 24.8 56.7 91.0 97.4 29.0 63.7 94.2 98.6 31.8</cell></row><row><cell cols="19">1 IN-1k? 91.7 74.5 82.4 61.7 85.5 43.0 83.0 94.2 24.4 58.5 91.8 97.9 28.6 66.2 94.3 98.9 31.8</cell></row><row><cell cols="19">2 IN-21k 96.3 84.7 99.7 67.7 92.6 52.5 87.4 97.4 24.6 66.5 93.7 99.1 29.4 73.4 95.5 99.5 33.4</cell></row><row><cell cols="19">3 IN-21k? 96.9 86.6 99.7 75.1 93.8 53.5 88.1 98.0 24.7 67.3 93.9 99.3 29.5 74.2 96.0 99.5 33.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison on COCO [41] object detection using Cascade Mask R-CNN [7,27] for 12 epochs. We report the number of parameters of the backbone.</figDesc><table><row><cell>#</cell><cell>Backbone</cell><cell cols="2">#Params IN-1k</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>0</cell><cell>Swin-T [43]</cell><cell>28M</cell><cell>81.2</cell><cell>48.1</cell><cell>67.1</cell><cell>52.1</cell><cell>31.1</cell><cell>51.2</cell><cell>63.5</cell></row><row><cell>1</cell><cell>TinyViT-21M</cell><cell>21M</cell><cell>83.1</cell><cell>49.6 (+1.5)</cell><cell>68.5</cell><cell>54.2</cell><cell>32.3</cell><cell>53.2</cell><cell>64.8</cell></row><row><cell>2</cell><cell>TinyViT-21M?</cell><cell>21M</cell><cell>84.8</cell><cell>50.2 (+2.1)</cell><cell>69.4</cell><cell>54.4</cell><cell>32.9</cell><cell>53.9</cell><cell>65.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>This supplementary material presents the details of Section 3.2. Besides, two extra experiments show how fast the proposed distillation method is and the result of distillation with ground-truth.-Model Architectures. We elaborate the model architectures of TinyViT of Section 3.2. -How fast the distillation method is? We compare the training cost between our proposed fast pretraining distillation and the conventional method, to show the effectiveness of the proposed method. -Distillation with ground-truth. We show why to use soft labels only to distill student models.Our proposed TinyViT architecture is shown in Tab.1. It is a hierarchical structure with 4 stages, for the convenience of dense prediction downstream tasks like Swin [43] and LeViT [24]. The attention biases [24] and a 3 ? 3 depthwise convolution between attention and MLP are introduced to capture local information [69,15]. The factors {? D1?4 , ? N1?4 , ? W2?4 , ? R , ? M , ? E } can be contracted to form tiny model families. We start with a 21M model and generate a set of candidate models around the basic model by adjusting the contraction factors. Then we select models that satisfy both constraints on the number of parameters and throughput, and evaluate them on 99% train and 1% val data sampled from ImageNet-1k train set. The models with the best validation accuracy will be utilized for further reduction in the next step until the target is achieved. In TinyViT model family, all models share the same factors: {? N1 , ? N2 , ? N3 , ? N4 } = {2, 2, 6, 2}, {? W2 , ? W3 , ? W4 } = {7, 14, 7} and {? R , ? M , ? E } = {4, 4, 32}. For the embeded dimensions {? D1 , ? D2 , ? D3 , ? D4 }, TinyViT-21M: {96, 192, 384, 576}, TinyViT-11M: {64, 128, 256, 448} and TinyViT-5M: {64, 128, 160, 320}.</figDesc><table><row><cell>A Model Architectures</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 :</head><label>1</label><figDesc>An elastic base architecture of TinyViT. embed dim ?D 2 , head ?D 2 /?E, window size ?W 2 ? ?W 2 , , head ?D 3 /?E, window size ?W 3 ? ?W 3 , , head ?D 4 /?E, window size ?W 4 ? ?W 4 ,</figDesc><table><row><cell></cell><cell>Block</cell><cell></cell><cell>Configuration</cell><cell></cell><cell>Output</cell></row><row><cell>Patch Embed</cell><cell>Stacked Conv</cell><cell></cell><cell>kernel size 3 ? 3, stride 2, padding 1</cell><cell cols="2">? 2</cell><cell>56 ? 56</cell></row><row><cell>Stage 1</cell><cell>MBConv [33]</cell><cell></cell><cell>embed dim ?D 1 , expansion ratio ?R</cell><cell cols="2">? ?N 1</cell><cell>56 ? 56</cell></row><row><cell>Downsampling</cell><cell>MBConv [33]</cell><cell></cell><cell cols="2">embed dim ?D 1 , stride 2, hidden/output dim ?D 2</cell><cell>? 1</cell><cell>28 ? 28</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>Stage 2</cell><cell>Transformer [65]</cell><cell>?</cell><cell></cell><cell></cell><cell>? ? ?N 2 28 ? 28</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mlp ratio ?M</cell><cell></cell></row><row><cell>Downsampling</cell><cell>MBConv [33]</cell><cell></cell><cell cols="2">embed dim ?D 2 , stride 2, hidden/output dim ?D 3</cell><cell>? 1</cell><cell>14 ? 14</cell></row><row><cell>Stage 3</cell><cell>Transformer [65]</cell><cell>? ?</cell><cell>embed dim ?D 3 mlp ratio ?M</cell><cell></cell><cell>? ? ? ?N 3 14 ? 14</cell></row><row><cell>Downsampling</cell><cell>MBConv [33]</cell><cell></cell><cell cols="2">embed dim ?D 3 , stride 2, hidden/output dim ?D 4</cell><cell>? 1</cell><cell>7 ? 7</cell></row><row><cell>Stage 4</cell><cell>Transformer [65]</cell><cell>? ?</cell><cell>embed dim ?D 4 mlp ratio ?M</cell><cell></cell><cell>? ? ? ?N 4 7 ? 7</cell></row><row><cell>Classifier</cell><cell>AvgPool+LayerNorm+Linear</cell><cell></cell><cell cols="3">output dim: the number of classes</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2. 3d object representations for fine-grained categorization</title>
		<ptr target="https://github.com/facebookresearch/fvcore/" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<title level="m">Are we done with imagenet? arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Bommasani</surname></persName>
		</author>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Glit: Neural architecture search for global and local image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Auto-scaling vision transformers without training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mobile-former: Bridging mobilenet and transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<title level="m">Conditional positional encodings for vision transformers. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchetti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>isic). arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL-HLT</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">NASVit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using deep learning for image-based plant disease detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salath?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in plant science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pcg: A family of simple fast space-efficient statistically good algorithms for random number generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>O&amp;apos;neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<title level="m">Do imagenet classifiers generalize to imagenet? In: ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Imagenet-21k pretraining for the masses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02436</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Talking-heads attention. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Is label smoothing truly incompatible with knowledge distillation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A fast knowledge distillation framework for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Vitas: Vision transformer architecture search. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Understanding and improving knowledge distillation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<title level="m">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Co-scale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Nvit: Vision transformer compression and parameter redistribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">arXiv</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unified visual transformer compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet. ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">A new foundation model for computer vision</title>
		<meeting><address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<publisher>ArXiv</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: From single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Minivit: Compressing vision transformers with weight multiplexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rest: An efficient transformer for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<title level="m">Meta learning for knowledge distillation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Vision transformer pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD Workshop on Model Mining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
