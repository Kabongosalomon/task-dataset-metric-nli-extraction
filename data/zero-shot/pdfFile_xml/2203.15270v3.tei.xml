<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAT: Mask-Aware Transformer for Large Hole Image Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
							<email>wenboli@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<email>zlin@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
							<email>kunzhou@link.cuhk.edu.cnwangyi@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
							<email>luqi@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MAT: Mask-Aware Transformer for Large Hole Image Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Input Ours Input Ours 1 Ours 2 Input Ours 1 Ours 2 Figure 1. The proposed MAT supports photo-realistic and pluralistic large hole image inpainting. The first example is a real-world highresolution image and the other two examples (512 ? 512) are from Places [78] and FFHQ [26] datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recent studies have shown the importance of modeling long-range interactions in the inpainting problem. To achieve this goal, existing approaches exploit either standalone attention techniques or transformers, but usually under a low resolution in consideration of computational cost. In this paper, we present a novel transformer-based model for large hole inpainting, which unifies the merits of transformers and convolutions to efficiently process highresolution images. We carefully design each component of our framework to guarantee the high fidelity and diversity of * Corresponding author recovered images. Specifically, we customize an inpaintingoriented transformer block, where the attention module aggregates non-local information only from partial valid tokens, indicated by a dynamic mask. Extensive experiments demonstrate the state-of-the-art performance of the new model on multiple benchmark datasets. Code is released at https://github.com/fenglinglwb/MAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image completion (a.k.a. inpainting) is a fundamental problem in computer vision, which aims to fill missing regions with plausible contents. It has many applications including image editing <ref type="bibr" target="#b22">[23]</ref>, image re-targeting <ref type="bibr" target="#b8">[9]</ref>, photo restoration <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> and object removal <ref type="bibr" target="#b2">[3]</ref>.</p><p>In inpainting, modeling the contextual information is crucial, especially for large masks. Creating reasonable structures and textures for the missing areas demands contextual understanding, using distant information according to non-local priors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b55">56]</ref> in images. Previous works employ stacked convolutions to reach large receptive fields and model long-range relationships, which works well on aligned (e.g., faces, bodies) and texture-heavy (e.g., forests, water) data. When processing images with complicated structures (i.e., the first example in the 2 nd row in <ref type="figure" target="#fig_10">Figure 1</ref>), it is difficult for fully convolutional neural networks (CNNs) to characterize the semantic correspondences between distant areas. This is mainly due to the inherent properties of CNNs, the slow growth of the effective receptive field and the inevitable dominance of nearby pixels. To explicitly model long-range dependencies in inpainting, <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref> propose to employ attention modules in the CNN-based generator. However, limited by the quadratic computational complexity, the attention module is merely applied to relatively small-scale feature maps with a few times, where long-range modeling is not fully exploited.</p><p>In contrast to applying attention modules to CNNs, transformer <ref type="bibr" target="#b51">[52]</ref> is a natural architecture to handle non-local modeling, where attention is a basic component in every block. Recent advances <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b76">77]</ref> adopt transformer structures to address the inpainting problem. Nonetheless, affected by the complexity issue, existing works only employ transformers to infer low-resolution predictions (e.g. 32 ? 32) for subsequent processing, hence the produced image structure is coarse, compromising the final image quality, especially on large-scale masks.</p><p>In this paper, we develop a new inpainting transformer, capable of generating high-resolution completed results for large mask inpainting. Due to the lack of useful information in some regions (this is common when the given mask rules out most pixels), we find the commonly utilized transformer block (LN?MSA?LN?FFN) exhibits inferior performance in adversarial training. In this regard, we customize the vanilla Transformer block to increase optimization stability and also improve performance, by removing the conventional layer normalization <ref type="bibr" target="#b0">[1]</ref> and replacing the residual learning with fusion learning using feature concatenation. We analyze why these modifications are crucial for learning and empirically demonstrate they are non-trivial. Also, to handle possible heavy interactions between all tokens extracted from the high-resolution input, we propose a new variant of multi-head self-attention (MSA), named multi-head contextual attention (MCA). It computes non-local relations only using partial valid tokens. The selection of adopted tokens is indicated by a dynamic mask, which is initialized by the input mask and updated with spatial constraints and long-range interactions, improving the efficiency at no cost of effectiveness. Additionally, we incorporate a novel style manipulation module into the proposed framework, inherently supporting pluralistic generation. As shown in <ref type="figure" target="#fig_10">Fig. 1</ref>, our method successfully fills large holes with visually realistic and exceptionally diverse contents. Our contributions are summarized as:</p><p>? We develop a novel inpainting framework MAT. It is the first transformer-based inpainting system capable of directly processing high-resolution images.</p><p>? We meticulously design components of MAT. The proposed multi-head contextual attention conducts longrange dependency modeling efficiently by exploiting valid tokens, indicated by a dynamic mask. We also propose a modified transformer block to make training large masks more stable. Moreover, we design a novel style manipulation module to improve diversity.</p><p>? MAT sets new state of the arts on multiple benchmark datasets including Places <ref type="bibr" target="#b77">[78]</ref> and CelebA-HQ <ref type="bibr" target="#b24">[25]</ref>. It also enables pluralistic completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image completion has been a longstanding problem in computer vision. Early diffusion-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> propagate neighboring undamaged information to the holes. Within an internal or external searching space, patch-based or exemplar-based approaches <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref> borrow patches with similar appearance based on human-defined distance metrics to complete missing regions. Patch-Match <ref type="bibr" target="#b2">[3]</ref> proposes a multi-scale patch searching strategy to accelerate the inpainting process. Moreover, partial differential equation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref> and global or local image statistics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref> are vastly studied in the literature. Though traditional methods can often obtain visually realistic results, the lack of high-level understanding hinders them from generating semantically reasonable contents.</p><p>In the few years, deep learning has achieved great success on the image completion. Pathak et al. <ref type="bibr" target="#b41">[42]</ref> bring the adversarial training <ref type="bibr" target="#b15">[16]</ref> to inpainting and utilize an encoder-decoder-based architecture to fill holes. Afterwards, numerous variants <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b68">69]</ref> of the U-Net structure <ref type="bibr" target="#b44">[45]</ref> have been developed for image completion. Besides, more sophisticated networks or learning strategies are proposed to generate high-quality images, including global and local discrimination <ref type="bibr" target="#b21">[22]</ref>, contextual attention <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>, partial <ref type="bibr" target="#b32">[33]</ref> and gated <ref type="bibr" target="#b66">[67]</ref> convolution, etc. Multi-stage generation has also received a great amount of attention, where intermediate clues like object edges <ref type="bibr" target="#b39">[40]</ref>, foreground contours <ref type="bibr" target="#b62">[63]</ref>, structures <ref type="bibr" target="#b43">[44]</ref> and semantic segmentation maps <ref type="bibr" target="#b48">[49]</ref> are extensively exploited. To allow for high-resolution image inpainting, a few attempts have been made to develop progressively generation systems, such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>. Recently, researchers switch their focus to more challenging settings, among which the most representative problems are pluralistic generation and large hole filling. For the former, Zheng et al. <ref type="bibr" target="#b75">[76]</ref> propose a probabilistically principled framework with two parallel paths, capable of producing multiple plausible solutions. UCTGAN <ref type="bibr" target="#b73">[74]</ref> projects the instance image space and masked image space into a common low-dimensional manifold space via optimizing the KL-divergence to allow diverse generations of missing contents. Later on, <ref type="bibr" target="#b54">[55]</ref> and <ref type="bibr" target="#b67">[68]</ref> take advantage of bidirectional attention or auto-regressive transformers to accomplish this goal. Although these methods improve the diversity, their completion and inference performances are limited due to the variational training and rasterscan-order-based generation. On the other hand, some works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b76">77]</ref> are proposed to solve the large hole inpainting problem. For example, CoModGAN <ref type="bibr" target="#b74">[75]</ref> leverages the modulation techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> to handle largescale missing regions. In this work, we develop a novel framework to simultaneously achieve high-quality pluralistic generation and large hole filling, bringing the best of long-range context interaction and unconditional generation to the image completion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a masked image, formulated as I M = I ? M, image completion aims to hallucinate visually appealing and semantically appropriate contents for missing areas. In this work, we present a mask-aware transformer (MAT) for large mask inpainting, supporting conditional long-range interactions. Besides, in light of the ill-posed nature of image completion problem, i.e., there could be numerous plausible solutions to fill the large holes, our approach is designed to support pluralistic generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our proposed MAT architecture consists of a convolutional head, a transformer body, a convolutional tail and a style manipulation module, bringing the merits of transformers and convolutions into full play. Specifically, a convolutional head is used to extract tokens, then the main body with five stages of transformer blocks at varying resolutions (with different numbers of tokens) models long-range interactions via the proposed multi-head contextual attention (MCA). For the output tokens from the body, a convolution-based reconstruction module is adopted to upsample the spatial resolution to the input size. Moreover, we adopt another Conv-U-Net to refine highfrequency details, leaning upon the local texture refinement capability and efficiency of CNNs. At last, we design a style manipulation module, enabling the model to deliver diverse predictions by modulating the weights of convolutions. All components in our method are detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutional Head</head><p>The convolutional head takes in the incompleted image I M and the given mask M, and produces 1 /8 sized feature maps used for tokens. It contains four convolutional layers, one for changing the input dimension and others for downsampling the resolution.</p><p>We utilize a convolutional head mainly for two reasons. First, the incorporation of local inductive priors in early visual processing remains vital for better representation <ref type="bibr" target="#b42">[43]</ref> and optimizability <ref type="bibr" target="#b59">[60]</ref>. On the other hand, it is designed for fast downsampling to reduce computational complexity and memory cost. Also, we empirically find this design is better than the linear projection head used in ViT <ref type="bibr" target="#b12">[13]</ref>, as validated in the supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transformer Body</head><p>The transformer body processes tokens by building longrange correspondences. It contains five stages of the proposed adjusted transformer blocks, with an efficient attention mechanism guided by an additional mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Adjusted Transformer Block</head><p>We propose a new transformer block variant to handle the optimization of masks with large holes. In detail, we remove the layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref> and employ fusion learning (using feature concatenation) instead of residual learning. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we concatenate the input and output of attention and use a fully connected (FC) layer:</p><formula xml:id="formula_0">X ? k,? = FC( [MCA(X k,??1 ), X k,??1 ] ) ,<label>(1)</label></formula><formula xml:id="formula_1">X k,? = MLP(X ? k,? ) ,<label>(2)</label></formula><p>where X k,? is the output of the MLP module of the ?-th block in the k-th stage. After several transformer blocks, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, we adopt a convolution layer with a global residual connection. Note that we abandon the positional embedding in the transformer block since <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b61">62]</ref> have shown that 3 ? 3 convolutions are sufficient to provide positional information for transformers. Thus, the flowing only depends on the feature similarity, which promotes long-range interactions.</p><p>Analysis. The general architecture of transformer <ref type="bibr" target="#b51">[52]</ref> contains two sub-modules, a multi-head self-attention (MSA) module and an MLP module. Layer normalization is applied before every module and a residual connection <ref type="bibr" target="#b19">[20]</ref> after every module. Whereas, we observe unstable optimization using the general block when handling large-scale masks, sometimes incurring gradient exploding. We attribute this training issue to the large ratio of invalid tokens (their values are nearly zero). In this circumstance, layer normalization may magnify useless tokens overwhelmingly, leading to unstable training. Besides, residual learning generally encourages the model to learn high-frequency contents. However, considering most tokens are invalid at the beginning, it is difficult to directly learn high-frequency details without proper low-frequency basis in GAN training, which makes the optimization harder. Replacing such residual learning with concatenation leads to obviously superior results, as verified in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Multi-Head Contextual Attention</head><p>To handle a large number of tokens (up to 4096 tokens for 512 ? 512 images) and low fidelity in the given tokens (at most 90% tokens are useless), our attention module exploits shifted windows <ref type="bibr" target="#b35">[36]</ref> and a dynamical mask, capable of conducting non-local interactions using a few feasible tokens. The output is computed as the weighted sum of valid tokens, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, which is formulated as</p><formula xml:id="formula_2">Att(Q, K, V) = Softmax( QK T + M ? ? d k )V ,<label>(3)</label></formula><p>where Q, K, V are the query, key, value matrices and 1</p><formula xml:id="formula_3">? d k</formula><p>is the scaling factor. The mask M ? is expressed as:</p><formula xml:id="formula_4">M ? ij = 0, if token j is valid , -?, if token j is invalid ,<label>(4)</label></formula><p>where ? is a large positive integer (100 in our experiments). In this case, the aggregation weights of invalid tokens are nearly 0. After each attention, we shift the positions of w ?w sized windows by (? w 2 ?, ? w 2 ?) pixels, enabling crosswindow connections.</p><p>Mask Updating Strategy. The mask (M ? ) points out whether a token is valid or invalid, which is initialized by the input mask and automatically updated during propagation. The updating follows a rule that all tokens in a window are updated to be valid after attention as long as there is at least one valid token before. If all tokens in a window are invalid, they remain invalid after attention. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, going through an attention from (a) to (b), all tokens in the top left window become valid, while tokens in other windows are still invalid. After several times of window shift and attention, the mask is updated to be fully valid.</p><p>Analysis. For images dominated by missing regions, the default attention strategy not only fails to borrow visible information to inpaint the holes, but also undermines the effective valid pixels. To reduce color discrepancy or blurriness, we propose to only involve valid tokens (selected by a dynamic mask) for computing relations. The effectiveness of our design is manifested in Sec 4.3.</p><formula xml:id="formula_5">U S U S U (a) (b) (c) (f) (e) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Style Manipulation Module</head><p>Inspired by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, we design a style manipulation module to endow our framework with pluralistic generation. It manipulates the output by changing the weight normalization of convolution layers in the reconstruction procedure with an additional noise input. To enhance the representation ability of noise inputs, we enforce the imageconditional style s c to learn from both the image feature X and the noise-unconditional style s u , formulated as</p><formula xml:id="formula_6">s u = E(n) ,<label>(5)</label></formula><formula xml:id="formula_7">X ? = B ? X + (1 ? B) ? Resize(s u ) ,<label>(6)</label></formula><formula xml:id="formula_8">s c = F(X ? ) ,<label>(7)</label></formula><p>where B is a random binary mask, on which values are set to 1 with a probability of p and to 0 with 1 ? p, E and F are mapping functions. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the style representation is obtained by fusing both style representations:</p><formula xml:id="formula_9">s = A(s u , s c ),<label>(8)</label></formula><p>where A is a mapping function. Then the weights W of convolutions are baked as</p><formula xml:id="formula_10">W ? ijk = W ijk ? s i ,<label>(9)</label></formula><formula xml:id="formula_11">W ?? ijk = W ? ijk i,k W ? ijk 2 + ? ,<label>(10)</label></formula><p>where i, j, k denotes the input channels, output channels and spatial footprint of the convolution, respectively, and ? is a small constant. The modulation of different style representations leads to pluralistic outputs. Also, we incorporate the noise injection <ref type="bibr" target="#b25">[26]</ref> into our framework to further enhance the diversity of generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>To improve the quality and diversity of the generation, we adopt the non-saturating adversarial loss <ref type="bibr" target="#b15">[16]</ref> for both two stages to optimize our framework, regardless of the pixel-wise MAE or MSE loss that usually leads to averaged blurry results. We also use the R 1 regularization <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref>, written as R 1 = E x ??D(x)?. Besides, we adopt the perceptual loss <ref type="bibr" target="#b23">[24]</ref> with an empirically low coefficient since we notice it enables easier optimization.</p><p>Adversarial Loss. We calculate the adversarial loss as</p><formula xml:id="formula_12">L G = ?Ex [log (D (x))] ,<label>(11)</label></formula><formula xml:id="formula_13">L D = ?E x [log (D (x))] ? Ex [log (1 ? D (x))] ,<label>(12)</label></formula><p>where x andx are the real and generated images. We apply adversarial loss to both two-stage generations in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Perceptual Loss. The perceptual loss is formulated as</p><formula xml:id="formula_14">L P = i ? i ?? i (x) ? ? i (x)? 1 ,<label>(13)</label></formula><p>where ? i (?) denotes the layer activation of a pre-trained VGG-19 <ref type="bibr" target="#b47">[48]</ref> network. We only consider the high-level features of conv 4 4 and conv <ref type="bibr">5 4</ref> , allowing for variations of inpainted results, with scaling coefficients ? i as 1 4 and 1 2 . Overall Loss. The overall loss of the generator is</p><formula xml:id="formula_15">L = L G + ?R 1 + ?L P .<label>(14)</label></formula><p>where ? = 10 and ? = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>We conduct experiments on the Places365-Standard <ref type="bibr" target="#b77">[78]</ref> and the CelebA-HQ <ref type="bibr" target="#b24">[25]</ref> datasets at 512 ? 512 resolution. Specifically, on the Places dataset, we use the 1.8 million and 36.5 thousand images from train and validation sets to train and evaluate our models, respectively. Images are randomly cropped or padded to 512 ? 512 size during training while centrally cropped or padded for evaluation. For CelebA-HQ, train and validation splits are organized with 24,183 and 2,993 images. Though trained on 512 ? 512 images, we show our model generalizes well to a larger resolution in the supplementary material.</p><p>In terms of the large hole setting, following <ref type="bibr" target="#b74">[75]</ref>, we opt for perceptual metrics including FID <ref type="bibr" target="#b20">[21]</ref>, P-IDS <ref type="bibr" target="#b74">[75]</ref> and U-IDS <ref type="bibr" target="#b72">[73]</ref> for evaluation. We suggest that it is inappropriate to use the pixel-wise L1 distance, PSNR and SSIM <ref type="bibr" target="#b57">[58]</ref>, since preliminary works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref> have shown that these metrics correlate weakly with human perception regarding image quality, especially for the ill-posed large-scale image completion problem. Though LPIPS <ref type="bibr" target="#b72">[73]</ref> is calculated in the deep feature space, the pixel-wise evaluation still greatly punishes diverse inpainting systems for large holes. Thus we only use it for reference in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In our framework, we set the numbers of convolution channels and FC dimensions to 180 for the head, body, and reconstruction modules. The block numbers and window sizes of 5-level transformer groups are {2, 3, 4, 3, 2} and {8, 16, 16, 16, 8}, respectively. The last Conv-U-Net firstly downsamples the resolution to <ref type="bibr">1 32</ref> and then upsamples to the original size, where the numbers of convolution layers and channels at different scales are borrowed from <ref type="bibr" target="#b26">[27]</ref>. The mapping network consists of 8 FC layers and the style manipulation module is implemented with convolutions followed with an AvgPool layer. Different from <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b76">77]</ref>, our transformer architecture is without pre-training.</p><p>All experiments are carried out on 8 NVidia V100 GPUs. Following <ref type="bibr" target="#b74">[75]</ref>, we train our models for 50M images on Places365-Standard and 25M images on CelebA-HQ. The batch size is 32. We adopt an Adam optimizer with ? 1 = 0 and ? 2 = 0.99 and set the learning rate to 1 ? 10 ?3 . The free-form mask is described in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we tease apart which components of our framework contribute most to the final performance. To enable a quick exploration, we only use 100K training images in Places <ref type="bibr" target="#b77">[78]</ref> (? 5.6%) at 256 ? 256 resolution and train the models for 5M samples (10% of the full setting). We adopt the first 10K validation images for evaluation. The  <ref type="table">Table 1</ref>. Ablation study of the framework components. "A" represents our full model. "B" replaces transformers with convolutions. "C" replaces our adjusted transformer block with the original design <ref type="bibr" target="#b51">[52]</ref>. "D" means using the conventional attention strategy. "E" removes the noise style manipulation. "F" limits the output size of first-stage generation to 64 ? 64.</p><p>quantitative comparison is shown in <ref type="table">Table 1</ref>.</p><p>Conv-Transformer Architecture. We explore whether the long-range context relations modeled by transformers are useful for filling large holes. Replacing the transformer blocks with convolution blocks (model "B" in <ref type="table">Table 1</ref>), we find an obvious performance drop on all metrics, especially on P-IDS and U-IDS, indicating that the inpainted images lose some fidelity. Moreover, we show some visual examples in <ref type="figure" target="#fig_4">Fig. 5</ref>. Compared to the fully convolutional network, our MAT takes advantage of distant context to reconstruct the structure of net and texture of dinosaur skeleton well, showing the effectiveness of long-range interactions.</p><p>Adjusted Transformer Block. In our framework, we develop a novel transformer block since the conventional design easily leads to unstable optimization, in which case we need to lower the learning rate of transformer body. As illustrated in <ref type="table">Table 1</ref>, our design (model "A") obtains superior performance, 0.39 improvement on FID, than model "C" with the original transformer block. As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, we notice our design produces more visually appealing results, supporting high-quality image completion. Especially for the first example, even though the missing area is extremely large, our method can still recover a semantically consistent and visually realistic indoor scene.</p><p>Multi-Head Contextual Attention. To quickly fill the missing regions with realistic contents, we propose a multihead contextual attention (MCA). To make a deeper understanding, we build a model without partial aggregation from valid tokens. The quantitative results are shown as model "D" in <ref type="table">Table 1</ref>. It is noted that FID drops by 0.1 yet other metrics do not change too much. We suggest the proposed contextual attention is helpful for maintaining color consistency and reducing blurriness. As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, the model without MCA generates contents with incorrect colors for the first example, while producing blurry artifacts for the second example. Both the quantitative and qualitative results validate the power of our MCA.</p><p>Style Manipulation Module. To deal with large masks, apart from the conditional long-range interaction, we also introduce unconditional generation. To quantify the unconditional generative capability of our framework, we strip the noise style manipulation. From the results of model "E" in <ref type="table">Table 1</ref>, we find a large gap on P-IDS and U-IDS, showing the modulation of stochastic noise styles further improves the naturalness of completed images.</p><p>High Resolution in Reconstruction. Due to quadratically increased computational complexity, existing works <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b76">77]</ref> adopt transformers to synthesize low-resolution results, typically 32 ? 32, for subsequent processing. By contrast, our MAT architecture takes advantage of its computational efficiency to enable high-resolution outputs in the recon- struction stage. As illustrated in <ref type="table">Table 1</ref>, our full model "A" achieves significant improvement over model "F", demonstrating the importance of high-resolution prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State of the Arts</head><p>We compare the proposed MAT with a number of stateof-the-art approaches. For a fair comparison, we use publicly available models to test on the same masks. As illustrated in <ref type="table">Table 2</ref>, MAT achieves state-of-the-art performance on both CelebA-HQ and Places. Especially, even if we only use a subset Places365-Standard (1.8M images) to train our model, much fewer than CoModGAN <ref type="bibr" target="#b74">[75]</ref> (8M images) and Big LaMa <ref type="bibr" target="#b50">[51]</ref> (4.5M images), MAT still yields promising results. Besides, our method is much more parameter-efficient than the second-best CoModGAN and transformer-based ICT <ref type="bibr" target="#b54">[55]</ref>. As illustrated in <ref type="figure" target="#fig_9">Fig 8,</ref> compared to other methods, the proposed MAT restores more photo-realistic images with fewer artifacts. For example, our method successfully recovers visually pleasing flowers and regular building structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Pluralistic Generation</head><p>The inherent diversity of our framework mainly sources from the style manipulation. As shown in <ref type="figure">Fig. 6</ref>, style variants lead to different completions. From the first example in <ref type="figure">Fig. 6</ref>, we observe a change from a pursed smile to a toothy laugh. And the second example shows different face contours and appearances. As for the final one, we find different window and roof structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations and Failure Cases</head><p>Trained without semantic annotations, MAT usually struggles when processing objects with a variety of shapes, e.g., running animals. As shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, our method fails to recover the cat and car due to the lack of semantic context understanding. Also, limited by the downsampling and pre-   <ref type="table">Table 2</ref>. Quantitative comparison on Places <ref type="bibr" target="#b77">[78]</ref> and CelebA-HQ <ref type="bibr" target="#b24">[25]</ref>. " ?": Our Mat, CoModGAN <ref type="bibr" target="#b74">[75]</ref> and LaMa <ref type="bibr" target="#b50">[51]</ref>   defined window sizes in attention, we need to pad or resize an image to make its size a multiple of 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a mask-aware transformer (MAT) for pluralistic large hole image inpainting. Taking advantage of the proposed adjusted transformer architecture and partial attention mechanism, the proposed MAT achieves state-ofthe-art performance on multiple benchmarks. Also, we design a style modulation module to improve the diversity of generation. Extensive qualitative comparisons have demonstrated the superiority of our framework in terms of image quality and diversity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>As illustrated in Sec. 3.1, the proposed MAT is a twostage framework, where the first stage consists of a convolutional head, a transformer body and a convolutional reconstruction tail while the second stage is a Conv-U-Net. And the discriminator follows the design of CoModGAN <ref type="bibr" target="#b74">[75]</ref>.</p><p>Given an H ? W input, the head first applies a convolution to change the number of channels from 4 (image 3 + mask 1) to 180 and then adopts three strided convolutions (stride = 2) to downsample the feature size to H 8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Free-Form Mask Sampling and Statistics</head><p>Referring to DeepFill v2 <ref type="bibr" target="#b66">[67]</ref>, we sample rectangles and brush strokes with random sizes, shapes and locations to generate free-form masks. During training, we use a large mask sampling strategy. The number of up to full-size or half-size rectangles is uniformly sampled within [0, 3] or [0, 5]. The number of strokes is randomly sampled within <ref type="bibr">[0,</ref><ref type="bibr" target="#b8">9]</ref>, with a random brush width within <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b47">48]</ref> and vertex number within <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>. During testing, apart from the large mask setup, we also introduce a small mask sampling strategy, where the number of up to full-size or half-size rectangles is within [0, 2] or [0, 3] and the number of strokes is within [0, 4], while other settings remain unchanged. Note that our model is trained on large masks and is evaluated on both small and large mask settings. As shown in <ref type="figure" target="#fig_0">Fig. A.2</ref>, we present the mask statistics on the Places Val set <ref type="bibr" target="#b77">[78]</ref> that is used for evaluation. It is observed that large masks are very aggressive and diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tokenization</head><p>As described in Sec. A, we adopt a stack of convolutions (the convolutional head) to extract tokens for the transformer body, which is specially tailored to the inpainting problem. Compared to the linear projection of ViT <ref type="bibr" target="#b12">[13]</ref>, our design owns two merits. First, stacked convolutions can gradually fill the holes, producing more effective tokens. Second, the multi-scale downsampled features can be passed to the decoder through shortcut connections, improving the optimization. As illustrated in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Configuration</head><p>Following the same experimental setting as ablation study, we explore several model variants in terms of feature width, block number and window size of the transformer body, leaving Conv-U-Net unchanged. The results are shown in the <ref type="table">Table D.</ref>2. The performance is positively correlated to the model capacity and attention range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. CelebA-HQ 256 ? 256 Results</head><p>We provide the quantitative results on 256?256 CelebA-HQ <ref type="bibr" target="#b24">[25]</ref>. As illustrated in <ref type="table">Table F</ref>.3, our MAT yields significant improvements on FID <ref type="bibr" target="#b20">[21]</ref>, P-IDS <ref type="bibr" target="#b74">[75]</ref> and U-IDS <ref type="bibr" target="#b72">[73]</ref> metrics over other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. LPIPS Results</head><p>As discussed in Sec. 4.1, LPIPS <ref type="bibr" target="#b72">[73]</ref> is not an appropriate measure for large mask inpainting, especially for pluralistic generation systems, since there could be numerous plausible solutions to fill the holes. Therefore, we provide the LPIPS results only for reference. As shown in <ref type="table">Table F</ref>.4, our method achieves superior or comparable performance on the CelebA-HQ <ref type="bibr" target="#b24">[25]</ref> and Places <ref type="bibr" target="#b77">[78]</ref> datasets. Note that we only use 22.5% of full data to train our Places model.  <ref type="bibr" target="#b24">[25]</ref> and Places <ref type="bibr" target="#b77">[78]</ref> datasets. " ?": CoModGAN <ref type="bibr" target="#b74">[75]</ref> and LaMa <ref type="bibr" target="#b50">[51]</ref> use 8M and 4.5M Places images to train their models, while our model is only trained on Places365-Standard (1.8M images). The LaMa models on CelebA-HQ and Places are different in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Generalization to A Higher Resolution</head><p>Though trained on 512 ? 512 images, our model generalizes well to larger resolutions. For example, we transfer our model and Big LaMa <ref type="bibr" target="#b50">[51]</ref> trained at 512 ? 512 resolution to 1024 ? 1024. Compared to Big LaMa (FID? 6.31, PIDS? 4.98%), our model (FID? 5.83, P-IDS? 9.51%) obtains superior results on Places under the large mask setting. We suggest that maintaining a resolution consistency during training and testing yields better visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Diversity-Fidelity Tradeoff</head><p>To evaluate the fidelity and diversity, apart from FID (depending on both diversity and fidelity), we also follow [?,?] to use Improved Precision and Recall to separately measure sample fidelity (precision) and diversity (recall). As shown in <ref type="table">Table H</ref>.5, our method obtains better FID, higher recall yet slightly lower precision compared to CoModGAN on Places. It is noted that we use much less training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Additional Qualitative Results</head><p>We present more visual comparisons on the Places <ref type="bibr" target="#b77">[78]</ref> dataset between our MAT and other state-of-the-art methods. As shown in <ref type="figure" target="#fig_3">Fig J.4 and Fig J.5</ref>, our method generates more photo-realistic results with few artifacts, manifesting the effectiveness of MAT. Due to potential copyright issues with CelebA-HQ <ref type="bibr" target="#b24">[25]</ref>, we do not provide visual comparisons on this dataset. If necessary, you can process CelebA-HQ images with the provided code and model, or contact the authors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Licenses of Face Images</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The proposed mask-aware transformer (MAT) for pluralistic inpainting, which consists of a convolutional head, a transformer body and a convolutional tail for reconstruction together with a Conv-U-Net for refinement. The mask updating strategy is described in Sec. 3.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Structure of a single transformer stage. "TB" refers to an adjusted transformer block and "MCA" represents the proposed multi-head contextual attention. The valid tokens are denoted as ? and invalid tokens are ?. The blue arrow indicates the output of attention is computed as the weighted sum of valid tokens (indicated by blue arrows) while ignoring invalid tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Toy example of mask updating. The feature map is initially partitioned into 2 ? 2 windows (in orange). "U" means a mask updating after attention and "S" indicates the window shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual examples for ablation study. Model A is our full model, while model B, C, D refer to models replacing transformers with convolutions, using the conventional transformer block and multi-head attention, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Input Ours 2 1 3 Figure 6 .</head><label>2136</label><figDesc>Ours Ours Visual examples with different style representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Failure cases of our method (MAT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>IDS(%)? U-IDS(%)? FID? P-IDS(%)? U-IDS(%)? FID? P-IDS(%)? U-IDS(%)? FID? P-IDS(%)? U-IDS(%)? MAT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>use 8M, 8M and 4.5M training images on Places, respectively, while our other model (without " ?") is only trained on a subset (1.8M images). The LaMa models on Places and CelebA are different in size. The results of LPIPS and 256 ? 256 CelebA are provided in the supplementary. The best and second best results are in red and blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative comparison (512?512) with state-of-the-art methods. Our results are more visually realistic, containing more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A. 1 .</head><label>1</label><figDesc>Examples of free-form masks (512 ? 512). Visible and invisible pixels are in white and black colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A. 2 .</head><label>2</label><figDesc>Small and large mask (512 ? 512) statistics on the Places Val set<ref type="bibr" target="#b77">[78]</ref>. The are totally 36500 masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure J. 4 .Figure J. 5 .</head><label>45</label><figDesc>Qualitative comparison (512 ? 512) with state-of-the-art methods on the Places dataset. Zoom in for a better view. Qualitative comparison (512 ? 512) with state-of-the-art methods on the Places dataset. Zoom in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>W 8 . The feature is transformed to tokens as input to the transformer body. The body is composed of five stages of transformer blocks, where the block numbers are {2, 3, 4, 3, 2} and the corresponding feature sizes are</figDesc><table><row><cell>{ H 8 ? W 8 , H 16 ? W 16 , H 32 ? W 32 , H 16 ? W 16 , H 8 ? W 8 }. The downsam-</cell></row><row><cell>pling and upsampling are realized by convolutions. The de-</cell></row><row><cell>tailed structure of a transformer block is shown in Sec. 3.3.</cell></row><row><cell>Then the output tokens from the body are converted to a 2D</cell></row><row><cell>feature, passed to the reconstruction tail. The convolutional</cell></row><row><cell>tail upsamples the feature size from H 8 ? W 8 to H ? W and</cell></row><row><cell>generates a completed image, during which style modula-</cell></row><row><cell>tion is applied to all layers to enable pluralistic generation.</cell></row><row><cell>The second-stage Conv-U-Net takes in the coarse predic-</cell></row><row><cell>tion and the input mask for subsequent high-fidelity detail</cell></row><row><cell>rendering. It first downsamples the feature size to H 32 ? W 32</cell></row><row><cell>and then upsamples the size back to H ? W . Shortcut</cell></row><row><cell>connections are adopted at each resolution. The number</cell></row><row><cell>of convolution channels in the encoder starts from 64 and</cell></row><row><cell>is doubled after each downsampling, with a maximum of</cell></row><row><cell>512, while the decoder uses a symmetrical setting. Besides,</cell></row><row><cell>all decoding layers are modulated by the image-conditional</cell></row><row><cell>and noise-unconditional style representations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table C.1 andFig. C.3, stacked convolutions obtain obviously superior results. The model using linear projection is more likely to generate unpleasing artifacts and fail to borrow surrounding textures to fill the holes, while our MAT successfully recovers high-fidelity contents. Both the quantitative and qualitative results demonstrate the effectiveness of our MAT.Table C.1. Quantitative comparison between linear projection and stacked convolutions for token extraction. We use the same training setting as the ablation study (Sec. 4.3).</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="4">FID? P-IDS (%)? U-IDS(%)?</cell></row><row><cell cols="2">Stacked Conv. (Ours)</cell><cell>5.97</cell><cell>13.17</cell><cell cols="2">29.23</cell></row><row><cell cols="2">Linear Projection</cell><cell>10.54</cell><cell>5.77</cell><cell cols="2">20.86</cell></row><row><cell>Input</cell><cell cols="2">Linear Proj.</cell><cell>Ours</cell><cell>GT</cell></row><row><cell cols="6">Figure C.3. Qualitative comparison between linear projection and</cell></row><row><cell cols="4">stacked convolutions (ours) for tokenization.</cell><cell></cell></row><row><cell cols="2">Model Feature Dim.</cell><cell>Block Num.</cell><cell>Window Size</cell><cell cols="2">FID?</cell></row><row><cell>Ours</cell><cell>180</cell><cell cols="3">{2, 3, 4, 3, 2} {8, 16, 16, 16, 8}</cell><cell>5.97</cell></row><row><cell>V1</cell><cell>90</cell><cell cols="3">{2, 3, 4, 3, 2} {8, 16, 16, 16, 8}</cell><cell>6.28</cell></row><row><cell>V2</cell><cell>180</cell><cell cols="3">{1, 1, 2, 1, 1} {8, 16, 16, 16, 8}</cell><cell>6.18</cell></row><row><cell>V3</cell><cell>180</cell><cell>{2, 3, 4, 3, 2}</cell><cell>{8, 8, 8, 8, 8}</cell><cell></cell><cell>6.09</cell></row></table><note>Table D.2. Ablation study on model configuration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>IDS? U-IDS? FID? P-IDS? U-IDS? MAT (Ours) 2.94 20.88 32.01 5.16 13.90 25.13 Table F.3. Quantitative results on CelebA-HQ at 256 ? 256 size. The results of P-IDS and U-IDS are shown in percentage (%).</figDesc><table><row><cell cols="6">Small Mask FID? P-LaMa [51] Method 3.98 8.82 22.57 8.75 2.34 Large Mask 8.77</cell></row><row><cell>ICT [55]</cell><cell></cell><cell>5.24 4.51</cell><cell cols="3">17.39 10.92 0.90</cell><cell>5.23</cell></row><row><cell>MADF [79]</cell><cell cols="2">10.43 6.25</cell><cell cols="3">14.62 23.59 0.50</cell><cell>1.44</cell></row><row><cell cols="3">AOT GAN [70] 9.64 5.61</cell><cell cols="3">14.62 22.91 0.47</cell><cell>1.65</cell></row><row><cell cols="3">DeepFill v2 [67] 5.69 6.62</cell><cell cols="3">16.82 13.23 0.84</cell><cell>2.62</cell></row><row><cell cols="3">EdgeConnect [40] 5.24 5.61</cell><cell cols="3">15.65 12.16 0.84</cell><cell>2.31</cell></row><row><cell>Method</cell><cell></cell><cell>#Param. ?10 6</cell><cell cols="3">CelebA-HQ Small Large Small Large Places</cell></row><row><cell>MAT (Ours)</cell><cell></cell><cell>60</cell><cell cols="3">0.065 0.125 0.099 0.189</cell></row><row><cell cols="2">CoModGAN [75]  ?</cell><cell>109</cell><cell cols="3">0.073 0.140 0.101 0.192</cell></row><row><cell>LaMa [51]  ?</cell><cell></cell><cell>27/51</cell><cell cols="3">0.075 0.143 0.086 0.166</cell></row><row><cell>ICT [55]</cell><cell></cell><cell>150</cell><cell cols="2">0.105 0.195</cell><cell>-</cell><cell>-</cell></row><row><cell>MADF [79]</cell><cell></cell><cell>85</cell><cell cols="3">0.068 0.130 0.095 0.181</cell></row><row><cell>AOT GAN [70]</cell><cell></cell><cell>15</cell><cell cols="3">0.074 0.145 0.101 0.195</cell></row><row><cell>HFill [65]</cell><cell></cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>0.148 0.284</cell></row><row><cell>DeepFill v2 [67]</cell><cell></cell><cell>4</cell><cell cols="3">0.117 0.221 0.113 0.213</cell></row><row><cell cols="2">EdgeConnect [40]</cell><cell>22</cell><cell cols="3">0.101 0.208 0.114 0.275</cell></row></table><note>Table F.4. LPIPS [73] comparison on 512 ? 512 CelebA-HQ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table H.5. Precision and Recall results of our MAT and Co-ModGAN on Places.</figDesc><table><row><cell>Method</cell><cell cols="4">Training Data FID? Precision? Recall?</cell></row><row><cell>MAT (Ours)</cell><cell>1.8M</cell><cell>2.90</cell><cell>0.925</cell><cell>0.951</cell></row><row><cell>CoModGAN</cell><cell>8M</cell><cell>2.92</cell><cell>0.929</cell><cell>0.942</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Strong-continuation, contrast-invariant inpainting with a third-order optimal pde</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1934" to="1938" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly-and self-supervised learning for content-aware deep image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object removal by exemplar-based inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>II-II</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image inpainting using nonlocal texture matching and nonlinear filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundaresh</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">J</forename><surname>Rodr?guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1705" to="1719" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inpainting and zooming using sparse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed-Jalal</forename><surname>Fadili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fionn</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="79" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple pyramids based image inpainting using local patch statistics and steering kernel feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmoy</forename><surname>Ghorai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumitra</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sekhar</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhabatosh</forename><surname>Chanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5495" to="5509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets. NIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A combined pde and texture synthesis approach to inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Grossauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive image inpainting with full-resolution residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2496" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. NIPS, 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sc-fegan: Face editing generative adversarial network with user&apos;s sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1745" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Examplar-based inpainting based on local geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josselin</forename><surname>Olivier Le Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Gautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Laplacian patchbased image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><surname>Ho Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2727" to="2735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning how to inpaint from global image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent feature reasoning for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7760" to="7768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking image inpainting via a mutual encoderdecoder with feature equalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="725" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coherent semantic attention for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4170" to="4179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Region-wise generative adversarial imageinpainting for large missing areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12507</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ebrahimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00212</idno>
		<title level="m">Generative image inpainting with adversarial edge learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Onion-peel networks for deep video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4403" to="4412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08810</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structureflow: Image inpainting via structure-aware appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeji</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03356</idno>
		<title level="m">Spg-net: Segmentation prediction and guidance network for image inpainting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="861" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Mashikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Remizova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Silvestrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naejin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshith</forename><surname>Goka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07161</idno>
		<title level="m">Resolution-robust large mask inpainting with fourier convolutions</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bringing old photos back to life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2747" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Old photo restoration via deep latent space translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07047</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">High-fidelity pluralistic image completion with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14031</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image inpainting via generative multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<title level="m">Piotr Doll?r, and Ross Girshick. Early convolutions help transformers see better</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image inpainting with learnable bidirectional attention maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8858" to="8867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Foreground-aware image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5840" to="5848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Shift-net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Contextual residual aggregation for ultra high-resolution image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daesik</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongliang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiying</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12335</idno>
		<title level="m">Diverse image inpainting with bidirectional and autoregressive transformers</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning pyramid-context encoder network for highquality image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Aggregated contextual transformations for high-resolution image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01431</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting with iterative confidence feedback and guided upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with progressive generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1939" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Uctgan: Diverse image inpainting based on unsupervised cross-space translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5741" to="5750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Large scale image completion via co-modulated generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pluralistic image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1438" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Tfill: Image completion via a transformer-based architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00845</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Image inpainting by end-to-end cascaded refinement with mask awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">All face images used in the paper and supplementary material are from the FFHQ [26] dataset. Here we provide the detailed information on source and license</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<idno>license: CC BY-NC 2.0</idno>
		<ptr target="https://creativecommons.org/licenses/by-nc/2.0/" />
		<title level="m">? Face image in Fig.1 of main paper</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<idno>license: CC BY 2.0</idno>
		<ptr target="https://creativecommons.org/licenses/by/2" />
		<title level="m">? Face image in Fig.2 of main paper</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<idno>license: CC BY-NC 2.0</idno>
		<ptr target="https" />
		<title level="m">? The first face image in Fig.6 of main paper</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">? The second face image in Fig.6 of main paper</title>
		<idno>license: CC BY 2.0</idno>
		<ptr target="https://creativecommons.org/licenses/by/2" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
