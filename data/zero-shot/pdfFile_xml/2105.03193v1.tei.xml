<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 NETWORK PRUNING THAT MATTERS: A CASE STUDY ON RETRAINING VARIANTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duong</forename><forename type="middle">H</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binh-Son Hua VinAI Research and VinUniversity</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Vietnam</roleName><forename type="first">Vinai</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Binh-Son Hua VinAI Research and VinUniversity</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 NETWORK PRUNING THAT MATTERS: A CASE STUDY ON RETRAINING VARIANTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy <ref type="bibr" target="#b26">(Renda et al., 2020)</ref>, but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule <ref type="bibr" target="#b27">(Smith &amp; Topin, 2019)</ref>. By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining -a detail often overlooked by practioners during the implementation of network pruning.</p><p>However, the use of large networks exacerbate the gap between research and practice since real-world applications usually require running neural networks in low-resource environments for numerous purposes: reducing memory, latency, energy consumption, etc. To adopt those networks to resourceconstrained devices, network pruning <ref type="bibr" target="#b12">(LeCun et al., 1990;</ref><ref type="bibr" target="#b6">Han et al., 2015;</ref> is often exploited to remove dispensable weights, filters and other structures from neural networks. The goal of pruning is to reduce overall computational cost and memory footprint without inducing significant drop in performance of the network.</p><p>A common approach to mitigating performance drop after pruning is retraining: we continue to train the pruned models for some more epochs. In this paper, we are interested in approaches based on learning rate schedules to control the retraining. A well-known practice is fine-tuning, which aims to train the pruned model with a small fixed learning rate. More advanced learning rate schedules exist, which we generally refer to as retraining. The retraining step is a critical part in implementing network pruning, but it has been largely overlooked and tend to vary in each implementation including differences in learning rate schedules, retraining budget, hyperparameter choices, etc.</p><p>Recently, <ref type="bibr" target="#b26">Renda et al. (2020)</ref> proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding (LRW). Specifically, instead of fine-tuning the pruned networks with a fixed learning rate, usually the last learning rate from the original training schedule <ref type="bibr" target="#b6">(Han et al., 2015;</ref>, the authors suggested using the learning rate schedule from the previous t epochs (i.e. rewinding). This seemingly subtle change in learning rate schedule led to an important result: LRW was shown to achieve comparable performance to more complex and computationally expensive 1 arXiv:2105.03193v1 [cs.LG] 7 May 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Training neural networks is an everyday task in the era of deep learning and artificial intelligence. Generally speaking, given data availability, large and cumbersome networks are often preferred as they have more capacity to exhibit good data generalization. In the literature, large networks are considered easier to train than small ones <ref type="bibr" target="#b24">(Neyshabur et al., 2018;</ref><ref type="bibr" target="#b0">Arora et al., 2018;</ref><ref type="bibr" target="#b25">Novak et al., 2018;</ref><ref type="bibr" target="#b1">Brutzkus &amp; Globerson, 2019)</ref>. Thus, many breakthroughs in deep learning are strongly correlated to increasingly complex and over-parameterized networks.</p><p>Published as a conference paper at ICLR 2021 pruning algorithms while only utilizing simple norm-based pruning. Unfortunately, the authors did not provide the analysis to justify the improvement. In general, it is intriguing to understand the importance of a learning rate schedule and how it affects the final performance of a pruned model.</p><p>In this work, we study the behavior of pruned networks under different retraining settings. We found that the efficacy from retraining with learning rate rewinding is rooted in the use of a large learning rate, which helps pruned networks to converge faster after pruning. We demonstrate that the success of learning rate rewinding over fine-tuning is not exclusive to the learning rate schedule coupling with the original training process. Retraining with a large learning rate could outperform fine-tuning even with some modest retraining, e.g., for a few epochs, and regardless of network compression ratio.</p><p>We argue that retraining is of paramount importance to regain the performance in network pruning and should not be overlooked when comparing two pruning algorithms. This is evidenced by our extensive experiments: (1) randomly pruned network can outperform methodically pruned network with only (hyper-parameters free) modifications of the learning rate schedule in retraining, and (2) a simple baseline such as norm-based pruning can perform as well as as other complex pruning methods by using a large learning rate restarting retraining schedule.</p><p>The contributions of our work are as follows.</p><p>? We document a thorough experiment on learning rate schedule for the retraining step in network pruning with different pruning configurations;</p><p>? We show that learning rate matters: pruned models retrained with a large learning rate consistently outperform those trained by conventional fine-tuning regardless of specific learning rate schedules;</p><p>? We present a novel and counter-intuitive result achieved by solely applying large learning rate retraining: a randomly pruned network and a simple norm-based pruned network can perform as well as networks obtained from more sophisticated pruning algorithms.</p><p>Given the significant impact of learning rate schedule in network pruning, we advocate the following practices: learning rate schedule should be considered as a critical part of retraining when designing pruning algorithms. Rigorous ablation studies with different retraining settings should be made for a fair comparison of pruning algorithms. To facilitate reproducibility, we would release our implementation upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY AND METHODOLOGY</head><p>Pruning is a common method to produce compact and high performance neural networks from their original large and cumbersome counterparts.We can categorize pruning approaches into three classes: Pruning after training -which consists of three steps: training the original network to convergence, prune redundant weights based on some criteria, and retrain the pruned model to regain the performance loss due to pruning <ref type="bibr" target="#b6">Han et al., 2015;</ref><ref type="bibr" target="#b21">Luo et al., 2017;</ref><ref type="bibr" target="#b32">Ye et al., 2018;</ref><ref type="bibr" target="#b30">Wen et al., 2016;</ref><ref type="bibr" target="#b9">He et al., 2017)</ref>; Pruning during training -we update the "pruning mask" while training the network from scratch, thus, allowing pruned neurons to be recovered <ref type="bibr" target="#b35">(Zhu &amp; Gupta, 2017;</ref><ref type="bibr" target="#b11">Kusupati et al., 2020;</ref><ref type="bibr" target="#b31">Wortsman et al., 2019;</ref><ref type="bibr" target="#b19">Lin et al., 2020b;</ref><ref type="bibr" target="#b8">He et al., 2019;</ref>; Pruning before training -Inspired by the Lottery Ticket Hypothesis <ref type="bibr" target="#b3">(Frankle &amp; Carbin, 2019)</ref>, some recent works try to find the sparsity mask at initialization and train the pruned network from scratch without changing the mask <ref type="bibr" target="#b13">(Lee et al., 2019;</ref><ref type="bibr" target="#b28">Tanaka et al., 2020;</ref>.</p><p>In this work, we are mainly concerned with the first category i.e. pruning after training which has the largest body of work to our knowledge. Traditionally, the last step is referred to as fine-tuning, i.e., continue to train the pruned model with a small learning rate obtained from the last epoch of the original model. This seemly subtle step is often overlooked when designing pruning algorithms.</p><p>Particularly, we found that the implementation of previous pruning algorithms have many notable differences in their retraining step: some employed a small value of learning rate (e.g. 0.001 on ImageNet) to fine-tune the network <ref type="bibr" target="#b22">(Molchanov et al., 2016;</ref><ref type="bibr" target="#b6">Han et al., 2015)</ref> for a small number of epochs, e.g., 20 epochs in the work by ; some used a larger value of learning rate (0.01) with much longer retraining budgets, e.g., 60, 100 and 120 epochs respectively on ImageNet <ref type="bibr" target="#b36">(Zhuang et al., 2018;</ref><ref type="bibr" target="#b5">Gao et al., 2020;</ref><ref type="bibr" target="#b14">Li et al., 2020)</ref>  <ref type="figure">Figure 1</ref>: Learning rate with different schedules on CIFAR when retraining for 72 epochs. In (a), the learning rate is fixed to the last learning rate of original training (i.e. 0.001). In (b), the learning rate is "rewound" to previous 72 epochs (which is 0.01), and is dropped to 0.001 after 32 epochs. In (c), after warming up the learning rate, we drop its value by the factor of 10? at 50% and 75% of remaining epochs. In (d), we warm up the learning rate from the lowest to the highest value (of standard training) for the first few epochs, then decay the learning rate according to cosine function.</p><p>instead of conventional step-wise schedule. Despite such difference, the success of each pruning algorithm is only attributed to the pruning algorithm itself. This motivates us to ask the question: do details like learning rate schedule used for retraining matter?</p><p>In this section, we strive to understand the behavior of pruned models under different retraining configurations and how they impact the final performance. Specifically, we conduct experiments with different retraining schedules on simple baselines such as 1 -norm filters pruning  and magnitude-based weights pruning <ref type="bibr" target="#b6">(Han et al., 2015)</ref>. We show that the efficacy of several pruning algorithms can be boosted by simply modifying the learning rate schedule. More importantly, the performance gain by retraining can be remarkable: the accuracy loss can drop to zero and in some cases better accuracy than baseline models can be achieved.</p><p>To analyze the effect of retraining a pruned network, we based on learning rate rewinding <ref type="bibr" target="#b26">(Renda et al., 2020)</ref> and experiment with different retraining settings. Although in the previous work, <ref type="bibr" target="#b26">Renda et al. (2020)</ref> demonstrated the efficacy of learning rate rewinding across datasets and pruning criteria, there is a lack of understanding of the actual reason behind the success of this technique. Here we hypothesize that the initial pruned network is a suboptimal solution, staying in a local minima. Learning rate rewinding succeeds because it uses a large learning rate to encourage the pruned networks to converge to another, supposingly better, local minima. Our experiment setups are as follows.</p><p>Retraining techniques. To verify this conjecture empirically, we conduct experiments with different learning rate schedules including learning rate rewinding <ref type="bibr" target="#b26">(Renda et al., 2020)</ref> while varying pruning algorithms, network architectures and datasets. In this work, we consider the following retraining techniques:</p><p>1. FINE-TUNING (FT) Fine-tuning is the most common retraining techniques <ref type="bibr" target="#b6">(Han et al., 2015;</ref>. In this approach, we continue train the pruned networks for t epochs with the last (smallest) learning rate of original training.</p><p>2. LEARNING RATE REWINDING (LRW) <ref type="bibr" target="#b26">Renda et al. (2020)</ref> propose to reuse the learning rate schedule of the original training when retraining pruned networks. Specifically, when retraining for t epochs, we reuse the learning rate schedule from the previous t epochs, i.e., rewinding.</p><p>3. SCALED LEARNING RATE RESTARTING (SLR): In this approach, we employ the learning rate schedule that is proportionally identical to the standard training. For example, the learning rate is dropped by a factor of 10? at 50% and 75% of retraining epochs on CIFAR, which is akin to original training learning rate adjustment. The original learning rate schedule can be found in Appendix A.</p><p>4. CYCLIC LEARNING RATE RESTARTING (CLR): Instead of using stepwise learning rate schedule as scaled learning rate restarting, we leverage the 1-cycle <ref type="bibr" target="#b27">(Smith &amp; Topin, 2019)</ref>, which is shown to give faster convergence speed than conventional approaches.  <ref type="figure">Figure 2</ref>: One-shot structured pruning on CIFAR-10 dataset using 1 -norm filters pruning  while varying retraining budgets. As can be seen, learning rate schedule matters. Schedules that employ large learning rates (LRW, SLR, CLR) are significantly better than fine-tuning. Among them, CLR performs best in most cases. Note that for the last two strategies (SLR and CLR), we warmup the learning rate for 10% of total retraining budget. For simplicity, we always use the largest learning rate of the original training for learning rate restarting. Specifically, the learning rate is increased from the smallest learning rate of original training to the largest one according to cosine function. The detailed learning rate schedule of each technique is depicted in <ref type="figure">Figure 1</ref>. See also Appendix F for the choice of warmup epochs.</p><p>Pruning algorithms We consider the following dimensions in our experiments. For pruning methods, we use 1 -norm filters pruning (PFEC)  and (global) magnitude-based weights pruning (MWP) <ref type="bibr" target="#b6">(Han et al., 2015)</ref> and evaluate them on the CIFAR-10, CIFAR-100 and ImageNet dataset. We examine both variations of pruning namely one-shot pruning and iterative pruning when comparing the proposed retraining techniques. Furthermore, we also experiment the CLR schedule on HRank <ref type="bibr" target="#b17">(Lin et al., 2020a)</ref>, Taylor Pruning (TP) <ref type="bibr" target="#b23">(Molchanov et al., 2019)</ref> and Soft Filter Pruning (SFP) <ref type="bibr" target="#b7">(He et al., 2018)</ref>.</p><p>Our implementation and hyperparameters of 1 -norm filters pruning and magnitude weight pruning are based on the public implementation of , which is shown to obtain comparable results with the original works. For remaining algorithms, we use official implementations with hyperparameters specified according to their papers. The detailed configurations of training and fine-tuning is provided in the Appendix B for interested readers.</p><p>Evaluation. For CIFAR-10 and CIFAR-100, we run each experiment three times and report "mean ? std". For ImageNet, we run each experiment once. These settings are kept consistently across architectures, pruning algorithms, retraining techniques, and ablation studies unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A CASE STUDY ON RETRAINING IN NETWORK PRUNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RETRAINING COST AND PERFORMANCE TRADE-OFF</head><p>We first investigate the performance of retraining techniques while varying the retraining budget. <ref type="figure">Figure 2</ref> illustrates the results of 1 -norm filters pruning (PFEC)  with different retraining techniques on CIFAR-10. We can observe that, larger values of learning rate always attain higher performance than fine-tuning regardless of number of retraining epochs. Furthermore, with some additional retraining budgets, e.g., 80 to 120 epochs compare to 40 epochs in original works, we can attain much higher compression ratio with almost no accuracy drop (see <ref type="figure">Figure 5</ref> in Appendix). <ref type="figure" target="#fig_0">Figure 3</ref> reports the accuracy of ResNet-56 pruned with magnitude-based weights pruning <ref type="bibr" target="#b6">(Han et al., 2015)</ref> in a one-shot manner. We study the impact of different retraining schedules on low and high sparsity i.e. 50% and 90% respectively. It can be observed that under both setting learning rate restarting consistently outperforms fine-tuning across number of retraining epochs.</p><p>To verify the effectiveness of CLR on large-scale datasets, we conduct experiment using 1 -norm filters pruning with ResNet-34 on ImageNet in <ref type="figure" target="#fig_0">Figure 3</ref>(c). We find that CLR achieve lower performance than fine-tuning for low epochs. This is expected phenomenon since <ref type="bibr" target="#b10">(Huang et al., 2017)</ref> also found that learning rate restarting on ImageNet usually requires training for 45 epochs while it only needs 40 epochs on CIFAR-10. When retraining with number of epochs higher than 45, CLR can reach higher accuracy than fine-tuning.</p><p>Thus, the value of LRW can be a good heuristic to choose the learning rate for retraining after pruning.</p><p>A concrete example is that we achieve significant gains with Taylor pruning when retraining for only 25 epochs as shown in <ref type="table" target="#tab_11">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODEL SIZE AND PERFORMANCE TRADE-OFF</head><p>In this section, we investigate the trade-off of between model size and performance of compact models under a fixed retraining budgets. <ref type="bibr" target="#b26">Renda et al. (2020)</ref> found that learning rate rewinding usually saturate at half of original training, thus, we perform on retraining for 80 epochs on CIFAR-10 and 45 epochs on ImageNet.</p><p>We first experiment with iterative 1 -norm filters pruning on CIFAR-10 and report the results in <ref type="figure" target="#fig_1">Figure 4</ref>(a,b), we can observe that SLR and CLR also perform comparable or better than LRW in this setting. We then conduct experiments with iterative unstructured pruning on ImageNet with ResNet-50 to verify the superior of CLR compared to fine-tuning in <ref type="figure" target="#fig_1">Figure 4</ref>(c), we find that MWP with CLR significantly outperform fine-tuning and even increase the accuracy of the network until 80% sparsity (of convolutional layers).</p><p>In our experiments, CLR usually reaches slightly higher accuracy than LRW and SLR. A possible explanation is that 1-cycle schedule gives better convergence speed than conventional schedules (Smith &amp; Topin, 2019), thus, bringing much better results when employing "low" retraining budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EVALUATION WITH OTHER PRUNING ALGORITHMS</head><p>We investigate whether the effectiveness of learning rate restarting holds with other pruning algorithms than simple norm-based pruning. As the performance of larger learning rate schedules such as LRW, SLR, and CLR are rather similar, we select CLR for use in this experiment. Particularly, we examine CLR with SFP <ref type="bibr" target="#b7">(He et al., 2018)</ref> in this section.. <ref type="bibr" target="#b7">(He et al., 2018)</ref>. Previous sections demonstrate that the success of large learning rate hold for a wide range of pruning algorithms, network architectures and datasets. However, we note that all examined pruning algorithms followed the pruning-retraining paradigm, i.e., it is necessary to retrain the pruned model. In this part, we investigate regularizing where we gradually, softly prune the network during the course of training. In contrast to pruning-retraining, the latter approach does not require further training after final pruning. That being said, we still make a case study to verify if retraining does improve the performance of models pruned with this type of algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Filters Pruning</head><p>To make a fair comparison between fine-tuning and no fine-tuning, we randomly split the conventional training set of CIFAR-10/CIFAR-100 (including 50000 images) to train (90% images of total set) and val (10% remaining images) set and we then report the result of best-validation models on "standard" test set (including 5000 images).</p><p>We report the performance of pruned network without fine-tuning, with fine-tuning and with learning rate restarting (CLR) in <ref type="table" target="#tab_2">Table 1</ref>. We find that applying fine-tuning on these model usually give very fast convergences speed (roughly in first 20 epochs). However, fine-tuned models exhibit negligible gain then pruned models without fine-tuning. In contrast, CLR significantly improves the performance even with small retraining budgets. Interested readers can find our results with other pruning algorithms namely HRank <ref type="bibr" target="#b17">(Lin et al., 2020a)</ref> and Taylor pruning <ref type="bibr" target="#b23">(Molchanov et al., 2019)</ref> in the Appendix C.</p><p>3.4 SCALING ORIGINAL TRAINING  present empirical evidence that 3-stage pipeline of structured pruning is inferior to training compact models from scratch if training budgets of compact models are scaled proportion to FLOPs ratio between original and pruned model. Following <ref type="bibr" target="#b2">Evci et al. (2019)</ref>; , we conduct experiments to compare this strong baseline with 1 -norm filters pruning while employing CLR on CIFAR-10, CIFAR-100 and ImageNet. Specifically, we compare pruned networks retrained with fine-tuning and CLR mechanism with Scratch-B(udget) and Scratch-E(pochs) which train pruned networks from scratch with the same computational budget and epochs respectively 1 .</p><p>When evaluating with CIFAR-10 and CIFAR-100, we found that the validation accuracies of Scratch-B, Scratch-E, and CLR are very similar to each other, and therefore, the comparisons and ranking could be very sensitive to the variance of the accuracy. To reduce variance, we employ the following training scheme. First, we re-run each experiment 5 times on CIFAR-10 and CIFAR-100 (more than previous experiments to reduce noise). Second, we randomly (and independently between each run) split the original training set of CIFAR-10 to training (90% images of total set) and validation set (10% images of total set) and report the performance of best validation models on original test set.</p><p>Note that for ImageNet, we re-run the experiments 3 times. We found that for ImageNet, the variance is negligible and thus we simply report the best validation accuracy.</p><p>For retraining, we adopt the same retraining budgets as in ) (40 and 20 epochs for CIFAR-10 and ImageNet respectively). Since the total number of retraining epochs is  . Configurations of Model and Pruned Model are both from the original paper . The results of "Scratch-E" and "Scratch-B" on ImageNet are taken directly from work of . Top-and second-ranked results are highlighted in bold blue and blue. limited, for ImageNet, we restart the learning rate to 0.01 (instead of 0.1 in the previous section) to guarantee the convergence of the models.</p><p>The results of pruned network via PFEC are shown in <ref type="table" target="#tab_3">Table 2</ref>. We can observe that for CIFAR-10, the results of fine-tuning and Scratch-B/E are aligned with the prior work of  while CLR achieve slightly better results than Scratch-E and comparable with Scratch-B. However, PFEC+CLR significantly outperform both fine-tuning and Scratch-B by a large margin.</p><p>Discussion. Our empirical results suggest that practitioners should employ retraining with large learning rate schedules as an alternative technique for fine-tuning to obtain compact models with better performance after network pruning. In our results, cyclic learning rate restarting (CLR) is slightly more efficient than scaled learning rate restarting (SLR) and learning rate rewinding (LRW).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INTERPLAY OF PRUNING ALGORITHMS AND RETRAINING SCHEMES</head><p>Section 3 suggests that learning rate schedule can significantly improve the performance of several pruning algorithms. We notice that there are notable differences between settings of implementation of network pruning especially in retraining phase. In this section, we show that the difference between implementation could easily lead to misleading results and unfair comparisons between pruning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A STRONG BASELINE: 1 -NORM FILTERS PRUNING WITH CLR</head><p>In this section, we demonstrate that even with same retraining budgets, utilizing simple CLR with 1 -norm filters pruning can achive comparable or exceed the performance of more sophisticated saliency metrics without meticulous hyperparameters searching. The implementation details are as follows.</p><p>Our baseline For pruning, we adopt the pretrained of Torchvision and apply 1 -norm Filters Pruning on these models. In our implementation, the number of removed filters in each block are approximately equal so that the final pruned models have similar number of parameters with the compared one. Thus, if any, this setting should favor other approaches involving laborious sensitive analysis. For retraining, we apply the CLR learning rate schedule while choosing the maximum value of learning rate according to learning rate rewinding. For pruning algorithms that only retrain for small number of epochs (e.g. 25 epochs in case of Taylor Pruning) we also restart the learning rate value to the slightly higher value of 0.01.</p><p>Generative Adversarial Learning (GAL) <ref type="bibr" target="#b18">Lin et al. (2019)</ref> suggest to leverage generative adversarial learning, which learns a sparse soft mask in a label-free and an end-to-end manner, to effectively  solve the optimization problem. The original work retrain the pruned network for 30 epochs with batch size of 32 and initial learning rate of 0.01 (which is decayed by factor of 10 every 10 epochs). Thus, we use CLR with learning rate of 0.01 and batch size of 32 in our experiments when comparing with GAL. <ref type="table" target="#tab_5">Table 3</ref> shows the results of PFEC + CLR when retrained under same settings with GAL.</p><p>We can see that PFEC+CLR can create a very strong baseline compare with GAL.</p><p>HRankPlus An extension of HRank <ref type="bibr" target="#b17">(Lin et al., 2020a)</ref> which is better than HRank in both retraining efficiency and performance 2 . Inspired by the discovery that average rank of feature maps of a filter is consistent, the authors suggested to iteratively remove low-rank feature maps that contain less information. In the original implementation, the authors retrain the pruned networks for 90 epochs with initial learning rate of 0.1 and also utilize label smoothing. For simplicity, we do not use label smoothing in our implementation. As shown in <ref type="table" target="#tab_2">Table 12</ref>, PFEC+CLR also attain similar results with HRankPlus approach.</p><p>Additional results on CIFAR-10 and Discrimination-aware Channel Pruning <ref type="bibr" target="#b36">(Zhuang et al., 2018)</ref>, Taylor Pruning <ref type="bibr" target="#b23">(Molchanov et al., 2019)</ref>, Provable Filters Pruning <ref type="bibr" target="#b16">(Liebenwein et al., 2020)</ref> are reported in Appendix D for interested readers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RANDOM PRUNING WITH LEARNING RATE RESTARTING</head><p>In last subsection, we demonstrated that slight modification in retraining of simple 1 -norm Filters Pruning can exceed the perfomance of much more sophisticated pruning algorithms. In this subsection, we investigate the interplay between pruning saliency metrics and retraining configurations by comparing accuracy of randomly pruned networks with the original performance of methodically pruned networks.</p><p>We directly adopt the original implementation from the authors of Taylor Pruning, HRankPlus for comparisons. For PFEC and MWP we make use of the implementation of . We selected these works because they do not utilize a similar learning rate value as LRW in the original implementation, i.e., the authors applied a smaller value then the heuristic of LRW. The detailed training recipe of the original works can be found in Appendix E.1. To make the randomly pruned network have the same structure as the methodically pruned networks, i.e., equal number of filters per layer, we propose to replace the importance score of each neuron estimated by these methods by a uniformly random score value.</p><p>For retraining randomly pruned networks, we adopt the same principle at described in Section 4.1. <ref type="table" target="#tab_7">Table 5</ref> presents novel results of performance of random pruning + CLR with corresponding results of considered pruning algorithms. It is worth pointing out that we report the original results of all Comprehensive results of random pruning with different compression ratio for 1 -norm Filters Pruning and MWP is reported in Appendix E.2.</p><p>These results suggest that retraining techniques, e.g., learning rate restarting and learning rate schedule, play a pivotal role to final performance. Thus, in order to perform fair comparison of different methods, one should be cautious of this seemingly subtle detail. While it is unclear to set up a fair comparison between pruning algorithms that belong to different categories, e.g., pruning before/during/after training, iterative vs. oneshot pruning, we advocate standardizing (a set of) retraining configurations for each algorithm group or thorough hyperparamters searching to find the best configuration of each pruning method when evaluating different algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND CONCLUSION</head><p>In this work, we conducted extensive experiments to show that learning rates do matter in achieving good performance of pruned neural networks. We concluded that compared to traditional fine-tuning, learning rate restarting in general is an efficient way to retrain pruned networks to recover performance drop due to pruning. The success of learning rate rewinding is accounted by the use of large learning rates in the retraining. We believe that these findings help raise awareness of proper use of learning rate schedule when desiging pruning algorithms, standardizing empirical experiments and allowing fair comparisons. Our takeaway message is:</p><p>Pruning algorithms should be compared in the same retraining configurations.</p><p>Our work is not without limitations. So far we investigated with hyperparameters identical to those of the original implementations of the pruning algorithms, and only experiment with different learning rate schedules. We also limited our experiments to the image classification task in computer vision. Considering more datasets and other domains is a great research avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A NETWORK PRUNING FORMULATION</head><p>Let us start with a formal definition of the network pruning problem. Let ? ? R D be the parameters of a (large) neural network that needs to be pruned, D be the dataset for training. The parameters ? is updated to minimize the loss function L(?; D). Define sparsity mask m ? {0, 1} D indicating if a weight should be kept or removed. Similar to <ref type="bibr" target="#b22">Molchanov et al. (2016)</ref>, we can formulate the pruning algorithms as finding the optimal sparsity mask such that:</p><formula xml:id="formula_0">m * = argmin m L(? * m; D) ? L(? * ; D)<label>(1)</label></formula><p>where denotes the Hadamard (element-wise) product, ? * represents the (locally) optimal solution of ?. An alternative approach is minimizing the different between final output of two networks <ref type="bibr" target="#b18">(Lin et al., 2019)</ref>:</p><formula xml:id="formula_1">m * = argmin m F(? * m; D) ? F(? * ; D) 2 2</formula><p>(2)</p><p>where F : R D ? R D is the function mapping input feature (i.e. images) to the final output (before softmax).</p><p>Theoretically speaking, as the pruning formulation only takes the loss function into account, and there is a caveat that pruning can result in a set of weights that belong to a bad local minima, making the network generalize poorly. In practice, it is common to observe such issues from the performance drop of the pruned model. In such cases, a fine-tuning process can be applied to further optimize the weights, mitigating the accuracy drop. However, as what fine-tuning means, the weights might be only slightly adjusted without any guarantee about the final network performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TRAINING CONFIGURATION</head><p>The implementation of 1 -norm filters pruning (PFEC)  and magnitude-based weights pruning (MWP) <ref type="bibr" target="#b6">Han et al. (2015)</ref> are adopted from the work of . The implementations of other algorithms are taken from the authors' official repositories.</p><p>For simplicity, we adopt Pytorch's pretrained models for ImageNet. The unpruned models (used for 1 -norm filters pruning) are trained with below configurations. <ref type="table">Table 6</ref>: Training configuration for unpruned models. To train CIFAR-10, we use Nesterov SGD with ? = 0.9, batch size 64, weight decay 0.0001 for 160 epochs. To train ImageNet, we use Nesterov SGD with ? = 0.9, batch size 32, weight decay 0.0001 for 90 epochs. In addition results in <ref type="figure">Figure 2</ref>, we conduct experiments to examine impact of different retraining techniques to networks pruned with low and high compression ratios. The results are shown in <ref type="figure">Figure  5</ref>.   <ref type="figure">Figure 5</ref>: Results from pruning with high compression ratios for one-shot structured pruning on CIFAR-10 dataset using 1 -norm filters pruning . With a proper learning rate schedule, it is possible to achieve almost no accuracy drop while having much more compact models and using slightly more training budgets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EVALUATING CLR WITH OTHER PRUNING ALGORITHMS</head><p>Taylor Pruning <ref type="bibr" target="#b23">(Molchanov et al., 2019)</ref>. In this method, the authors estimated the contributions of each filter to the final performance by utilizing the first-and second-order Taylor expansion and performing iterative pruning. In the original implementation, the authors fine tuned the network with a small learning rate (0.001) for 25 epochs on ImageNet. In our implementation, we opt to evaluate the accuracy of network retrained with learning rate restarting (CLR) with a relatively small budget. Therefore, we keep the retraining budget the same and only increase the learning rate to 0.01. <ref type="table" target="#tab_11">Table 8</ref> shows the accuracy of ResNet-50 on ImageNet when pruning 72% of network weights. It can be seen that for a small retraining budget, employing learning rate restarting also gives significant gain in performance.</p><p>HRank <ref type="bibr" target="#b17">(Lin et al., 2020a)</ref>. Inspired by the discovery that average rank of feature maps of a filter is consistent, the authors suggested to iteratively remove low-rank feature maps that contain less information. Particularly, we iteratively remove filters in each layer and then retrain the network for 30 epochs each time. We examine the performance of pruned models with HRank on CIFAR while employing CLR at each fine-tuning step. Due to the heavily expensive computational cost for retraining, we only run each model once for HRank. In the original version of this method, the authors used the learning rate of 0.01 and drop by 10? at epochs 5 and 10. In our implementation, we restart the learning rate to 0.1 and apply the CLR schedule.</p><p>The quantitative results are shown in <ref type="table" target="#tab_10">Table 7</ref>. In our experiments, CLR also attains comparable or better results than proposed retraining methods in original work. D STRONG BASELINES WITH 1 -NORM FILTERS PRUNING D.1 CIFAR-10</p><p>Furthermore, we show that by naively neglecting retraining configurations (i.e. budget), we can also reach state-of-the-art results on CIFAR-10 with <ref type="bibr">PFEC Li et al. (2016)</ref>. Specifically, we prune two "standard" models namely ResNet-56 and ResNet-110 and retrain with CLR for 152 epochs, and then we compare with more sophisticated algorithms in <ref type="table" target="#tab_12">Table 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 IMAGENET</head><p>Taylor Pruning <ref type="bibr" target="#b23">(Molchanov et al., 2019)</ref> In this method, the authors estimated the contributions of each filter to the final performance by utilizing the first-and second-order Taylor expansion and performing iterative pruning.</p><p>Original work retrains pruned networks for 25 epochs with learning rate of 0.001 with the exception for Taylor-FO-BN-56%. <ref type="table" target="#tab_2">Table 10</ref> demonstrates the performance of PFEC+CLR compare with original works of <ref type="bibr" target="#b23">Molchanov et al. (2019)</ref>. In most cases, PFEC+CLR attains higher or comparable accuracy with Taylor Pruning. Discrimination-aware Channel Pruning <ref type="bibr" target="#b36">(Zhuang et al., 2018)</ref> The authors introduce additional discrimination-aware losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error.</p><p>We use the retraining budget of 60 epochs and set initial learning rate to 0.01 same as <ref type="bibr" target="#b36">Zhuang et al. (2018)</ref>. However, we employ the 1-cycle learning rate instead of stepwise learning rate as original work. The detailed comparison is presented in <ref type="table" target="#tab_2">Table 11</ref>. Provable Filters Pruning <ref type="bibr" target="#b16">(Liebenwein et al., 2020)</ref> This algorithm uses a small batch of input data points to assign a saliency score to each filter and constructs an importance sampling distribution where filters that highly affect the output are sampled with correspondingly high probability.</p><p>Original implementation of <ref type="bibr" target="#b16">Liebenwein et al. (2020)</ref> retrain the pruned network on ImageNet for 90 epochs with standard learning rate schedule (drop learning rate by factor of 10 after 30 epochs) with batch size of 256 and weight decay of 0.0001. In our implementation, pruned networks are trained for 90 epochs with maximum learning rate of 0.1 while keeping the same configurations for all other hyperparameters as original work. We compare the configuration between original implementation of aforementioned pruning methods and corresponding random pruning in <ref type="table" target="#tab_2">Table 13</ref>. Note that for HRankPlus, the original implementation employs different values of weight decay for each model: 0.005, 0.006, 0.005, 0.002 for VGG-16, ResNet-56, ResNet-110, DenseNet-40 respectively. We found that these value is relatively higher than "conventional" values (0.0001) making the models hard to converge after restarting. Thus, we use weight decay of 0.0005 through out experiments with HRankPlus. In all our experiments, other details such as batch size, retraining budget are set to similar value of original implementation.   <ref type="figure">Figure 6</ref>: One-shot structured pruning on CIFAR-10 dataset using PFEC  and randomly filters pruning with different retraining schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 RANDOM PRUNING WITH VARIOUS COMPRESSION RATIO</head><p>1 -norm Filters Pruning <ref type="figure">Figure 6</ref> illustrates the performance of networks pruned with PFEC and random pruning on CIFAR-10 and CIFAR-100 when retraining with 40 epochs -the same setting used by . We can see that randomly pruned networks consistently achieve superior performance than methodically pruned networks (fine-tuned with standard learning rate schedule) in terms of accuracy. However, random pruning obtain lower accuracy than PFEC when using identical retraining techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Magnitude-based Weights Pruning</head><p>We extend the scope of experiment in Sec 4.2 to unstructured pruning and analyze performance of MWP with CLR. <ref type="figure">Figure 7</ref> represents the results of random pruning with CLR and methodically pruning while varying difference compression ratio in both iterative and oneshot pruning manner. Specifically, we retrain the trimmed network for 40 epochs. We can observe that though Random Pruning + CLR can achieve higher accuracy with low sparsity, the performance of randomly pruned network immensely reduced with the increasing of compression ratio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F PERFORMANCE OF CLR WITH DIFFERENT WARMUP EPOCHS</head><p>To avoid the laborious hyperparameters tuning process, we simply warm up the learning rate for CLR and SLR for the first 10% budget of retraining. In this section, we investigate whether the performance of pruned network is sensitive to this hyperparameter. <ref type="figure">Figure 8</ref> presents the results of pruned networks obtained with fine-tuning and CLR while varying number of warmup epochs namely 0, 5, 10, 15% of total retraining. We can observe that there is no significant diversity in results of CLR with different number of warmup epochs and all of them exceed performance of fine-tuning by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G TRAINING FROM SCRATCH WITH CLR</head><p>In this section, we conduct experiment where we train the unpruned models from scratch with CLR schedule. Then, we compare performance of networks pruned from these baseline with those pruned from models trained with conventional step wise learning rate schedule. <ref type="table" target="#tab_2">Table 14</ref> presents the accuracy of pruned ResNet-56 models on CIFAR-10 with different training and retraining recipes. We use the budget of 160 for training the baseline networks. For retraining the trimmed networks, we examine the budget of 40 and 56 epochs. We can see that both CLR and SLR consistently exceed the performance of fine-tuning under these configurations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>One-shot unstructured pruning on CIFAR-10 dataset using MWP<ref type="bibr" target="#b6">(Han et al., 2015)</ref> ((a) and (b)) and structured pruning on ImageNet with 1 -norm filters pruning(Li et al., 2016) (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Iterative pruning on CIFAR-10 dataset using 1 -norm filters pruning ((a)   and (b)) and on ImageNet using magnitude-based weights pruning<ref type="bibr" target="#b6">(Han et al., 2015)</ref> ((c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>One-shot and Iterative unstructured pruning on CIFAR-10 dataset using MWP<ref type="bibr" target="#b6">(Han et al., 2015)</ref> and randomly weights pruning with different retraining schemes. 1 -norm Filters Pruning on CIFAR-10 with different number of warmup epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results (accuracy) for soft filters pruning<ref type="bibr" target="#b7">(He et al., 2018)</ref>. The "w/o FT" column demonstrate the performance of networks follow original work (do not retrain after final prune). "FT" column indicates results of networks when fine-tuning for 200 more epochs. "CLR-x" columns show accuracy of networks after retraining with CLR for x more epochs."%PF" indicates number of pruned filters.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">%PF w/o FT</cell><cell>FT</cell><cell>CLR-50</cell><cell>CLR-100</cell><cell>CLR-200</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>30</cell><cell>92.74?0.28</cell><cell cols="3">92.85?0.21 92.57?0.19 93.00?0.02</cell><cell>93.18 ? 0.25</cell></row><row><cell></cell><cell>ResNet-56</cell><cell>40</cell><cell>91.95?0.29</cell><cell cols="3">91.94?0.26 92.19?0.16 92.23?0.12</cell><cell>92.78 ? 0.34</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell>ResNet-110</cell><cell>40</cell><cell>92.25?0.55</cell><cell cols="3">92.23?0.50 92.30?0.37 92.79?0.38</cell><cell>92.91 ? 0.41</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>30</cell><cell cols="4">68.92 ?0.33 69.41?0.38 70.27?0.40 70.24?0.43</cell><cell>70.29 ? 0.46</cell></row><row><cell></cell><cell>ResNet-56</cell><cell>40</cell><cell>67.69?0.41</cell><cell cols="3">67.64?0.50 68.78?0.23 69.12 ? 0.22 68.88?0.44</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>30</cell><cell>70.93?0.58</cell><cell cols="3">70.94?0.58 71.77?0.60 71.74?0.63</cell><cell>72.00 ? 0.21</cell></row><row><cell></cell><cell>ResNet-110</cell><cell>40</cell><cell>68.41?0.26</cell><cell cols="3">69.04?0.41 70.22?0.14 70.05?0.27</cell><cell>70.34 ? 0.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results (accuracy) for 1 -norm based filter pruning</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparing the performance of pruned network via PFEC + CLR and GAL on ImageNet. The results of GAL are taken directly from original papers.</figDesc><table><row><cell>Model</cell><cell>Unpruned Top-1</cell><cell>Method</cell><cell cols="3">Param ? % FLOPs ? % Top-1</cell></row><row><cell></cell><cell></cell><cell>GAL-0.5</cell><cell>17.2</cell><cell>43.0</cell><cell>71.95</cell></row><row><cell>ResNet-50</cell><cell>76.15</cell><cell>PFEC + CLR</cell><cell>17.6</cell><cell>-</cell><cell>75.26</cell></row><row><cell></cell><cell></cell><cell>GAL-1</cell><cell>42.6</cell><cell>61.4</cell><cell>69.88</cell></row><row><cell></cell><cell></cell><cell>PFEC + CLR</cell><cell>43.1</cell><cell>-</cell><cell>73.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparing the performance of pruned network via PFEC + CLR and HRankPlus on ImageNet. The results of HRankPlus are taken directly from official Github repository.</figDesc><table><row><cell>Model</cell><cell>Unpruned Top-1</cell><cell>Method</cell><cell cols="3">Param ? % FLOPs ? % Top-1</cell></row><row><cell></cell><cell></cell><cell>HRankPlus</cell><cell>40.8</cell><cell>44.8</cell><cell>75.56</cell></row><row><cell>ResNet-50</cell><cell>76.15</cell><cell>PFEC + CLR</cell><cell>41.4</cell><cell>-</cell><cell>75.59</cell></row><row><cell></cell><cell></cell><cell>HRankPlus</cell><cell>56.7</cell><cell>62.8</cell><cell>74.19</cell></row><row><cell></cell><cell></cell><cell>PFEC + CLR</cell><cell>56.9</cell><cell>-</cell><cell>74.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of networks when applying random pruning and methodically pruning algorithms. "Original" column presents accuracy of pruned network reported in original papers. "R-CLR" presents the results of Random Pruning with CLR.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Model</cell><cell cols="2">Params ? % FLOPs ? %</cell><cell>Original</cell><cell>R-CLR</cell></row><row><cell></cell><cell></cell><cell>ResNet-56</cell><cell>70.0</cell><cell>74.1</cell><cell>92.32</cell><cell>92.40 ? 0.16</cell></row><row><cell cols="2">CIFAR-10 HRankPlus</cell><cell>ResNet-110 DenseNet-40</cell><cell>68.3 61.9</cell><cell>71.6 59.9</cell><cell>93.23 93.66</cell><cell>93.37 ? 0.04 93.71 ? 0.05</cell></row><row><cell></cell><cell></cell><cell>VGG-16</cell><cell>87.3</cell><cell>78.6</cell><cell>93.10</cell><cell>93.06 ? 0.07</cell></row><row><cell></cell><cell>PFEC</cell><cell>ResNet-34 A ResNet-34 B</cell><cell>2.3 10.8</cell><cell>15.9 24.2</cell><cell cols="2">72.96 ? 0.06 73.47 ? 0.08 72.50 ? 0.08 73.05 ? 0.05</cell></row><row><cell>ImageNet</cell><cell>Taylor Pruning</cell><cell>ResNet-50 72% ResNet-50 81% ResNet-50 91%</cell><cell>44.5 30.1 11.4</cell><cell>45.0 35.0 20.0</cell><cell>74.50 75.48 76.43</cell><cell>74.91 75.54 75.93</cell></row><row><cell cols="7">methodical pruning are taken directly from their papers respectively. In our experiments, Random</cell></row><row><cell cols="7">Pruning + CLR could surpass the performance of sophisticated saliency metrics even on large-scale</cell></row><row><cell cols="7">and challenging dataset such as ImageNet with only minimal change in learning rate schedule and</cell></row><row><cell cols="2">initial learning rate value.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Results (accuracy) for HRank ilters pruning (Lin et al., 2020a) on CIFAR-10. "Pruned + FT ?" is the model pruned from the large model with original fine-tuning scheme that are reported. "Prune + FT" and "Prune + CLR" are results of our runs with original retraining and CLR respectively. Configurations of Model and Pruned Model are both from the original paper.</figDesc><table><row><cell>VGG-16 (BN)</cell><cell>93.93</cell><cell>92.34</cell><cell>91.97</cell><cell>92.53</cell><cell>65.3%</cell><cell>82.10%</cell></row><row><cell>ResNet-56</cell><cell>93.26</cell><cell>93.17</cell><cell>92.97</cell><cell>93.16</cell><cell>50.0%</cell><cell>42.4%</cell></row><row><cell>ResNet-110</cell><cell>93.50</cell><cell>94.23</cell><cell>93.84</cell><cell>93.90</cell><cell>37.9%</cell><cell>38.7%</cell></row><row><cell>DenseNet-40</cell><cell>94.81</cell><cell>94.24</cell><cell>94.10</cell><cell>94.10</cell><cell>40.8%</cell><cell>30.5%</cell></row></table><note>Model Unpruned Prune + FT ? Prune + FT Prune + CLR FLOPs ? % Params ? %</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Results (accuracy)  for Taylor filters pruning<ref type="bibr" target="#b23">(Molchanov et al., 2019)</ref> on ImageNet. All columns have same meaning with corresponding columns inTable 7.</figDesc><table><row><cell>Model</cell><cell cols="5">Unpruned Prune + FT Prune + CLR FLOPs ? % Params ? %</cell></row><row><cell>Taylor-FO-BN-56%</cell><cell></cell><cell>71.69</cell><cell>72.51</cell><cell>67.2</cell><cell>66.8</cell></row><row><cell>Taylor-FO-BN-72%</cell><cell>76.15</cell><cell>74.50</cell><cell>75.22</cell><cell>45.0</cell><cell>44.5</cell></row><row><cell>Taylor-FO-BN-81%</cell><cell></cell><cell>75.48</cell><cell>75.67</cell><cell>35.0</cell><cell>30.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Results of ResNet-56 and ResNet-110 on CIFAR-10. The performance of other pruning algorithms are taken directly from original papers.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="5">Unpruned Params ? FLOPs ? Prune Acc. ?</cell></row><row><cell></cell><cell>PFEC (Li et al., 2016)</cell><cell>93.04</cell><cell>14.1</cell><cell>27.6</cell><cell>93.06</cell><cell>-0.02</cell></row><row><cell></cell><cell>NISP (Yu et al., 2018)</cell><cell>93.26</cell><cell>42.4</cell><cell>35.5</cell><cell>93.01</cell><cell>0.25</cell></row><row><cell>ResNet-56</cell><cell>HRank (Lin et al., 2020a)</cell><cell>93.26</cell><cell>42.4</cell><cell>50.0</cell><cell>93.17</cell><cell>0.09</cell></row><row><cell></cell><cell>GAL (Lin et al., 2019)</cell><cell>93.26</cell><cell>-</cell><cell>37.6</cell><cell>92.98</cell><cell>0.28</cell></row><row><cell></cell><cell>PFEC + CLR (ours)</cell><cell>93.21</cell><cell>48.7</cell><cell>-</cell><cell>93.29</cell><cell>-0.07</cell></row><row><cell></cell><cell>PFEC (Li et al., 2016)</cell><cell>93.53</cell><cell>32.6</cell><cell>38.7</cell><cell>93.30</cell><cell>-0.23</cell></row><row><cell>ResNet-110</cell><cell>GAL-0.5 (Lin et al., 2019) HRank (Lin et al., 2020a)</cell><cell>93.50 93.50</cell><cell>44.8 59.2</cell><cell>48.4 58.2</cell><cell>92.74 93.36</cell><cell>0.76 0.14</cell></row><row><cell></cell><cell>PFEC+CLR (ours)</cell><cell>93.56</cell><cell>64.2</cell><cell>-</cell><cell>93.69</cell><cell>-0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Comparing the performance of pruned network via PFEC + CLR and Taylor Pruning on ImageNet. The results of Taylor Pruning are taken directly from original papers.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="4">Param ? % FLOPs ? % Unpruned Top-1 Top-1</cell></row><row><cell>ResNet-34</cell><cell>Taylor-FO-BN-82% PFEC + CLR</cell><cell>21.1 22.5</cell><cell>22.3 -</cell><cell>73.31 73.30</cell><cell>72.83 73.01</cell></row><row><cell></cell><cell>Taylor-FO-BN-56%</cell><cell>66.8</cell><cell>67.2</cell><cell>76.18</cell><cell>71.69</cell></row><row><cell></cell><cell>PFEC + CLR</cell><cell>68.8</cell><cell>-</cell><cell>76.15</cell><cell>70.70</cell></row><row><cell></cell><cell>Taylor-FO-BN-72%</cell><cell>44.5</cell><cell>45.0</cell><cell>76.18</cell><cell>74.50</cell></row><row><cell>ResNet-50</cell><cell>PFEC + CLR</cell><cell>44.5</cell><cell></cell><cell>76.15</cell><cell>75.04</cell></row><row><cell></cell><cell>Taylor-FO-BN-81%</cell><cell>30.1</cell><cell>35.0</cell><cell>76.18</cell><cell>75.48</cell></row><row><cell></cell><cell>PFEC + CLR</cell><cell>30.1</cell><cell>-</cell><cell>76.15</cell><cell>75.79</cell></row><row><cell></cell><cell>Taylor-FO-BN-91%</cell><cell>11.4</cell><cell>20.0</cell><cell>76.18</cell><cell>76.43</cell></row><row><cell></cell><cell>PFEC + CLR</cell><cell>12.5</cell><cell>-</cell><cell>76.15</cell><cell>76.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Comparing the performance of pruned network via PFEC + CLR and DCP on ImageNet. The results of DCP are taken directly from original papers.</figDesc><table><row><cell>Model</cell><cell cols="2">Param ? % FLOPs ? %</cell><cell>Method</cell><cell>Unpruned Top-1</cell><cell>Top-1</cell></row><row><cell></cell><cell>28.1</cell><cell>27.1</cell><cell>DCP</cell><cell>69.64</cell><cell>69.21</cell></row><row><cell></cell><cell>31.9</cell><cell>-</cell><cell>PFEC + CLR</cell><cell>69.76</cell><cell>69.31 ? 0.06</cell></row><row><cell>ResNet-18</cell><cell>47.1</cell><cell>46.1</cell><cell>DCP</cell><cell>69.64</cell><cell>67.35</cell></row><row><cell></cell><cell>50.6</cell><cell>-</cell><cell>PFEC + CLR</cell><cell>69.76</cell><cell>67.38</cell></row><row><cell></cell><cell>65.7</cell><cell>64.1</cell><cell>DCP</cell><cell>69.64</cell><cell>64.12</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>PFEC + CLR</cell><cell>69.76</cell><cell>64.08</cell></row><row><cell></cell><cell>33.3</cell><cell>35.7</cell><cell>DCP</cell><cell>76.01</cell><cell>76.40</cell></row><row><cell></cell><cell>33.7</cell><cell>-</cell><cell>PFEC + CLR</cell><cell>76.15</cell><cell>76.03</cell></row><row><cell>ResNet-50</cell><cell>51.4</cell><cell>55.5</cell><cell>DCP</cell><cell>76.01</cell><cell>74.95</cell></row><row><cell></cell><cell>51.5</cell><cell>-</cell><cell>PFEC + CLR</cell><cell>76.15</cell><cell>75.16</cell></row><row><cell></cell><cell>65.9</cell><cell>71.1</cell><cell>DCP</cell><cell>76.01</cell><cell>72.75</cell></row><row><cell></cell><cell>66.1</cell><cell>-</cell><cell>PFEC + CLR</cell><cell>76.15</cell><cell>72.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Comparing the performance of pruned network via PFEC + CLR and Provable Filters Pruning (PFP) on ImageNet. The results of PFP are taken directly from original paper<ref type="bibr" target="#b16">(Liebenwein et al., 2020)</ref>.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="4">Param ? % FLOPs ? % Unpruned Top-1 Top-1</cell></row><row><cell></cell><cell>PFP (lowest top-1 err.)</cell><cell>18.0</cell><cell>10.8</cell><cell>76.13</cell><cell>75.91</cell></row><row><cell>ResNet-50</cell><cell>PFEC + CLR</cell><cell>18.1</cell><cell>-</cell><cell>76.15</cell><cell>76.56</cell></row><row><cell></cell><cell>PFP (within 1.0% top-1)</cell><cell>44.0</cell><cell>30.1</cell><cell>76.13</cell><cell>75.21</cell></row><row><cell></cell><cell>PFEC + CLR</cell><cell>44.3</cell><cell>-</cell><cell>76.15</cell><cell>75.38</cell></row><row><cell cols="3">E RANDOM PRUNING WITH CLR</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">E.1 RETRAINING CONFIGURATION</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Training configurations of original pruning methods and random pruning.</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell></cell><cell></cell><cell cols="2">Original</cell><cell>Random Pruning</cell></row><row><cell></cell><cell>ResNet-56 ResNet-110</cell><cell>? =</cell><cell>? ? ?</cell><cell>0.01 0.001</cell><cell>t ? [0, 150) t ? [151, 225)</cell></row><row><cell>HRankPlus</cell><cell>DenseNet-40</cell><cell></cell><cell>? ?</cell><cell cols="2">0.0001 t ? [226, 300)</cell><cell>? ? ?max = 0.1 ? ?init = 0.001</cell></row><row><cell></cell><cell>VGG-16</cell><cell>? =</cell><cell>? ? ?</cell><cell>0.01 0.001</cell><cell>t ? [0, 50) t ? [51, 100)</cell><cell>? ? ?min = 0.00001</cell></row><row><cell></cell><cell></cell><cell></cell><cell>? ?</cell><cell cols="2">0.0001 t ? [101, 150)</cell></row><row><cell>PFEC</cell><cell>ResNet-34</cell><cell cols="3">? = 0.001</cell><cell>t ? [0, 20)</cell><cell>? ? ?init = 0.0001 ? ?max = 0.01</cell></row><row><cell>Taylor Pruning</cell><cell>ResNet-50</cell><cell cols="3">? = 0.001</cell><cell>t ? [0, 25)</cell><cell>? ? ?min = 0.000001</cell></row><row><cell>1.25 1.50 1.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Compression Ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Performance of ResNet-56 pruned from models trained with CLR and conventional stepwise learning rate schedule on CIFAR-10. "Epochs" column indicates the number of epochs for retraining trimmed networks. "Schedule" colum indicates the learning rate schedule used for training baseline (unpruned) networks. The best and second best methods are highlighted in bold blue and blue respectively.</figDesc><table><row><cell cols="3">Epochs Schedule Param ? %</cell><cell>Baseline</cell><cell>Fine-tuning</cell><cell>SLR</cell><cell>CLR</cell></row><row><cell></cell><cell>Step-wise</cell><cell>13.7</cell><cell cols="2">93.15 ? 0.36 92.81 ? 0.49</cell><cell>92.93 ? 0.15</cell><cell>93.14 ? 0.37</cell></row><row><cell>40</cell><cell>CLR</cell><cell>13.7</cell><cell cols="2">93.45 ? 0.17 93.06 ? 0.20</cell><cell>93.03 ? 0.16</cell><cell>93.15 ? 0.11</cell></row><row><cell></cell><cell>Step-wise</cell><cell>34.9</cell><cell cols="3">93.15 ? 0.36 92.25 ? 0.35 92.83 ? 0.16</cell><cell>92.81 ? 0.32</cell></row><row><cell></cell><cell>CLR</cell><cell>34.9</cell><cell cols="2">93.45 ? 0.17 92.50 ? 0.07</cell><cell>92.63 ? 0.05</cell><cell>93.03 ? 0.16</cell></row><row><cell></cell><cell>Step-wise</cell><cell>13.7</cell><cell cols="2">93.15 ? 0.36 92.81 ? 0.41</cell><cell>92.86 ? 0.28</cell><cell>93.22 ? 0.37</cell></row><row><cell>56</cell><cell>CLR</cell><cell>13.7</cell><cell cols="2">93.45 ? 0.17 93.04 ? 0.16</cell><cell>93.22 ? 0.05</cell><cell>93.37 ? 0.25</cell></row><row><cell></cell><cell>Step-wise</cell><cell>34.9</cell><cell cols="2">93.15 ? 0.36 92.48 ? 0.45</cell><cell>92.89 ? 0.11</cell><cell>93.26 ? 0.17</cell></row><row><cell></cell><cell>CLR</cell><cell>34.9</cell><cell cols="2">93.45 ? 0.17 92.63 ? 0.13</cell><cell>92.83 ? 0.03</cell><cell>93.29 ? 0.25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For Scratch-E, we use 160 (original training) + 40 (retraining) = 200 epochs while training from scratch instead of 160 as in </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/lmbxmu/HRankPlus</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06509</idno>
		<title level="m">On the optimization of deep networks: Implicit acceleration by overparameterization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Why do larger models generalize better? a theoretical perspective via the xor problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="822" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Samuel</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11134</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<title level="m">The state of sparsity in deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discrete model compression with resource constraint for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangqian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1899" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06866</idno>
		<title level="m">Soft filter pruning for accelerating deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Filter pruning via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03231</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Snip: Single-shot network pruning based on connection sensitivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Eagleeye: Fast sub-net evaluation for efficient neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02491</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Provable filter pruning for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Liebenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cenk</forename><surname>Baykal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hrank: Filter pruning using high-rank feature map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1529" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards optimal structured cnn pruning via generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqian</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic model pruning with feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards understanding the role of over-parametrization in generalization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12076</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Abolafia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08760</idno>
		<title level="m">Sensitivity and generalization in neural networks: an empirical study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Comparing rewinding and fine-tuning in neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02389</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidenori</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05467</idno>
		<title level="m">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discovering neural wirings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2684" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00124</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinmian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2133" to="2144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nisp: Pruning networks using neuron importance score propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Hsin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9194" to="9203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discrimination-aware channel pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="875" to="886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
