<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Moving-camera Background Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Erez</surname></persName>
						</author>
						<title level="a" type="main">A Deep Moving-camera Background Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>unsupervised</term>
					<term>background model</term>
					<term>background subtraction</term>
					<term>moving camera</term>
					<term>joint alignment</term>
					<term>regularization-free</term>
					<term>deep learning</term>
					<term>video analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000?0002?4545?6664] , Ron Shapira Weber 1[0000?0003?4579?0678] , and Oren Freifeld 1[0000?0001?9816?9709]</p><p>Ben-Gurion University of the Negev, Be'er Sheva, Israel Abstract. In video analysis, background models have many applications such as background/foreground separation, change detection, anomaly detection, tracking, and more. However, while learning such a model in a video captured by a static camera is a fairly-solved task, in the case of a Moving-camera Background Model (MCBM), the success has been far more modest due to algorithmic and scalability challenges that arise due to the camera motion. Thus, existing MCBMs are limited in their scope and their supported camera-motion types. These hurdles also impeded the employment, in this unsupervised task, of end-to-end solutions based on deep learning (DL). Moreover, existing MCBMs usually model the background either on the domain of a typically-large panoramic image or in an online fashion. Unfortunately, the former creates several problems, including poor scalability, while the latter prevents the recognition and leveraging of cases where the camera revisits previously-seen parts of the scene. This paper proposes a new method, called DeepMCBM, that eliminates all the aforementioned issues and achieves state-of-the-art results. Concretely, first we identify the difficulties associated with joint alignment of video frames in general and in a DL setting in particular. Next, we propose a new strategy for joint alignment that lets us use a spatial transformer net with neither a regularization nor any form of specialized (and non-differentiable) initialization. Coupled with an autoencoder conditioned on unwarped robust central moments (obtained from the joint alignment), this yields an end-to-end regularization-free MCBM that supports a broad range of camera motions and scales gracefully. We demonstrate DeepMCBM's utility on a variety of videos, including ones beyond the scope of other methods. Our code is available at https://github.com/BGU-CS-VIL/DeepMCBM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The unsupervised video-analysis task this paper focuses on is learning a background model in a video captured by a moving camera. In the simpler case where the camera is static, such models have been used successfully in many computer-vision applications such as background/foreground separation, change or anomaly detection, and tracking. Static-camera solutions, however, cannot be easily extended to the moving-camera case since we do not know, a-priori, how the video frames should be aligned to each other. Thus, most of the tools traditionally used in background models become less applicable; e.g., methods based on learning a low-dimensional subspace via Robust Principal Component Analysis (RPCA) assume that the frames are aligned to each other.</p><p>Seemingly, there is a straightforward solution: "simply" align the frames to each other to reduce the problem back to the static-camera case, and then build a static-camera background model based on the aligned frames. However, this is more complicated than it might seem. First, the alignment problem itself is often difficult. For example, methods based on creating a panoramic image by sequentially aligning each pair of consecutive frames suffer from drift errors. Moreover, such methods cannot exploit the information conveyed in situations where the camera revisits (possibly from a different viewpoint) a previously-seen region in the scene. This, among other considerations, motivates solutions based on Joint Alignment (JA) of the frames. However, even in this formulation the problem is often still hard to solve, partially due to reasons we analyze later in ? 4. Second, and regardless of how the alignment is done, there is the issue of scalability which pertains to not only the alignment problem itself but also the subsequent learning of the background model: when the accumulative motion of the camera throughout the video is substantial, the domain of the panoramic image can be huge so background models learned in that domain must scale gracefully. Furthermore, in such cases, when a frame is warped (i.e., aligned) towards the panorama, it captures only a small portion of the latter. This means that most of the data in the panoramic version of the warped images is missing. This is problematic in our context since existing solutions for subspace learning in the presence of missing data usually struggle in such cases. Therefore, the missing-data issue, together with the scalability requirement, considerably complicates the task. Due to the above reasons, the success in the case of a Moving-camera Background Model (MCBM) is lagging far behind its static-camera counterpart. Moreover, the difficulties above have also largely prevented the use of Deep Learning (DL) for this task. This is unfortunate not only because the idea of harnessing the power of DL is attractive but also since it hinders the usage of MCBMs within larger end-to-end pipelines.</p><p>With this in mind, the goal of this paper is to provide an effective and scalable DL-based MCBM. To that aim, we start by identifying more precisely what makes JA of video frames challenging: first in the general case and then in the more specific DL context. Next, we design a new JA strategy based on a regularization-free Spatial Transformer Net (STN) and a JA loss involving a memory aspect. Our method requires no auxiliary tools (such as the brittle and non-differentiable initialization used in <ref type="bibr">[10]</ref>) that would prohibit its usage within end-to-end pipelines. We also propose a new deep module for learning a background model. The model, based on a Conditional Autoencoder (CAE) and the output of the JA module, is learned in the small domain of the input frames instead of the much-larger panoramic domain. This eliminates scalability issues and targets the goal of estimating the background more directly. Importantly, this module too can be used within end-to-end pipelines. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the type of results obtained by the proposed modules. Taken together, the proposed two modules give rise to a new and highly-effective MCBM method, coined DeepMCBM, which supports a broad range of camera motions and scales gracefully. We demonstrate DeepMCBM's utility on a variety of videos, including ones beyond the scope of competing methods.</p><p>Our key contributions are: 1) a DL module, for jointly aligning video frames, that relies on an STN-based optimization and a new training strategy that requires neither regularization nor initialization; 2) a DL background-modeling module that leverages the JA via a CAE conditioned on unwarped robust central moments derived from the JA; 3) together, these two modules form an end-to-end unsupervised MCBM that achieves SOTA results, that scales gracefully, and that supports a wide range of camera motions.</p><p>2 Related Work <ref type="bibr">STN [26]</ref> is a DL module that learns and applies a parameterized input-dependent spatial transformation. Given a parameterized transformation family and an input image f , the STN's output consists of a parameter vector ? and a warped image obtained by warping f using T ? (a transformation parameterized by ?). During training, the differentiation of a loss propagates through the STN. In practice, however, and despite their elegance, potential strength, and usage in numerous papers, STNs are often hard to train. Part of our solution addresses exactly such a case, where we take an STN-based optimization problem that was thought to be too difficult <ref type="bibr">[10]</ref> and show how it can, in fact, be solved easily, without resorting to a regularization or a sophisticated limiting initialization.</p><p>Static-camera background models. Early methods were pixelwise (e.g., [41]) but later the focus has shifted to subspace estimation using Robust Principal Component Analysis and its variants (e.g., <ref type="bibr">[44,</ref><ref type="bibr">7,</ref><ref type="bibr">50,</ref><ref type="bibr" target="#b19">20]</ref>). While those models usually do not scale well, there also exist scalable RPCA models (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">8]</ref>).</p><p>Image alignment. In <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">34]</ref>, pairwise homographies are estimated between consecutive frames while [27] uses a multi-layer homography. An adaptive panoramic image is built in <ref type="bibr">[47,</ref><ref type="bibr">32]</ref> while [43] relies on the assumption that a PTZ camera is used. Most of the works above make stringent assumptions about the camera motion and estimate transformations between pairs of images, sometimes even sequentially. This approach, however, can lead to accumulative errors and/or significant distortions. To avoid such issues, AutoStitch <ref type="bibr">[6]</ref> employs bundle adjustment. However, publicly-available implementations of AutoStitch scale poorly with the number of images (e.g., cannot handle more than a few hundreds of frames). This is unlike the proposed approach which scales gracefully. Alignment methods relying on depth or expensive 3D information/reconstruction include <ref type="bibr">[35,</ref><ref type="bibr">30,</ref><ref type="bibr">29,</ref><ref type="bibr">46]</ref>. Unlike those works, and similarly to, e.g., <ref type="bibr">[10]</ref>, the JA approach in this paper is purely 2D-based.</p><p>MCBMs. Online RPCA methods (e.g., <ref type="bibr">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19]</ref>) were extended to the case of camera jitter <ref type="bibr" target="#b22">[23]</ref> as well as more significant motions <ref type="bibr" target="#b17">[18]</ref>. DECOLOR [49] is another MCBM, based on motion detection, that is restricted to small motions. IncPCP-PTI <ref type="bibr">[9]</ref> targets a PTZ-camera setting by updating a low-dimensional subspace with the help of an estimated rigid motion between consecutive frames. Several MCBMs are built by first aligning the frames to each other, and then, in the usually-large domain of the obtained panoramic image, learning a background model from the warped images using a static-camera background model that can handle missing data (since each warped image covers only a portion of the panoramic domain). A prime example for such methods is PRPCA <ref type="bibr">[34]</ref>. Also of note are methods targeting moving-object detection in a moving camera; e.g., <ref type="bibr">[48,</ref><ref type="bibr">39,</ref><ref type="bibr">4]</ref>. These works, however, cannot detect changes unrelated to motion and also do not scale well.</p><p>STN-based JA. As we explain in ? 4, STN-based JA poses several difficulties. On that note, the closest work to ours is JA-POLS <ref type="bibr">[10]</ref> which handles some of the difficulties via the usage of a non-differentiable and non-robust initialization, together with a fairlyrestrictive regularization. While JA-POLS is effective in cases where it is applicable, it is limited in the camera-motion types it supports and is not an end-to-end solution. We will return to JA-POLS in more detail later on.</p><p>Learning background models in the panoramic domain. Once alignment is obtained, in principle a background model can be learned. However, panoramic-size models (e.g., [34]) do not scale while using an ensemble of Partially-overlapping Local Subspaces (POLS) <ref type="bibr">[10]</ref> is cumbersome and also suffers from the fact the number of models grows with the size of the panorama. Either way, the existing methods do not offer an end-to-end solution that can be used easily within DL pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries: Joint Alignment (JA)</head><p>Let (f n ) N n=1 be the frames of the input video and assume the size of each frame is h ? w pixels. Let C be the number of input channels; e.g., C = 3 for RGB images (the case considered in this paper). Let ? ? R 2 denote the rectangular h ? w common domain of each f n , and let ? n denote the (latent) parameter vector of the spatial transformation associated with the sought-after alignment of f n : ? ? R C . The transformation itself, denoted by T ?n , is viewed as an R 2 ? R 2 map (not just ? ? R 2 ). The value of d = dim(? n ) depends on the transformation family; e.g., in the affine case, d = 6.</p><formula xml:id="formula_0">The warped version of ? is ? n ? T ?n (?) ? {x : ?x ? ? ? s.t. T ?n (x ? ) = x} ? R 2 .</formula><p>Mathematically, we define the warped image as g n : ? n ? R C using the equality</p><formula xml:id="formula_1">g n (T ?n (x ? )) = f n (x ? ) ?x ? ? ? .<label>(1)</label></formula><p>However, due to technical reasons related to image warping [42], it is more convenient and customary to define g n via the inverse transformation of T ?n :</p><formula xml:id="formula_2">g n x ? g n (x) = f n ((T ?n ) ?1 (x)) ?x ? ? n .<label>(2)</label></formula><p>Note that g n depends on ? n and f n . Let H and W be the height and width, respectively, of a rectangle, denoted by ? scene ? R 2 , that is large enough to contain n ? n . We now define a mask that will be useful for reasons to become clear shortly. Let M ? be a single-channel h ? w image whose domain is ? and whose values are all equal to 1. Let M n : ? scene ? [0, 1] be a non-binary H ? W mask obtained by image warping of M ? , according to T ?n , using zero padding and a bilinear interpolation kernel. That is, for any integral location x in ? scene , the value of M n at x, denoted by M n x , is given by</p><formula xml:id="formula_3">M n x = M ? x ? x ? = T ??n (x) ? R 2<label>(3)</label></formula><p>where M ? x ? is interpolated from the values of M ? at the 4 integral locations nearest to x ? where whenever any of those integral locations falls outside ? the value of M ? at that location is taken to be zero. Thus, M n x = 0 if all those 4 locations are outside ?, M n x = 1 it they all fall inside it, and 0 &lt; M n x &lt; 1 otherwise. Let g n x,c denote the value of g n x at channel c. We will refer to p x,c ? (g n x,c ) N n=1 where c ? {1, . . . , C} as the C pixel stacks at location x. Similarly, we define the mask stack at location x as m x ? (M n x ) N n=1 . Note that p x,c and m x depend on (? n ) N n=1 . A joint-alignment loss, to be minimized w.r.t. (? n ) N n=1 , may be formulated in terms of</p><formula xml:id="formula_4">L JA = func(((p x,c ) C c=1 , m x ) x??scene ) .<label>(4)</label></formula><p>For example, in the early works on congealing (e.g., <ref type="bibr">[33,</ref><ref type="bibr">31,</ref><ref type="bibr">25,</ref><ref type="bibr">24]</ref>) that loss was based on entropy minimization. Later, other researchers <ref type="bibr">[11,</ref><ref type="bibr" target="#b11">12]</ref> showed the benefits of a loss based on least squares. A robust variant (used in <ref type="bibr">[10]</ref>) of the latter is</p><formula xml:id="formula_5">L JA = 1 N N n=1 1 C C c=1 x??scene M n x ? JA (g n x,c ? ? x,c ) x??scene M n x<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">? x,c = N n=1 M n x g n x,c N n=1 M n x</formula><p>and ? JA is a differentiable robust error function <ref type="bibr">[5]</ref>. Let ? be the mean of the warped images; i.e., the value of ? at location x and channel c is ? x,c . Note that ? may be viewed as the "moving target" to which the frames should be aligned. It "moves", during the optimization, in the following sense. As the alignment of the frames keeps changing, ? changes too since it is computed using the (weighted) average of the warped images. Assuming that the parameterization ? n ? T ?n is differentiable and that the transformation family is sufficiently well-behaved (as is the case, e.g., (a) A typical problem: if ?scene is not very large, the process is prone to a poor global minimum.</p><p>(b) A typical problem: drastic spatial changes in ? (note also that the end result is quite blurry). with the affine group or, more generally, spaces of diffeomorphisms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">40,</ref><ref type="bibr">38,</ref><ref type="bibr">28]</ref>), the loss in Eq. (5) is differentiable. Thus, if ? n is predicted using an STN (so, in particular, ? n is a differentiable function of f n , the STN's input), the loss can, at least in principle, be minimized using standard DL training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Identifying Key Challenges in Solving Joint-alignment Problems</head><p>Below we discuss three issues that might arise when solving JA problems: 1) poor global minima; 2) the need of regularization; 3) the need of a good initialization.</p><p>Usually when trying to minimize a loss, reaching a global minimum is hard or even impossible, and if this feat happens to be achieved, it is deemed to be the ultimate success. Sadly, global minima of L JA , while being (very) easy to achieve, reflect, in fact, an ultimate failure; e.g., the non-negative L JA can attain its global minimum (i.e., zero) when all the frames are shrunk to an infinitesimally-small point. A similar phenomenon occurs if all the frames are warped outside ? scene (e.g., see <ref type="figure" target="#fig_1">Figure 2a</ref>) or if the frames are warped such that there will be no pairwise overlap between them.</p><p>A popular solution in such cases is adding some type of regularization over (? n ) N n=1 . However, while various forms of regularization have been suggested, each of them imposes a certain bias; e.g., the regularization term in [31] favors symmetric distributions while the one in <ref type="bibr">[10]</ref> pushes the (affine) transformations towards the Special Euclidean group, denoted by SE(2). The implied assumptions in both these cases are limiting. Likewise, penalizing the size of the transformations (e.g., by penalizing some norm of ? n ) is problematic when the accumulative motion of the camera is large, while regularization favoring temporal smoothness is not always compatible with real camera motions. Another issue is the need of hyperparameter tuning for the weight of the regularization term. Moreover, finding a combination of a regularization type and a weight that will work well for a sufficiently-large variety of videos is difficult.</p><p>JA is usually a difficult non-convex problem. Thus, a good initialization can be useful; e.g., in JA-POLS <ref type="bibr">[10]</ref> an STN-based JA module had to rely on an initialization based on SE-Sync <ref type="bibr">[37]</ref>. The latter provides a useful globally-optimal solution to a differentbut-related problem: the estimation of absolute transformations that are consistent as possible with noisy measurements of pairwise relative transformations between pairs of frames, where both the latent absolute transformations and the observed relative ones are in SE <ref type="bibr">(2)</ref>. With that initialization, the STN needs to solve an easier problem and does so over the more expressive Affine group.</p><p>There are, however, several problems with the JA approach in <ref type="bibr">[10]</ref> (we will later also discuss problems related to the background-modeling approach in <ref type="bibr">[10]</ref>). First, preprocessing and heuristics are needed for extracting the relative transformations. Second, in cases where some of the true latent absolute transformations are far from SE(2) (e.g.: when the video contains a significant accumulative variation in the distance between the camera and the scene; when the camera zoom is changing; when there is a strong perspective effect; etc.), the initialization breaks and this leads in turn to JA-POLS' failure. Moreover, SE-Sync is neither robust nor differentiable w.r.t. the input frames. As there is no easy way to differentiate SE-Sync w.r.t. the input frames, the STN-based JA module in JA-POLS cannot be used in an end-to-end DL pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">An Additional Challenge with Joint Alignment When Using Batches</head><p>Typically, due to the data size and as it is almost always the case in DL, the optimization is done batch by batch where each batch consists of a subset (selected at random) of the frames from the entire video. A single epoch then represents a full pass over the entire data, and the frames are reshuffled between epochs. This typically-necessary batch-by-batch processing creates an optimization difficulty which might appear to be minor but is, in fact, far more critical than it may seem (we will revisit this point in ? 5.1). The issue is that the mean image ? (from Eq. <ref type="formula" target="#formula_5">(5)</ref>) is a function of the entire video, not just the frames in the current batch. A seemingly-obvious solution is to hold ? fixed during each epoch -so it does not affect the computation of the loss' gradient -and then, at the end of each epoch, recompute ?. However, a problem that arises with that approach is that the difference between the alignment targets (that is, the previous ? and the recomputed one) in each pair of consecutive epochs might be large, making the optimization difficult since the optimal transformations for one target might be quite far from those that are optimal for the next target. For an illustration, see <ref type="figure" target="#fig_1">Figure 2b</ref>. A different approach, used in <ref type="bibr">[10]</ref>, picks the target ? to be the mean of only the (warped) frames in the current batch. Besides the fact that this is somewhat inconsistent with the cost-function formulation, that approach too can cause significant changes in the targets between consecutive batches. The jumping-target problem complicates the optimization more than one may expect. This is especially an issue at the beginning of the process when the frames are completely misaligned. For example, in retrospect, this is partly why JA-POLS <ref type="bibr">[10]</ref> had to rely on the SE-Sync-based initialization scheme: as shown in <ref type="bibr">[10]</ref>, except in the simple case where the accumulative camera motion is small, without that initialization JA-POLS usually fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Proposed Method: DeepMCBM</head><p>The proposed modules of joint alignment (using an STN) and background modeling (using a CAE) are presented in ? 5.1 and ? 5.2, respectively. Together, they form the proposed method, DeepMCBM. The goal of the STN straining is 1) to jointly align the Algorithm 1: Training an STN for Joint Alignment</p><formula xml:id="formula_7">Input: N epochs , N batches , ?(?), data loader Data: (f n ) N n=1</formula><p>Output: A trained STN for Joint Alignment</p><formula xml:id="formula_8">1 Initialize accumulators G ? R H?W ?C and M ? R H?W // see text 2 for e ? {1, . . . , N epochs } do 3 for i ? {1, . . . , N batches } do 4 (f b ) b?B ? data loader // Load batch: B ? {1, . . . , N } 5 (? b , g b ) b?B ? STN((f b ) b?B ) // Note that g b = f b ? T ? b 6 (M b ) b?B ? (M ? ? T ? b ) b?B // Warp masks 7 G, M, L batch ? Algorithm 2( G, M, (g b ) b?B , (M b ) b?B ) // Update G and M; measure L batch (i.e.</formula><p>, the batch loss) <ref type="bibr">8</ref> Perform an optimization step to minimize the L batch loss. <ref type="bibr">9</ref> (G, M) ? (?G, ?M) // Keep the history, but downweight it video frames, implicitly forming a panoramic image, and 2) to learn how to warp an input frame towards that panoramic image. The goal of the CAE training is to learn the variability in the differences between the panoramic image and the input frames, while taking the warping into account but ignoring the foreground objects. The conditioning is done using the robust version of the panoramic pixelwise mean and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A Regularization-free Strategy for Joint Alignment</head><p>Having identified, in ? 4.1, that the jumps in the values of ? cause a major difficulty in the STN-based optimization of L JA (Eq. <ref type="formula" target="#formula_5">(5)</ref>), we design a simple but surprisingly-effective optimization strategy, summarized in Algorithm 1 (which, in turn, uses Algorithm 2 as its subroutine). During the training epochs, instead of computing ? using only the current batch (as was done in <ref type="bibr">[10]</ref>), or instead of recomputing ? from scratch each epoch, we construct our ? from the warped frames in the current batch while also taking into account, albeit with a lower weight, all the warped frames from the previous epochs as well as the previous batches in the current epoch. The proposed algorithm uses accumulators, denoted by G and M. The former is used to accumulate weighted sums of the values of the pixels in the warped frames while the latter serves a similar purpose with the values of the pixels in the warped masks. Concretely, let e denote the index of the current epoch and let e ? denote the index of some previous epoch. When evaluating the loss in a batch during epoch e, the contribution of the results from epoch e ? becomes smaller and smaller as the "time" difference, e ? e ? , grows. This is done in line 9 in Algorithm 1 by multiplying the accumulators of the warped frames and the warped masks by a positive factor ? where ? &lt; 1 (we use ? = 0.9). As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the resulting targets (i.e., the ? sequence formed during the optimization) change smoothly between epochs. Importantly, this behaviour has a profound and fourfold positive effect: 1. No complicated initialization is needed. As the optimization becomes much easier, the initial transformations are simply taken to be the identity. 2. Regularization-free JA. No form of regularization on (? n ) N n=1 is Algorithm 2: Update (?, G, M) and measure the loss on the batch <ref type="figure" target="#fig_1">Figure 2a</ref>, the process is more stable and successful. Also, even when ? nears the border of ?scene, it never goes outside it.</p><formula xml:id="formula_9">Input: G, M, (g b ) b?B , (M b ) b?B Output: G, M, L batch 1 G ? G + B b=1 g b // update warped-image accumulator 2 M ? M + B b=1 M b // update warped-mask accumulator 3 ? ? 0H?W ?C 4 for x ? {x : x ? ?scene and Mx ? 0} do in parallel 5 for c ? {1, . . . , C} do in parallel 6 ?x,c ? Gx,c Mx 7 L batch ? 1 B B b=1 1 C C c=1 x??scene M b x ?(g b x,c ? ?x,c) / x??scene M b x (a) Compared with</formula><p>(b) Compared with <ref type="figure" target="#fig_1">Figure 2b</ref>, the drastic jumps are eliminated. Also, with the proposed term the results are less affected by the specified size of ?scene. needed; e.g., there is no need to worry about the poor global minima from ? 4. Since the optimization is gradient-based and since each epoch lingers in the "history" of the process for many epochs before its effective weight decays to zero (due to the repeating multiplications by ? ? (0, 1)), such undesired cases are eliminated altogether. For instance, as the stack of the original frames overlaid over each other (from the first epoch) contributes to the computation of ?, either shrinking the frames to a point or moving them outside ? scene will incur a loss. Our regularization-free JA is in sharp contrast to many algorithms including classical works (e.g., <ref type="bibr">[31]</ref>) and more recent ones (e.g., <ref type="bibr">[10]</ref>).</p><p>3. Higher expressiveness. The formulation lets us increase the expressiveness of the transformation family as needed. For example, JA-POLS is so crucially dependent on its SE-Sync initialization and SE-based regularization, that the affine transformations it predicts are nearly in SE(2) themselves. In contrast, our method can not only predict more general transformations in the Affine group but also use broader transformation families. In our experiments we demonstrate this using the group of homographies but one may also try richer STNs such as those based on diffemorphisms [40, <ref type="bibr">1,</ref><ref type="bibr" target="#b13">14]</ref>. 4. Our JA module can be used in end-to-end pipelines. This is unlike not only non-DL methods but also JA-POLS [10] whose non-differentiable initialization prevents its JA ?1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STN Conditional Decoder</head><p>Conditional Encoder <ref type="figure">Fig. 4</ref>: The background-modeling module. After the STN module was trained using Algorithm 1, the robust panoramic moments, ? r and v r , are computed. A CAE is trained for a robust reconstruction task, using the transformation parameters, (? n ) N n=1 , estimated by the (frozen) STN. The CAE's output is f n , the estimated background associated with f n and the conditioning is done by (un)warping ? r and v r towards each input training image, f n . During test time the process is similar, except that the transformation being used is the one predicted by the STN.</p><formula xml:id="formula_10">? ? ? ?1 ? , ? ? ? ? ?1</formula><p>module from being used in an end-to-end manner. The technical details of the training process appear in our Supplemental Material (Supmat).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Background Modeling in ? (not ? scene ) via a Conditional Autoencoder</head><p>Upon the training of the STN, the frames become jointly aligned. In principle, at this point all that is left to do is to learn a background model using either non-DL methods (e.g., based on either pixelwise mixture models or RPCA methods; see ? 2) or deep ones (such as using a robust loss when training an autoencoder for reconstruction). However, there are several problems with this approach. First, it does not scale well: if the accumulating motion of the camera throughout the video is large, the panoramic image (of the entire scene covered throughout the video) can be huge. Moreover, in such a case even scalable RPCA methods will have to face an additional problem: since the domain of each warped image captures only a small region inside the domain of the panoramic image, it means that most of the pixels will represent missing data. Thus, one would need an RPCA method which can not only scale well but also succeed in situations where more than, say, 90%-95% of the data is missing. Also important is the following. Recall that given an input image, our goal is to estimate a background image, of the same size, that corresponds to that image. Thus, why should we even bother with trying to learn a panoramic-size background model? In <ref type="bibr">[10]</ref>, the discussion above motivated the learning of multiple local RPCA models and then, for estimating the background of a given image, only a subset of those models whose domains overlapped with the frame of interest were used. That solution, however, means that the number of models to be learned grows with the size of the panoramic image. Moreover, its non-DL formulation was another reason why JA-POLS was not an end-to-end method.</p><p>Here we propose a better alternative, whose pipeline is summarized in <ref type="figure">Figure 4</ref>: use a CAE to learn a background model whose domain is small. This has two advantages:</p><p>1) It does not compromise the end-to-end nature of the method. 2) We need to learn only a single model (unlike in <ref type="bibr">[10]</ref>) and its domain is small, fixed, and does not grow with the size of panoramic image (unlike in PRPCA). Concretely, rather than learning a background model (or models) whose domain is ? scene , we train a CAE on the original (i.e., non-warped) input frames, using a robust reconstruction error and, for each input frame f n , conditioning both the encoder and the decoder on (robust versions of) the mean and variance of the pixel stacks, but not before unwarping those central moments from ? scene towards f n . We now provide the details. The first and second central moments, denoted by ? r and v r , respectively, are C-channel H ? W images defined on ? scence and computed rubustly using trimmed averaging as follows. Fix x ? ? scene , let N x = |{n : M n x &gt; 0}|, and let (g <ref type="bibr">(1)</ref> x,c , . . . , g</p><p>x,c ) be the order statistics of p x,c . The values of ? r and v r at x in channel c are computed, respectively, by</p><formula xml:id="formula_12">? r x,c = 1 (1?2?)Nx (1??)Nx i=?Nx g (i) x,c v r x,c = 1 (1?2?)Nx (1??)Nx i=?Nx (g (i) x,c ? ? r x,c ) 2 .<label>(6)</label></formula><p>Such trimmed averaging is a standard technique for computing robust moments <ref type="bibr" target="#b20">[21]</ref>. The trimming parameter, ?, was empirically set to ? = 0.3 as it provided a good balance between sample size and robustness. That said, the results when using any other value in the wide range between 20% and almost 50% were similar. Next, when f n is fed into the CAE, the encoder and the decoder are conditioned by</p><formula xml:id="formula_13">? n ? ? r | ?n ? (T ?n ) ?1 and v n ? v r | ?n ? (T ?n ) ?1<label>(7)</label></formula><p>which are h ? w images (with C channels) defined on ? and are nothing more than the portion of ? r and v r that is relevant for f n . Using a code whose length was only 4, the CAE was trained with the following loss:</p><formula xml:id="formula_14">L AE = N n=1 C c=1 x ? ?? ? recon (f n x ? ,c ? f n x ? ,c )<label>(8)</label></formula><p>f n = Decoder(Encoder(f n ; ? n , v n ); ? n , v n )</p><p>where f n is the output of the CAE and ? recon is a differentiable robust error function. We remark that, by design, the fact that ? n and v n are of the same dimensions as the input, f n , also means it is easy to implement the conditioning via a convolutional layer. For more details about the CAE (whose architecture is based on the AE from <ref type="bibr">[2]</ref>) as well as other training details, see our Supmat. Finally, ? recon should usually be more robust than ? JA . The reason is that while in JA the influence of foreground objects is relatively small, in the CAE-based reconstruction it is important, in every pixel, to eliminate the outliers (i.e., the foreground pixels) as much as possible. Thus, we use the smoothed ? 1 loss (which is closely-related to Huber's function <ref type="bibr">[5]</ref>) for ? JA and the Geman-McClure error function <ref type="bibr" target="#b16">[17]</ref> for ? recon . See Supmat for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We experimented with 4 variants of the Given an estimate of the background, subtracting it from the original frame yields a difference that can serve to determine foreground/background separation. To quantify the results in a threshold-independent way, for each method and each video we computed the Receiver Operating Characteristic (ROC) curve (using the ground truth) and its Area Under the Curve (AUC). The ROC curves are included in Supmat. We emphasize that our method is unsupervised and the ground truth information was used only for evaluation. <ref type="table" target="#tab_3">Table 1</ref>, summarizing the AUC results, shows that DeepMCBM, especially with its CAE variants, is, overall, the leading method. In cases where DeepMCBM is not the first it is typically the runner-up. Moreover, unlike some competitors, DeepMCBM was applicable in all cases considered. The visual examples also illustrate how the CAE helps achieving a better estimate of the background than that one obtained by merely using the unwarped ? r . We remark that our fixed code size, 4, is so small since: 1) the goal is not a typical reconstruction but to filter out foreground objects; 2) our AE is conditional so it is unsurprising a small size suffices. We could have made the code size video-dependent and thus improve results even further, but felt that a fixed size is simpler and makes a comparison with other methods fairer.</p><p>Predicting background for previously-unseen misaligned frames. In the comparison above, we focused on background/foreground estimation in the input videos on which the competing models (ours included) were learned. However, like JA-POLS, but unlike all the other methods, our method can predict the background in frames that were not included in the learning (more accurately, some of the competing methods can predict the background in the next constitutive frame, but they are unable to do so for misaligned frames in general such as those that are not consecutive). Due to space limits, we demonstrate that capability of DeepMCBM in the Supmat.  <ref type="table" target="#tab_3">Table 1</ref> shows, the AE usually improves performance. In particular, its role is especially important when a foreground object spends a long portion of time in a static position (e.g., the dog in the dog-gooses or the flamingo cases, the robust mean alone still tends to capture some "ghosting" artifacts (as usually do all the competing methods) while the CAE helps correctly identifying that object as belonging to the foreground. The importance of the memory-based approach was also demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="figure" target="#fig_2">Figure 3</ref>. In particular, the JA failures in <ref type="figure" target="#fig_1">Figure 2</ref> imply that no subsequent background model could be built there, making a quantitative comparison (between using the memory term and not using it) a moot point. Finally, note that a basic (i.e., unconditional AE) that knows nothing about the alignment has no chance here as it can only either simply reconstruct the entire frames (i.e., with the undesired foreground objects) or fail in the reconstruction. Thus, when simply dropping the conditioning from our CAE, the resulting AE fails badly in background modeling; e.g., its AUC for the Tennis video is 0.701 while DeepMCBM's AUC score is 0.963.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The proposed DeepMCBM is an end-to-end DL solution for modeling background in a video from a moving camera. It supports a wide range of camera-motion types and sizes, scales gracefully, and achieves SOTA results. While we experimented with either affine transformations or homographies, DeepMCBM also supports more expressive transformations. The proposed regularization-free STN-based JA strategy may find usage in other applications, thereby the potential impact of this work may be broader than MCBMs. One limitation of our work is that, since DL involved, the training is slower in comparison to some competitors (JA-POLS excluded). However, we believe the SOTA results together with the other benefits DeepMCBM brings (end-to-end; scalability; the ability to predict background for previously-unseen misaligned frames; etc.) justifies it. The main failure case of the method is when foreground objects are large and much closer to the camera than the background is. Ben-Gurion University of the Negev, Be'er Sheva, Israel {ergu,ronsha}@post.bgu.ac.il, orenfr@cs.bgu.ac.il</p><p>Abstract. The supplemental material includes, in addition to this document, select examples of videos showing the results of our method next to the original videos. Additionally, and due to space limits, additional visual comparisons between the different methods (using PDF presentations so it be will easy to browse back and forth between the frames) are available at https: //github.com/BGU-CS-VIL/DeepMCBM.</p><p>As for this document, it contains the following: <ref type="table" target="#tab_3">Table 1</ref> in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A comparison of ROC curves of different methods on various videos. These curves correspond to the AUC values reported in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A demonstration of DeepMCBM's capability of predicting the background in previously-unseen misaligned frames from the video it was trained on (a capability lacking in most other methods, except JA-POLS <ref type="bibr">[4]</ref>).</p><p>3. The details of the robust error functions that we used.</p><p>4. The technical details of the evaluation procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The technical details of the architecture and training process.</p><p>6. An explanation how, in the affine Spatial Transformer Net, the invertibility of the affine transformations is guaranteed via the matrix exponential.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Predicting Background to Previously-unseen Misaligned Frames</head><p>In the paper, our comparison of DeepMCBM to the other methods focused on their own playground: i.e., given an input sequence of video frames, the task was viewed as an optimization problem whose overarching goal is to estimate the background in those frames. However, like JA-POLS <ref type="bibr">[4]</ref> but unlike the other competitors, DeepMCBM, being based on (unsupervised) learning, also possesses a generalization capability in the sense it can estimate the background in previously-unseen misaligned frames from the video it was trained on. In contrast, other methods (JA-POLS excluded), do not have a readilyavailable mechanism that enables them to receive such misaligned frames and predict their alignment and background estimation -at least not without solving additional optimization steps. Note that in a static camera background model this is a non-issue since, by definition, the frames (both train and test) are always aligned; however, in the moving-camera case the situation is different due to the misalignment. Remark: Note that this generalization capability should not be confused with an online-learning setting -which some competitors aim for -where each time the next consecutive frame arrives their model is being updated (using further optimization).</p><p>To showcase the generalization capability, in each of the "tennis" and "flamingo" sequences (from <ref type="bibr">[10]</ref>) we partitioned the video sequence into train and test sets. We did it by letting the test set (in each of the two videos) consist of 10% of the frames (chosen at random from the original sequence) while letting the train test consist of the remaining 90% of the frames. In each video we let both DeepMCBM and its competitor, JA-POLS, train only on the train set. Next, we evaluated the models, using their prediction functionalities, on the test sets. <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="table" target="#tab_3">Table 1</ref> summarize the results in terms of the corresponding ROC curve and AUC scores, respectively, and show that DeepMCBM outperforms JA-POLS in this type of evaluation as well. Note that here we intentionally used our CAE/Aff variant (and not CAE/Hom) to highlight the fact that even when using the same transformation type as JA-POLS (i.e., affine), DeepMCBM beats the latter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Robust Error Functions</head><p>We have used the smoothed ? 1 loss (which is closely-related to Huber's function <ref type="bibr">[3]</ref>) for ? JA and Geman-McClure's error function <ref type="bibr">[5]</ref> for ? recon :</p><formula xml:id="formula_16">? JA (?; ?) = 0.5? 2 /? if |?| ? ? |?| ? 0.5? otherwise ? recon (?; s) = ? 2 ? 2 + s 2<label>(1)</label></formula><p>where in all of our experiments we used ? = 0.35 and s = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Details</head><p>In this section we explain how the ROC curves and AUC scores were obtained. Given an input sequence of N frames, (f n ) N n=1 , ground-truth annotations of foreground pixels, (a n ) N n=1 , and a background estimation sequence, ( f n ) N n=1 , we compute the pixelwise squared error (E n ) N n=1 , where E n x = 1 C C c=1 (f n x,c ? f n x,c ) 2 is the error in pixel x ? ? (recall that ?, a rectangle of height h and width w, was defined in the paper as the common domain of the original input frames), averaged across C channels. To have a unified comparison measure applicable in all videos and for all methods, we scale to the [0, 1] interval where the scaling is done (for the method under consideration) w.r.t. the entire video sequence. That is, we define the scaled error of that method in the specific video as</p><formula xml:id="formula_17">E n x = E n x ? min n,x ? (E n x ? ) max n,x ? (E n x ? ) ? min n,x ? (E n x ? ) .<label>(2)</label></formula><p>Next, for each threshold value ? (from a discrete set of evenly-spaced points between 0 and 1) we computed the True Positive Rate (TPR) and False Positive Rate (FPR):</p><formula xml:id="formula_18">TPR = 1 N ? h ? w N n=1 x?? 1(a x = 1 ? E n x ? ?) ;<label>(3)</label></formula><formula xml:id="formula_19">FPR = 1 N ? h ? w N n=1 x?? 1(a x = 0 ? E n x ? ?) .<label>(4)</label></formula><p>Computing the TPR and FPR for multiple threshold values lets us obtain the Receiver Operating Characteristic (ROC) curve for this (method,sequence) pair. <ref type="figure" target="#fig_0">Figure 1</ref> shows the resulted ROC curves while Area Under the (ROC) Curve (AUC) scores are summarized in <ref type="table" target="#tab_3">Table 1</ref> in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Architecture and Training Details</head><p>DeepMCBM's pipeline consists of two main parts:</p><p>1. a Spatial Transformer Net (STN); 2. a Conditional Autoencoder (CAE).</p><p>First, an STN is trained to learn an input-dependent function, that predicts the transformation parameter vector ?. These predictions are used to create the panoramic central moments as well as, later, to unwarp those moments back towards the input image. Once the STN training process has converged, the STN module is frozen and the CAE is trained to learn a background model in the input domain, where the conditioning is on the aforementioned unwapred moments. Below we describe the architecture and training details of the STN and CAE modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The STN</head><p>The STN we used consisted of a backbone and tailored regression heads for each available transformation type (in our case, either affine of homographies). The backbone we used was ResNet18 [6] whose output size was 1000. Each regression head consisted of two dense layers of size 1000 ? 32 and 32 ? d, where d is the dimension of the transformation parameters vector ? (d = 6 for affine transformations and d = 8 for homographies). In both the regressor heads we used a ReLU <ref type="bibr">[1]</ref> activation function. In all our experiments, we trained the backbone and the Affine head for the first 3000 epochs. In the variants that used homographies, we first did repeat the process above (that is, 3000 epochs for training the backbone and the affine head) and then switched to train the homographic head for additional 3000 epochs (recall that our affine transformations are invertible and that such transformations are a particular case of homographies; thus, the results from the affine head provide a good initialization for the homographic head). Thus, in total, the STN module was trained for about 6000 epochs with a step learning rate scheduler, decreasing the learning rate every 1000 epochs with an initial learning rate of 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The CAE</head><p>The second module in the DeepMCBM pipeline is a CAE, conditioned on the unwarped panoramic central moments. In our experiments, we conditioned the CAE on the first two such moments, but more generally, any number of moments can be used as well. Theoretically, using more moments implies that the distribution of the pixel stack is better captured.</p><p>Our CAE was based on the (unconditional) Autoencoder from <ref type="bibr">[2]</ref>. Using a similar architecture, we used 4 channels in each hidden convolutional layer and 2 channels for the last convolutional layer. In between the convolutional encoder and decoder we used dense layers of size 256 ? 4 and 4 ? 256 (thus, the code size was 4) with a ReLU <ref type="bibr">[1]</ref> activation function. All convolutional layers used a 5 ? 5 kernel with stride size 2.</p><p>Our conditioning is done in both the encoder and decoder parts. As the conditioning (the unwarped central moments) is in the same spatial dimensions as the input image, conditioning on the encoder size was done by a simple concatenation, along the channel dimension, of the input and the unwarped moments. This results in (N moments + 1) ? C input channels for the encoder where N moments is the number of the moments used (N moments = 2 in our experiments) and C is the number of channels of an input image (C = 3 in the RGB case). To condition the decoder, we concatenate the unwaped moments to the (classic) decoder output and then use a small Convolutional Neural Net (CNN) to integrate the decoder's output and the conditioning. This last CNN is composed of 3 convolutional layers with 50 channels for the hidden layesr and C channels for the output layer. All three layers use a 3 ? 3 kernel, and a ReLU <ref type="bibr">[1]</ref> activation function.</p><p>Using the robust reconstruction loss mentioned in the paper, we trained the CAE for 2000 epochs with a learning rate scheduler and a step size of 500 epochs and initial learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Using the Matrix Exponential within an STN</head><p>Let A be a 3 ? 3 real-valued matrix such that its last row is all zeros:</p><formula xml:id="formula_20">A = ? ? A 1 A 2 A 3 A 4 A 5 A 6 0 0 0 ? ? .<label>(5)</label></formula><p>Then, a known result (which is also widely-used in computer vision; see, e.g., <ref type="bibr">[8,</ref><ref type="bibr">9]</ref>) from the theory of matrix groups is that</p><formula xml:id="formula_21">T ? exp(A)<label>(6)</label></formula><p>(i.e., the matrix exponential of A) has the following form,</p><formula xml:id="formula_22">T = ? ? T 1 T 2 T 3 T 4 T 5 T 6 0 0 1 ? ?<label>(7)</label></formula><p>where, in addition, we have that det T &gt; 0 (in particular, T is invertible). Consequently, the matrix exponential provides a mapping from the unconstrained linear space R 6 into the Affine Group (namely, the space of invertible affine transformations -in this case from R 2 to R 2 ). Taken together with the fact that the matrix exponential is a differentiable function, this means that an STN <ref type="bibr">[7]</ref> can be easily set to produce only invertible transformations <ref type="bibr">[11,</ref><ref type="bibr">4]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Examples for several input frames (b) Alignment, visualized via the mean panoramic image (computed from the entire video) (c) Background estimation using the Conditional Autoencoder Typical results of the proposed module. Note that despite the fact that the dog spent long times being static in two locations (as is evident by the corresponding ghosting effects in (b)) the model succeeded in eliminating it from the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Typical problems in JA. Rightmost images are post-convergence results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Results analogous to those inFigure 2except they were obtained with the proposed memory-based approach. Rightmost images are post-convergence results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Visual Comparison: Select Results. Note the ghosting artifacts. N/A (1) large zoom changes failed JA-POLS completely, N/A (2) out-of-memory on a 256GB RAM machine, N/A (3) failed to run: Matlab process was killed Ablation Study. As</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>24. Huang, G., Mattar, M., Lee, H., Learned-Miller, E.G.: Learning to align from scratch. In: NIPS (2012) 5 25. Huang, G.B., Jain, V., Learned-Miller, E.: Unsupervised joint alignment of complex images. In: ICCV (2007) 5 26. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks. In: NeurIPS (2015) 3 27. Jin, Y., Tao, L., Di, H., Rao, N.I., Xu, G.: Background modeling from a free-moving camera by multi-layer homography algorithm. In: ICIP (2008) 4 28. Kaufman, I., Weber, R.S., Freifeld, O.: Cyclic diffeomorphic transformer nets for contour alignment. In: ICIP (2021) 6 29. Kendall, A., Grimes, M., Cipolla, R.: Posenet: A convolutional network for real-time 6-dof camera relocalization. In: ICCV (2015) 4 30. Klein, G., Murray, D.: Parallel tracking and mapping for small ar workspaces. In: International Symposium on Mixed and Augmented Reality (2007) 4 31. Learned-Miller, E.G.: Data driven image models through continuous joint alignment. IEEE TPAMI (2006) 5, 6, 9 32. Meneghetti, G., Danelljan, M., Felsberg, M., Nordberg, K.: Image alignment for panorama stitching in sparsely structured environments. In: Scandinavian Conference on Image Analysis (2015) 4 33. Miller, E.G., Matsakis, N.E., Viola, P.A.: Learning from one example through shared densities on transforms. In: CVPR (2000) 5 34. Moore, B.E., Gao, C., Nadakuditi, R.R.: Panoramic robust PCA for foreground-background separation on noisy, free-motion camera video. IEEE Transactions on Computational Imaging (2019) 4, 12, 14 35. Newcombe, R.A., Lovegrove, S.J., Davison, A.J.: Dtam: Dense tracking and mapping in real-time. In: ICCV (2011) 4 36. Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel?ez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017) 12 37. Rosen, D.M., Carlone, L., Bandeira, A.S., Leonard, J.J.: SE-Sync: A certifiably correct algorithm for synchronization over the special Euclidean group. The International Journal of Robotics Research (2019) 6 38. Shapira Weber, R., Eyal, M., Skafte Detlefsen, N., Shriki, O., Freifeld, O.: Diffeomorphic temporal alignment nets. In: NeurIPS (2019) 6 39. Sheikh, Y., Javed, O., Kanade, T.: Background subtraction for freely moving cameras. In: ICCV (2009) 4 40. Skafte Detlefsen, N., Freifeld, O., Hauberg, S.: Deep diffeomorphic transformer networks. In: CVPR (2018) 6, 9 41. Stauffer, C., Grimson, W.E.L.: Adaptive background mixture models for real-time tracking. In: CVPR (1999) 3 42. Szeliski, R.: Computer vision: algorithms and applications. Springer Science &amp; Business Media (2010) 5 43. Thurnhofer-Hemsi, K., L?pez-Rubio, E., Dom?nguez, E., Luque-Baena, R.M., Molina-Cabello, M.A.: Panoramic background modeling for ptz cameras with competitive learning neural networks. In: IJCNN (2017) 4 44. De la Torre, F., Black, M.J.: Robust principal component analysis for computer vision. In: ICCV (2001) 3 45. Wang, Y., Jodoin, P.M., Porikli, F., Konrad, J., Benezeth, Y., Ishwar, P.: Cdnet 2014: an expanded change detection benchmark dataset. In: CVPR Workshop (2014) 12 46. Wu, C.: Towards linear-time incremental structure from motion. In: International Conference on 3D Vision (2013) 4 47. Xue, K., Liu, Y., Chen, J., Li, Q.: Panoramic background model for PTZ camera. In: International Congress on Image and Signal Processing (2010) 4 48. Yalcin, H., Hebert, M., Collins, R., Black, M.J.: A flow-based approach to vehicle detection and background mosaicking in airborne video. In: CVPR (2005) 4 49. Zhou, X., Yang, C., Yu, W.: Moving object detection by detecting contiguous outliers in the low-rank representation. TPAMI (2012) 4, 12, 14 50. Zhou, Z., Li, X., Wright, J., Candes, E., Ma, Y.: Stable principal component pursuit. In: ISIT (2010) 3 An End-to-end Moving Camera Background Model ------Supplemental Material Guy Erez 1[0000?0002?4545?6664] , Ron Shapira Weber 1[0000?0003?4579?0678] , and Oren Freifeld 1[0000?0001?9816?9709]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 :</head><label>1</label><figDesc>A comparison of ROC curves of different methods on various videos. Note that sometimes some curves coincide (e.g., our CAE/Aff and CAE/Hom on the flamingo video).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>proposed DeepMCBM: 1. Basic/Aff: This version uses only the STN-based JA module, without the CAE. It estimates the background by simply unwarping the robust mean towards the input image. The transformations used in the STN belong to the Affine group (the invertibility of the transformations was guaranteed via the matrix exponential; see Supmat). 2. CAE/Aff: This version too uses the Affine STN but also uses the CAE module (for estimating the background). 3. The 13 videos that we tested on are ones typically used for evaluation of methods in this area and are taken from well-known datasets[45,36]. Those movies cover camera motions in a variety of types, sizes, speed, zoom changes, etc. It should be noted that, due to their scalability limitaitons, PRPCA and PanGAEA could not run on the ContinuousPan video as the covered scene in the latter was too large.</figDesc><table><row><cell>Ba-sic/Hom and 4. CAE/Hom: Similar to Basic/Aff and CAE/Aff, respectively, except that homographies are used instead of affine transformations. We compared those 4 variants with several methods: PRPCA [34]; JA-POLS [10]; PanGAEA [18]; DECOLOR [49]; PCP PTI [9]; PRAC [19].</cell></row></table><note>JA-POLS failed running on zoomInZoomOut (the significant zoom changes broke its key assumption). ?? contains a visual comparison, on select example videos, of DeepM- CBM (in its CAE/Hom variant), PRPCA, JA-POLS, and PanGAEA. Results of the other (and less successful) methods (DECOLOR; PCP PTI; PRAC), as well as more visual results (including videos) are in the Supmat.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 ROC Curves (Related to the Values fromTable 1 in the Paper)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>tennis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>flamingo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>continuousPan</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell cols="2">Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS PCP_PTI PRPCA DECOLOR PRAC PanGAEA</cell><cell></cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell cols="2">Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS PCP_PTI DECOLOR PRPCA PRAC PanGAEA</cell><cell></cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell>PCP_PTI DECOLOR PRAC Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS</cell></row><row><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 dog-gooses</cell><cell>0.8</cell><cell>1.0</cell><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 bmx-trees</cell><cell>0.8</cell><cell>1.0</cell><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 stunt</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell cols="2">Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS PCP_PTI PRPCA DECOLOR PRAC PanGAEA</cell><cell></cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell cols="2">Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS PCP_PTI PRPCA DECOLOR PRAC PanGAEA</cell><cell></cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell>Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS PCP_PTI PRPCA DECOLOR PRAC PanGAEA</cell></row><row><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 zoomInZoomOut</cell><cell>0.8</cell><cell>1.0</cell><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 horsejump-high</cell><cell>0.8</cell><cell>1.0</cell><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 sidewalk</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell cols="2">Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) PCP_PTI PRPCA DECOLOR PRAC PanGAEA</cell><cell></cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell cols="2">Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS PCP_PTI PRPCA DECOLOR PRAC PanGAEA</cell><cell></cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell>JA-POLS PCP_PTI PRPCA DECOLOR PRAC Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) PanGAEA</cell></row><row><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 stroller</cell><cell>0.8</cell><cell>1.0</cell><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 boxing-fisheye</cell><cell>0.8</cell><cell>1.0</cell><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6 swing</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.0 0.0 0.2 0.4 0.6</cell><cell>0.2</cell><cell cols="2">0.4 False Positive Rate 0.6 PCP_PTI 0.8 PRPCA DECOLOR PRAC Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) PanGAEA JA-POLS</cell><cell>1.0</cell><cell>True Positive Rate</cell><cell>0.0 0.2 0.4 0.6 0.0</cell><cell>0.2</cell><cell cols="2">0.4 False Positive Rate 0.6 PRAC PRPCA DECOLOR 0.8 PCP_PTI JA-POLS Ours (Basic/H) Ours (CAE/H) Ours (Basic/Aff) Ours (CAE/Aff) PanGAEA breakdance-flare</cell><cell>1.0</cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6 0.0 0.0</cell><cell>0.2</cell><cell>JA-POLS PCP_PTI PRPCA DECOLOR PRAC Ours (CAE/H) Ours (Basic/H) Ours (CAE/Aff) Ours (Basic/Aff) PanGAEA 0.8 False Positive Rate 0.4 0.6</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>True Positive Rate</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell cols="2">Ours (CAE/Aff) Ours (Basic/Aff) Ours (CAE/H) Ours (Basic/H) JA-POLS PCP_PTI PRPCA DECOLOR PRAC PanGAEA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>AUC scores for test and train sets. DeepMCBM in its CAE+Affine version</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>flamingo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tennis</cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>True Positive Rate</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours (CAE/Aff) JA-POLS</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours (CAE/Aff) JA-POLS</cell></row><row><cell></cell><cell>0.0</cell><cell>.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0.0 0.0</cell><cell>0.2</cell><cell>0.4 False Positive Rate 0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell cols="12">Fig. 2: A typical comparison of ROC curves, between the proposed method and JA-POLS [4], of the performance on test sets.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Erez et al.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by the Lynn and William Frankel Center at BGU CS and by Israel Science Foundation Personal Grant #360/21. G.E. was also funded by the VATAT National excellence scholarship for female Master's students in Hi-Tech-related fields.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end optimized image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Online identification and tracking of subspaces from highly incomplete information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
			<pubPlace>Allerton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Subspace tracking under dynamic dimensionality for online background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Seversky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the unification of line processes, outlier rejection, and robust statistics with applications in early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic panoramic image stitching using invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Intrinsic grassmann averages for online linear and robust subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Vemuri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Panning and jitter invariant incremental principal component pursuit for video background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">JA-POLS: a moving-camera background model via joint alignment and partially-overlapping local subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2020) 3, 4, 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Least squares congealing for unsupervised alignment of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Least-squares congealing for large numbers of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical moving object detection for mobile devices with camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohedano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garc?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCE</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning conditional deformable templates with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Highly-expressive spaces of well-behaved transformations: Keeping it simple</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformations based on continuous piecewise-affine velocity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Statistical methods for tomographic image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Mcclure</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>BISI</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panoramic video separation with online Grassmannian robust subspace estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical reprocs for separating sparse and low-dimensional signal sequences from their sum-part 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Foreground detection via robust low rank matrix decomposition including spatio-temporal constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Zahzah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Grassmann averages for scalable robust pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Incremental gradient on the grassmannian for online foreground and background separation in subsampled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iterative grassmannian optimization for robust image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end optimized image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the unification of line processes, outlier rejection, and robust statistics with applications in early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">JA-POLS: a moving-camera background model via joint alignment and partially-overlapping local subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020) 1, 4</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Statistical methods for tomographic image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Mcclure</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>BISI</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning visual flows: A Lie algebraic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Modeling and estimating persistent motion with geometric flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep diffeomorphic transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Skafte Detlefsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
