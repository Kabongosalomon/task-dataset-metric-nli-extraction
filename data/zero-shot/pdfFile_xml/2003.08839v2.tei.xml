<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
							<email>tabish.rashid@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
							<email>mikayel@samvelyan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De Witt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
							<email>gregory.farquhar@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
							<email>shimon.whiteson@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De Witt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Samvelyan</roleName><forename type="first">Schroeder</forename><surname>Rashid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Farquhar, Foerster, Whiteson</roleName><surname>De Witt</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Armenian University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Journal of Machine Learning Research</title>
						<imprint>
							<biblScope unit="volume">21</biblScope>
							<biblScope unit="page" from="1" to="52"/>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
					<note type="submission">Submitted 1/20; Revised 8/20; Published 8/20</note>
					<note>University of Oxford Editor: George Konidaris</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement Learning</term>
					<term>Multi-Agent Learning</term>
					<term>Multi-Agent Coordination * Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning (RL) holds considerable promise to help address a variety of cooperative multi-agent problems, such as coordination of robot swarms <ref type="bibr" target="#b25">(H?ttenrauch et al., 2017)</ref> and autonomous cars <ref type="bibr" target="#b6">(Cao et al., 2012)</ref>.</p><p>In many such settings, partial observability and/or communication constraints necessitate the learning of decentralised policies, which condition only on the local action-observation history of each agent. Decentralised policies also naturally attenuate the problem that joint action spaces grow exponentially with the number of agents, often rendering the application of traditional single-agent RL methods impractical.</p><p>Fortunately, decentralised policies can often be learned in a centralised fashion in a simulated or laboratory setting. This often grants access to additional state information, otherwise hidden from agents, and removes inter-agent communication constraints. The paradigm of centralised training with decentralised execution <ref type="bibr" target="#b43">(Oliehoek et al., 2008;</ref><ref type="bibr" target="#b30">Kraemer and Banerjee, 2016)</ref> has recently attracted attention in the RL community <ref type="bibr" target="#b28">(Jorge et al., 2016;</ref>. However, many challenges surrounding how to best exploit centralised training remain open.</p><p>One of these challenges is how to represent and use the action-value function that many RL methods learn. On the one hand, properly capturing the effects of the agents' actions requires a centralised action-value function Q tot that conditions on the global state and the joint action. On the other hand, such a function is difficult to learn when there are many agents and, even if it can be learned, offers no obvious way to extract decentralised policies that allow each agent to select only an individual action based on an individual observation.</p><p>The simplest option is to forgo a centralised action-value function and let each agent a learn an individual action-value function Q a independently, as in independent Q-learning (IQL) <ref type="bibr" target="#b66">(Tan, 1993)</ref>. However, this approach cannot explicitly represent interactions between the agents and may not converge, as each agent's learning is confounded by the learning and exploration of others.</p><p>At the other extreme, we can learn a fully centralised action-value function Q tot and then use it to guide the optimisation of decentralised policies in an actor-critic framework, an approach taken by counterfactual multi-agent (COMA) policy gradients , as well as work by <ref type="bibr" target="#b18">Gupta et al. (2017)</ref>. However, this requires on-policy learning, which can be sample-inefficient, and training the fully centralised critic becomes impractical when there are more than a handful of agents.</p><p>In between these two extremes, we can learn a centralised but factored Q tot , an approach taken by value decomposition networks (VDN) <ref type="bibr" target="#b63">(Sunehag et al., 2017)</ref>. By representing Q tot as a sum of individual value functions Q a that condition only on individual observations and actions, a decentralised policy arises simply from each agent selecting actions greedily with respect to its Q a . However, VDN severely limits the complexity of centralised action-value functions that can be represented and ignores any extra state information available during training.</p><p>In this paper, we propose a new approach called QMIX which, like VDN, lies between the extremes of IQL and COMA, but can represent a much richer class of action-value functions. Key to our method is the insight that the full factorisation of VDN is not necessary to extract decentralised policies. Instead, we only need to ensure that a global argmax performed on Q tot yields the same result as a set of individual argmax operations performed on each Q a . To this end, it suffices to enforce a monotonicity constraint on the relationship between Q tot and each Q a : ?Q tot ?Q a ? 0, ?a.</p><p>QMIX consists of agent networks representing each Q a , and a mixing network that combines them into Q tot , not as a simple sum as in VDN, but in a complex nonlinear way that ensures consistency between the centralised and decentralised policies. At the same time, it enforces the constraint of (1) by restricting the mixing network to have positive weights. We use hypernetworks <ref type="bibr" target="#b19">(Ha et al., 2017)</ref> to condition the weights of the mixing network on the state, which is observed only during training. As a result, QMIX can represent complex centralised action-value functions with a factored representation that scales well in the number of agents and allows decentralised policies to be easily extracted via inexpensive individual argmax operations.</p><p>To evaluate QMIX, as well as the growing number of other algorithms recently proposed for multi-agent RL <ref type="bibr" target="#b63">Sunehag et al., 2017)</ref>, we introduce the StarCraft Multi-Agent Challenge (SMAC) 1 . In single-agent RL, standard environments such as the Arcade Learning Environment <ref type="bibr" target="#b2">(Bellemare et al., 2013)</ref> and MuJoCo <ref type="bibr" target="#b52">(Plappert et al., 2018)</ref> have facilitated rapid progress. While some multi-agent testbeds have emerged, such as Poker <ref type="bibr" target="#b21">(Heinrich and Silver, 2016)</ref>, Pong <ref type="bibr" target="#b65">(Tampuu et al., 2017)</ref>, Keepaway Soccer <ref type="bibr" target="#b60">(Stone et al., 2005)</ref>, or simple gridworld-like environments <ref type="bibr" target="#b36">(Lowe et al., 2017;</ref><ref type="bibr">Zheng et al., 2017)</ref>, there are currently no challenging standard testbeds for centralised training with decentralised execution with the exception of the recently introduced Hanabi Challenge ( <ref type="bibr" target="#b1">Bard et al., 2020)</ref> which focusses on a setting with less than 5 agents.</p><p>SMAC fills this gap. It is built on the popular real-time strategy game StarCraft II and makes use of the SC2LE environment <ref type="bibr" target="#b69">(Vinyals et al., 2017)</ref>. Instead of tackling the full game of StarCraft with centralised control, it focuses on decentralised micromanagement challenges ( <ref type="figure" target="#fig_0">Figure 1</ref>). In these challenges, each of our units is controlled by an independent, learning agent that has to act based only on local observations, while the opponent's units are controlled by the hand-coded built-in StarCraft II AI. SMAC offers a diverse set of scenarios that challenge algorithms to handle high-dimensional inputs and partial observability, and to learn coordinated behaviour even when restricted to fully decentralised execution. In contrast to the diverse set of scenarios included in SMAC, the Hanabi Challenge focusses on a single task involving between 2-5 agents designed to test agents' ability to reason about the actions of the other agents (an ability in humans referred to as theory of mind <ref type="bibr" target="#b53">(Rabinowitz et al., 2018)</ref>).</p><p>To further facilitate research in this field, we also open-source PyMARL, a learning framework that can serve as a starting point for other researchers and includes implementations of several key multi-agent RL algorithms. PyMARL is modular, extensible, built on PyTorch, and serves as a template for dealing with some of the unique challenges of deep multi-agent RL in practice. We also offer a set of guidelines for best practices in evaluations using our benchmark, including the reporting of standardised performance metrics, sample efficiency, and computational requirements (see <ref type="bibr">Section 7)</ref>.</p><p>Our experiments on SMAC show that QMIX outperforms IQL, VDN, and COMA, both in terms of absolute performance and learning speed. In particular, our method shows considerable performance gains on the harder tasks in SMAC, and tasks with heterogeneous agents. Moreover, our analysis and ablations show both the necessity of conditioning on the state information and a flexible multi-layer network for mixing of agent Q-values in order to achieve consistent performance across tasks.</p><p>Since its introduction by <ref type="bibr" target="#b54">Rashid et al. (2018)</ref>, QMIX has become an important valuebased algorithm in discrete action environments. It has inspired a number of extensions and follow-up work (Section 2.4) and is a prominent point of comparison in any work that utilises SMAC (Section 2.5). Recently it is becoming increasingly common (and arguably necessary) to evaluate deep multi-agent RL algorithms on non-gridworld environments. This has been driven in part by the introduction of PyMARL and SMAC, which provide an open-source codebase and a standardised testbed for evaluating and comparing deep multi-agent RL algorithms. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent work in multi-agent RL has started moving from tabular methods <ref type="bibr" target="#b76">(Yang and Gu, 2004;</ref><ref type="bibr" target="#b4">Busoniu et al., 2008)</ref> to deep learning methods that can tackle high-dimensional state and action spaces <ref type="bibr" target="#b65">(Tampuu et al., 2017;</ref><ref type="bibr" target="#b50">Peng et al., 2017;</ref>. In this paper, we focus on the fully-cooperative setting in which all agents must maximise a joint reward signal in the paradigm of centralised training and decentralised execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Independent Learners</head><p>A natural approach to producing decentraliseable agents in a multi-agent system is to directly learn decentralised value functions or policies. Independent Q-learning <ref type="bibr" target="#b66">(Tan, 1993)</ref> trains independent action-value functions for each agent using Q-learning <ref type="bibr" target="#b74">(Watkins, 1989)</ref>. <ref type="bibr" target="#b65">Tampuu et al. (2017)</ref> extend this approach to deep neural networks using DQN <ref type="bibr" target="#b41">(Mnih et al., 2015)</ref>. While trivially achieving decentralisation, these approaches are prone to instability arising from the non-stationarity of the environment induced by simultaneously learning and exploring agents. <ref type="bibr" target="#b14">Foerster et al. (2017)</ref> addresses the issue of non-stationarity when using an experience replay with independent learners to some extent. <ref type="bibr" target="#b31">Lauer and Riedmiller (2000)</ref> ignore updates which decrease Q-value estimates in order to not prematurely underestimate an action's Q-value due to the exploratory actions of other agents. In a tabular setting, they prove this converges to the optimal policy in deterministic environments, provided there is a unique optimal joint-action. <ref type="bibr" target="#b40">Matignon et al. (2007)</ref> introduce Hysteretic Qlearning which instead uses a smaller learning rate for decreasing Q-value estimates, which is slightly more robust to stochasticity and the presence of multiple optimal joint actions. <ref type="bibr" target="#b44">Omidshafiei et al. (2017)</ref> utilises Hysteretic Q-learning in a multi-task deep RL setting. <ref type="bibr" target="#b49">Panait et al. (2008)</ref> introduce Leniency which ignores updates that decrease Q-value estimates with a probability that is decreasing during training. <ref type="bibr" target="#b75">Wei and Luke (2016)</ref> show that Leniency outperforms Hysteretic Q-learning in cooperative stochastic games, and <ref type="bibr" target="#b47">Palmer et al. (2018)</ref> extend Leniency to the deep RL setting and show benefits over Hysteretic and fully Independent Q-learners on deterministic and stochastic gridworlds with two agents. <ref type="bibr" target="#b48">Palmer et al. (2019)</ref> maintain a learned interval whose lower bound is approximately the minimum cumulative reward received when all agents are coordinating. They use this interval when decreasing Q-value estimates to distinguish between miscoordination between the agents and the stochasticity of the environment, which Hysteretic and Lenient learners do not distinguish between. <ref type="bibr" target="#b37">Lu and Amato (2019)</ref> utilise Distributional RL <ref type="bibr" target="#b3">(Bellemare et al., 2017)</ref> in combination with Hysteretic Q-learning in order to also distinguish between miscoordination and stochasticity, and argue their approach is more stable and robust to hyperparameters than the above methods. All of these approaches do not utilise extra state information available during a centralised training regime, nor do they attempt to learn joint action-value functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Centralised Execution</head><p>In settings where decentralised execution is not mandatory, the centralised learning of joint action-value function naturally handles the coordination problems and avoids the non-stationarity problem. Nonetheless, a centralised action-value function is impractical to scale since the joint action space grows exponentially in the number of agents. Classical approaches to scalable centralised learning include coordination graphs <ref type="bibr" target="#b17">(Guestrin et al., 2002)</ref>, which exploit conditional independencies between agents by decomposing a global reward function into a sum of agent-local terms. This can significantly reduce the computation required in order to find the maximum joint action (depending on the exact structure of the coordination graph supplied) through message passing or variable elimination algorithms. However, specifying an appropriate coordination graph (i.e. not fully-connected) can require significant domain knowledge. Sparse cooperative Q-learning <ref type="bibr" target="#b29">(Kok and Vlassis, 2006</ref>) is a tabular Q-learning algorithm that learns to coordinate the actions of a group of cooperative agents only in the states in which such coordination is necessary, encoding those dependencies in a coordination graph. Both methods require the dependencies between agents to be pre-supplied, whereas we do not require such prior knowledge. <ref type="bibr" target="#b7">Castellini et al. (2019)</ref> investigate the representational capacity of coordination graphs in a deep RL setting using one-step matrix games. <ref type="bibr" target="#b5">Bhmer et al. (2019)</ref> also extend coordination graphs to the deep RL setting, making use of parameter sharing and limiting the graphs to contain only pairwise edges, in order to consider more complex scenarios. <ref type="bibr" target="#b8">Chen et al. (2018)</ref> similarly factor the joint Q-function into pairwise interaction terms, performing maximisation using coordinate ascent instead of message-passing. QMIX (and VDN) correspond to the case of a degenerate fully disconnected coordination graph, thus enabling fully decentralised execution.</p><p>DIAL <ref type="bibr" target="#b13">(Foerster et al., 2016)</ref> utilises an end-to-end differentiable architecture that allows for a learned inter-agent communication to emerge via backpropagation. CommNet <ref type="bibr" target="#b62">(Sukhbaatar et al., 2016)</ref> use a centralised network architecture to exchange information between agents. BicNet <ref type="bibr" target="#b50">(Peng et al., 2017)</ref> utilise bidirectional RNNs for inter-agent communication in an actor-critic setting and additionally requires estimating individual agent rewards. MAGNet <ref type="bibr" target="#b39">(Malysheva et al., 2018)</ref> allow agents to centrally learn a shared graph structure that encodes the relevance of individual environment entities to each agent. Based on this graph, agents can communicate and coordinate their actions during centralised execution. <ref type="bibr" target="#b81">Zhao and Ma (2019)</ref> show that the performance of decentralised policies can be improved upon through a centralised communication scheme that uses a central informationfiltering neural network in between timesteps. Clustered Deep Q-Networks (CDQN) <ref type="bibr" target="#b46">(Pageaud et al., 2019)</ref> use a hierarchical approach in order to scale learning to more agents and larger joint action spaces. Low-level agents are clustered into groups, each of which is managed by a higher-level agent within a centralised execution setting. All low-level agents within a cluster execute the same action selected by a vote, which alleviates some of the non-stationarity of the independent learning setting. In contrast to these approaches, QMIX does not require any form of inter-agent communication during decentralised execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Centralised Training and Decentralised Execution</head><p>A number of methods have developed hybrid approaches that exploit the centralised training opportunity for training fully decentralised policies. <ref type="bibr" target="#b18">Gupta et al. (2017)</ref> present a centralised actor-critic algorithm with per-agent critics, which scales easily with the number of agents. Similarly, <ref type="bibr" target="#b36">Lowe et al. (2017)</ref> present MADDPG, which learns a centralised critic for each agent and apply this to competitive games with continuous action spaces. In contrast to <ref type="bibr" target="#b18">Gupta et al. (2017)</ref>, MADDPG takes advantage of the centralised training paradigm by incorporating information from the other agents into the critics, at the expense of scalability due to the increased input size. COMA  instead uses a single centralised critic to train decentralised actors in the fully-cooperative setting, estimating a counterfactual advantage function for each agent in order to address multi-agent credit assignment. <ref type="bibr" target="#b27">Iqbal and Sha (2018)</ref> devise an actor-critic algorithm with per-agent critics that share an attention mechanism in order to improve scalability compared to a single centralised critic. Their approach allows for the agents to have differing action spaces as well allowing for individual rewards. The authors do not compare against QMIX and do not evaluate on environments of similar complexity as StarCraft. Schroeder de Witt et al. (2018) construct a multi-agent actor-critic algorithm with a hierarchical actor policy that is able to use common knowledge between agents for coordination. Learning Individual Intrinsic Reward (LIIR) <ref type="bibr" target="#b10">(Du et al., 2019)</ref> learns an intrinsic reward function for each agent to supplement the team reward. The intrinsic reward function is trained such that it maximises the team reward function. As a bi-level optimisation process, LIIR is significantly less computationally efficient than QMIX and has not been demonstrated to scale to large numbers of agents. These approaches are all actor-critic variants which use the policy-gradient theorem, and thus are prone to get stuck sub-optimal local minima. Additionally, all but MADDPG are on-policy algorithms which can have poor sample efficiency compared to off-policy algorithms such as QMIX.</p><p>Lin et al. (2019) train a centralised action-value function Q which has access to observations of all agent. They then train the decentralised agents via supervised learning to mimic the actions of the centralised policy. Distilling a centralised policy into decentralised agent policies can be problematic since the centralised policy conditions on more information than any of the agents' policies. QMIX instead learns a factored joint Q-function that can be easily decentralised due to its architecture. <ref type="bibr" target="#b63">Sunehag et al. (2017)</ref> propose value decomposition networks (VDN), which allow for centralised value function learning with decentralised execution. Their algorithm decomposes a central action-value function into a sum of individual agent terms. VDN does not make use of additional state information during training and can represent only a limited class of centralised action-value functions. <ref type="bibr" target="#b59">Son et al. (2019)</ref> introduce QTRAN which learns a centralised, unrestricted, joint Q-function as well as a VDN-factored joint Q-function that is decentralisable. Since the unrestricted Q-value cannot be maximised efficiently, the VDN-factored Q is used to produce the (approximate) maximum joint-action. They show that solving a linear optimisation problem involving all joint actions, in which the VDN-factored Q-value matches the unrestricted Q-value for the approximated maximum joint action and over-estimates for every other action, results in decentralisable policies with the correct argmax. This allows QTRAN to represent a much larger class of joint Q-function than QMIX, as well as produce decentralisable policies for them. However, exactly solving the linear optimisation problem is prohibitively expensive. Thus, the authors instead optimise a soft approximation using L 2 penalties via stochastic gradient descent. In practice, optimising this loss is difficult which results in poor performance for QTRAN on complex environments such as SMAC. <ref type="bibr" target="#b38">Mahajan et al. (2019)</ref> show that the representational constraints of QMIX can prohibit it from learning an optimal policy. Importantly, they show that this is exacerbated by increased -greedy exploration. They introduce MAVEN, which conditions QMIX agents on a shared latent space whose value is chosen at the beginning of an episode by a hierarchical policy. Note that this requires access to the initial state and communication at the first timestep. A mutual information loss is added to encourage diversity of trajectories across the shared latent space, which allows for relatively-greedy agents (once conditioned on the latent variable) to still achieve committed exploration during training similar to Bootstrapped DQN <ref type="bibr" target="#b45">(Osband et al., 2016)</ref>. This results in significant performance gains on some scenarios in SMAC .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Extensions of QMIX</head><p>Action Semantics Networks (ASN) <ref type="bibr" target="#b72">(Wang et al., 2019b)</ref> extends QMIX with a novel agent network architecture that separates each agent's Q-value estimation of actions that influence other agents from those actions that do not. The Q-values for actions which influence another agent are computed using the relevant parts of the current agent's observation (e.g. the relative position of another agent). Thus, ASN requires intimate a priori knowledge about an environment's action semantics and the structure of the agent's observations, whereas QMIX does not. <ref type="bibr" target="#b71">Wang et al. (2019a)</ref> extend QMIX to a setting which allows for communication between agents. Their approach allows agents to condition their Q-values on messages from other agents, which take the form of a real-valued vector. An entropy loss and a mutual information loss incentivise the learning of succinct and expressive messages. Zhang et al. (2019) also allow for each agent's Q-values to condition on vector-valued messages from other agents. However, they only utilise the messages from other agents if their gap between the best and second-best action is small enough (interpreted as uncertainty in the best choice of action). Additionally, they minimise the variance of the messages across actions to limit noisy or uninformative messages. SMIX(?) <ref type="bibr" target="#b79">(Yao et al., 2019)</ref> replaces the 1-step Q-learning target of QMIX with a SARSA(?) target. They incorrectly claim that the Q-learning update rule is responsible for the representational limitations of QMIX, instead of the non-negative weights in the mixing network which enforces monotonicity. They also incorrectly claim that their method can represent a larger class of joint action-value functions than QMIX. Since they also use non-negative weights in their mixing network, SMIX(?) can represent exactly the same class of value functions as QMIX.  investigate transfer learning algorithms in a multi-agent setting using QMIX as a base.  use DIAYN <ref type="bibr" target="#b12">(Eysenbach et al., 2018)</ref> to learn lower-level skills that a higher level QMIX agent uses. <ref type="bibr" target="#b16">Fu et al. (2019)</ref> extend QMIX to allow for action spaces consisting of both discrete and continuous actions.</p><p>The applicability of QMIX in real-world application domains has also been considered such as robot swarm coordination <ref type="bibr" target="#b26">(Httenrauch et al., 2018)</ref> and stratospheric geoengineering <ref type="bibr" target="#b56">(Schroeder de Witt and Hornigold, 2019)</ref>.</p><p>QMIX has been extended from discrete to continuous action spaces <ref type="bibr">(Schroeder de Witt et al., 2020, COMIX)</ref>. COMIX uses the cross-entropy method to approximate the otherwise intractable greedy per-agent action maximisation step. Schroeder de <ref type="bibr" target="#b57">Witt et al. (2020)</ref> also present experiments with FacMADDPG, i.e., MADDPG with a factored critic, which show that QMIX-like network factorisations also perform well in actor-critic settings. COMIX has been found to outperform previous state-of-the-art MADDPG in continuous robotic control suite Multi-Agent Mujoco, thereby illustrating its versatility beyond SMAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">PyMARL and SMAC</head><p>A number of papers have established unit micromanagement in StarCraft as a benchmark for deep multi-agent RL. <ref type="bibr" target="#b67">Usunier et al. (2017)</ref> present an algorithm using a centralised greedy MDP and first-order optimisation which they evaluate on Starcraft: BroodWar <ref type="bibr" target="#b64">(Synnaeve et al., 2016)</ref>. <ref type="bibr" target="#b50">Peng et al. (2017)</ref> also evaluate their methods on StarCraft. However, neither requires decentralised execution. Similar to our setup in SMAC is the work of <ref type="bibr" target="#b14">Foerster et al. (2017)</ref>, who evaluate replay stabilisation methods for IQL on combat scenarios with up to five agents.  also uses this setting.</p><p>In this paper, we construct unit micromanagement tasks using the StarCraft II Learning Environment (SC2LE) <ref type="bibr" target="#b69">(Vinyals et al., 2017)</ref> as opposed to StarCraft, since it is actively supported by the game developers and offers a more stable testing environment. The full game of StarCraft II has already been used as an RL environment <ref type="bibr" target="#b69">(Vinyals et al., 2017)</ref>, and DeepMind's AlphaStar <ref type="bibr" target="#b70">(Vinyals et al., 2019)</ref> has recently shown an impressive level of play on a StarCraft II matchup using a centralised controller. By contrast, SMAC introduces strict decentralisation and local partial observability to turn the StarCraft II game engine into a new set of decentralised cooperative multi-agent problems.</p><p>Since their introduction, SMAC and PyMARL have been used by a number of papers to evaluate and compare multi-agent RL methods <ref type="bibr" target="#b73">Wang et al., 2019c;</ref><ref type="bibr" target="#b10">Du et al., 2019;</ref><ref type="bibr" target="#b71">Wang et al., 2019a;</ref><ref type="bibr" target="#b80">Zhang et al., 2019;</ref><ref type="bibr" target="#b38">Mahajan et al., 2019;</ref><ref type="bibr" target="#b79">Yao et al., 2019;</ref><ref type="bibr" target="#b5">Bhmer et al., 2019;</ref><ref type="bibr" target="#b72">Wang et al., 2019b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Hypernetworks and Monotonic Networks</head><p>QMIX relies on a neural network to transform the centralised state into the weights of another neural network, in a manner reminiscent of hypernetworks <ref type="bibr" target="#b19">(Ha et al., 2017)</ref>. This second neural network is constrained to be monotonic with respect to its inputs by keeping its weights positive. <ref type="bibr" target="#b11">Dugas et al. (2009)</ref> also investigate such functional restrictions for neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>A fully cooperative multi-agent sequential decision-making task can be described as a decentralised partially observable Markov decision process (Dec-POMDP) <ref type="bibr" target="#b42">(Oliehoek and Amato, 2016)</ref> consisting of a tuple G = S, U, P, r, Z, O, n, ? . s ? S describes the true state of the environment. At each time step, each agent a ? A ? {1, ..., n} chooses an action u a ? U , forming a joint action u ? U ? U n . This causes a transition on the environment according to the state transition function P (s |s, u) : S ? U ? S ? [0, 1]. All agents share the same reward function r(s, u) : S ? U ? R and ? ? [0, 1) is a discount factor.</p><p>We consider a partially observable scenario in which each agent draws individual observations z ? Z according to observation function O(s, a) : S ? A ? Z. Each agent has an action-observation history ? a ? T ? (Z ? U ) * , on which it conditions a stochastic policy ? a (u a |? a ) : T ? U ? [0, 1]. The joint policy ? has a joint action-value function:</p><formula xml:id="formula_1">Q ? (s t , u t ) = E s t+1:? ,u t+1:? [R t |s t , u t ], where R t = ?</formula><p>i=0 ? i r t+i is the discounted return. We operate in the framework of centralised training with decentralised execution, i.e. the learning algorithm has access to all local action-observation histories ? and global state s, but each agent's learnt policy can condition only on its own action-observation history ? a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Q-Learning</head><p>Deep Q-learning represents the action-value function with a deep neural network parameterised by ?. Deep Q-networks (DQNs) <ref type="bibr" target="#b41">(Mnih et al., 2015)</ref> use a replay memory to store the transition tuple s, u, r, s , where the state s is observed after taking the action u in state s and receiving reward r. ? is learnt by sampling batches of b transitions from the replay memory and minimising the squared TD error :</p><formula xml:id="formula_2">L(?) = b i=1 y DQN i ? Q(s, u; ?) 2 ,<label>(2)</label></formula><p>where y DQN = r + ? max u Q(s , u ; ? ? ). ? ? are the parameters of a target network that are periodically copied from ? and kept constant for a number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Recurrent Q-Learning</head><p>In partially observable settings, agents can benefit from conditioning on their entire actionobservation history. <ref type="bibr" target="#b20">Hausknecht and Stone (2015)</ref> propose deep recurrent Q-networks (DRQN) that make use of recurrent neural networks. Typically, gated architectures such as LSTM <ref type="bibr" target="#b23">(Hochreiter and Schmidhuber, 1997)</ref> or GRU <ref type="bibr" target="#b9">(Chung et al., 2014)</ref> are used to facilitate learning over longer timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Independent Q-Learning</head><p>Perhaps the most commonly applied method in multi-agent learning is independent Qlearning (IQL) <ref type="bibr" target="#b66">(Tan, 1993)</ref>, which decomposes a multi-agent problem into a collection of simultaneous single-agent problems that share the same environment. This approach does not address the non-stationarity introduced due to the changing policies of the learning agents, and thus, unlike Q-learning, has no convergence guarantees even in the limit of infinite exploration. In practice, nevertheless, IQL commonly serves as a surprisingly strong baseline even in mixed and competitive games <ref type="bibr" target="#b65">(Tampuu et al., 2017;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Value Decomposition Networks</head><p>By contrast, value decomposition networks (VDNs) <ref type="bibr" target="#b63">(Sunehag et al., 2017)</ref> aim to learn a joint action-value function Q tot (? , u), where ? ? T ? T n is a joint action-observation history and u is a joint action. It represents Q tot as a sum of individual value functions Q a (? a , u a ; ? a ), one for each agent a, that condition only on individual action-observation histories:</p><formula xml:id="formula_3">Q tot (? , u) = n i=1 Q i (? i , u i ; ? i ).<label>(3)</label></formula><p>Strictly speaking, each Q a is a utility function <ref type="bibr" target="#b17">(Guestrin et al., 2002)</ref> and not a value function since by itself it does not estimate an expected return. However, for terminological simplicity we refer to both Q tot and Q a as value functions. The loss function for VDN is equivalent to <ref type="formula" target="#formula_2">(2)</ref>, where Q is replaced by Q tot . An advantage of this representation is that a decentralised policy arises simply from each agent performing greedy action selection with respect to its Q a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QMIX</head><p>In this section, we propose a new approach called QMIX which, like VDN, lies between the extremes of IQL and centralised Q-learning. However, QMIX can represent a much richer class of action-value functions than VDN.</p><p>Key to our method is the insight that the full factorisation of VDN is not necessary in order to extract decentralised policies that are fully consistent with their centralised counterpart. As long as the environment is not adversarial, there exists a deterministic optimal policy conditioned on the full action-observation history. Hence, we only need to establish consistency between the deterministic greedy decentralised policies and the deterministic greedy centralised policy based on the optimal joint action-value function. When the greedy decentralised policies are determined by an argmax over the Q a , consistency holds if a global argmax performed on Q tot yields the same result as a set of individual argmax operations performed on each Q a :</p><formula xml:id="formula_4">argmax u Q tot (? , u) = ? ? ? argmax u 1 Q 1 (? 1 , u 1 ) . . . argmax u n Q n (? n , u n ) ? ? ? .<label>(4)</label></formula><p>This allows each agent a to participate in a decentralised execution solely by choosing greedy actions with respect to its Q a . As a side effect, if (4) is satisfied, then taking the argmax of Q tot , required by off-policy learning updates, is trivially tractable without an exhaustive evaluation of Q tot for the exponentially many joint actions. VDN's representation is sufficient to satisfy (4). However, QMIX is based on the observation that this representation can be generalised to the larger family of monotonic functions that also satisfy (4). Monotonicity in this context is defined as a constraint on the relationship between Q tot and each Q a :</p><formula xml:id="formula_5">?Q tot ?Q a ? 0, ?a ? A,<label>(5)</label></formula><p>which is sufficient to satisfy (4), as the following theorem shows.</p><formula xml:id="formula_6">Theorem 1 If ?a ? A ? {1, 2, ..., n}, ?Qtot ?Qa ? 0 then argmax u Q tot (? , u) = ? ? ? argmax u 1 Q 1 (? 1 , u 1 ) . . . argmax u n Q n (? n , u n ) ? ? ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>The discrete per-agent action-value scores Q a are fed into the monotonic function Q tot (Q 1 , Q 2 ). The maximum Q a for each agent is shown in blue, which corresponds to the maximum Q tot also shown in blue. The constraint <ref type="formula" target="#formula_4">(4)</ref> is satisfied due to the monotonicity of Q tot .</p><p>Proof Proof is provided in Appendix A. <ref type="figure">Figure 2</ref> illustrates the relationship between Q tot and individual Q a functions, and how monotonicity leads to a decentralisable argmax in an example with two agents with three possible actions. Each agent a produces scores Q a (u a ) for each discrete action that it can take (columns with the local argmax in blue). These are the inputs to a continuous monotonic mixing function Q tot (Q 1 , ..., Q N ), the output of which is represented by the monotonically increasing heatmap. The intersections of the blue lines indicate the estimated Q tot for each of the discrete joint actions. Due to the monotonicity of Q tot , these joint-action value estimates maintain the ordering corresponding to each agent's Q a , for that agent's actions when the other agents' actions remain fixed. The result is that the global greedy joint action of Q tot , indicated by the blue dot, corresponds to the set of decentralised greedy actions.</p><p>We now describe how QMIX enforces (5) in practice, by representing Q tot using an architecture consisting of agent networks, a mixing network, and a set of hypernetworks <ref type="bibr" target="#b19">(Ha et al., 2017)</ref>. <ref type="figure">Figure 3</ref> illustrates the overall setup.</p><p>For each agent a, there is one agent network that represents its individual value function Q a (? a , u a ). We represent agent networks as DRQNs that receive the current individual observation o a t and the last action u a t?1 as input at each time step, as shown in <ref type="figure">Figure 3c</ref>. We include the last actions since they are part of the action-observation history ? a on which the decentralised policy can condition. Due to the use of stochastic policies during training, it is necessary to provide the actual action that was executed in the environment. If weights are shared across the agent networks in order to speed up learning, agent IDs are included as part of the observations to allow for heterogeneous policies. The mixing network is a feed-forward neural network that takes the agent network outputs as input and mixes them monotonically, producing the values of Q tot , as shown in <ref type="figure">Figure 3a</ref>. To enforce the monotonicity constraint of (5), the weights (but not the biases) of the mixing network are restricted to be non-negative. This allows the mixing network to approximate any monotonic function arbitrarily closely in the limit of infinite width <ref type="bibr" target="#b11">(Dugas et al., 2009)</ref>.</p><p>The weights of each layer of the mixing network are produced by separate hypernetworks. Each hypernetwork takes the state s as input and generates the weights of one layer of the mixing network. Each hypernetwork consists of two fully-connected layers with a ReLU nonlinearity, followed by an absolute activation function, to ensure that the mixing network weights are non-negative. The output of the hypernetwork is then a vector, which is reshaped into a matrix of appropriate size. The biases are produced in the same manner but are not restricted to being non-negative. The first bias is produced by a hypernetwork with a single linear layer, and the final bias is produced by a two-layer hypernetwork with a ReLU nonlinearity. <ref type="figure">Figure 3a</ref> illustrates the mixing network and the hypernetworks.</p><p>The state is used by the hypernetworks rather than being passed directly into the mixing network because Q tot is allowed to depend on the extra state information in non-monotonic ways. Thus, it would be overly constraining to pass some function of s through the monotonic network alongside the per-agent values. Instead, the use of hypernetworks makes it possible to condition the weights of the monotonic network on s in an arbitrary way, thus integrating the full state s into the joint action-value estimates as flexibly as possible. The choice of nonlinearity for the mixing network is also an important consideration due to its interaction with the non-negative weights. A ReLU is not a good choice since a negative input to the mixing network is likely to remain negative (depending on the biases), which would then be zeroed out by the ReLU leading to no gradients for all agent networks. It is for this reason that we use an ELU.</p><p>QMIX is trained end-to-end to minimise the following loss:</p><formula xml:id="formula_7">L(?) = b i=1 y tot i ? Q tot (? , u, s; ?) 2 ,<label>(6)</label></formula><p>where b is the batch size of transitions sampled from the replay buffer, the DQN target is given by y tot = r + ? max u Q tot (? , u , s ; ? ? ), and ? ? are the parameters of a target network as in DQN. <ref type="formula" target="#formula_7">(6)</ref> is analogous to the standard DQN loss of (2). Since (4) holds, we can perform the maximisation of Q tot in time linear in the number of agents (as opposed to scaling exponentially in the worst case). The mixing network relies on centralised training, although it may be discarded after training to allow fully decentralised execution of the learned joint policy. In a setting with limited communication, the core algorithmic mechanism of QMIX can still be applied: the mixing network can be viewed as a manager that coordinates learning, but requires only the communication of low-dimensional action-values from, and gradients to, the decentralised agents. In a setting without communication constraints, we can further exploit the centralised training setting by, e.g., sharing parameters between agents for more efficient learning.</p><p>In Algorithm 1 we outline the pseudocode for the particular implementation of QMIX we use for all of our experiments. The choice to gather rollouts from an entire episode (line 17) before executing a single gradient descent step (line 24), as well as using per-agent -greedy action selection (line 10), is not a requirement for QMIX but is an implementation detail. However, the action selection based solely on agent network's Q-values (line 10), as well as calculation of Q tot and its target using the mixing network and hypernetworks (lines 19-20) are essential features of QMIX. The update of per-agent action-value functions and hypernetworks using the DQN-style loss with respect to Q tot and joint reward (lines 22-24) is also central to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representational Complexity</head><p>The value function class representable with QMIX includes any value function that can be factored into a nonlinear monotonic combination of the agents' individual value functions in the fully observable setting.</p><p>This follows since the mixing network is a universal function approximator of monotonic functions <ref type="bibr" target="#b11">(Dugas et al., 2009)</ref>, and hence can represent any value function that factors into a nonlinear monotonic combination of the agent's individual value functions. Additionally, we require that the agent's individual value functions order the values of the actions appropriately.</p><formula xml:id="formula_8">By this we mean that Q a is such that Q a (s t , u a ) &gt; Q a (s t , u a ) ?? Q tot (s t , (u ?a , u a )) &gt; Q tot (s t , (u ?a ,<label>u</label></formula><p>a )), i.e., they can represent a function that respects the ordering of the agent's actions in the joint-action value function. Since the agents' networks are universal function approximators <ref type="bibr" target="#b51">(Pinkus, 1999)</ref>, they can represent such a Q a . Hence QMIX is able to represent any value function that factors into a nonlinear monotonic combination of the agent's individual value functions.</p><p>In a Dec-POMDP, QMIX cannot necessarily represent the true optimal Q-function. This is because each agent's observations are no longer the full state, and thus they might not be able to distinguish the true state given their local observations. If the agent's value function ordering is then wrong, i.e., Q a (? a , u) &gt; Q a (? a , u ) when Q tot (s t , (u ?a , u)) &lt; while s t = terminal and t &lt; episode limit do 7:</p><p>for each agent a do 8:</p><formula xml:id="formula_9">? a t = ? a t?1 ? {(o t , u t?1 )} 9:</formula><p>= epsilon-schedule(step) 10:</p><formula xml:id="formula_10">u a t = argmax u a t Q(? a t , u a t ) with probability 1 ? randint(1, |U |)</formula><p>with probability 11:</p><p>end for 12:</p><p>Get reward r t and next state s t+1 13: </p><formula xml:id="formula_11">D = D ? {(s t , u t , r t ,<label>s</label></formula><formula xml:id="formula_12">Q tot = Mixing-network (Q 1 (? 1 t , u 1 t ), . . . , Q n (? n t , u n t ); Hypernetwork(s t ; ?)) 20:</formula><p>Calculate target Q tot using Mixing-network with Hypernetwork(s t ; ? ? )) end if 29: end while Q tot (s t , (u ?a , u )), then the mixing network would be unable to correctly represent Q tot given the monotonicity constraints.</p><p>QMIX thus expands upon the linear monotonic value functions that are representable by VDN. <ref type="table" target="#tab_2">Table 1a</ref> gives an example of a monotonic value function for the simple case of a two-agent matrix game. Note that VDN is unable to represent this simple monotonic value function. <ref type="table" target="#tab_3">Table 2a</ref> provides the optimal Q tot values for VDN, minimising a squared loss. Section 5 additionally provides results for the values that VDN learns in a function approximation setting.</p><p>However, the constraint in (5) prevents QMIX from representing value functions that do not factorise in such a manner. A simple example of such a value function for a two-agent matrix game is given in <ref type="table" target="#tab_2">Table 1b</ref>. Intuitively, any value function for which an agent's best action depends on the actions of the other agents at the same time step will not factorise Agent 2   <ref type="table" target="#tab_2">Table 1b</ref>.</p><formula xml:id="formula_13">A B Agent 1 A 0 1 B 1 8 (a) Agent 2 A B Agent 1 A 2 1 B 1 8 (b)</formula><p>appropriately, and hence cannot be perfectly represented by QMIX. <ref type="table" target="#tab_3">Table 2b</ref> provides the optimal Q tot values for QMIX when minimising a squared loss. For the example in <ref type="table" target="#tab_2">Table 1b</ref>, QMIX still recovers the optimal policy and learns the correct maximum over the Q-values. Formally, the class of value functions that cannot be represented by QMIX are called nonmonotonic <ref type="bibr" target="#b38">(Mahajan et al., 2019)</ref>.</p><p>Although QMIX cannot perfectly represent all value functions, its increased representational capacity allows it to learn Q-value estimates that are more accurate for computing the target values to regress onto during Q-learning targets (bootstrapping). In particular, we show in Section 5 that the increased representational capacity allows for QMIX to learn the optimal policy in an environment with no per-timestep coordination, compared to VDN which still fails to learns the optimal policy. In Section 6 we show that QMIX learns significantly more accurate maxima over Q tot in randomly generated matrix games, demonstrating that it learns better Q-value estimates for bootstrapping. Finally, we show in Section 9 that QMIX significantly outperforms VDN on the challenging SMAC benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Two-Step Game</head><p>To illustrate the effects of representational complexity of VDN and QMIX, we devise a simple two-step cooperative matrix game for two agents.</p><p>At the first step, Agent 1 chooses which of the two matrix games to play in the next timestep. For the first time step, the actions of Agent 2 have no effect. In the second step, both agents choose an action and receive a global reward according to the payoff matrices depicted in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>We train VDN and QMIX on this task for 5000 episodes and examine the final learned value functions in the limit of full exploration ( = 1). Full exploration ensures that each method is guaranteed to eventually explore all available game states, such that the representational capacity of the state-action value function approximation remains the only limitation. The full details of the architecture and hyperparameters used, as well as additional results are provided in Appendix B.    <ref type="table" target="#tab_6">Table 4</ref>, which shows the learned values for Q tot , demonstrates that QMIX's higher representational capacity allows it to accurately represent the joint-action value function whereas VDN cannot. This directly translates into VDN learning the suboptimal strategy of selecting Action A at the first step and receiving a reward of 7, whereas QMIX recovers the optimal strategy from its learnt joint-action values and receives a reward of 8.</p><p>The simple example presented in this section demonstrates the importance of accurate Q-value estimates, especially for the purpose of bootstrapping. In Section 6 we provide further evidence that QMIX's increased representational capacity allows it to learn more accurate Q-value, that directly lead to more accurate bootstrapped estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Random Matrix Games</head><p>To demonstrate that QMIX learns more accurate Q-values than VDN for the purpose of bootstrapping, we compare the learnt values of max u Q tot (s, u) on randomly generated matrix games. Single-step matrix games allow us to compare the learnt maximum Q-values across both methods in isolation. We focus on max u Q tot (s, u) because it is the only learnt quantity involved in the bootstrapping for 1-step Q-learning.</p><p>The maximum return for each matrix is 10, and all other values are drawn uniformly from [0, 10). Each individual seed has different values for the payoff matrix, but within each run they remain fixed. We set = 1 to ensure all actions are sampled and trained on equally.  <ref type="figure" target="#fig_3">Figure 4</ref> shows the results for a varying number of agents and actions. We can see that QMIX learns significantly more accurate maxima than VDN due to its larger representational capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">The StarCraft Multi-Agent Challenge</head><p>In this section, we describe the StarCraft Multi-Agent Challenge (SMAC) to which we apply QMIX and a number of other methods. SMAC is based on the popular real-time strategy (RTS) game StarCraft II. In a regular full game of StarCraft II, one or more humans compete against each other or against a built-in game AI to gather resources, construct buildings, and build armies of units to defeat their opponents.</p><p>Akin to most RTSs, StarCraft has two main gameplay components: macromanagement and micromanagement. Macromanagement refers to high-level strategic considerations, such as economy and resource management. Micromanagement (micro), on the other hand, refers to fine-grained control of individual units.</p><p>StarCraft has been used as a research platform for AI, and more recently, RL. Typically, the game is framed as a competitive problem: an agent takes the role of a human player, making macromanagement decisions and performing micromanagement as a puppeteer that issues orders to individual units from a centralised controller.</p><p>In order to build a rich multi-agent testbed, we instead focus solely on micromanagement. Micro is a vital aspect of StarCraft gameplay with a high skill ceiling, and is practiced in isolation by amateur and professional players. For SMAC, we leverage the natural multi-agent structure of micromanagement by proposing a modified version of the problem designed specifically for decentralised control. In particular, we require that each unit be controlled by an independent agent that conditions only on local observations restricted to a limited field of view centred on that unit (see <ref type="figure" target="#fig_4">Figure 5</ref>). Groups of these agents must be trained to solve challenging combat scenarios, battling an opposing army under the centralised control of the game's built-in scripted AI.</p><p>Proper micro of units during battles maximises the damage dealt to enemy units while minimising damage received, and requires a range of skills. For example, one important technique is focus fire, i.e., ordering units to jointly attack and kill enemy units one after another. When focusing fire, it is important to avoid overkill : inflicting more damage to units than is necessary to kill them. Other common micro techniques include: assembling units into formations based on their armour types, making enemy units give chase while maintaining enough distance so that little or no damage is incurred (kiting, <ref type="figure" target="#fig_0">Figure 1a</ref>), coordinating the positioning of units to attack from different directions or taking advantage of the terrain to defeat the enemy.</p><p>SMAC thus provides a convenient environment for evaluating the effectiveness of MARL algorithms. The simulated StarCraft II environment and carefully designed scenarios require learning rich cooperative behaviours under partial observability, which is a challenging task. The simulated environment also provides an additional state information during training, such as information on all the units on the entire map. This is crucial for facilitating algorithms to take full advantage of the centralised training regime and assessing all aspects of MARL methods. SMAC features the following characteristics that are common in many real-word multiagent systems:</p><p>? Partial observability: Like, e.g., self-driving cars and autonomous drones, agents in SMAC receive only limited information about the environment. ? Large number of agents: the scenarios in SMAC include up to 27 learning agents.</p><p>? Diversity: many scenarios feature heterogeneous units with resulting diversity in optimal strategies. ? Long-term planning: defeating the enemy in SMAC often requires the agents to perform a long sequence of actions. ? High-dimensional observation spaces: the inclusion of diverse units and the large size of the map yield an immense state space. ? Large per-agent action space: the agents can perform up to 70 actions at each time step. ? Coordinated teamwork: micromanagement of units requires the individual agents to execute strictly coordinated actions, which is essential to many MARL problems. ? Stochasticity: the behaviour of the enemy differs across individual runs. Also, the amount of time that the agents must wait until being able to shoot again is stochastic.</p><p>SMAC is one of the first MARL benchmarks that includes all of these features, making it useful for evaluating the effectiveness of methods for many aspects of learning decentralised multi-agent control. We hope that it will become a standard benchmark for measuring the progress and a grand challenge for pushing the boundaries of MARL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Scenarios</head><p>SMAC consists of a set of StarCraft II micro scenarios which aim to evaluate how well independent agents are able to learn coordination to solve complex tasks. These scenarios are carefully designed to necessitate the learning of one or more micro techniques to defeat the enemy. Each scenario is a confrontation between two armies of units. The initial position, number, and type of units in each army varies from scenario to scenario, as does the presence  The first army is controlled by the learned allied agents. The second army consists of enemy units controlled by the built-in game AI, which uses carefully handcrafted non-learned heuristics. At the beginning of each episode, the game AI instructs its units to attack the allied agents using its scripted strategies. An episode ends when all units of either army have died or when a pre-specified time limit is reached (in which case the game is counted as a defeat for the allied agents). The goal is to maximise the win rate, i.e., the ratio of games won to games played.</p><p>The complete list of challenges is presented in <ref type="table" target="#tab_8">Table 5</ref>. More specifics on the SMAC scenarios and environment settings can be found in Appendices D.1 and D.2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">State and Observations</head><p>At each timestep, agents receive local observations drawn within their field of view. This encompasses information about the map within a circular area around each unit and with a radius equal to the sight range ( <ref type="figure" target="#fig_4">Figure 5</ref>). The sight range makes the environment partially observable from the standpoint of each agent. Agents can only observe other agents if they are both alive and located within the sight range. Hence, there is no way for agents to distinguish between teammates that are far away from those that are dead.</p><p>The feature vector observed by each agent contains the following attributes for both allied and enemy units within the sight range: distance, relative x, relative y, health, shield, and unit type. Shields serve as an additional source of protection that needs to be removed before any damage can be done to the health of units. All Protoss units have shields, which can regenerate if no new damage is dealt. In addition, agents have access to the last actions of allied units that are in the field of view. Lastly, agents can observe the terrain features surrounding them, in particular, the values of eight points at a fixed radius indicating height and walkability.</p><p>The global state, which is only available to agents during centralised training, contains information about all units on the map. Specifically, the state vector includes the coordinates of all agents relative to the centre of the map, together with unit features present in the observations. Additionally, the state stores the energy of Medivacs and cooldown of the rest of the allied units, which represents the minimum delay between attacks. Finally, the last actions of all agents are attached to the central state.</p><p>All features, both in the state as well as in the observations of individual agents, are normalised by their maximum values. The sight range is set to nine for all agents. The feature vectors for both local observations and global state have a fixed size, where information about the ally/enemy is represented as a sequence ordered by the unit ID number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Action Space</head><p>The discrete set of actions that agents are allowed to take consists of move[direction] 3 , attack[enemy id], stop and no-op. 4 As healer units, Medivacs must use heal[agent id] actions instead of attack[enemy id]. The maximum number of actions an agent can take ranges between 7 and 70, depending on the scenario.</p><p>To ensure decentralisation of the task, agents can use the attack[enemy id] action only on enemies in their shooting range ( <ref type="figure" target="#fig_4">Figure 5</ref>). This additionally constrains the ability of the units to use the built-in attack-move macro-actions on enemies that are far away. We set the shooting range to six for all agents. Having a larger sight range than a shooting range forces agents to use move commands before starting to fire.</p><p>To ensure that agents only execute valid actions, a mask is provided indicating which actions are valid and invalid at each timestep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Rewards</head><p>The overall goal is to maximise the win rate for each battle scenario. The default setting is to use the shaped reward, which produces a reward based on the hit-point damage dealt and enemy units killed, together with a special bonus for winning the battle. The exact values and scales for each of these events can be configured using a range of flags. To produce fair comparisions we encourage using this default reward function for all scenarios. We also provide another sparse reward option, in which the reward is +1 for winning and -1 for losing an episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Evaluation Methodology</head><p>To ensure the fairness of the challenge and comparability of results, performance should be evaluated under standardised conditions. One should not undertake any changes to the environment used for evaluating the policies. This includes the observation and state spaces, action space, the game mechanics, and settings of the environment (e.g., frame-skipping rate). One should not modify the StarCraft II map files in any way or change the difficulty of the game AI. Episode limits of each scenario should also remain unchanged.</p><p>SMAC restricts the execution of the trained models to be decentralised, i.e., during testing each agent must base its policy solely on its own action-observation history and cannot use the global state or the observations of other agents. It is, however, acceptable to train the decentralised policies in centralised fashion. Specifically, agents can exchange individual observations, model parameters and gradients during training as well as make use of the global state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Evaluation Metrics</head><p>Our main evaluation metric is the mean win percentage of evaluation episodes as a function of environment steps observed, over the course of training. Such progress can be estimated by periodically running a fixed number of evaluation episodes (in practice, 32) with any exploratory behaviours disabled. Each experiment is repeated using a number of independent training runs and the resulting plots include the median performance as well as the 25-75% percentiles. We use five independent runs for this purpose in order to strike a balance between statistical significance and the computational requirements. We recommend using the median instead of the mean in order to avoid the effect of any outliers. We report the number of independent runs, as well as environment steps used in training.</p><p>We also include the computational resources used, as well as the wall clock time for running each experiment. SMAC provides functionality for saving StarCraft II replays, which can be viewed using a freely available client. The resulting videos can be used to comment on interesting behaviours observed. Each independent run takes between 8 to 16 hours, depending on the exact scenario, using Nvidia Geforce GTX 1080 Ti graphics cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">PyMARL</head><p>To make it easier to develop algorithms for SMAC, we have also open-sourced our software engineering framework PyMARL 5 . Compared to many other frameworks for deep reinforcement learning <ref type="bibr" target="#b61">(Stooke and Abbeel, 2019;</ref><ref type="bibr" target="#b22">Hill et al., 2018;</ref><ref type="bibr" target="#b24">Hoffman et al., 2020)</ref>, PyMARL is designed with a focus on the multi-agent setting. It is also intended for fast, easy development and experimentation for researchers, relying on fewer abstractions than e.g., RLLib <ref type="bibr" target="#b33">(Liang et al., 2018)</ref>. To maintain this lightweight development experience, PyMARL does not support distributed training.</p><p>PyMARL's codebase is organized in a modular fashion in order to enable the rapid development of new algorithms, as well as provide implementations of current deep MARL algorithms to benchmark against. It is built on top of PyTorch to facilitate the fast execution and training of deep neural networks, and take advantage of the rich ecosystem built around it. PyMARL's modularity makes it easy to extend, and components can be readily isolated for testing purposes.</p><p>Since the implementation and development of deep MARL algorithms come with a number of additional challenges beyond those posed by single-agent deep RL, it is crucial to have simple and understandable code. In order to improve the readability of code 5. PyMARL is available at https://github.com/oxwhirl/pymarl. and simplify the handling of data between components, PyMARL encapsulates all data stored in the buffer within an easy to use data structure. This encapsulation provides a cleaner interface for the necessary handling of data in deep MARL algorithms, whilst not obstructing the manipulation of the underlying PyTorch Tensors. In addition, PyMARL aims to maximise the batching of data when performing inference or learning so as to provide significant speed-ups over more naive implementations.</p><p>PyMARL features implementations of the following algorithms: QMIX, QTRAN <ref type="bibr" target="#b59">(Son et al., 2019)</ref>, COMA , VDN <ref type="bibr" target="#b63">(Sunehag et al., 2017)</ref>, and IQL <ref type="bibr" target="#b66">(Tan, 1993)</ref> as baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">SMAC Results</head><p>In this section, we present the results of our experiments of QMIX and other existing multi-agent RL methods using the new SMAC benchmark.</p><p>The evaluation procedure is similar to the one in <ref type="bibr" target="#b54">(Rashid et al., 2018)</ref>. The training is paused after every 10000 timesteps during which 32 test episodes are run with agents performing action selection greedily in a decentralised fashion. The percentage of episodes where the agents defeat all enemy units within the permitted time limit is referred to as the test win rate.</p><p>The   <ref type="figure" target="#fig_7">Figure 7</ref> plots the median test win percentage averaged across all scenarios in order to compare the algorithms across the entire SMAC suite. We also plot the performance of a simple heuristic AI that selects the closest enemy unit (ignoring partial observability) and attacks it with the entire team until it is dead, upon which the next closest enemy unit is selected. This is a basic form of focus-firing, which is a crucial tactic for achieving good performance in micromanagement scenarios. The relatively poor performance of the heuristic AI shows that the suite of SMAC scenarios requires more complex behaviour than naively focus-firing the closest enemy, making it an interesting and challenging benchmark.</p><p>Overall QMIX achieves the highest test win percentage and is the best performer on up to eight scenarios during training. Additionally, IQL, VDN, and QMIX all significantly outperform COMA, demonstrating the sample efficiency of off-policy value-based methods over on-policy policy gradient methods. We also compare to QTRAN on 3 scenarios in <ref type="figure" target="#fig_8">Figure 8</ref>. We can see that QTRAN fails to achieve good performance on 3s5z and takes far longer to reach the performance of VDN and QMIX on 2s3z. Since it barely beats IQL on relatively easy scenarios, we do not perform a more comprehensive benchmarking of QTRAN. In preliminary experiments, we found the QTRAN-Base algorithm slightly more performant and more stable than QTRAN-Alt. For more details on the hyperparameters and architectures considered, please see the Appendix D.3.</p><p>Based on the overall performances of all algorithms, we broadly group the scenarios into three categories: Easy, Hard, and Super-Hard. <ref type="figure" target="#fig_9">Figure 9</ref> shows that IQL and COMA struggle even on the Easy scenarios, performing poorly on four of the five scenarios in this category. This shows the advantage of learning a centralised but factored centralised Q tot . Even though QMIX exceeds 95% test win rate on all of five Easy scenarios, they serve an important role in the benchmark as sanity checks when implementing and testing new algorithms.</p><p>The Hard scenarios in <ref type="figure" target="#fig_0">Figure 10</ref> each present their own unique problems. 2c vs 64zg only contains 2 allied agents, but 64 enemy units (the largest in the SMAC benchmark) making the action space of the agents much larger than the other scenarios. bane vs bane contains a large number of allied and enemy units, but the results show that IQL easily finds a winning strategy whereas all other methods struggle and exhibit large variance. 5m vs 6m is an asymmetric scenario that requires precise control to win consistently, and in which the best performers (QMIX and VDN) have plateaued in performance. Finally, 3s vs 5z  requires the three allied stalkers to kite the enemy zealots for the majority of the episode (at least 100 timesteps), which leads to a delayed reward problem. The scenarios shown in <ref type="figure" target="#fig_0">Figure 11</ref> are categorised as Super Hard because of the poor performance of all algorithms, with only QMIX making meaningful progress on two of the five. We hypothesise that exploration is a bottleneck in many of these scenarios, providing a nice testbed for future research in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Analysis</head><p>In this section, we aim to understand the reasons why QMIX outperforms VDN in our experiments. In particular, we consider two possible causes: 1) the inclusion of the central state in the mixing network and 2) the nonlinearity of the mixing network.</p><p>We find that inclusion of the central state is extremely important for performance but that it alone is not sufficient to explain QMIX's increased performance. We further find that the mixing network learns nonlinear functions when required, but achieves good performance on the SMAC benchmark even when the learned solutions are mostly linear combinations of the agent's utilities. Studying this phenomenon further, we conclude that it is the flexibility of the mixing network's parameterisation, but not its nonlinearity, that accounts for QMIX's superior performance in these tasks.</p><p>Our ablation experiments focus on three scenarios, one for each of the three difficulty categories: 3s5z, 2c vs 64zg, and MMM2. We chose these three scenarios because they exhibit a large gap in performance between QMIX and VDN, in order to better understand the contribution of the components in QMIX towards that increased performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Role of the Central State</head><p>In order to disentangle the effects of the various components of the QMIX architecture, we consider the following two ablations:</p><p>? VDN-S: The output of VDN is augmented by a state-dependent bias.</p><p>? QMIX-NS: The weights of the mixing network are not a function of the state. The state-dependent bias remains.  <ref type="figure" target="#fig_0">Figure 12</ref> shows that VDN-S performs better than VDN, but still worse than QMIX, indicating that inclusion of the state accounts for some but not all of QMIX's performance. On the easy 3s5z scenario, QMIX-NS performs very poorly, which is surprising since it is a strict generalisation of VDN-S. However, on the other two scenarios it outperforms VDN-S, indicating that the extra flexibility afforded by a learnt mixing over a fixed summation can be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Role of Nonlinear Mixing</head><p>The results in <ref type="figure" target="#fig_0">Figure 12</ref> show that both a state dependent bias on VDN (VDN-S) and a learnt state-independent mixing (QMIX-NS) do not match the performance of QMIX. Thus, we consider another ablation of QMIX:</p><p>? QMIX-Lin: We remove the final layer and nonlinearity of the mixing network, making it a linear network. The state-dependant bias remains.  <ref type="figure" target="#fig_0">Figure 13</ref> shows that whilst QMIX-Lin matches QMIX on the easy 3s5z scenario, it underperforms QMIX on the other two scenarios we considered. This suggests that the ability to perform a nonlinear mixing, through the inclusion of a nonlinearity and additional hidden layer in the mixing network, plays an important role QMIX's performance. To confirm this, we examine the function the mixing network learns. <ref type="figure" target="#fig_0">Figure 14</ref>: The learnt mixing of QMIX on 2c vs 64zg at the end of training for timesteps 0 (left) and 50 (right). Circles indicate the Q tot -values for the discrete joint-action space. <ref type="figure" target="#fig_0">Figure 14</ref> shows the learnt mixing of the network at the end of training for the 2c vs 64zg scenario with two agents trained using QMIX. We generated a sample trajectory by greedily selecting actions and plotted the mixing function at different timesteps. The learnt function is approximately linear for the vast majority of the timesteps in the agent's Q-values, similarly to the first timestep of the episode shown in <ref type="figure" target="#fig_0">Figure 14 (left)</ref>. The only timesteps in which we observe a noticeable nonlinearity are at the end of the episode (53 timesteps total). When trained for longer we still observe that the the mixing function remains linear for the vast majority of timesteps. <ref type="figure" target="#fig_0">Figure 15</ref>: The learnt mixing of QMIX on 3s5z at the end of training for timesteps 0 (left) and 50 (right), for agents 2 (top) and 7 (bottom). <ref type="figure" target="#fig_0">Figure 15</ref> shows the learnt mixing for 3s5z as a function of a single Q a , keeping the Q-values for the other agent's fixed. The mixing network clearly learns an approximately linear mixing, which is consistent across all timesteps and agents (only timesteps 0 and 50 for agents 2 and 7 are shown since they are representative of all other timesteps and agents). This is a surprising result. QMIX is certainly capable of learning nonlinear mixing functions in practice, and thereby outperforming the more restrictive VDN decomposition. As an example, <ref type="figure" target="#fig_0">Figure 16</ref> visualises the learnt mixing function for the didactic two-step game from section 5, clearly showing how a nonlinear fit allows QMIX to represent the true optimal value function.</p><p>Yet, we observe mostly linear mixing functions in solutions to the SMAC benchmark tasks. If QMIX is learning a mostly linear mixing, then why does it outperform QMIX-Lin? One difference is that QMIX's mixing network is compromised of two fully-connected layers, rather than the single linear layer of QMIX-Lin. If the nonlinearity is not essential, it may be that the extra layer in the mixing network is responsible for the performance increase. To test this hypothesis, we consider another ablation:</p><p>? QMIX-2Lin: QMIX, without a nonlinear activation function in the mixing network.</p><p>QMIX-2Lin's mixing network is compromised of two linear layers without any nonlinearity between them. Thus, it can only learn a linear transformation of Q a into Q tot . QMIX-Lin's mixing network is a single linear layer, thus both QMIX-Lin and QMIX-2Lin's mixing networks share the same representational capacity. QMIX's mixing network, on the other hand, has an ELU non-linearity between its two weight layers and thus can represent non-linear transformations of Q a to Q tot .</p><p>We first consider a simple regression experiment in which the states, agent Q-values, and targets are randomly generated and fixed. Further details on the regression task are included in Appendix E. <ref type="figure" target="#fig_0">Figure 17</ref>: The loss on a regression task.</p><p>We compare the loss of QMIX (with an ELU non-linearity), QMIX-Lin, and QMIX-2Lin in <ref type="figure" target="#fig_0">Figure 17</ref>, which illustrates that QMIX and QMIX-2Lin perform similarly, and both perform a faster optimisation of the loss than QMIX-Lin. Even though QMIX-Lin and QMIX-2Lin can both learn only linear mixing functions, their parametrisation of such a linear function are different. Having an extra layer is a better parametrisation in this scenario because it allows for gradient descent to optimise the loss faster <ref type="bibr" target="#b0">(Arora et al., 2018)</ref>.  <ref type="figure" target="#fig_0">Figure 18</ref> confirms that the more flexible parameterisation of the mixing network is largely responsible for the increased performance. However, the results on 2c vs 64zg show that QMIX performs slightly better than QMIX-2Lin, indicating that in some scenarios nonlinear mixing can still be beneficial. <ref type="figure" target="#fig_0">Figure 14 (right)</ref> shows an example of when such a nonlinearity is learnt on this task.</p><p>We also investigate changing the nonlinearity in the mixing network from ELU, which is largely linear, to Tanh. <ref type="figure" target="#fig_0">Figure 19</ref>: The learnt mixing of QMIX on 2c vs 64zg using a tanh nonlinearity at the end of training for timesteps 0 (left) and 50 (right). <ref type="figure" target="#fig_0">Figures 19 and 20</ref> show the learnt mixings when using a tanh non-linearity. The mixing network using tanh learns a more nonlinear mixing, especially for 3s5z, than with ELU. <ref type="figure" target="#fig_0">Figure 21</ref> shows how this translates to performance: there are perhaps some small gains in learning speed for some random seeds when using tanh. However, the effect of an additional layer (in improving the optimisation dynamics) is much more significant than encouraging a more non-linear mixing through the choice of nonlinearity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Role of RNN in Agent Network</head><p>Finally, we compare the necessity of using an agent's action-observation history by comparing the performance of an agent network with and without an RNN in <ref type="figure" target="#fig_18">Figure 22</ref>. On the easy scenario of 3s5z we can see that an RNN is not required to use action-observation information from previous timesteps, but on the harder scenario of 3s vs 5z it is crucial to learning how to kite effectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Role of the COMA Critic Architecture</head><p>In addition to the algorithmic differences between QMIX and COMA (QMIX is an off-policy Q-learning algorithm; COMA is an on-policy actor-critic algorithm), the architecture used to estimate the relevant Q-values differs. COMA's critic is a feed-forward network, whereas QMIX's architecture for estimating Q tot contains RNNs (the agent networks). The results of <ref type="figure" target="#fig_18">Figure 22</ref> raise the question: To what extent do the architectural differences between QMIX and COMA affect final performance? To test this, we change the architecture of the COMA critic to match that of QMIX more closely. Specifically it now consists of per-agent components with the same architecture and inputs as the agent networks used by QMIX, which feed Q a values for their chosen action into a mixing network. Since there is no requirement for monotonicity in this mixing network, we simplify its architecture significantly to a feed-forward network with ReLU non-linearities and two hidden layers of 128 units. We test two variants of the mixing network, one with access to the state and one without. The former takes the state as input in addition to the agent's chosen action Q a -values. The output of the mixing network is |U | units, and we mask out the Q a -value for the agent whose Q-values are being produced.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5">Role of Network Size and Depth</head><p>Another architectural consideration is the overall size of the network used to estimate Q tot for VDN and VDN-S. Since QMIX has an additional mixing network, the overall architecture is deeper during training. In order to understand the effect of this depth on performance, we consider the following variants of VDN and VDN-S.</p><p>? VDN Wider: We increase the size of the agent's hidden layer from 64 to 128.</p><p>? VDN Deeper: We add an additional layer to the agent network after the GRU, with a ReLU non-linearity.</p><p>? VDN-S Bigger: We increase the size of the state-dependent bias to 2 hidden layers of 64 units.</p><p>In our other experiments, we have used the same agent networks for each method, to ensure a fair comparison of the performance of the decentralised policies. By increasing the capacity of the agent networks for VDN Wider and VDN Deeper, we are now using larger agent networks than all other methods. In this sense, the comparisons to these variants are no longer fair. <ref type="figure" target="#fig_4">Figure 25</ref> shows the results for VDN Wider and VDN Deeper. As expected, increasing the capacity of the agent networks increases performance. In particular, increasing the depth results in a large performance increase on the challenging MMM2. However, despite the larger agent networks, VDN Wider and VDN Deeper still do not match the performance of QMIX with smaller, less expressive agent networks. <ref type="figure" target="#fig_5">Figure 26</ref> shows the results for VDN-S Bigger, in which we increase the size of the state-dependent bias. The results show that this has a minimal impact on performance over VDN-S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Conclusion</head><p>This article presented QMIX, a deep multi-agent RL method that allows end-to-end learning of decentralised policies in a centralised setting and makes efficient use of extra state information. QMIX allows the learning of a rich joint action-value function, which admits tractable decompositions into per-agent action-value functions. This is achieved by imposing a monotonicity constraint on the mixing network.  To evaluate QMIX and other multi-agent RL algorithms, we introduced SMAC as a set of benchmark problems for cooperative multi-agent RL. SMAC focuses on decentralised micromanagement tasks and features 14 diverse combat scenarios which challenge MARL methods to handle partial observability and high-dimensional inputs. Additionally, we open-source PyMARL, a framework for cooperative deep MARL algorithms that features implementations of IQL, VDN, QMIX, QTRAN and COMA.</p><p>Our results on SMAC show that QMIX improves the final performance over other relevant deep multi-agent RL algorithms, most notably VDN. We include a detailed analysis through a series of ablations and visualisations for why QMIX outperforms VDN, showing that two components of QMIX are crucial to the performance the algorithm: a state-dependent bias and a learnt mixing of the agent's utilities into Q tot that is conditioned on the state. Through our analysis, we found that the learnt mixing need only be linear to recover the majority of QMIX's performance, but that the parametrisation for this linear function is an important consideration.</p><p>In the future, we aim to extend SMAC with new challenging scenarios that feature a more diverse set of units and require a higher level of coordination amongst agents. Particularly, we plan to make use of the rich skill set of StarCraft II units, and host scenarios that require the agents to utilise the features of the terrain. With harder multi-agent coordination problems, we aim to explore the gaps in existing multi-agent RL approaches and motivate further research in this domain.</p><p>There are several interesting avenues for future research:</p><p>? Investigating the performance gap between value-based methods and policy-based methods. In our experiments, QMIX (and VDN) significantly outperform COMA in all scenarios, both in terms of sample efficiency and final performance. The primary differences between the methods are the use of Q-learning vs the Policy Gradient theorem, use of off-policy data from the replay buffer and the architecture used to estimate the joint-action Q-Values.</p><p>? Expanding upon the joint-action Q-values representable by QMIX, to allow for the representation of coordinated behaviour. Importantly, this must scale gracefully to the deep RL setting.</p><p>? Better algorithms for multi-agent exploration that go beyond -greedy. There have been many advances with regards to exploration in single-agent RL, dealing with both sparse and dense rewards environments. Taking inspiration from these, or developing entirely new approaches that improve exploration in a cooperative multi-agent RL setting is an important problem. SMAC's super-hard scenarios provide a nice benchmark for evaluating these advances.</p><p>Lianmin Zheng, Jiacheng Yang, Han Cai, Weinan Zhang, Jun Wang, and Yong Yu. Magent: A many-agent reinforcement learning platform for artificial collective intelligence. arXiv preprint arXiv:1712.00600, 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Monotonicity</head><p>Theorem 2 If ?a ? A ? {1, 2, ..., n}, ?Qtot ?Qa ? 0 then</p><formula xml:id="formula_14">argmax u Q tot (? , u) = ? ? ? argmax u 1 Q 1 (? 1 , u 1 ) . . . argmax u n Q n (? n , u n ) ? ? ? .</formula><p>Proof Since ?Qtot ?Qa ? 0 for ?a ? A, the following holds for any (u 1 , . . . , u n ) and the mixing network function Q tot (?) with n arguments:</p><formula xml:id="formula_15">Q tot (Q 1 (? 1 , u 1 ), . . . , Q a (? a , u a ), . . . , Q n (? n , u n )) ? Q tot (max u 1 Q 1 (? 1 , u 1 ), . . . , Q a (? a , u a ), . . . , Q n (? n , u n )) . . . ? Q tot (max u 1 Q 1 (? 1 , u 1 ), . . . , max u a Q a (? a , u a ), . . . , Q n (? n , u n )) . . . ? Q tot (max u 1 Q 1 (? 1 , u 1 ), . . . , max u a Q a (? a , u a ), . . . , max u n Q n (? n , u n )).</formula><p>Therefore, the maximiser of the mixing network function is: <ref type="figure">u a )</ref>, . . . , max u n Q n (? n , u n )).</p><formula xml:id="formula_16">(max u 1 Q 1 (? 1 , u 1 ), . . . , max u a Q a (? a ,</formula><p>Thus, max u Q tot (? , u) := max u=(u 1 ,...,u n ) Q tot (Q 1 (? 1 , u 1 ), . . . , Q n (? n , u n )) = Q tot (max u 1 Q 1 (? 1 , u 1 ), . . . , max u n Q n (? n , u n )).</p><p>Letting u * = (u 1 * , . . . , u n * ) = ? ? ? argmax u 1 Q 1 (? 1 , u 1 ) . . . argmax u n Q n (? n , u n ) ? ? ?, we have that:</p><formula xml:id="formula_17">Q tot (Q 1 (? 1 , u 1 * ), . . . , Q n (? n , u n * )) = Q tot (max u 1 Q 1 (? 1 , u 1 ), . . . , max u n Q n (? n , u n )) = max u Q tot (? , u)</formula><p>Hence, u * = argmax u Q tot (? , u), which proves (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Two Step Game</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Architecture and Training</head><p>The architecture of all agent networks is a DQN with a single hidden layer comprised of 64 units with a ReLU nonlinearity. Each agent performs independent greedy action selection, with = 1. We set ? = 0.99. The replay buffer consists of the last 500 episodes, from which we uniformly sample a batch of size 32 for training. The target network is updated every 100 episodes. The learning rate for RMSprop is set to 5 ? 10 ?4 . We train for 10k timesteps. The size of the mixing network is 8 units. All agent networks share parameters, thus the agent id is concatenated onto each agent's observations. We do not pass the last action taken to the agent as input. Each agent receives the full state as input.</p><p>Each state is one-hot encoded. The starting state for the first timestep is State 1. If Agent 1 takes Action A, it transitions to State 2 (whose payoff matrix is all 7s). If agent 1 takes Action B in the first timestep, it transitions to State 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Learned Value Functions</head><p>The learned value functions for the different methods on the Two Step Game are shown in <ref type="table" target="#tab_10">Tables 6 and 7</ref>    <ref type="figure" target="#fig_7">Figure 27</ref> shows the loss for the different methods. <ref type="table" target="#tab_12">Table 8</ref> shows the final testing reward for each method.  The architecture of all agent networks is a DQN with 2 hidden layers comprised of 64 units with a ReLU nonlinearity. Each agent performs independent greedy action selection, with = 1. The replay buffer consists of the last 500 episodes, from which we uniformly sample a batch of size 32 for training. The target network is updated every 100 episodes. The learning rate for RMSprop is set to 5 ? 10 ?4 . We train for 20k timesteps. The mixing network is identical to the SMAC experiments. All agent networks share parameters, thus the agent id is concatenated onto each agent's observations. We do not pass the last action taken to the agent as input. Each agent receives the full state (a vector of 1s with dimension 5) as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. SMAC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Scenarios</head><p>Perhaps the simplest scenarios are symmetric battle scenarios, where the two armies are composed of the same units. Such challenges are particularly interesting when some of the units are extremely effective against others (this is known as countering), for example, by dealing bonus damage to a particular armour type. In such a setting, allied agents must deduce this property of the game and design an intelligent strategy to protect teammates vulnerable to certain enemy attacks.</p><p>SMAC also includes more challenging scenarios, for example, in which the enemy army outnumbers the allied army by one or more units. In such asymmetric scenarios it is essential to consider the health of enemy units in order to effectively target the desired opponent.</p><p>Lastly, SMAC offers a set of interesting micro-trick challenges that require a higher-level of cooperation and a specific micro trick to defeat the enemy. An example of such scenario is the corridor scenario ( <ref type="figure" target="#fig_5">Figure 6d</ref>). Here, six friendly Zealots face 24 enemy Zerglings, which requires agents to make effective use of the terrain features. Specifically, agents should collectively wall off the choke point (the narrow region of the map) to block enemy attacks from different directions. The 3s vs 5z scenario features three allied Stalkers against five enemy Zealots <ref type="figure" target="#fig_0">(Figure 1a</ref>). Since Zealots counter Stalkers, the only winning strategy for the allied units is to kite the enemy around the map and kill them one after another. Some of the micro-trick challenges are inspired by StarCraft Master challenge missions released by Blizzard (Blizzard Entertainment, 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Environment Setting</head><p>SMAC makes use of the StarCraft II Learning Environment (SC2LE ) <ref type="bibr" target="#b69">(Vinyals et al., 2017)</ref> to communicate with the StarCraft II engine. SC2LE provides full control of the game by making it possible to send commands and receive observations from the game. However, SMAC is conceptually different from the RL environment of SC2LE. The goal of SC2LE is to learn to play the full game of StarCraft II. This is a competitive task where a centralised RL agent receives RGB pixels as input and performs both macro and micro with the player-level control similar to human players. SMAC, on the other hand, represents a set of cooperative multi-agent micro challenges where each learning agent controls a single military unit.</p><p>SMAC uses the raw API of SC2LE. Raw API observations do not have any graphical component and include information about the units on the map such as health, location coordinates, etc. The raw API also allows sending action commands to individual units using their unit IDs. This setting differs from how humans play the actual game, but is convenient for designing decentralised multi-agent learning tasks.</p><p>Furthermore, to encourage agents to explore interesting micro strategies themselves, we limit the influence of the StarCraft AI on our agents. In the game of StarCraft II, whenever an idle unit is under attack, it automatically starts a reply attack towards the attacking enemy units without being explicitly ordered. We disable such automatic replies towards the enemy attacks or enemy units that are located closely by creating new units that are the exact copies of existing ones with two attributes modified: Combat: Default Acquire Level is set to Passive (default Offensive) and Behaviour: Response is set to No Response (default Acquire). These fields are only modified for allied units; enemy units are unchanged.</p><p>The sight and shooting range values might differ from the built-in sight or range attribute of some StarCraft II units. Our goal is not to master the original full StarCraft II game, but rather to benchmark MARL methods for decentralised control.</p><p>The game AI is set to level 7, very difficult. Our experiments, however, suggest that this setting does significantly impact the unit micromanagement of the built-in heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Architecture and Training</head><p>The architecture of all agent networks is a DRQN with a recurrent layer comprised of a GRU with a 64-dimensional hidden state, with a fully-connected layer before and after. Exploration is performed during training using independent -greedy action selection, where each agent a performs -greedy action selection over its own Q a . Throughout the training, we anneal linearly from 1.0 to 0.05 over 50k time steps and keep it constant for the rest of the learning. We set ? = 0.99 for all experiments. The replay buffer contains the most recent 5000 episodes. We sample batches of 32 episodes uniformly from the replay buffer, and train on fully unrolled episodes, performing a single gradient descent step after every episode. The target networks are updated after every 200 training episodes. The Double Q-Learning update rule from <ref type="bibr" target="#b68">(Van Hasselt et al., 2016)</ref> is used for all Q-Learning variants (IQL, VDN, QMIX and QTRAN).</p><p>To speed up the learning, we share the parameters of the agent networks across all agents. Because of this, a one-hot encoding of the agent id is concatenated onto each agent's observations. All neural networks are trained using RMSprop 6 with learning rate 5 ? 10 ?4 .</p><p>The mixing network consists of a single hidden layer of 32 units, utilising an ELU non-linearity. The hypernetworks consist of a feedforward network with a single hidden layer of 64 units with a ReLU non-linearity. The output of the hypernetwork is passed through an absolute function (to acheive non-negativity) and then resized into a matrix of appropriate size.</p><p>The architecture of the COMA critic is a feedforward fully-connected neural network with the first 2 layers having 128 units, followed by a final layer of |U | units. We set ? = 0.8. We utilise the same -floor scheme as in  for the agents policies, linearlly annealing from 0.5 to 0.01 over 100k timesteps. For COMA we roll-out 8 episodes and train on those episodes. The critic is first updated, performing a gradient descent step for each timestep in the episode, starting with the final timestep. Then the agent policies are updated by a single gradient descent step on the data from all 8 episodes.</p><p>The architecture of the centralised Q for QTRAN is similar to the one used in <ref type="bibr" target="#b59">(Son et al., 2019)</ref>. The agent's hidden states (64 units) are concatenated with their chosen action (|U | units) and passed through a feedforward network with a single hidden layer and a ReLU non-linearity to produce an agent-action embedding (64 + |U | units). The network is shared across all agents. The embeddings are summed across all agents. The concatentation of the state and the sum of the embeddings is then passed into the Q network. The Q network consists of 2 hidden layers with ReLU non-linearities with 64 units each. The V network 6. We set ? = 0.99 and do not use weight decay or momentum. takes the state as input and consists of 2 hidden layers with ReLU non-linearities with 64 units each. We set ? opt = 1 and ? nopt min = 0.1.</p><p>We also compared a COMA style architecture in which the input to Q is the state and the joint-actions encoded as one-hot vectors. For both architectural variants we also tested having 3 hidden layers. For both network sizes and architectural variants we performed a hyperparameter search over ? opt = 1 and ? nopt min ? {0.1, 1, 10} on all three of the scenarios we tested QTRAN on and picked the best performer out of all configurations.</p><p>VDN-S uses a state-dependent bias that has the same architecture as the final bias in QMIX's mixing network, a single hidden layer of 64 units with a ReLU non-linearity. VDN-S Bigger uses 2 hidden layers of 64 units instead.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Table of Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Regression Experiment</head><p>For the regression experiment we simulate a task with 4 agents. There are 10 states, of dimensionality 10, in which each dimension is uniformly sampled from [?1, 1]. For each of the 10 states, the agent Q-values are uniformly sampled from [?1, 1]. The Q tot targets for each state are uniformly sampled from [0, 10]. The agent Q-values, targets and states are kept fixed, only the parameters of the mixing network may change. We use a mixing embed dimension of 32, and train for a total of 2000 timesteps. At each timestep we uniformly sample a state, add it to the replay buffer of size 200, and execute a training step. Each training step consists of sampling a minibatch of 32 from the replay buffer and minimising the L 2 loss between the Q tot targets and the network's outputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Decentralised unit micromanagement in StarCraft II, where each learning agent controls an individual unit. The goal is to coordinate behaviour across agents to defeat all enemy units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Mixing network structure. In red are the hypernetworks that produce the weights and biases for mixing network layers shown in blue. (b) The overall QMIX architecture. (c) Agent network structure. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>|D| &gt; batch-size then 17: b ? random batch of episodes from D 18: for each timestep t in each episode in batch b do 19:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The median max u Q tot (s, u) for {2, 3, 4} agents with {2, 3, 4} actions across 10 runs for VDN and QMIX. 25%-75% quartile is shown shaded. The dashed line at 10 indicates the correct value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The cyan and red circles respectively border the sight and shooting range of the agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Screenshots of two SMAC scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>architectures and training details are presented in Appendix D.3. A table of the results is included in Appendix D.4. Data from the individual runs are available at the SMAC repository (https://github.com/oxwhirl/smac).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Left: The median test win %, averaged across all 14 scenarios. Heuristic's performance is shown as a dotted line. Right: The number of scenarios in which the algorithm's median test win % is the highest by at least 1/32 (smoothed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Three scenarios including QTRAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Easy scenarios. The heuristic AI's performance shown as a dotted black line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Hard scenarios. The heuristic AI's performance shown as a dotted black line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Super Hard scenarios. The heuristic AI's performance shown as a dotted black line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Ablations for state experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Ablations for mixing network linearity experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>The learnt mixing of QMIX on the two-step game from Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Ablations for linear experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 20 :</head><label>20</label><figDesc>The learnt mixing of QMIX on 3s5z, using a tanh nonlinearity, at the end of training for timesteps 0 (left) and 50 (right), for agents 2 (top) and 7 (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 21 :</head><label>21</label><figDesc>The Median test win % comparing QMIX with an ELU nonlinearity (QMIX) and QMIX with a tanh nonlinearity in the mixing network (QMIX-Tanh).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 22 :</head><label>22</label><figDesc>Comparing agent networks with and without RNNs (QMIX-FF) on two scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 23 :</head><label>23</label><figDesc>Comparing the different architectures for the COMA critic without the state, on two scenarios.Figures 23 and 24show that the architectural change has little effect on the performance of COMA, with the addition of the state actually degrading performance in 2s3z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 24 :</head><label>24</label><figDesc>Comparing the different architectures for the COMA critic with the state on two scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 25 :</head><label>25</label><figDesc>Comparing larger agent networks for VDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 26 :</head><label>26</label><figDesc>Comparing a larger state-dependent bias for VDN-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 27 :</head><label>27</label><figDesc>Loss for all six methods on the Two Step Game. The mean and 95% confidence interval is shown across 30 independent runs.IQL VDN VDN-S QMIX QMIX-Lin QMIX-NS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 QMIX 1: Initialise ?, the parameters of mixing network, agent networks and hypernetwork. 2: Set the learning rate ? and replay buffer D = {} 3: step = 0, ? ? = ? 4: while step &lt; step max do</figDesc><table><row><cell>5:</cell><cell>t = 0, s 0 = initial state</cell></row><row><cell>6:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="6">: (a) An example of a monotonic payoff matrix, (b) a nonmonotonic payoff matrix.</cell></row><row><cell></cell><cell cols="2">Agent 2</cell><cell></cell><cell cols="2">Agent 2</cell></row><row><cell></cell><cell>A</cell><cell>B</cell><cell></cell><cell>A</cell><cell>B</cell></row><row><cell>Agent 1</cell><cell>A -1.5 B 2.5</cell><cell>2.5 6.5</cell><cell>Agent 1</cell><cell>A 4/3 B 4/3</cell><cell>4/3 8</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>(a) Optimal Q tot values for VDN on the example in Table 1a. (b) Optimal Q tot values for QMIX on the example in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">State 1</cell><cell cols="2">State 2A</cell><cell cols="2">State 2B</cell></row><row><cell>(a)</cell><cell></cell><cell>A</cell><cell>B</cell><cell>A</cell><cell>B</cell><cell>A</cell><cell>B</cell></row><row><cell></cell><cell>A</cell><cell>6.94</cell><cell>6.94</cell><cell>6.99</cell><cell>7.02</cell><cell>-1.87</cell><cell>2.31</cell></row><row><cell></cell><cell>B</cell><cell>6.35</cell><cell>6.36</cell><cell>6.99</cell><cell>7.02</cell><cell>2.33</cell><cell>6.51</cell></row><row><cell></cell><cell></cell><cell>A</cell><cell>B</cell><cell>A</cell><cell>B</cell><cell>A</cell><cell>B</cell></row><row><cell>(b)</cell><cell>A</cell><cell>6.93</cell><cell>6.93</cell><cell>7.00</cell><cell>7.00</cell><cell>0.00</cell><cell>1.00</cell></row><row><cell></cell><cell>B</cell><cell>7.92</cell><cell>7.92</cell><cell>7.00</cell><cell>7.00</cell><cell>1.00</cell><cell>8.00</cell></row></table><note>Payoff matrices of the two-step game after the Agent 1 chose the first action. Action A takes the agents to State 2A and action B takes them to State 2B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Q tot on the two-step game for (a) VDN and (b) QMIX.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>SMAC challenges. Note that list of SMAC scenarios has been updated from the earlier version. All scenarios, however, are still available in the repository.or absence of elevated or impassable terrain. Figures 1 and 6 include screenshots of several SMAC micro scenarios.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Q tot on the 2 step game for (a) VDN, (b) QMIX, (c) QMIX-NS, (d) QMIX-Lin</cell></row><row><cell>and (e) VDN-S</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Q a for IQL on the 2 step game</figDesc><table><row><cell>B.3 Results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>The final Test Reward acheived.</figDesc><table><row><cell>Appendix C. Random Matrix Games</cell></row><row><cell>C.1 Architecture and Training</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table D .</head><label>D</label><figDesc>4 shows the final median performance (maximum median across the testing intervals within the last 250k of training) of the algorithms tested. The mean test win %, across 1000 episodes, for the heuristic-based ai is also shown.</figDesc><table><row><cell></cell><cell cols="5">IQL COMA VDN QMIX Heuristic</cell></row><row><cell>2s vs 1sc</cell><cell>100</cell><cell>98</cell><cell>100</cell><cell>100</cell><cell>0</cell></row><row><cell>2s3z</cell><cell>75</cell><cell>43</cell><cell>97</cell><cell>99</cell><cell>90</cell></row><row><cell>3s5z</cell><cell>10</cell><cell>1</cell><cell>84</cell><cell>97</cell><cell>42</cell></row><row><cell>1c3s5z</cell><cell>21</cell><cell>31</cell><cell>91</cell><cell>97</cell><cell>81</cell></row><row><cell>10m vs 11m</cell><cell>34</cell><cell>7</cell><cell>97</cell><cell>97</cell><cell>12</cell></row><row><cell>2c vs 64zg</cell><cell>7</cell><cell>0</cell><cell>21</cell><cell>58</cell><cell>0</cell></row><row><cell>bane vs bane</cell><cell>99</cell><cell>64</cell><cell>94</cell><cell>85</cell><cell>43</cell></row><row><cell>5m vs 6m</cell><cell>49</cell><cell>1</cell><cell>70</cell><cell>70</cell><cell>0</cell></row><row><cell>3s vs 5z</cell><cell>45</cell><cell>0</cell><cell>91</cell><cell>87</cell><cell>0</cell></row><row><cell>3s5z vs 3s6z</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>0</cell></row><row><cell>6h vs 8z</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>0</cell></row><row><cell>27m vs 30m</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>49</cell><cell>0</cell></row><row><cell>MMM2</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>69</cell><cell>0</cell></row><row><cell>corridor</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>The Test Win Rate % of the various algorithms.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Four directions: north, south, east, or west. 4. Dead agents can only take no-op action while live agents cannot.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713). It was also supported by the Oxford-Google DeepMind Graduate Scholarship, the UK EPSRC CDT in Autonomous Intelligent Machines and Systems, Chevening Scholarship, Luys Scholarship and an EPSRC grant (EP/M508111/1, EP/N509711/1). This work is linked to and partly funded by the project Free the Drones (FreeD) under the Innovation Fund Denmark and Microsoft. The experiments were made possible by a generous equipment grant from NVIDIA.</p><p>We would like to thank Frans Oliehoek and Wendelin Boehmer for helpful comments and discussion. We also thank Oriol Vinyals, Kevin Calderone, and the rest of the SC2LE team at DeepMind and Blizzard for their work on the interface.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A convergence analysis of gradient descent for deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02281</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The hanabi challenge: A new frontier for ai research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nolan</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibl</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="page">103216</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey of Multiagent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep Coordination Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendelin</forename><surname>Bhmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00091</idno>
		<idno>arXiv: 1910.00091</idno>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Overview of Recent Progress in the Study of Distributed Multi-agent Coordination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanrong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="427" to="438" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The representational capacity of action-value networks for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1862" to="1864" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03738</idno>
		<idno>arXiv: 1809.03738</idno>
		<ptr target="http://arxiv.org/abs/1809.03738" />
		<title level="m">Factorized Q-Learning for Large-Scale Multi-Agent Systems</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LIIR: Learning individual intrinsic reward in multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4405" to="4416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating functional knowledge in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Blisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1239" to="1262" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06070</idno>
		<title level="m">Diversity is all you need: Learning skills without a reward function</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2137" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 34th International Conference on Machine Learning</title>
		<meeting>The 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1146" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Counterfactual multi-agent policy gradients</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep multi-agent reinforcement learning with discrete-continuous hybrid action spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04959</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiagent Planning with Factored MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1523" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cooperative Multi-agent Control Using Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykel</forename><surname>Egorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous Agents and Multiagent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Recurrent Q-Learning for Partially Observable MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from self-play in imperfectinformation games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1603.01121</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Ernestus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anssi</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<ptr target="https://github.com/hill-a/stable-baselines" />
		<editor>Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feryal</forename><surname>Behbahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Baumli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gmez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00979</idno>
		<ptr target="https://arxiv.org/abs/2006.00979" />
		<title level="m">Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Guided Deep Reinforcement Learning for Swarm Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>H?ttenrauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian?o?i?</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS 2017 Autonomous Robots and Multirobot Systems (ARMS) Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep Reinforcement Learning for Swarm Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Httenrauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06613</idno>
		<idno>arXiv: 1807.06613</idno>
		<ptr target="http://arxiv.org/abs/1807.06613" />
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Actor-Attention-Critic for Multi-Agent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shariq</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02912</idno>
		<idno>arXiv: 1810.02912</idno>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to play guess who? and inventing a grounded language as a consequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>K?geb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Gustavsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative Multiagent Reinforcement Learning by Payoff Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1789" to="1828" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning as a rehearsal for decentralized planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Landon</forename><surname>Kraemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bikramjit</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page" from="82" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An algorithm for distributed reinforcement learning in cooperative multi-agent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning. Citeseer</title>
		<meeting>the Seventeenth International Conference on Machine Learning. Citeseer</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning in sequential social dilemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Joel Z Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Marecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 16th Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RLlib: Abstractions for distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Tong Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katia</forename><surname>Debord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Estabridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Hewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cesma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02311</idno>
		<title level="m">Centralized expert supervises multi-agents</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Value function transfer for deep multi-agent reinforcement learning based on n-step returns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="457" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6382" to="6393" />
		</imprint>
	</monogr>
	<note>OpenAI Pieter Abbeel, and Igor Mordatch</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Decentralized likelihood quantile networks for improving performance in deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueguang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Amato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06319v4</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MAVEN: Multi-Agent Variational Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7611" to="7622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep Multi-Agent Reinforcement Learning with Relevance Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Malysheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyong</forename><surname>Tegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chae-Bong</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Kudenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shpilman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12557</idno>
		<idno>arXiv: 1811.12557</idno>
		<ptr target="http://arxiv.org/abs/1811.12557" />
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La?titia</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><forename type="middle">Le</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fort-Piat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A Concise Introduction to Decentralized POMDPs. SpringerBriefs in Intelligent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimal and Approximate Q-value Functions for Decentralized POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frans</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Matthijs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep Decentralized Multi-task Multi-Agent RL under Partial Observability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayegan</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Vian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">International Foundation for Autonomous Agents and Multiagent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pageaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vronique</forename><surname>Deslandres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilissa</forename><surname>Lehoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salima</forename><surname>Hassas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS &apos;19</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS &apos;19<address><addrLine>Richland, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2156" to="2158" />
		</imprint>
	</monogr>
	<note>Multiagent Learning and Coordination with Clustered Deep Q-Network</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lenient multi-agent deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Bloembergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 17th International Conference on Autonomous Agents and MultiAgent Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="443" to="451" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">International Foundation for Autonomous Agents and Multiagent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS &apos;19</title>
		<editor>Montreal QC, Canada</editor>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS &apos;19<address><addrLine>Richland, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
	<note>Negative Update Intervals in Deep Multi-Agent Reinforcement Learning</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Theoretical advantages of lenient learners: An evolutionary game theoretic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liviu</forename><surname>Panait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="423" to="457" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenkun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10069</idno>
		<title level="m">Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Approximation theory of the mlp model in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta numerica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciek</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Multi-goal reinforcement learning: Challenging robotics environments and request for research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Machine theory of mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Perbet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4218" to="4227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
		<idno>PMLR 80</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The StarCraft Multi-Agent Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Man</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2186" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hornigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07366</idno>
		<idno>arXiv: 1905.07366</idno>
		<ptr target="http://arxiv.org/abs/1905.07366" />
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
	<note>physics, stat</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deep Multi-Agent Reinforcement Learning for Decentralized Continuous Cooperative Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Alexandre</forename><surname>Kamienny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendelin</forename><surname>Bhmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06709</idno>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Schroeder De Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendelin</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Boehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11702</idno>
		<idno>arXiv: 1810.11702</idno>
		<ptr target="http://arxiv.org/abs/1810.11702" />
		<title level="m">Multi-Agent Common Knowledge Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghwan</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daewoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><forename type="middle">Ju</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Earl</forename><surname>Hostallero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung</forename><surname>Yi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05408</idno>
		<idno>arXiv: 1905.05408</idno>
		<ptr target="http://arxiv.org/abs/1905.05408" />
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Keepaway soccer: From machine learning testbed to benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robot Soccer World Cup</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="93" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01500</idno>
		<title level="m">rlpyt: A research code base for deep reinforcement learning in pytorch</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrunas</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 17th International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Auvolat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothe</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Richoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00625</idno>
		<title level="m">TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multiagent cooperation and competition with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardi</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tambet</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorian</forename><surname>Kodelja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristjan</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaan</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Vicente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<ptr target="http://us.battle.net/sc2/en/blog/4544189/new-blizzard-custom-game-starcraft-master-3-1-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="2018" to="2029" />
		</imprint>
	</monogr>
	<note>Blizzard Entertainment. New blizzard custom game: Starcraft master</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04782</idno>
		<title level="m">StarCraft II: A New Challenge for Reinforcement Learning</title>
		<meeting><address><addrLine>Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Grandmaster level in starcraft ii using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7782</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning nearly decomposable value functions via communication minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjie</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Action semantics network: Considering the effects of actions in multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">From few to more: Large-scale dynamic multiagent curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge England</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Lenient learning in independent-learner stochastic cooperative games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermo</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2914" to="2955" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Multiagent reinforcement learning for multi-robot systems: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbing</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Hierarchical cooperative multi-agent reinforcement learning with skill discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Borovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03558</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Mean field multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minne</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05438</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">SMIX($\lambda$): Enhancing centralized value functions for cooperative multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Efficient communication in multi-agent reinforcement learning via variance based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Sai Qian Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3230" to="3239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning Efficient Communication in Cooperative Multi-Agent Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems<address><addrLine>Richland, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2321" to="2323" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
