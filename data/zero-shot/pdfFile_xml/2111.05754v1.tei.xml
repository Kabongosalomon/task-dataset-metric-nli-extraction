<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prune Once for All: Sparse Pre-Trained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Zafrir</surname></persName>
							<email>ofir.zafrir@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Larey</surname></persName>
							<email>ariel.larey@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Boudoukh</surname></persName>
							<email>guy.boudoukh@intel.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haihao</forename><surname>Shen</surname></persName>
							<email>haihao.shen@intel.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
							<email>moshe.wasserblat@intel.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prune Once for All: Sparse Pre-Trained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based language models are applied to a wide range of applications in natural language processing. However, they are inefficient and difficult to deploy. In recent years, many compression algorithms have been proposed to increase the implementation efficiency of large Transformer-based models on target hardware. In this work we present a new method for training sparse pre-trained Transformer language models by integrating weight pruning and model distillation. These sparse pre-trained models can be used to transfer learning for a wide range of tasks while maintaining their sparsity pattern. We demonstrate our method with three known architectures to create sparse pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed sparse pre-trained models we trained transfer their knowledge to five different downstream natural language tasks with minimal accuracy loss. Moreover, we show how to further compress the sparse models' weights to 8bit precision using quantization-aware training. For example, with our sparse pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a compression ratio of 40X for the encoder with less than 1% accuracy loss. To the best of our knowledge, our results show the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based pre-trained language models (LM) such as BERT <ref type="bibr" target="#b2">[Devlin et al., 2019]</ref> and RoBERTa <ref type="bibr" target="#b15">[Liu et al., 2019]</ref> have become the standard approach for a wide range of natural language processing (NLP) tasks. Recently, we witness the emergence of models, larger by several orders of magnitude, such as GPT-2 <ref type="bibr" target="#b19">[Radford et al., 2019]</ref>, T-NLG <ref type="bibr" target="#b22">[Rosset, 2020]</ref>, GPT-3 <ref type="bibr" target="#b0">[Brown et al., 2020]</ref>, and Switch-C <ref type="bibr" target="#b3">[Fedus et al., 2021]</ref>. These models advance the state-of-the-art results in several NLP tasks such as question answering and text classification. However, this trend toward bigger models raises several concerns. As the computational and memory resources required to run inference increase with the model's size, it becomes very expensive and challenging to deploy these models in production environments and on edge devices. Moreover, these large amounts of computational resources incur a steep environmental cost <ref type="bibr" target="#b27">[Strubell et al., 2019]</ref>.</p><p>Model compression of large LM is a growing field of study as a result of these concerns. Weight pruning is a compression method that has been shown to be very effective at reducing the memory footprint of a model <ref type="bibr">[Han et al., 2015, Zhu and</ref><ref type="bibr">Gupta, 2018]</ref>. However, weight pruning of large Transformer-based LMs to high sparsity ratios requires specialized pruning methods <ref type="bibr">[Sanh et al.,</ref> 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2111.05754v1 [cs.CL] 10 Nov 2021 2020, <ref type="bibr" target="#b5">, Gordon et al., 2020</ref><ref type="bibr" target="#b13">, Lagunas et al., 2021</ref>. Moreover, most of the pruning methods require task specific modifications and tuning to produce quality results. <ref type="bibr" target="#b5">Gordon et al. [2020]</ref> found that, in terms of accuracy, it does not matter whether BERT is pruned during the pre-training phase or during the transfer learning phase. This suggests that a LM can be pruned once during pre-training and then fine-tuned to any downstream task without task-specific tuning.</p><p>In this paper, we present a new method, Prune Once for All (Prune OFA), that leverages weight pruning and model distillation to produce pre-trained Transformer-based language models with a high sparsity ratio. We apply our method to BERT-Base, BERT-Large and DistilBERT <ref type="bibr" target="#b23">[Sanh et al., 2019]</ref> to produce sparse pre-trained models for these model architectures. We then show how these sparse models can be fine-tuned to produce task-specific sparse models with minimal accuracy loss for SQuADv1.1 <ref type="bibr" target="#b20">[Rajpurkar et al., 2016]</ref> as well as for four tasks from the GLUE Benchmark <ref type="bibr" target="#b29">[Wang et al., 2018]</ref>. We also show that it is possible to further compress the models using quantization-aware training to achieve state-of-the-art results in terms of compression-to-accuracy ratio.</p><p>The main contributions of this work are threefold: 1) We introduce a new architecture-agnostic method of training sparse pre-trained language models. 2) We demonstrate how to fine-tune these sparse models on downstream tasks to create sparse and quantized models, removing the burden of pruning and tuning for a specific language task. 3) We publish our compression research library with example scripts to reproduce our work for other architectures, along with our sparse pre-trained models presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Large language models are over-parameterized and difficult to deploy. Therefore, the problem of compressing these models with minimum accuracy loss for downstream tasks is widely explored.  suggests the Movement Pruning method designed especially for transfer learning. Neural Magic implements Gradual Magnitude Pruning. 1 Both methods suggest pruning BERT-Base while fine-tuning to downstream tasks paired with model distillation, and present results showing 90% sparsity for several tasks. However, both methods require a long fine-tuning time as well as tuning pruning related hyper-parameters for every task. Our method, on the other hand, requires no tuning of special pruning hyper-parameters per task because we prune the model once for all tasks. Furthermore, we present better or comparable results at a much lower computation budget at the transfer learning phase. <ref type="bibr" target="#b5">Gordon et al. [2020]</ref> explored the effect of weight pruning during transfer learning and concluded that pruning BERT-Base at the pre-training phase does not degrade the performance of the model compared to pruning at fine-tuning. We improve upon the suggested method and present better results at a much higher sparsity ratio.  explored the Lottery Ticket Hypothesis <ref type="bibr" target="#b4">[Frankle and Carbin, 2018]</ref> for BERT pre-trained models. More specifically, they analyzed the possibility of finding winning tickets in a BERT-Base pre-trained model that transfer to other downstream tasks. The authors concluded that winning tickets found while pre-training on a Masked-LM task, transfer well to other downstream tasks. <ref type="bibr" target="#b13">Lagunas et al. [2021]</ref> presented a structured pruning method, removing rows, columns and attention heads, while achieving less than 1% loss in F1 for a BERT architecture on SQuADv1.1. <ref type="bibr" target="#b17">Mishra et al. [2021]</ref> performed structured 2:4 pruning on BERT while further pre-training BERT; The method produced a 50% sparse model which can be fine-tuned without accuracy loss. <ref type="bibr" target="#b16">Michel et al. [2019]</ref> explored the significance of each head in the multi-head attention mechanism of BERT and presented a method for pruning attention heads with their associated weights.</p><p>Other works propose knowledge distillation to compress Transformer models to a smaller dense counter part that can be tuned to downstream tasks <ref type="bibr" target="#b23">[Sanh et al., 2019</ref><ref type="bibr" target="#b9">, Jiao et al., 2020</ref><ref type="bibr" target="#b28">, Sun et al., 2020</ref>. Quantization of Transformer-based language models is also a well known method for compression. <ref type="bibr" target="#b25">Shen et al. [2020]</ref> proposes a method to quantize BERT at a different bit-width per layer. Other works implement quantization-aware training to quantize BERT to 8bits <ref type="bibr" target="#b10">[Kim et al., 2021</ref><ref type="bibr">, Zafrir et al., 2019</ref>. <ref type="bibr" target="#b1">Zhang et al. [2020]</ref> created a method of producing a ternary weight BERT. <ref type="bibr" target="#b11">Kim and Hassan [2020]</ref> presented a compression pipeline for Transformer models that includes model distillation, quantization and head pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weight pruning</head><p>Weight pruning is the process of forcing some of the neural network's weights to zero. Weight pruning can be either unstructured where individual weights are pruned, or structured where structured groups of weights are pruned, e.g. blocks, channels, layers. Weight pruning results in sparse neural networks that reduce the computation and the memory footprint of the trained model.</p><p>In this paper we focus on unstructured weight pruning. Zhu and Gupta <ref type="bibr">[2018]</ref> presented a method of Gradual Magnitude Pruning (GMP) to gradually prune weights with low magnitude during training. During training, every f steps the lowest magnitude weights are pruned until reaching the temporal sparsity ratio s t for time step t, defined by</p><formula xml:id="formula_0">s t = s f + (s i ? s f ) 1 ? t ? t s t e ? t s 3<label>(1)</label></formula><p>where s i and s f are the initial and final sparsity ratios, and t s and t e are the pruning start and end time steps.</p><p>In a recent paper, <ref type="bibr" target="#b21">Renda et al. [2020]</ref> presented a pruning algorithm based on IMP (Iterative Magnitude Pruning) <ref type="bibr" target="#b6">[Han et al., 2015]</ref> and Learning Rate Rewinding (LRR). IMP consist of two steps: prune a portion of the model and continue fine-tuning it to recover from the induced pruning error. These two steps are repeated until the desired sparsity ratio is achieved. In LRR, the learning rate scheduler is rewound to its state before the pruning step at the beginning of the fine-tune step. We propose to incorporate the principle of learning rate rewinding into GMP by rewinding the learning rate scheduler to its state at time t s every f steps. After t e the scheduler continues with its original setting until training ends. Appendix C visualizes how LRR combined with GMP modifies the learning rate scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Knowledge distillation</head><p>Knowledge distillation, introduced by Hinton et al. <ref type="bibr">[2015]</ref>, is the process of training a student network to reproduce the behavior of a teacher model. When distillation is used to fit the predictions of the teacher model, soft cross-entropy loss between the student and the teacher soft probabilities is computed as follows:</p><formula xml:id="formula_1">L kd = ? i t i ? log (s i )<label>(2)</label></formula><p>where s i is the soft-probability estimated by the student, and t i is its corresponding soft-probability estimated by the teacher for the same input sample. The soft probabilities are calculated using a softmax function with temperature T .</p><p>Commonly, the teacher is a large model that achieves high performance, and the student is based on a smaller architecture. In this paper, we propose to leverage the model distillation method for the pruning process. We focus on an approach where both teacher and student share the same architecture, but differ in their sparsity ratio. In this case, the teacher is a dense model that was trained on a target task, and the student is a model with a fixed sparsity or one undergoing pruning. Distillation-duringpruning can be applied to language models during both the pre-training and fine-tuning phases. In the pre-training phase, the teacher is a pre-trained language model, and in the fine-tuning phase, the teacher is a language model fine-tuned to a target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Prune Once for All</head><p>The notion of pruning language models such as BERT <ref type="bibr" target="#b2">[Devlin et al., 2019]</ref> while pre-training has already been explored by  and <ref type="bibr" target="#b5">Gordon et al. [2020]</ref>. However, fine-tuning the sparse model for a specific language task resulted in either poor results or a low sparsity ratio. In this section we will introduce our novel method, Prune OFA, for creating sparse pre-trained language models that can be later fine-tuned to downstream tasks with minimal accuracy loss at high sparsity ratios. A visualization of our method is presented in <ref type="figure">Figure 1</ref>. The method takes as its input a pre-trained language model and outputs a sparse language model of the same architecture. The method consists of two steps, teacher preparation and student pruning. The sparse pre-trained model we trained is the  <ref type="figure">Figure 1</ref>: Prune OFA method model we use for transfer learning while maintaining its sparsity pattern. We call the method Prune Once for All since we show how to fine-tune the sparse pre-trained models for several language tasks while we prune the pre-trained model only once.</p><p>Teacher preparation The first step of Prune OFA is to obtain a model optimized on the pre-training dataset for some pre-training task with objective L PT as shown in <ref type="figure">Figure 1</ref>. <ref type="bibr">2</ref> The same dataset will be used for pruning the student in the next step. This model will initialize the student and teacher models in the student pruning step.</p><p>Student pruning A student model is initialized from the teacher prepared in the teacher preparation step. The student is then fine-tuned on a linear combination of the pre-training task, from the teacher preparation step, and the knowledge distillation objective L kd :</p><formula xml:id="formula_2">L = ? P T L P T + ? kd L kd<label>(3)</label></formula><p>while being pruned with GMP + LRR methods. The output model of this process is a sparse pretrained LM that can be used without additional pruning for transfer learning to produce sparse models for a specific downstream task.</p><p>Pattern-lock We wish to keep the sparsity pattern of the sparse pre-trained model created by Prune OFA in place during the fine-tuning process. We propose a method called pattern-lock that prevents the zeros found in the model from changing while training the model. Pattern-lock is described in more details in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental setup</head><p>Datasets We use the English Wikipedia dataset (2500M words) for training the models on the pre-training task. We split the data into train (95%) and validation (5%) sets. Both sets are preprocessed as described in the models' original papers <ref type="bibr" target="#b2">[Devlin et al., 2019</ref><ref type="bibr" target="#b23">, Sanh et al., 2019</ref>. We process the data to use the maximum sequence length allowed by the models, however, we allow shorter sequences at a probability of 0.1. We evaluate our sparse pre-trained models on several common benchmarks for transfer learning; a question answering task, SQuADv1.1 containing 89K training examples <ref type="bibr" target="#b20">[Rajpurkar et al., 2016]</ref>, and the following text classification tasks from the GLUE Benchmark: MNLI, QQP, QNLI and SST-2 containing 393K, 364K, 105K, and 67K training examples respectively <ref type="bibr" target="#b29">[Wang et al., 2018</ref><ref type="bibr" target="#b30">, Williams et al., 2018</ref><ref type="bibr" target="#b8">, Iyer et al., 2017</ref><ref type="bibr" target="#b26">, Socher et al., 2013</ref>.</p><p>Applying Prune Once for All We showcase our method by applying Prune OFA on three different architectures of different sizes; BERT-Base, BERT-Large and DistilBERT. Since we don't have the original processed training data used to train BERT-Base, BERT-Large and DistilBERT we run an additional step to fine-tune the pre-trained models using the processed training data we prepared. Next, we execute the student pruning step to obtain our sparse pre-trained models. We prune BERT-Base and DistilBERT to {85%, 90%} sparsity ratios and BERT-Large to a 90% sparsity ratio. Pruning is applied to all Linear layers in the Transformer encoder including the pooler layer if it exists. Exact hyper-parameters and additional details are summarized in Appendix E  Transfer learning After creating our sparse pre-trained models we fine-tune them to the following NLP tasks: SQuADv1.1, QNLI, MNLI, SST-2 and QQP. We use default hyper-parameters for each task and conduct a grid search for learning rate, weight decay, warmup ratio and number of training epochs hyper-parameters. For each task we report the mean of two different runs with different seeds that achieved the best result on the task's development set. We further improve the results of our sparse models by integrating knowledge distillation. For each task and model, we create a task teacher based on the original dense pre-trained model fine-tuned to the task. For SQuADv1.1 and QQP we report the result that maximizes F1, and for MNLI we report the result that maximizes the mismatched accuracy. For exact hyper-parameters and additional details see Appendix E.</p><p>Comparison with fine-tune pruning We compare our Prune OFA method with fine-tune pruning where we prune the dense pre-trained model during fine-tuning to a downstream task. For that purpose, we implement GMP pruning coupled with knowledge distillation and run experiments using the same teacher and hyper-parameters used in the Prune OFA transfer learning experiments.</p><p>Quantization We implemented quantization-aware training similar to <ref type="bibr">Q8BERT [Zafrir et al., 2019]</ref>. For details on the differences between our method and Q8BERT see Appendix D. For each task, we pick the best-performing model for this task and perform quantization-aware training on it. We use slightly different hyper-parameters for this training session as described in Appendix E.2. We report the mean of two different runs with different seeds that achieved the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In <ref type="table" target="#tab_1">Table 1</ref> we present our experimental results for pruning BERT-Base to a 85% and 90% sparsity ratio using Prune OFA. We also present results of other pruning methods applied to BERT-Base as well as results of the fine-tune pruning experiments we conducted. Results not marked in the column Transfer with KD do not use model distillation in the transfer learning phase. The best result in each category is marked with bold font. We observe that our method achieves better results than other previous pruning works while pre-training at a higher sparsity ratio. When comparing our Prune OFA method against other fine-tune pruning methods, we observe that our method produces the best results at 85% and 90% sparsity ratios. Moreover, we show accuracy degradation lower than The results for pruning BERT-Large to a 90% sparsity ratio are presented in <ref type="table" target="#tab_2">Table 2</ref>. These results fall within the range of 1% accuracy loss for all tasks but the MNLI task. We conclude that the 90% sparse BERT-Large (30.2M non-zero parameters) model we trained has better accuracy in comparison to dense BERT-Base (85M non-zero parameters).</p><p>Our results for pruning DistilBERT to a 85% and 90% sparsity ratio are presented in <ref type="table" target="#tab_3">Table 3</ref> with our results for the fine-tune pruning experiments we conducted. In both sparsity ratios our method produces better accuracy results compared to fine-tune pruning (the best result in each category is marked with bold font). Furthermore, at the 85% sparsity ratio our results are within the range of 1% relative accuracy loss in all tasks but QQP.</p><p>Tables 1, 2 and 3 present quantization results, designated with a +QAT suffix. Applying quantizationaware training on our resultant sparse models decreases the accuracy of the model further by an average of 0.67% relative to the full precision model's accuracy. The results for the 85% sparse model +QAT are better than for the 90% sparse model with full precision in all the tasks for BERT-Base and in 3/5 tasks for DistilBERT. Furthermore, the 85% sparse and quantized model are smaller than the 90% sparse model by a factor of 0.375.</p><p>An ablation study was conducted to test how each component of the Prune OFA method affects the ability of the pre-trained model to transfer its knowledge to downstream tasks, as described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and future work</head><p>We introduced Prune OFA, an architecture-agnostic method for producing sparse pre-trained language models. We also showed how these sparse models can be used to obtain fine-tuned sparse models without the burden of task-specific pruning. Our results suggest that using these sparse pre-trained models for transfer learning produces results with minimal performance degradation loss w.r.t their dense counterpart for a variety of NLP tasks. We further demonstrated that integrating quantization can lead to more efficient sparse and quantized models at a small cost to the model's accuracy.</p><p>A possible direction for future research is to explore whether a large and sparse pre-trained model is better at capturing and transferring natural language knowledge than a smaller dense model of the same architecture with similar non-zero parameters count.</p><p>We hope that the release of our code and sparse pre-trained models to the community will help develop more efficient models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We are grateful to Ella Charlaix of HuggingFace for her fruitful comments and corrections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ablation study</head><p>In this section we analyze how each step of the Prune OFA method affects the final results. We compare the models in the same fashion as in Section 7, by comparing the different results of the sparse pre-trained models on downstream tasks. In the ablation study we focus on BERT-Base pruned to 85% fine-tuned to SQuADv1.1 and MNLI. All the results from the ablation study are present in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Teacher preparation</head><p>The teacher preparation step is only done in the case the original processed training data of the pre-trained model is not available. Since our objective is to prune the model it is always better to start from a model that is better optimized to the data used for pruning, hence the teacher preparation step. To measure the effect of the teacher preparation step we prune two models, a model that uses BERT-Base pre-trained model as initialization, and a model that uses the output of the teacher preparation step as initialization. Then, we fine-tune them both to SQuADv1.1 and MNLI tasks and compare their results. We see notable improvement when executing with the teacher preparation step in both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student pruning</head><p>We compare the results of a model pruned with LRR to a model that was pruned without LRR, meaning the learning rate schedule remained the default linear decay with warmup schedule. For SQuADv1.1 we observe a significant improvement in both benchmarks. However, in MNLI case we don't see any improvement in the mismatched accuracy which we try to maximize, but there is a significant improvement in the matched accuracy. We observe that applying knowledge distillation during the student pruning step improves both tasks results. Knowledge distillation seems less significant in SQuADv1.1 case and more significant in MNLI case. In addition, we see that combining LRR and knowledge distillation achieves better results than either method separately. We conclude that applying LRR while pruning improves fine-tuning results and therefore a crucial part of our algorithm.</p><p>Transfer learning with knowledge distillation We saw that using knowledge distillation while fine-tuning to downstream tasks improves the results significantly. We test whether our method still improves accuracy results of sparse models when fine-tuned with model distillation. From the results at the bottom of <ref type="table" target="#tab_4">Table 4</ref> we deduce that our method is orthogonal to knowledge distillation while fine-tuning and improves the accuracy results of both tasks further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Pattern-lock details</head><p>Following is a detailed description of the Pattern-lock method used when fine-tuning our sparse pre-trained models. Before training, Pattern-lock method initializes a mask M l for each sparse layer l with weight W l , representing the layer's sparsity pattern.</p><formula xml:id="formula_3">M l uv = 1 W l uv = 0 0 W l uv = 0<label>(4)</label></formula><p>Then, while training, the loss L gradient w.r.t the weights is modified to</p><formula xml:id="formula_4">?L ?W l uv = ?L ?W l uv M l uv = 1 0 M l uv = 0<label>(5)</label></formula><p>ensuring that a weight that was initially 0 will stay 0 through-out fine-tuning.</p><p>C Visualization of Learning Rate Rewinding with Gradual Magnitude Pruning <ref type="figure" target="#fig_0">Figure 2</ref> demonstrates how a linear decay learning rate scheduler with warmup is modified with LRR against the same scheduler without LRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Quantization method differences from Q8BERT</head><p>We have implemented our own version of quantization-aware training which is similar to Q8BERT with the following differences: 1) Activations are quantized using asymmetric quantization instead of symmetric quantization. 2) Embedding vectors are not quantized and represented in full precision. 3) Models are quantized after fine-tuning to a downstream task in a seperate learning session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Reproducibility E.1 Implementation</head><p>Our Prune OFA method, GMP, model distillation and quantization-aware training are implemented in our Model Compression Research Package using PyTorch <ref type="bibr" target="#b18">[Paszke et al., 2019]</ref>. 4 Our library offers several architecture agnostic pruning and other compression methods that can be plugged into any training session with a few lines of code. We invite the researches community to use our library to accelerate their research in pruning and neural networks compression.  We use the HuggingFace/transformers library and the available example scripts to train our Transformer-based models . We have modified the example scripts to include our methods and make them available in our library's examples.</p><p>All the datasets mentioned in the paper are downloaded and processed using the HuggingFace/datasets library <ref type="bibr" target="#b14">[Lhoest et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Training details &amp; hyper-parameters</head><p>Teacher preparation We execute the teacher preparation step on all models. The pre-training objectives for both BERT models and DistilBERT are the same as in the original paper. For BERT models, the objectives are masked language-modeling (MLM) and next sentence predicition (NSP), and for DistilBERT the objective is MLM only. The hyper-parameters used are detailed in <ref type="table" target="#tab_5">Table 5</ref>. We use Adam optimizer <ref type="bibr" target="#b12">[Kingma and Ba, 2015]</ref> with learning rates {5e-5, 1e-4, 1e-4} for {BERT-Base, BERT-Large, DistilBERT}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student pruning</head><p>We run student pruning with the same objectives, hyper-parameters and optimizer we used at the teacher preparation step ( Transfer learning For transfer learning experiments of either Prune OFA or fine-tune pruning we use the hyper-parameters in <ref type="table" target="#tab_6">Table 6</ref> coupled with Adam optimizer. When combining knowledge distillation in the transfer learning phase, in our experiments we found that it is best to optimize only on knowledge distillation objective and ignore the ground truth labels. Quantization For quantization-aware training experiments of Prune OFA we use the hyperparameters in <ref type="table" target="#tab_8">Table 7</ref> coupled with Adam optimizer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Learning rate and sparsity scheduler. Both figures show a linear decay learning rate scheduler with t wu warmup steps against a sparsity scheduler defined by Equation 1. (a) learning scheduler without rewinding. (b) learning scheduler with rewinding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Prune OFA BERT-Base results compared to other pruning methods88.50 84.06 84.51 92.13 91.16 91.20 88.13    </figDesc><table><row><cell>Model</cell><cell>Sparsity</cell><cell>Transfer with KD</cell><cell cols="2">SQuAD EM F1</cell><cell cols="4">MNLI (m/mm) SST-2 QNLI Acc Acc Acc Acc</cell><cell>QQP Acc</cell><cell>F1</cell></row><row><cell cols="6">Reference 80.80 Chen et al. [2020] 0% 70% N/A 86.54 82.59</cell><cell>N/A</cell><cell cols="2">91.86 89.44 90.03 N/A</cell></row><row><cell>Gordon et al. [2020]</cell><cell>80%</cell><cell></cell><cell>N/A</cell><cell cols="2">N/A 75.90</cell><cell>N/A</cell><cell cols="2">88.10 85.30 86.90 N/A</cell></row><row><cell>Prune OFA</cell><cell>85%</cell><cell></cell><cell cols="6">78.59 86.63 81.67 82.53 91.34 89.95 90.69 87.41</cell></row><row><cell>Fine-tune pruning Prune OFA</cell><cell>85%</cell><cell>+ +</cell><cell cols="6">78.00 86.16 82.45 83.05 88.82 87.79 90.87 87.65 81.10 88.42 82.71 83.67 91.46 90.34 91.15 88.00</cell></row><row><cell>Prune OFA +QAT</cell><cell>85%</cell><cell>+</cell><cell cols="6">80.84 88.24 81.40 82.51 91.46 89.76 91.09 88.01</cell></row><row><cell>Neural Magic 3</cell><cell></cell><cell>+</cell><cell cols="3">79.40 87.20 N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Sanh et al. [2020]</cell><cell>90%</cell><cell>+</cell><cell cols="4">76.60 84.90 81.20 81.80</cell><cell>N/A</cell><cell>N/A</cell><cell>90.20 86.80</cell></row><row><cell>Prune OFA</cell><cell></cell><cell>+</cell><cell cols="6">79.83 87.25 81.45 82.43 90.88 89.07 90.93 87.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">: Prune OFA BERT-Large results</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Sparsity</cell><cell>SQuAD EM F1</cell><cell>MNLI (m/mm) SST-2 QNLI Acc Acc Acc Acc</cell><cell>QQP Acc</cell><cell>F1</cell></row><row><cell>Reference</cell><cell>0%</cell><cell cols="4">83.99 90.93 86.39 86.58 93.54 92.42 91.59 88.67</cell></row><row><cell>Prune OFA</cell><cell>90%</cell><cell cols="4">83.35 90.20 83.74 84.20 92.95 91.39 91.48 88.43</cell></row><row><cell>Prune OFA + QAT</cell><cell>90%</cell><cell cols="4">83.22 90.02 83.47 84.08 92.72 91.45 91.41 88.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Prune OFA DistilBERT results compared to fine-tune pruning 1% relative to the results of the dense pre-trained model at 85% sparsity with the exception of the MNLI-matched benchmark. Note that for MNLI, the reported results were selected based on the best model's mismatched accuracy found in our grid-search; when searching for the best matched result we reduce the accuracy gap to ? 1% accuracy loss at the expense of increased accuracy loss for mismatched: 83.09/83.36 (m/mm).</figDesc><table><row><cell>Model</cell><cell>Sparsity</cell><cell>SQuAD EM F1</cell><cell cols="3">MNLI (m/mm) SST-2 QNLI Acc Acc Acc Acc</cell><cell>QQP Acc</cell><cell>F1</cell></row><row><cell>Reference</cell><cell>0%</cell><cell cols="2">77.70 85.80 82.20</cell><cell>N/A</cell><cell>91.30 89.20</cell><cell>N/A 88.50</cell></row><row><cell>Fine-tune pruning Prune OFA</cell><cell>85%</cell><cell cols="5">76.16 84.55 81.22 81.92 88.88 86.60 90.18 86.80 78.10 85.82 81.35 82.03 90.60 88.31 90.29 86.97</cell></row><row><cell>Prune OFA +QAT</cell><cell>85%</cell><cell cols="5">77.03 85.13 80.66 81.14 88.93 87.97 90.22 86.92</cell></row><row><cell>Fine-tune pruning Prune OFA</cell><cell>90%</cell><cell cols="5">74.63 83.42 80.47 81.32 88.25 84.91 89.97 86.57 76.91 84.82 80.68 81.47 90.02 87.66 90.05 86.67</cell></row><row><cell>Prune OFA +QAT</cell><cell>90%</cell><cell cols="5">75.62 83.87 78.80 80.40 88.47 87.20 89.97 86.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Prune OFA 85% sparse BERT-Base ablation study results</figDesc><table><row><cell>Teacher preparation</cell><cell>LRR</cell><cell>Pre-train distillation</cell><cell>Transfer distillation</cell><cell>SQuAD EM F1</cell><cell cols="2">MNLI (m/mm) Acc Acc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">78.11 86.13 81.14</cell><cell>81.74</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell><cell cols="2">78.00 86.31 81.22</cell><cell>82.01</cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell><cell cols="2">78.41 86.51 81.39</cell><cell>82.01</cell></row><row><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell cols="2">78.30 86.41 81.57</cell><cell>82.13</cell></row><row><cell>+</cell><cell>+</cell><cell>+</cell><cell></cell><cell cols="2">78.59 86.63 81.67</cell><cell>82.53</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell>+</cell><cell cols="2">80.77 88.08 82.20</cell><cell>82.83</cell></row><row><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell cols="2">81.10 88.42 82.71</cell><cell>83.67</cell></row><row><cell cols="7">O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth</cell></row><row><cell cols="7">Workshop on Energy Efficient Machine Learning and Cognitive Computing -NeurIPS Edition</cell></row><row><cell cols="6">(EMC2-NIPS), pages 36-39, 2019. doi: 10.1109/EMC2-NIPS53020.2019.00016.</cell></row><row><cell cols="7">W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, and Q. Liu. Ternarybert: Distillation-aware</cell></row><row><cell cols="7">ultra-low bit bert. In Proceedings of the 2020 Conference on Empirical Methods in Natural</cell></row><row><cell cols="4">Language Processing (EMNLP), pages 509-521, 2020.</cell><cell></cell><cell></cell></row></table><note>M. Zhu and S. Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. ICLR, 2018.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters used with Prune OFA</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Warmup ratio</cell><cell>0.01</cell></row><row><cell>Batch size</cell><cell>256</cell></row><row><cell>Weight decay</cell><cell>0.01</cell></row><row><cell>Max steps</cell><cell>100k</cell></row><row><cell cols="2">Learning rate decay Linear + LRR</cell></row><row><cell>Sequence length</cell><cell>512</cell></row><row><cell>? P T</cell><cell>0.5</cell></row><row><cell>? kd</cell><cell>0.5</cell></row><row><cell>Temperature</cell><cell>2.0</cell></row><row><cell>Pruning start</cell><cell>0</cell></row><row><cell>Pruning policy end</cell><cell>50k</cell></row><row><cell>Pruning end</cell><cell>80k</cell></row><row><cell>Pruning interval</cell><cell>1k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">: Hyper-parameters used for transfer learning</cell></row><row><cell>Hyper-parameter</cell><cell>SQuAD</cell><cell>GLUE</cell></row><row><cell>Learning rate</cell><cell cols="2">{1.5e-4, 1.8e-4} {1e-4, 1.2e-4, 1.5e-5}</cell></row><row><cell>Batch size</cell><cell>12</cell><cell>32</cell></row><row><cell>Weight decay</cell><cell>{0, 0.01}</cell><cell></cell></row><row><cell>Epochs</cell><cell>8</cell><cell>{3, 6, 9}</cell></row><row><cell>Learning rate decay</cell><cell>Linear</cell><cell></cell></row><row><cell>Warmup ratio</cell><cell cols="2">{0, 0.01, 0.1}</cell></row><row><cell>Sequence length</cell><cell>384</cell><cell>128</cell></row><row><cell>? P T</cell><cell>0.0</cell><cell></cell></row><row><cell>? kd</cell><cell>1.0</cell><cell></cell></row><row><cell>Temperature</cell><cell>2.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>) with learning rates {1.5e-4, 1e-4, 1.5e-4} for {BERT-Base, BERT-Large, DistilBERT}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters used for quantization-aware training</figDesc><table><row><cell>Hyper-parameter</cell><cell>SQuAD</cell><cell>GLUE</cell></row><row><cell>Learning rate</cell><cell cols="2">{1e-6, 1e-5} {5e-8, 1e-7, 1e-6, 1e-5}</cell></row><row><cell>Batch size</cell><cell>12</cell><cell>32</cell></row><row><cell>Weight decay</cell><cell>{0, 0.01}</cell><cell></cell></row><row><cell>Epochs</cell><cell>2</cell><cell>3</cell></row><row><cell>Learning rate decay</cell><cell>Linear</cell><cell></cell></row><row><cell>Warmup ratio</cell><cell cols="2">{0, 0.01, 0.1}</cell></row><row><cell>Sequence length</cell><cell>384</cell><cell>128</cell></row><row><cell>? P T</cell><cell>0.0</cell><cell></cell></row><row><cell>? kd</cell><cell>1.0</cell><cell></cell></row><row><cell>Temperature</cell><cell>2.0</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/neuralmagic/sparseml/tree/main/integrations/ huggingface-transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, the pre-training task for BERT-Base is masked language-modeling combined with next sentence prediction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Results taken from Neural Magic's sparse model zoo: https://sparsezoo.neuralmagic.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/IntelLabs/Model-Compression-Research-Package</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis for pre-trained bert networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15834" to="15846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compressing bert: Studying the effects of weight pruning on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network" />
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.02531" />
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs. data. quora. com</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Csernai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<title level="m">Integer-only bert quantization. ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fastformers: Highly efficient transformer models for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</title>
		<meeting>SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Block pruning for faster transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lagunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charlaix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Del Moral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lesage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matussi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bekman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5148649</idno>
		<idno>huggingface/datasets: 1.11.0</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5148649" />
		<imprint>
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Latorre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08378</idno>
		<title level="m">Accelerating sparse deep neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Comparing rewinding and fine-tuning in neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Turing-nlg: A 17-billion-parameter language model by microsoft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Q-bert: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8815" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobilebert: a compact task-agnostic bert for resource-limited devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2018</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
