<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Fitzgerald</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hench</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charith</forename><surname>Peris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mackie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Rottmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Sanchez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Nash</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Urbach</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishesh</forename><surname>Kakarala</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swetha</forename><surname>Ranganath</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Crist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Britan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Leeuwis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
						</author>
						<title level="a" type="main">MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the MASSIVE dataset-Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly. . 2021. Parsinlu: A suite of language understanding challenges for persian. Diederik P. Kingma and Jimmy Ba. 2017. Adam: A method for stochastic optimization. P. J. Price. 1990. Evaluation of spoken language systems: the ATIS domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Description</head><p>Natural Language Understanding (NLU) is a machine's ability to understand the meaning and relevant entities from text. For instance, given the utterance what is the temperature in new york, an NLU model might classify the intent as weather_query and fill the slots as weather_descriptor: temperature and place_name: new york. Our particular focus of NLU is one component of Spoken Language Understanding (SLU), in which raw audio is first converted to text before NLU is performed <ref type="bibr">(Young, 2002;</ref><ref type="bibr">Wang et al., 2005;</ref><ref type="bibr">Tur and Mori, 2011)</ref>. SLU is the foundation of voice-based virtual assistants like Alexa, Siri, and Google Assistant. Though virtual assistants have advanced incredibly in the past decade, they still only support a small fraction of the world's 7,000+ languages <ref type="bibr">(Simons, 2022)</ref>. Challenges *Corresponding author, jgmf@amazon.com. All authors were associated with Amazon at the time of publication. for multilingualism span the software stack and a variety of operational considerations, but one difficulty in creating massively multilingual NLU models is the lack of labeled data for training and evaluation, particularly data that is realistic for the task and that is natural for each given language. High naturalness typically requires human-based vetting, which is often costly.</p><p>We present MASSIVE (Multilingual Amazon SLU Resource Package (SLURP) for Slot filling, Intent classification, and Virtual assistant Evaluation), a new 1M-example dataset composed of realistic, human-created virtual assistant utterance text spanning 51 languages, 60 intents, 55 slot types, and 18 domains. With the English seed data included, there are 587k train utterances, 104k dev utterances, 152k test utterances, and 153k utterances currently held out for the MMNLU-22 competition, which will be released after the competition. We have released our data, code, and models 1 . MASSIVE was created by localizing the SLURP NLU dataset (created only in English) in a parallel manner. SLURP is described further in Section 2, linguistic analyses of the dataset in Section 3, and the localization process in Section 4.3. Results for Massively Multilingual NLU (MMNLU) modeling, in which a single model can perform NLU on any of the incoming languages, are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prior researchers have emphasized the need to explore the unique challenges of low-resource languages <ref type="bibr">(Simpson et al., 2008;</ref><ref type="bibr">Strassel and Tracey, 2016;</ref><ref type="bibr">Cruz and Cheng, 2020;</ref><ref type="bibr" target="#b2">Lakew et al., 2020;</ref><ref type="bibr">Marivate et al., 2020;</ref><ref type="bibr">Magueresse et al., 2020;</ref><ref type="bibr">Goyal et al., 2021)</ref>, while the growing number and size of language models (mBERT <ref type="bibr">(Devlin, 2018)</ref>, <ref type="bibr">RoBERTa (Liu et al., 2019b)</ref>, XLM <ref type="bibr" target="#b3">(Lample and Conneau, 2019)</ref>, <ref type="bibr">XLM-R (Conneau et al., 2020)</ref>, mBART <ref type="bibr">(Liu et al., 2020)</ref>, MARGE <ref type="bibr" target="#b4">(Lewis et al., 2020)</ref>, and mT5 <ref type="bibr">(Xue et al., 2021)</ref> pre-trained on massively multilingual corpora have allowed for significant improvements in supporting them. However, the creation of evaluation datasets for specific tasks has not kept pace. Some tasks, such as Named Entity Recognition (NER) or translation, lend themselves to mining existing corpora <ref type="bibr">(Tiedemann, 2012;</ref><ref type="bibr">Pan et al., 2017;</ref><ref type="bibr">Hu et al., 2020)</ref>, while others such as NLU, the focus here, require the creation of new data and schema-specific annotations. Beyond the cost, even identifying a sufficient number of speakers for data generation and quality control can be difficult. Most studies have thus focused on collecting data for one such low-resource language and determining the utility of multilingual models or cross-lingual learning from more readily available languages. Moreover, such datasets are often isolated collections, creating an environment of multiple datasets not easily comparable across the different languages or tasks. There have been exceptions, such as SQuAD <ref type="bibr">(Rajpurkar et al., 2016)</ref> and XQuAd <ref type="bibr" target="#b0">(Artetxe et al., 2019)</ref>, ATIS <ref type="bibr">(Price, 1990)</ref>, its Hindi and Turkish extension <ref type="bibr">(Upadhyay et al., 2018)</ref>, and MultiATIS++ <ref type="bibr">(Xu et al., 2020)</ref>, and Snips <ref type="bibr">(Coucke et al., 2018)</ref> with its addition of French <ref type="bibr">(Saade et al., 2019)</ref>, where researchers have extended popular English benchmark datasets to new languages. This work focuses on the general multi-domain NLU task and builds off the SLURP <ref type="bibr">(Bastianelli et al., 2020)</ref> benchmark dataset to extend to an unprecedented 50 new languages.</p><p>For the task of NLU, the ATIS dataset has been popular in the NLP community since its first release. MultiATIS++ was one of the first efforts to extend an NLU dataset across a significant number of languages (nine), yet remained in the limited domain of airline bookings. While proving an asset, it has been questioned what is left to learn from such a dataset <ref type="bibr">(Tur et al., 2010)</ref>. Facebook released a general Intelligent Virtual Assistant (IVA) dataset across the domains of <ref type="bibr">Alarm, Reminder, and Weather (Schuster et al., 2019)</ref> created for the purpose of demonstrating cross-lingual transfer learning; and so did not need to be parallel or have an equal number of datapoints, resulting in far fewer examples in Thai (5k) compared to Spanish (7.6k) and English (43k). The Snips datasets (both the original English only and the English and French releases) are most similar to the NLU contained in the MASSIVE dataset, spanning smart home and music domains for a generic voice-based virtual assistant.</p><p>The first iteration for the foundation of the MAS-SIVE dataset was the NLU Evaluation Benchmarking Dataset, with 25k utterances across 18 domains <ref type="bibr">(Liu et al., 2019a)</ref>. The authors updated the dataset and added audio and ASR transcriptions in the release of the Spoken Language Understanding Resource Package (SLURP) <ref type="bibr">(Bastianelli et al., 2020)</ref>, allowing for full end-to-end Spoken Language Understanding (SLU) evaluation similar to the Fluent Speech Commands dataset <ref type="bibr">(Lugosch et al., 2019)</ref> and Chinese Audio-Textual Spoken Language Understanding (CATSLU) <ref type="bibr">(Zhu et al., 2019</ref>). An overview of selected existing NLU datasets can be seen in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We release the MASSIVE dataset along with baselines from large pre-trained models fine-tuned on the NLU slot and intent prediction tasks. Early cross-lingual and multilingual NLU modeling approaches used projection or alignment methods <ref type="bibr">(Yarowsky et al., 2001)</ref>, focusing on string matching, edit distance, or consonant signatures <ref type="bibr">(Ehrmann et al., 2011)</ref>, lookup lexicons for lowresource languages <ref type="bibr">(Mayhew et al., 2017)</ref>, and aligning <ref type="bibr">(Xie et al., 2018)</ref> or jointly training word embeddings <ref type="bibr">(Singla et al., 2018)</ref>. More recently, researchers have borrowed encoders from pre-trained neural translation models before building subsequent classifiers and NER models <ref type="bibr">(Eriguchi et al., 2018;</ref><ref type="bibr">Schuster et al., 2019)</ref>, also focusing on language-agnostic and language specific features to learn what information to share between languages <ref type="bibr">(Chen et al., 2019b)</ref>. Generative parsing has been demonstrated using sequence-to-sequence models and pointer networks <ref type="bibr">(Rongali et al., 2020)</ref>. With the rise of BERT and large pre-trained language models, we have also seen impressive demonstrations of zero-shot performance, where subword tokenization WordPiece overlap helps but is not even necessary to realize improvements <ref type="bibr">(Pires et al., 2019;</ref><ref type="bibr">K et al., 2020)</ref>, as well as production multilingual NLU improvements with distillation and full fine-tuning <ref type="bibr">(FitzGerald et al., 2022)</ref>. The translation task has then been incoporated in the pretraining (Wang et al., 2021) of these models or even as part of the final NLU hypothesis for streamlined multilingual production systems (FitzGerald,  3 Language Selection and Linguistic Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Selection</head><p>The languages in MASSIVE were chosen according to the following considerations. First, we acquired cost and worker availability estimates for over 100 languages, providing a constraint to our choices given our fixed budget. Second, we determined existing languages available in major virtual assistants, such that the dataset could be used to benchmark today's systems. Third, we categorized the full pool of languages according to their genera as taken from the World Atlas of Linguistic Structures (WALS) database <ref type="bibr">(Dryer and Haspelmath, 2013)</ref>, where a genus is a language group that is clear to most linguists without systematic comparative analysis. Genus is a better indicator of typological diversity, which we sought to maximize, than language family <ref type="bibr">(Dryer, 1989)</ref>. Fourth, we used the eigenvector centrality of Wikipedia articles, tweets, and book translations <ref type="bibr">(Ronen et al., 2014)</ref> as proxies for the internet influence and thus the resource availability of a given language, particularly for self-supervised pretraining applications, and we chose languages spanning the breadth of resource availability. Fifth, we examined the script of each language, seeking to increase script diversity to drive experimentation in tokenization and normalization. Ultimately, we created 50 new, distinct text corpora, representing 49 different spoken languages. Mandarin Chinese was collected twice, once with native speakers who use the traditional set of characters, and once with native speakers who use the modern simplified set of characters. There are 14 language families in the dataset. The term "language family" usually refers to a group of languages which are known to be genetically related, that is, they all descend from a common ancestor language. In MASSIVE, we also include "language isolates" as families. These are languages that have no clear relationship to any known language. Our choices are given in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scripts</head><p>There are 21 distinct scripts used in the dataset. The majority of languages in MASSIVE (28 including English) use some variety of the Latin alphabet, which is also the most widely used script in the world. The Arabic script is used for three languages, the Cyrillic script for two languages, and the remaining 18 languages have "unique" scripts, in the sense that only one language in the dataset uses that script. Fourteen scripts are unique to a single language, although they may belong to a larger family of writing systems. For example, the Dravidian languages in <ref type="bibr">MASSIVE</ref>   class of scripts. The other two scripts are unique in that only one language in the dataset uses them, but they are more widely used in the real world: Ge'ez and Chinese. Ge'ez is represented by Amharic in the dataset, but is used for several languages in East Africa, such as Tigrinya. The Chinese script is represented by Mandarin, but is used by other languages in China such as Cantonese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Types</head><p>MASSIVE consists of utterances directed at a device, rather than a person, which has some consequences for the type of linguistic patterns it contains. Specifically, the corpus primarily consists of interrogatives (i.e., questions) and imperatives (commands or requests). There are relatively few declarative utterances in the set. This is in contrast to many large datasets from other sources (e.g., wikipedia, movie scripts, newspapers) which contain a high proportion of declaratives, since the language is collected from situations where humans are communicating with humans.</p><p>In the context of a voice assistant, a user typically asks a device to perform an action or answer a question, so declaratives are less common. For instance, a person might use an imperative "tell me if it calls for rain today" or ask a question "will it rain today," but they would not tell their device "it's raining today." When declaratives are used with voice assistants, they generally have the pragmatic effect of a directive. For instance, a virtual assistant can respond to the declarative "it's cold in here" by turning up the temperature <ref type="bibr">(Thattai et al., 2020)</ref>. Although syntactically it looks like a declarative, such an utterance has the force of an imperative.</p><p>The standard unit of analysis in linguistics is the declarative sentence, and there is relatively less known about imperatives and questions. MASSIVE presents an opportunity to study these sentence forms, and the parallel nature of the corpus makes cross-linguistic comparisons even easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Word Order</head><p>Languages have intricate rules for ordering words depending on the word-type and sentence-type. In English, the word order for statements ("you are leaving") is different from questions ("are you leaving?"). This is not mandatory, and sometimes the pitch of the voice is enough to indicate a question (e.g. "you're leaving?" with a rising intonation). When considering word order at a typological level, it is common to simplify the situation and consider only affirmative declarative sentences and only three grammatical elements: the verb (V), its subject (S), and its object (O). This makes for six possible word orders: SVO, SOV, VOS, VSO, OVS, and OSV. All six orders have been documented, although the overwhelming majority of languages use Subject-initial ordering, while Object-initial ordering is extremely rare.</p><p>In MASSIVE, 39 languages are subject-initial (24 SVO and 15 SOV), while only three are verb-initial (VSO specifically). No object-initial languages are represented. Five languages are marked in WALS as having no preferred word order, and four do not have any word order data at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Imperative Marking</head><p>The languages in MASSIVE have a variety of ways of indicating the imperative mood of an utterance. The majority of them (33) use some kind of verb morphology, such as adding a suffix. About half of those languages (18) have distinct imperative marking for singular or plural addressees. The utterances in MASSIVE are technically directed at a single addressee, the voice assistant, but since some languages use the plural as an indicator of politeness (see below) all varieties of imperatives will likely occur in this dataset. There are ten languages without any special morphology, and they indicate imperative through other means, such as word order or vocabulary choice.</p><p>Ten languages in the dataset have a specialized distinction between imperatives, for commands directed at another individual, and "hortatives", where the command also includes the speaker. English verbs are not directly marked for hortative, but the auxiliary verb "let" can convey the mood instead. For example, "write this down" is an imperative and only the addressee need write anything, while "let's write this down" is a hortative and the speaker is also expected to write. The pervasiveness of hortatives in the context of a voice assistant is an open question.</p><p>Four languages have "optative" moods, which are subtly different from imperatives. In the optative, a speaker expresses a wish or desire, as opposed to giving a direct command. However, in the right context, an optative may carry the same pragmatic weight as an imperative, and strongly imply that someone ought to do something. English has no specific optative form, but a similar mood can be conveyed using conditionals. For example, "buy this bag for me" is an imperative while "if only someone would buy me this bag" is closer to an optative. Optative forms are not well studied in linguistics, as they require specific contexts which can be difficult to create during field work, but they may be more common in device-directed utterances.</p><p>Lastly, some languages distinguish between imperatives, when telling someone to do something, and "prohibitives", when telling someone not to do something. In the MASSIVE set, there are 18 languages with specialized negative particles which can only co-occur with imperative verbs. Vietnamese for instance uses the words "ch?ng" or "kh?ng" to negate declarative sentences, but uses "ch?" or "dung" to negate imperatives. Another ten languages have special verbs for the prohibitive, although these may overlap with other grammatical features of the language. In Spanish, for example, the prohibitive form of a verb is the same as the subjunctive form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Politeness</head><p>Many languages encode different levels of politeness through their use of pronouns. Many European languages distinguish between "familiar" and "formal" pronouns, with the "formal" pronouns often morphologically identical to a plural. In French, the second-person singular "tu" is used between friends, while the second-person plural "vous" is used when speaking to a group, or to an individual of higher social rank (such as an employee to a manager). These politeness systems are heavily influenced by social context, and the MASSIVE dataset gives us a chance to see how people adapt their language when speaking to a virtual assistant instead of another human.</p><p>Nearly half of the languages in MASSIVE (21) make a two-way formal/informal distinction in their second-person pronouns. This is probably due to the fact that most MASSIVE languages are European, and the binary politeness distinctions are the most common strategy in that family. A further eight languages have more than two levels of formality, such as informal, formal, and honorific. Seven languages have an "avoidance" strategy, which means that pronouns are omitted entirely in a polite scenario. Finally, eleven languages have no data on politeness in WALS at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Collection Setup and Execution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Heldout Evaluation Split</head><p>We randomly sampled a subset of the English seed data which was then paraphrased by professional annotators, resulting in new, more challenging utterances, including 49% more slots per utterance. These utterances were localized along with the other splits to be used as a held out evaluation set for the Massively Multilingual NLU-22 competition and workshop 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Vendor Selection and Onboarding</head><p>The MASSIVE dataset was collected using a customized workflow powered by Amazon MTurk. We required a vendor pool with the capability and resources to collect a large multilingual dataset. Our original vendor pool consisted of five vendors adjudicated based on previous engagements. This vendor pool was reduced to three based on engagement and resource availability. Vendors for each language were selected based on their resource availability and proposed cost. A majority of languages were supported by a single vendor, while some languages required cross-vendor support to be completed with the required quality and within the required timeline.</p><p>We offered two mechanisms to vendors for evaluating workers to be selected for each language. The first, which was used to select workers for the translation task, was an Amazon MTurk-hosted fluency test where workers listen to questions and statements in the relevant language and were evaluated using a multiple-choice questionnaire. The second, which was used to select workers for the judgment task, was a test with a set of three judgments that the vendor could use to assess if workers were able to detect issues in the translated utterances. In order to further improve worker selection quality, we created a translator quiz using the Amazon MTurk instructions that were created for translation and judgment tasks, coupled with customized locallanguage examples. The workers were required to prove that they understood the instructions for the project based on a series of questions.</p><p>Before commencing operations, an initial pilot run of this customized workflow was completed in three languages. A few workers per vendor were chosen to engage in this exercise. The pilot run helped improve clarity of instructions, determine reporting methods, and share open questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Collection Workflows</head><p>The collection was conducted by locale on an individual utterance level. Each utterance from the "train," "dev," "test," and "heldout" splits of the SLURP dataset went through two sequential task workflows and a judgment workflow. The first task is slot translation or localization (see <ref type="figure" target="#fig_0">Figure 1</ref>). Workers are presented the entire utterance with colored highlighting of the slot values for the utterance (if any) and then presented with each slot value and its corresponding label individually. The worker is asked to either localize or translate the slot, depending on whether the value should be translated (e.g., "tomorrow") or localized (e.g., the movie "La La Land", which in French is "Pour l'amour d'Hollywood." Other entities like regionally known songs or artists could also be localized to a more relevant, known song or artist for that language or region). There is also an option to keep the slot as is, such as for names (e.g., "Taylor Swift") or proper nouns where the original English spelling should be retained. The metadata of the released dataset includes whether the worker elected to "localize," "translate," or keep the slot "unchanged," primarily for the purposes of researchers evaluating machine translation systems, where it would be unreasonable to expect the system to "localize" to a specific song name the worker selected.</p><p>After the slot task, the second worker is asked to translate or localize the entire phrase using the slot task output provided by the first worker (see <ref type="figure" target="#fig_1">Figure 2</ref>). The phrase worker can decide to keep the slot as it was translated, modify it, or remove it entirely if it is not relevant for the language in that scenario. This worker is also responsible for aligning grammatical genders or prepositional affixes to any of the slots.</p><p>Note that this two-step system alleviates the annotation burden often encountered with such work. Traditionally in such collections, workers would be given a light annotation guide and asked to highlight spans of the slots in a translated or localized utterance. In this system, the first step of slot translation and subsequent insertion obviates the need for workers to understand nuanced span notation, which can be complex for highly inflected languages (prepositions outside the span in English would not be carried over in the localization, but would be in the traditional span annotation workflow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quality Assurance</head><p>The output of the second workflow (the fully localized utterance) is judged by three workers for (1) whether the utterance matches the intent semantically, (2) whether the slots match their labels semantically, (3) grammaticality and naturalness, (4) spelling, and (5) language identification-English or mixed utterances are acceptable if that is natural for the language, but localizations without any tokens in the target language were not accepted. See <ref type="figure" target="#fig_2">Figure 3</ref> for how this is presented to the Amazon MTurk worker. These judgments are also included in the metadata of the dataset. In addition to the workers judging each other's work, the collection system had alarms in place for workers with high rejection rates, high rates of slot deletion, and high rates of English tokens in the translations. Workers were also monitored to see if their tasks were primarily machine translated. Such workers were removed from the pool and all of their work was resubmitted to be completed by the other workers.</p><p>Additionally, the authors performed several deep dives into languages with which they were familiar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Benchmarking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>As initial model benchmarks, we fine-tuned publicly-available pre-trained language models on the MASSIVE dataset and evaluated them on intent classification and slot filling. Our models of choice for this exercise were XLM-Roberta (XLM-R; Conneau et al. 2020) and mT5 <ref type="bibr">(Xue et al., 2021)</ref>.</p><p>In the case of XLM-R, we utilized the pretrained encoder with two separate classification heads trained from scratch, based on JointBERT (Chen et al., 2019a). The first classification head used the pooled output from the encoder to predict the intent and the second used the sequence output to predict the slots. As pooling for the intent classification head, we experimented with using hidden states from the first position, averaged hidden states across the sequence, and the maximally large hidden state from the sequence.</p><p>With mT5, we explored two separate architectures. In one architecture, we only used the pre-trained encoder extracted from mT5, and we trained two classification heads from scratch similarly to the XLM-R setup. We refer to this setup as mT5 Encoder-Only. In the other architecture, we used the full sequence-to-sequence mT5 model in text-to-text mode, where the input is "Annotate:" followed by the unlabeled utterance. The decoder output is a sequence of labels (including the Other label) for all of the tokens followed by the intent. We did not add the slots and intents to the vocabulary, but we instead allowed them to be tokenized into subwords. We refer to this model as mT5 Text-to-Text. For all models, we used the Base size, which corresponds to 270M parameters for XLM-R, 258M parameters for mT5 Encoder-Only, and 580M parameters for mT5 Text-to-Text, including 192M parameters for embeddings for all three.</p><p>For each model, we performed 128 trials of hyperparameter tuning using the Tree of Parzen Estimators algorithm and Asynchronous Successive Halving Algorithm (ASHA) <ref type="bibr">(Li et al., 2018a)</ref> for scheduling, which are both part of the hyperopt library (Bergstra et al., 2013) integrated into the ray[tune] library <ref type="bibr">(Liaw et al., 2018)</ref>, which is itself integrated into the Trainer from the transformers library (Wolf et al., 2020), which we used for modeling and for our pretrained models. Our hyperparameter search spaces, sampling types, and final choices are given in <ref type="table" target="#tab_11">Table 5</ref>. We trained our models with the Adam optimizer (Kingma and Ba, 2017) and chose the best performing model checkpoint based on overall exact match accuracy across all locales. Hyperparameter tuning and fine-tuning was performed using single p3dn.24xlarge instances (8 x Nvidia v100) for XLM-R and mT5 Text-to-Text and a single g4dn.metal instance (8 x Nvidia T4) for mT5 Encoder-Only. Hyperparameter tuning times were less than 4 days per model and training times were less than 1 day per model.</p><p>Our dataset includes several languages where white spacing is not used as a word delimiter. In some cases, spaces do occur, but they might serve as phrase delimiters or denote the end of a sentence. Three of these written languages, Japanese, Chinese (Traditional), and Chinese (Simplified), do not use spaces anywhere except to identify the end of a sentence. For these languages, we separate each character in the unlabeled input with a whitespace. We leave exploration of more sophisticated techniques (such as MeCab for Japanese <ref type="bibr">;</ref><ref type="bibr" target="#b1">Kudo 2005)</ref> to future work. We use the default spacing provided by annotators for all other languages.</p><p>Zero-shot performance was also assessed, in which the models were trained on English data, validation was performed on all languages, and testing was performed on all non-English locales. <ref type="table" target="#tab_5">Table 3</ref> shows the results for each model and training setup, including those for the best performing locale, the worst performing locale, and locale-averaged results for intent accuracy, microaveraged slot F1 score, and exact match accuracy. Zero-shot exact match performance is 25-37 points worse than that of full-dataset training runs. Additionally, the variance in task performance across locales is significantly greater for the zero-shot setup than for full-dataset training. For example, there is a 15 point difference in exact match accuracy between the highest and lowest locales for mT5 Text-to-Text when using the full training set, while the gap expands to 44 points with zero-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analysis</head><p>We compared the pretraining data quantities by language for XLM-R to its per-language task performance values, and in the zero shot setup, we found a Pearson correlation of 0.54 for exact match  accuracy, 0.58 for intent accuracy, and 0.46 for micro-averaged slot F1 score. In the full dataset training setup, the correlations decrease to 0.42 for exact match accuracy, 0.47 for intent accuracy, and 0.24 for micro-averaged slot F1 score. This suggests that the constant per-language data quantities in MASSIVE help to mitigate the effects of the language-skewed pretraining data distribution.</p><p>In Thai, for which spacing is optional, the model can learn from artificial spacing in the input (around where the slots will be) to improve task performance. For Khmer, the workers had a difficult time adapting their translations and localizations to properly-slotted outputs given the space-optional nature of the language. Additionally, for Japanese and Chinese, we added spaces between all characters when modeling. These single-character inputs differ from the non-spaced inputs used during pretraining, which would be chunked into groups of characters by the tokenizer with corresponding embeddings. By splitting into single characters, we don't allow the model to the use embeddings learned for chunks of characters. This is a likely major cause of the drop in exact match accuracy for Japanese from 58.3% when training on the full dataset to 9.4% for zero shot. In the zero shot setup, the model relies solely on pretrained data representations, and individually-spaced characters are rare in the pretraining data. That said, character spacing was necessary in order to properly assign the slots to the right characters. As mentioned in Section 5.1, we leave exploration of more sophisticated spacing techniques for slot filling (such as MeCab; <ref type="bibr" target="#b1">Kudo 2005)</ref> to future work.</p><p>Discounting for artificial spacing effects, Germanic genera and Latin scripts performed the best overall (See Appendix E), which is unsurprising given the amount of pretraining data for those genera and scripts, as well as the quantity of Germanic and Latin-script languages in MASSIVE. Within the Germanic genera, Swedish, English, Danish, Norwegian, and Dutch all performed comparably (within 95% confidence bounds) for exact match accuracy. Icelandic was the lowest-performing Germanic language, likely due to a lack of pretraining data, as well as to its linguistic evolution away from the others due to isolated conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have released a truly MASSIVE multilingual dataset for NLU spanning 51 typologically diverse languages. Our hope is that MASSIVE will encourage many new innovations in massively multilingual NLU, other NLP tasks such as machine translation, and new linguistic analyses, such as with imperative morphologies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Linguistic Characteristics</head><p>Additional linguistic characteristics of our languages are given in <ref type="table" target="#tab_8">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The Collection System</head><p>Screenshots from our collection workflow are given in <ref type="figure" target="#fig_0">Figures 1, 2, and 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters</head><p>The hyperparameter search spaces and the chosen hyperparameters are given in Tables 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Results for All Languages</head><p>Results for all languages are given for exact match accuracy in <ref type="table">Table 7</ref>, intent accuracy in <ref type="table">Table 8</ref>, and micro-averaged slot-filling F1 in <ref type="table">Table 9</ref>.</p><p>E A summary of model performance on language characteristics</p><p>We pick our best performing model, mT5 Text-to-Text, and provide a summary of its performance on different language characteristics in Figures 4 and 5.       <ref type="bibr">[7, 8, 9, 10, 11] [7, 8, 9, 10, 11]</ref>    <ref type="bibr">[7, 8, 9, 10, 11] [7, 8, 9, 10, 11]</ref>   <ref type="table">Table 6</ref>: The zero-shot hyperparameter search space, the sampling technique, and the chosen hyperparameter for our 3 models. The search space for the "quniform" and "qloguniform" sampling techniques is given as [min, max, increment].</p><p>Figure 5: mT5 Text-to-Text performance grouped by Script, Family, Order, Politeness, Imperative Morphology, Imperative Hortative, Optative and Prohibitive. As with <ref type="figure">Figure 4</ref>, the categories of each language characteristic are sorted by exact match accuracy for readability. The number of languages falling into each category is provided in the bar chart in the lowest panel for each characteristic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Slot localization task as presented to Amazon MTurk worker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Phrase localization task as presented to Amazon MTurk worker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Judgment task as presented to Amazon MTurk worker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Selected NLU benchmark datasets with number of languages, utterances per language, domain count, intent count, and slot count.</figDesc><table /><note>2020). Researchers have propped up training data by translating and projecting labels into the target language (Xu et al., 2020) and discovered more so- phisticated approaches to alignment such as trans- late and fill using mT5 to train the filler (Nicosia et al., 2021). Recent work has even delved into the application of these techniques to lower-resource languages such as Persian. For example, ParsiNLU explores a variety of NLU tasks for Parsi, fine- tuning mT5 of various sizes (Khashabi et al., 2021). Similarly these techniques have also been used, even a bit earlier, for text summarization (Farahani et al., 2021).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The 51 languages of MASSIVE, including scripts and genera.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>? 1.2 79.0 ? 1.5 85.3 ? 0.2 86.8 ? 0.7 67.6 ? 0.4 76.8 ? 0.1 73.4 ? 1.6 58.3 ? 1.8 66.6 ? 0.2 ? 1.1 79.1 ? 1.5 86.1 ? 0.2 85.7 ? 0.7 64.5 ? 0.4 75.4 ? 0.1 72.3 ? 1.6 57.8 ? 1.8 65.9 ? 0.2</figDesc><table><row><cell>Model</cell><cell></cell><cell>Intent Acc (%)</cell><cell></cell><cell></cell><cell>Slot F1 (%)</cell><cell></cell><cell cols="2">Exact Match Acc (%)</cell></row><row><cell></cell><cell>High</cell><cell>Low</cell><cell>Avg</cell><cell>High</cell><cell>Low</cell><cell>Avg</cell><cell>High</cell><cell>Low</cell><cell>Avg</cell></row><row><cell cols="2">mT5 Base 87.9 Text-to-Text en-US</cell><cell>km-KH</cell><cell></cell><cell>th-TH</cell><cell>ja-JP</cell><cell></cell><cell>th-TH</cell><cell>ja-JP</cell></row><row><cell cols="2">mT5 Base 89.0 Encoder-Only en-US</cell><cell>km-KH</cell><cell></cell><cell>th-TH</cell><cell>ja-JP</cell><cell></cell><cell>th-TH</cell><cell>ja-JP</cell></row><row><cell cols="10">XLM-R Base 88.3 ? 1.2 77.2 ? 1.5 85.1 ? 0.2 83.5 ? 0.7 63.3 ? 0.4 73.6 ? 0.1 70.1 ? 1.6 55.8 ? 1.8 63.7 ? 0.2</cell></row><row><cell></cell><cell>en-US</cell><cell>km-KH</cell><cell></cell><cell>th-TH</cell><cell>ja-JP</cell><cell></cell><cell>th-TH</cell><cell>ja-JP</cell></row><row><cell></cell><cell></cell><cell cols="5">(a) Test results when using the full training set</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>Intent Acc (%)</cell><cell></cell><cell></cell><cell>Slot F1 (%)</cell><cell></cell><cell cols="3">Exact Match Acc (%)</cell></row><row><cell></cell><cell>High</cell><cell>Low</cell><cell>Avg</cell><cell>High</cell><cell>Low</cell><cell>Avg</cell><cell>High</cell><cell>Low</cell><cell>Avg</cell></row><row><cell>mT5 Base</cell><cell cols="9">79.9 ? 1.4 25.7 ? 1.6 62.9 ? 0.2 64.3 ? 0.7 13.9 ? 0.3 44.8 ? 0.1 53.2 ? 1.8 9.4 ? 1.0 34.7 ? 0.2</cell></row><row><cell>Text-to-Text</cell><cell>nl-NL</cell><cell>ja-JP</cell><cell></cell><cell>de-DE</cell><cell>ja-JP</cell><cell></cell><cell>sv-SE</cell><cell>ja-JP</cell></row><row><cell>mT5 Base</cell><cell cols="9">76.4 ? 1.5 27.1 ? 1.6 61.2 ? 0.2 59.5 ? 1.0 6.3 ? 0.2 41.6 ? 0.1 44.3 ? 1.8 4.2 ? 0.7 28.8 ? 0.2</cell></row><row><cell>Encoder-Only</cell><cell>nl-NL</cell><cell>ja-JP</cell><cell></cell><cell>th-TH</cell><cell>ja-JP</cell><cell></cell><cell>sv-SE</cell><cell>ja-JP</cell></row><row><cell cols="10">XLM-R Base 85.2 ? 1.3 44.8 ? 1.8 70.6 ? 0.2 68.4 ? 0.7 15.4 ? 0.3 50.3 ? 0.1 57.9 ? 1.8 9.8 ? 1.1 38.7 ? 0.2</cell></row><row><cell></cell><cell>sv-SE</cell><cell>ja-JP</cell><cell></cell><cell>sv-SE</cell><cell>ja-JP</cell><cell></cell><cell>sv-SE</cell><cell>ja-JP</cell></row><row><cell></cell><cell></cell><cell cols="5">(b) Zero-shot test results after training only on en-US</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Modeling results for (a) training runs on the full training dataset and (b) zero-shot training runs, in which training was performed only with en-US data, validation was performed with all locales, and testing was performed on all locales except for en-US. Each table includes the highest locale, the lowest locale, and locale-averaged results for intent accuracy, micro-averaged slot F1 score, and exact match accuracy. Intervals for 95% confidence are given assuming normal distributions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Ehrmann, Marco Turchi, and Ralf Steinberger. 2011. Building a multilingual named entityannotated corpus using annotation projection. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages 118-124, Hissar, Bulgaria. Association for Computational Linguistics. Steven Pinker, and C?sar A. Hidalgo. 2014. Links that speak: The global language network and its association with global fame. Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the First International Conference on Human Language Technology Research. Su Zhu, Zijian Zhao, Tiejun Zhao, Chengqing Zong, and Kai Yu. 2019. Catslu: The 1st chinese audiotextual spoken language understanding challenge. In 2019 International Conference on Multimodal Interaction, ICMI '19, pages 521-525, New York, NY, USA. Association for Computing Machinery.</figDesc><table><row><cell>Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojan-ski, and Verena Rieser. 2020. SLURP: A spoken lan-guage understanding resource package. In Proceed-ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7252-7262, Online. Association for Computational Linguistics. James Bergstra, Daniel Yamins, and David Cox. 2013. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision ar-chitectures. In Proceedings of the 30th International Conference on Machine Learning, volume 28 of Pro-ceedings of Machine Learning Research, pages 115-123, Atlanta, Georgia, USA. PMLR. Qian Chen, Zhu Zhuo, and Wen Wang. 2019a. Bert for joint intent classification and slot filling. ArXiv, abs/1902.10909. Xilun Chen, Ahmed Hassan Awadallah, Hany Has-san, Wei Wang, and Claire Cardie. 2019b. Multi-source cross-lingual model transfer: Learning what ing of the Association for Computational Linguis-tics, pages 3098-3112, Florence, Italy. Association for Computational Linguistics. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm?n, Edouard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso-8451, Online. Association for Computational Lin-guistics. Alice Coucke, Alaa Saade, Adrien Ball, Th?odore Bluche, Alexandre Caulier, David Leroy, Cl?ment Doumouro, Thibault Gisselbrecht, Francesco Calt-agirone, Thibaut Lavril, Ma?l Primet, and Joseph Dureau. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. Jan Christian Blaise Cruz and Charibeth Cheng. 2020. Establishing baselines for text classification in low-resource languages. Jacob Devlin. 2018. Multiligual bert. model selection and training. arXiv preprint Association (ELRA). 2018. Tune: A research platform for distributed Portoro?, Slovenia. European Language Resources Moritz, Joseph E Gonzalez, and Ion Stoica. and Evaluation (LREC'16), pages 3273-3280, Richard Liaw, Eric Liang, Robert Nishihara, Philipp International Conference on Language Resources Xiujun Li, Yu Wang, Siqi Sun, Sarah Panda, Jingjing resource languages. In Proceedings of the Tenth logue systems. resources for technology development in low challenge: Building end-to-end task-completion dia-LORELEI language packs: Data, tools, and Liu, and Jianfeng Gao. 2018b. Microsoft dialogue Stephanie Strassel and Jennifer Tracey. 2016. Ekaterina Gonina, Moritz Hardt, Benjamin Recht, and Ameet S. Talwalkar. 2018a. Massively parallel hyperparameter tuning. ArXiv, abs/1810.05934. tational Linguistics. 220, Melbourne, Australia. Association for Compu-Linguistics (Volume 2: Short Papers), pages 214-nual Meeting of the Association for Computational Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, gual representations. In Proceedings of the 56th An-2018. A multi-task approach to learning multilin-Karan Singla, Dogan Can, and Shrikanth Narayanan. ciation for Computational Linguistics, pages 8440-language resources for less-resourced languages, 7. monly taught languages: Lessons learned toward interoperability between people in the creation of creation of basic language resources. Collaboration: man language technology resources for less com-Kathryn Baker, and Boyan Onyshkevych. 2008. Hu-Heather Simpson, Christopher Cieri, Kazuaki Maeda, Dallas, TX, USA. of the World, twenty-fifth edition. SIL International, Gary Simons, editor. 2022. Ethnologue: Languages to share. In Proceedings of the 57th Annual Meet-Linguistics. neapolis, Minnesota. Association for Computational (Long and Short Papers), pages 3795-3805, Min-guistics: Human Language Technologies, Volume 1 Chapter of the Association for Computational Lin-ings of the 2019 Conference of the North American for multilingual task oriented dialog. In Proceed-Mike Lewis. 2019. Cross-lingual transfer learning Sebastian Schuster, Sonal Gupta, Rushin Shah, and the edge. Primet. 2019. Spoken language understanding on Francesco Caltagirone, Thibaut Lavril, and Ma?l Leroy, Cl?ment Doumouro, Thibault Gisselbrecht, Dureau, Adrien Ball, Th?odore Bluche, David Alaa Saade, Alice Coucke, Alexandre Caulier, Joseph Alessandro Vespignani, Proceedings of the National Academy of Sciences, 111(52):E5616-E5622. Subendhu Rongali, Luca Soldaini, Emilio Monti, and Wael Hamza. 2020. Don't parse, generate! a se-2020. mantic parsing. Proceedings of The Web Conference quence to sequence architecture for task-oriented se-tional Linguistics. David Yarowsky, Steve J. Young. 2002. Talking to machines (statistically Shahar Ronen, Bruno Gon?alves, Kevin Z. Hu, pages 483-498, Online. Association for Computa-</cell><cell>slot filling, translation, intent classification, and lan-guage identification: Initial results using mbart on multiatis++. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-ishnan, Marc'Aurelio Ranzato, Francisco Guzman, benchmark for low-resource and multilingual ma-chine translation. Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Ku-task oriented dialog using hierarchical representa-tions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2787-2792, Brussels, Belgium. Association Linting Xue, Noah Constant, Adam Roberts, Mi-sociation for Computational Linguistics. tional Linguistics. Processing (EMNLP), pages 5052-5063, Online. As-5001, Florence, Italy. Association for Computa-ence on Empirical Methods in Natural Language mar, and Mike Lewis. 2018. Semantic parsing for ciation for Computational Linguistics, pages 4996-lingual NLU. In Proceedings of the 2020 Confer-ceedings of the 57th Annual Meeting of the Asso-End-to-end slot alignment and recognition for cross-How multilingual is multilingual BERT? In Pro-Weijia Xu, Batool Haider, and Saab Mansour. 2020. Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. Computational Linguistics. and Angela Fan. 2021. The flores-101 evaluation In ACL. pages 369-379, Brussels, Belgium. Association for lingual name tagging and linking for 282 languages. Empirical Methods in Natural Language Processing, Nothman, Kevin Knight, and Heng Ji. 2017. Cross-sources. In Proceedings of the 2018 Conference on Xiaoman Pan, Boliang Zhang, Jonathan May, Joel lingual named entity recognition with minimal re-guistics: EMNLP 2021, pages 3272-3284, Punta Smith, and Jaime Carbonell. 2018. Neural cross-tational Linguistics. Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A. Cana, Dominican Republic. Association for Compu-ciation for Computational Linguistics. Prakash, Stephen Rawls, Andy Rosenbaum, An-jali Shenoy, Saleh Soltan, Mukund Harakere Srid-har, Liz Tan, Fabian Triefenbach, Pan Wei, Haiyang Yu, Shuai Zheng, Gokhan Tur, and Prem Natarajan. 2022. Alexa teacher model: Pretraining and distill-ing multi-billion-parameter encoders for natural lan-guage understanding systems). In Proceedings of Discovery and Data Mining, KDD. ACM. Jack G. M. FitzGerald. 2020. Stil -simultaneous Massimo Nicosia, Zhongdi Qu, and Yasemin Altun. 2021. Translate &amp; Fill: Improving zero-shot mul-Findings of the Association for Computational Lin-System Demonstrations, pages 38-45, Online. Asso-tilingual semantic parsing with synthetic data. In Empirical Methods in Natural Language Processing: cessing. In Proceedings of the 2020 Conference on Transformers: State-of-the-art natural language pro-the 28th ACM SIGKDD Conference on Knowledge Quentin Lhoest, and Alexander M. Rush. 2020. sociation for Computational Linguistics. Teven Le Scao, Sylvain Gugger, Mariama Drame, ing, pages 2536-2545, Copenhagen, Denmark. As-Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, on Empirical Methods in Natural Language Process-icz, Joe Davison, Sam Shleifer, Patrick von Platen, recognition. In Proceedings of the 2017 Conference ric Cistac, Tim Rault, R?mi Louf, Morgan Funtow-Cheap translation for cross-lingual named entity Chaumond, Clement Delangue, Anthony Moi, Pier-Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Kazawa, and Wolfgang Macherey. 2018. Zero-shot cross-lingual classification using multilingual neural machine translation. Mehrdad Farahani, Mohammad Gharachorloo, and Mo-hammad Manthouri. 2021. Leveraging parsbert and pretrained mt5 for persian abstractive text summa-rization. 2021 26th International Computer Confer-ence, Computer Society of Iran (CSICC). Jack FitzGerald, Shankar Ananthakrishnan, Konstan-tine Arkoudas, Davide Bernardi, Abhishek Bha-gia, Claudio Delli Bovi, Jin Cao, Rakesh Chada, Amit Chauhan, Luoxin Chen, Anurag Dwarakanath, Satyam Dwivedi, Turan Gojayev, Karthik Gopalakr-ishnan, Thomas Gueudre, Dilek Hakkani-Tur, Wael Hamza, Jonathan Hueser, Kevin Martin Jose, Haidar Khan, Beiye Liu, Jianhua Lu, Alessandro Manzotti, Pradeep Natarajan, Karolina Owczarzak, Gokmen Oz, Enrico Palumbo, Charith Peris, Chandana Satya Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized bert pretraining ap-proach. Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Speech Model Pre-Training for End-to-End Spo-ken Language Understanding. In Proc. Interspeech 2019, pages 814-818. Alexandre Magueresse, Vincent Carles, and Evan Heet-derks. 2020. Low-resource languages: A review of Vukosi Marivate, Tshephisho Sefara, Vongani Chabal-ala, Keamogetswe Makhaya, Tumisho Mokgonyane, Rethabile Mokoena, and Abiodun Modupe. 2020. Investigating an approach for low resource lan-guage dataset creation, curation and classification: Language Resources Association (ELRA). ing Magazine, 22:16-31. guages, pages 15-20, Marseille, France. European ken language understanding. IEEE Signal Process-workshop on Resources for African Indigenous Lan-Ye-Yi Wang, Li Deng, and Alex Acero. 2005. Spo-Setswana and sepedi. In Proceedings of the first line. Association for Computational Linguistics. guistics: ACL-IJCNLP 2021, pages 2011-2020, On-Findings of the Association for Computational Lin-learning with unsupervised machine translation. In Hui Jiang. 2021. Exploring cross-lingual transfer Chao Wang, Judith Gaspers, Thi Ngoc Quynh Do, and past work and future challenges. pages 6034-6038. Acoustics, Speech and Signal Processing (ICASSP), ing. In 2018 IEEE International Conference on zero-shot cross-lingual spoken language understand-Hakkani-T?r Dilek, and Larry Heck. 2018. (almost) Shyam Upadhyay, Manaal Faruqui, Gokhan T?r, Vikrant Singh Tomar, and Yoshua Bengio. 2019. tic information from speech. guage understanding: Systems for extracting seman-uation (LREC'12), Istanbul, Turkey. European Lan-guage Resources Association (ELRA). Gokhan Tur, Dilek Hakkani-T?r, and Larry Heck. 2010. What is left to be understood in atis? In 2010 IEEE Spoken Language Technology Workshop, pages 19-24. IEEE. Gokhan Tur and Renato De Mori. 2011. Spoken lan-Maud Akiko Eriguchi, Melvin Johnson, Orhan Firat, Hideto Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey tional Conference on Language Resources and Eval-speaking). In INTERSPEECH.</cell></row><row><cell>arXiv:1807.05118. Govind Thattai, Gokhan Tur, and Prem Natarajan.</cell><cell>for Computational Linguistics. hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya</cell></row><row><cell>2020. New alexa features: Interactive teaching by</cell><cell>Barua, and Colin Raffel. 2021. mT5: A massively</cell></row><row><cell>Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and customers.</cell><cell>multilingual pre-trained text-to-text transformer. In</cell></row><row><cell>Verena Rieser. 2019a. Benchmarking natural lan-</cell><cell>Proceedings of the 2021 Conference of the North</cell></row><row><cell>guage understanding services for building conversa-J?rg Tiedemann. 2012. Parallel data, tools and inter-</cell><cell>American Chapter of the Association for Computa-</cell></row><row><cell>tional agents. faces in opus. In Proceedings of the Eight Interna-</cell><cell>tional Linguistics: Human Language Technologies,</cell></row></table><note>Matthew S. Dryer. 1989. Large linguistic areas and lan- guage sampling. Studies in Language, 13:257-292. Matthew S. Dryer and Martin Haspelmath, editors. 2013. WALS Online. Max Planck Institute for Evo- lutionary Anthropology, Leipzig.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Additional linguistic characteristics of the MASSIVE languages.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>The full-dataset hyperparameter search space, the sampling technique, and the chosen hyperparameter for our 3 models. The search space for the "quniform" and "qloguniform" sampling techniques is given as[min, max,  increment].</figDesc><table><row><cell></cell><cell>XLM-R Base</cell><cell>mT5 Text-to-Text</cell><cell>mT5 Encoder-Only</cell></row><row><cell>Adam ? 1</cell><cell>[0.8, 0.9, 0.99]</cell><cell>[0.8, 0.9, 0.99]</cell><cell>[0.8, 0.9, 0.99]</cell></row><row><cell></cell><cell>choice</cell><cell>choice</cell><cell>choice</cell></row><row><cell></cell><cell>0.99</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell>Adam ? 2</cell><cell>[0.95, 0.99, 0.999, 0.9999]</cell><cell>[0.95, 0.99, 0.999, 0.9999]</cell><cell>[0.95, 0.99, 0.999, 0.9999]</cell></row><row><cell></cell><cell>choice</cell><cell>choice</cell><cell>choice</cell></row><row><cell></cell><cell>0.9999</cell><cell>0.999</cell><cell>0.9999</cell></row><row><cell>Adam</cell><cell>[1e-06, 1e-07, 1e-08, 1e-09]</cell><cell>[1e-06, 1e-07, 1e-08, 1e-09]</cell><cell>[1e-06, 1e-07, 1e-08, 1e-09]</cell></row><row><cell></cell><cell>choice</cell><cell>choice</cell><cell>choice</cell></row><row><cell></cell><cell>1e-09</cell><cell>1e-09</cell><cell>1e-08</cell></row><row><cell>Batch Size</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dropout, Attention</cell><cell>[0.0, 0.5, 0.05]</cell><cell></cell><cell>[0.0, 0.5, 0.05]</cell></row><row><cell></cell><cell>quniform</cell><cell></cell><cell>quniform</cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell>0.4</cell></row><row><cell>Dropout, Feedforward</cell><cell>[0.0, 0.5, 0.05]</cell><cell>[0.0, 0.5, 0.05]</cell><cell>[0.0, 0.5, 0.05]</cell></row><row><cell></cell><cell>quniform</cell><cell>quniform</cell><cell>quniform</cell></row><row><cell></cell><cell>0.25</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Encoder Layer Used</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> mmnlu-22.github.io   </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 9</ref><p>: Micro-averaged slot-filling F1 by language for our three models using the full dataset and the zero-shot setup. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno>abs/1910.11856</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mecab : Yet another part-ofspeech and morphological analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumitsu</forename><surname>Kudo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Low resource neural machine translation: A benchmark for five african languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surafel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Lakew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pre-training via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.257</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2950" to="2962" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
