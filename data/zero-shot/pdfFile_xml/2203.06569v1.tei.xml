<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Ravaut</surname></persName>
							<email>mathieuj001@e.ntu</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Infocomm Research (I2R)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<email>srjoty@ntu.edu.sgnfychen@i2r.a-star.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Salesforce Research Asia</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Infocomm Research (I2R)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through finetuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a secondstage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PE-GASUS, we push ROUGE scores by 5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https://github.com/ ntunlp/SummaReranker.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, sequence-to-sequence neural models have enabled great progress in abstractive summarization <ref type="bibr">(See et al., 2017;</ref><ref type="bibr">Lin et al., 2021)</ref>. In the news domain, they have surpassed the strong LEAD-3 extractive baseline. With the rise of transfer learning since <ref type="bibr">BERT (Devlin et al., 2019)</ref>, leading approaches typically fine-tune a base pretrained model that either follows a general text generation training objective like T5 <ref type="bibr">(Raffel et al., 2019)</ref>, <ref type="bibr">BART (Lewis et al., 2020)</ref>, <ref type="bibr">ERNIE (Zhang et al., 2019b)</ref> and <ref type="bibr">ProphetNet (Qi et al., 2021)</ref>, or an objective specifically tailored for summarization like in PEGASUS <ref type="bibr">(Zhang et al., 2020)</ref>.</p><p>Most of these sequence-to-sequence models are history-based, where an output sequence is represented as a sequence of decisions and the probabil-* Equal contribution.  ity of the sequence is computed as a product of decision probabilities. This is also known as the autoregressive factorization. To transform the sequence of probabilities into summaries, beam search is commonly used. While auto-regressive decoding with beam search is simple and has many advantages, it can be difficult to encode global constraints such as grammaticality, coherence and factual consistency within this framework, properties that are believed to be useful in discriminating among candidate outputs. If the model starts decoding in a bad direction, mistakes might propagate, carry over the mistake of previous tokens to the generation of new ones, and the model has no way to know that it should adjust the decoding. Furthermore, these models are typically trained with teacher forcing <ref type="bibr">(Williams and Zipser, 1989)</ref>, which leads to an inherent discrepancy between training time and inference time known as the exposure bias problem <ref type="bibr">(Bengio et al., 2015;</ref><ref type="bibr">Sun and Li, 2021)</ref>. Decoding methods such as beam search maintain a list of top-k best candidates, and output a single best one. In the case of beam search, candidates are sorted by decreasing log-probability, and the last (k ? 1) hypotheses are discarded. However, these (k ? 1) other hypotheses often contain considerably better sequences in terms of different evaluation measures. This observation holds over other decoding methods: diverse beam search <ref type="bibr">(Vi-jayakumar et al., 2016)</ref>, top-k sampling <ref type="bibr">(Fan et al., 2018)</ref> and top-p sampling <ref type="bibr">(Holtzman et al., 2019)</ref>. In <ref type="table" target="#tab_1">Table 1</ref>, we illustrate this phenomenon with the oracle scores (maximum scores over the pool of candidates) for four popular decoding methods and five metrics on the <ref type="bibr">CNN-DailyMail (Hermann et al., 2015)</ref> dataset with a PEGASUS model. The oracle ROUGE-1 scores are up to 10 points higher (+22.8%) than the top beam baseline. Moreover, oracle gains significantly increase when mixing several generation methods together, reaching an improvement of more than 13 ROUGE-1 points (+30.5%). Such a gap is larger than the progress made by research in the whole field of neural abstractive summarization in the last five years <ref type="bibr">(Nallapati et al., 2016;</ref><ref type="bibr">Dou et al., 2021)</ref>. This suggests that current abstractive models are not exploited to their full capacity, calling for better methods to identify the best summary candidate.</p><p>Given this assessment, we investigate whether it is possible to train a second-stage summarization model which learns to select the best summary among a set of candidates obtained from a base model and with a decoding process, which itself can potentially involve a set of decoding methods (e.g., beam search variants). This way, the model would recover the gap that separates it with the oracle. This raises the question of what makes a summary candidate the optimal one? Admittedly, summarization has been an underconstrained task and its evaluation is complex and remains an active research area <ref type="bibr">(Kryscinski et al., 2019;</ref><ref type="bibr">Fabbri et al., 2021;</ref><ref type="bibr">Koto et al., 2021)</ref>. To build a flexible approach, we use a multi-task learning framework based on a mixture-of-experts architecture in order to optimize jointly over several measures.</p><p>To design a robust re-ranker, we systematically explore the dimensions of summary re-ranking: base model, decoding process, and evaluation measure. Our system, named SummaReranker, is flexible and multi-task: it can be trained with any set of evaluation metrics. It is considerably less computationnally expensive to train than the singlestage summarization models that it is plugged on. We apply our system across three different datasets {CNN-DailyMail, XSum, Reddit TIFU} and two base models {PEGASUS, BART}. Optimizing ROUGE metrics leads to relative performance improvements from 1.31% to 9.34% depending on the dataset. It outperforms recently proposed second-stage summarization approaches <ref type="bibr">RefSum (Liu et al., 2021)</ref> and <ref type="bibr">SimCLS (Liu and Liu, 2021)</ref> and sets a new state-of-the-art on <ref type="bibr">CNN-DailyMail and XSum (Narayan et al., 2018)</ref>. We present extensive quantitative results coupled with a qualitative human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Re-ranking has been adopted in several branches of NLP for long. In syntactic parsing, <ref type="bibr">Collins and Koo (2005)</ref> were the first to employ a re-ranker on the outputs of a base parser, followed by <ref type="bibr">Charniak and Johnson (2005)</ref>, who used a Maximum Entropy re-ranker. Passage re-ranking is used as the first stage of question-answering systems, to retrieve relevant passages where the answer might lay <ref type="bibr">(Kratzwald and Feuerriegel, 2018;</ref><ref type="bibr">Nogueira and Cho, 2019)</ref>. Some recent question-answering models also propose to perform answer re-ranking, to refine the answer selection <ref type="bibr">(Kratzwald et al., 2019;</ref><ref type="bibr">Iyer et al., 2021)</ref>. Re-ranking has also been used in neural machine translation. Checkpoint reranking (Pandramish and Sharma, 2020) generates several translation candidates with multiple model checkpoints, based on the observation (similar to the one we made in ?1) that the oracle across checkpoints is of higher quality than just the last checkpoint. <ref type="bibr">Bhattacharyya et al. (2021)</ref> use an energy-based model on top of BERT to select translation candidates with higher BLEU score.</p><p>In abstractive summarization, second-stage approaches such as re-ranking remain underexplored. <ref type="bibr">Recently, RefSum (Liu et al., 2021)</ref> defined a second-stage summarization framework which helps address the problem of the train-test distribution mismatch in second-stage models. With a base GSum model <ref type="bibr">(Dou et al., 2021)</ref>, the authors reach a 46.18 state-of-the-art ROUGE-1 on <ref type="bibr">CNN-DailyMail. In SimCLS (Liu and Liu, 2021)</ref>, the authors train a second-stage model with contrastive learning, using a ranking loss to select the best summary candidate from a pool of 16 diverse beam search candidates, reaching 46.67 ROUGE-1 on CNN-DailyMail. Our approach differs from Ref-Sum and SimCLS in terms of model architecture and loss function, as well as summary candidate generation process. In contrast with RefSum, we use a single base model, but mix several decoding methods, as our goal is single-model improvement. Unlike SimCLS, we do not use a ranking loss, but directly model the probability that a summary candidate is the best one. To the best of our knowl-edge, we are the first ones to propose a multi-task re-ranking system for abstractive summarization. This enables practitioners to leverage the recent rich literature in automatic abstractive summarization evaluation <ref type="bibr">(Lin, 2004;</ref><ref type="bibr">Zhang et al., 2019a;</ref><ref type="bibr">Zhao et al., 2019a;</ref><ref type="bibr">Yuan et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Re-ranking Framework</head><p>Our approach follows the paradigm of second-stage models. Specifically, given a source document S, a base model B, and a set of decoding methods D, we get a pool of m summary candidates C = {C 1 , . . . , C m }. Given an evaluation metric ? in a set of metrics M, we get associated scores for each candidates S ? = {?(C 1 ), . . . , ?(C m )}. Our goal is to train a model f ? parameterized by ? to explicitly identify the best summary candidate C * ? according to the metric, which is given by:</p><formula xml:id="formula_0">C * ? = arg max C i ?C {?(C 1 ), . . . , ?(C m )} (1)</formula><p>We frame this problem as a binary classification. C * ? is the positive candidate, while other candidates are treated as negative. For a metric ?, the re-ranker f ? is trained with a binary cross-entropy loss:</p><formula xml:id="formula_1">L ? = ?y i log p ? ? (C i ) ? (1 ? y i ) log(1 ? p ? ? (C i )) (2) where y i = 1 if C i = C *</formula><p>? , otherwise y i = 0. Binary classification has been successfully employed for re-ranking in prior work <ref type="bibr">(Nallapati, 2004;</ref><ref type="bibr">Nogueira and Cho, 2019)</ref>. While multi-way classification could be an alternative, we noticed that for each generation method, a significant fraction of candidates share the same score for one or several metrics, while it is rare that all candidates share the same score (Appendix C-D). Thus, there is not enough signal to distinguish m candidates into m different classes, but enough for two classes.</p><p>To optimize for N different metrics M = {? 1 , . . . , ? N } simultaneously, we use a separate prediction head (tower) for each and we minimize the average over metric losses defined as:</p><formula xml:id="formula_2">L = 1 N ??M L ?<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>We first need to get a good representation of the summary candidate. To use contextual information, we concatenate the source with the candidate, Once we have a joint representation of the source with the candidate (noted x), we perform multi-task learning in order to optimize for the desired metrics. Since metrics are different, yet may be strongly correlated (e.g., ROUGE variants), we adopt a mixtureof-experts (MoE) architecture. In particular, we follow the sparse MoE approach <ref type="bibr">(Shazeer et al., 2017)</ref>, which introduces experts dropout. To adapt it to multi-task training, we use the multi-gate approach proposed in <ref type="bibr">Zhao et al. (2019b)</ref>. Given E experts E 1 , . . . , E E and N prediction towers T 1 , . . . , T N , the prediction for an input summary representation x for a metric ? indexed by k ? {1, . . . , N } is:</p><formula xml:id="formula_3">f k ? (x) = T k ( E i=1 softmax(W k x) (i) E i (x)) (4)</formula><p>where W k is the weight matrix associated with gate k. The corresponding prediction probability is:</p><formula xml:id="formula_4">p ? ? = sigmoid(f k ? (x))<label>(5)</label></formula><p>Experts are shared across all tasks, and through the softmax gates the model learns how much weight to assign to each expert for each task. Our SummaReranker model architecture is shown in <ref type="figure">Fig. 1</ref>. In practice, the shared bottom MLP consists in two fully-connected layers with ReLU activation <ref type="bibr">(Glorot et al., 2011)</ref>. Each expert E i is also a two-layer MLP with ReLU, and each prediction tower T k is a single-layer MLP. We set the number E of experts to be equal to twice the number of tasks (N ), and the experts dropout to 50%, so that the effective number of experts being used during training matches N . Our model has 370.09 million trainable parameters, representing a slight 4.14% increase due to the mixture-of-experts compared to the off-the-shelf RoBERTa-large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tackling Training and Inference Gap</head><p>Second-stage learning approaches may suffer from an inherent distribution bias. Indeed, the base model has a different output distribution on the training set than on the validation and test sets. Thus, it is ineffective to train a second-stage model on the training set outputs of the base model.</p><p>To resolve this distribution shift, we shuffle the training set and randomly split it into equal parts, then fine-tune a pre-trained model on each half. Then, to build a training set for the re-ranker, we infer with each model on the half that it was not trained on. At testing time, we face two options:</p><p>? Base setup: in this setup, we infer on the test set with one of the two base models trained on half the training set, then apply the re-ranker.</p><p>Since the base models are trained on less data, their performance on the test set worsens. However, we will show that SummaReranker brings improvements which more than compensate this performance drop.</p><p>? Transfer setup: this setup consists in applying SummaReranker on top of a base model trained on the whole training set. Note that SummaReranker is still trained in the same fashion as before. There could be a distribution mismatch in this setting too, since SummaReranker needs to rank summary candidates of a potentially higher quality (generated by a model trained on the full data) than the summaries that it was trained on  (generated by a model trained on half the data). Nevertheless, SummaReranker still transfers well and considerably improves the performance of the base model in this transfer setup.</p><p>If D is made of multiple decoding methods {? 1 , ..., ? j }, each producing several candidates, the overall candidate set may be large, slowing down inference. Thus, to explore lower-resource inference setups, we separate the sets of decoding methods D train and D test used for training and inference, respectively, and enforce that D test ? D train .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scope &amp; Datasets</head><p>Throughout our experiments, we vary all the three dimensions of our re-ranking framework: the base model B, the set of decoding methods D and the set of scoring metrics M.</p><p>As base models, we use PEGASUS (Zhang et al., 2020) and <ref type="bibr">BART (Lewis et al., 2020)</ref>, each one in their large version, as they are leading summarization models with publicly available checkpoints. We obtain pre-trained and fine-tuned checkpoints from the HuggingFace transformers library <ref type="bibr">(Wolf et al., 2020)</ref>.</p><p>For decoding methods (D), we experiment with beam search (referred to as 1), diverse beam search (2), top-k sampling (3) and top-p sampling (4). For each decoding method, we set the number of candidates to 15, as it is close to the maximum which could fit in a standard 11GB RAM GPU when doing generation with PEGASUS-large.</p><p>As set of metrics, we first use ROUGE <ref type="bibr">(Lin and Hovy, 2003)</ref>, in its commonly used three flavours of ROUGE-1 (noted R-1), ROUGE-2 (noted R-2)  and ROUGE-L (noted R-L) for summarization evaluation. We also leverage recently introduced model based evaluation methods BERTScore (noted BS) (Zhang et al., 2019a) and BARTScore (noted BaS) (Yuan et al., 2021), which both rely on contextual word embeddings from pre-trained language models. Thus, our total set of metrics is M = {R-1, R-2, R-L, BS, BaS}. As seen in <ref type="table" target="#tab_4">Table 2</ref>, R-1 and R-L are strongly correlated (Pearson correlation score of 0.977). BARTScore is the least correlated to other metrics, suggesting that it captures aspects complementary to the other four. We train SummaReranker on the following datasets, covering multiple domains:</p><p>? CNN-DailyMail (Hermann et al., 2015) contains 93k and 220k articles from the CNN and Daily-Mail newspapers, respectively. We use the non anonymized version from (See et al., 2017).</p><p>? XSum (Narayan et al., 2018) contains 227k articles from the BBC for years 2010 -2017. While also in the news domain, XSum is by design significantly more abstractive than CNN/DM and is made of single-sentence summaries.</p><p>? Reddit TIFU (Kim et al., 2019) contains 120k posts from the popular online Reddit forum. As in other summarization works (Zhang et al., 2020), we use the TIFU-long subset, containing 37k posts. As there is no official split, we build a random 80:10:10 split for training:validation:test.</p><p>We refer to <ref type="table" target="#tab_6">Table 3</ref> for statistics on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training &amp; Inference Details</head><p>To help the model better discriminate between candidates, we found that sampling was useful.  This means that at training time, the model only sees two candidates per data point. We scale the pool of candidates that these two are sampled from to four decoding methods, totalling 60 summary candidates per source document. We train SummaReranker for five epochs. We use the Adafactor optimizer (Shazeer and Stern, 2018), with maximum learning rate 1e-5, warming up the learning rate linearly over the first 5% training steps. Training on CNN/DM takes four days on a single RTX 2080 Ti GPU.</p><p>For inference, we need to output a single candidate. After getting predicted probabilities across each metric ? ? M, we output the candidate maximizing the sum of predicted probabilities. Note that relaxing inference to allow for a different best candidate for each metric would improve performance, but is not practical. We perform inference with the model checkpoint maximizing the sum of the scores for the metrics on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Base Setup Results</head><p>First, we investigate how our model performs in the base setup described in ?3. We apply SummaReranker on top of PEGASUS and BART models fine-tuned on each half. For each model, we decode using beam search (1) and diverse beam search (2). The latter performs better for PEGASUS, while the former is better for BART. We then apply SummaReranker optimized jointly for R-1, R-2, and R-L on  top of each of the two base models, for each decoding method, and finally when using both decoding methods. Results are shown in <ref type="table" target="#tab_8">Table 4</ref>. SummaReranker improves a base PEGASUS by 4.57% to 7.21% with 15 candidates, and 8.70% to 9.36% with 30 candidates. With BART, Sum-maReranker improves by 3.94% to 11.65% with 15 candidates, and 7.98% with 30 candidates. When using several decoding methods, we compare the reranker performance with the best baseline among decoding methods. Notably, with SummaReranker, PEGASUS and BART models trained on 50% of the training set now surpass their counterparts trained on the whole training set, achieving 46.19 R-1 with PEGASUS and 45.96 R-1 with BART. This is better than <ref type="bibr">GSum (Dou et al., 2021)</ref>, the best reported summarization model on CNN/DM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transfer Setup Results</head><p>Next, we look at how SummaReranker performs in the transfer setup. That means, we apply it on top of PEGASUS and BART models fine-tuned on the entire dataset, using public checkpoints. We also include R3F <ref type="bibr" target="#b0">(Aghajanyan et al., 2020)</ref> and <ref type="bibr">GSum (Dou et al., 2021)</ref> in our single-stage model comparison. In terms of second-stage approaches, we compare SummaReranker with <ref type="bibr">RefSum (Liu et al., 2021)</ref> and <ref type="bibr">SimCLS (Liu and Liu, 2021)</ref>. Note that SummaReranker is trained as usual, on the outputs of two base models each trained on 50%.</p><p>We first optimize for ROUGE metric {R-1, R-2, R-L} with multi-task training on CNN/DM (Table 5). With two decoding methods, PEGASUS + SummaReranker sets a new state of the art on CNN/DM with 47.16 R-1, 22.55 R-2 and 43.87 R-L, corresponding to gains of 2.60/1.65/2.29 R-1/2/L or +5.44% from our diverse beam search baseline. As expected, the relative gains in transfer setup are lower than in base setup. Next, we optimize modelbased metrics, and note the difficulty in improving BERTScore, compared to BARTScore. Optimizing jointly ROUGE and model-based metrics improves all metrics, but does not match the results when training only ROUGE. Interestingly, performance gains saturate when adding two extra decoding methods (top-k and top-p sampling), despite gains in the oracle scores observed in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>To assert statistical significance of performance gains, we perform a t-test between SummaReranker scores and scores from the base model with each of the decoding methods being used, and mark with ? results where the p-value is smaller than 0.05 for all these decoding methods.</p><p>We also show experts utilization (obtained with  softmax weights from the gates) for the model optimized on all five metrics in <ref type="figure" target="#fig_0">Fig. 2</ref>. Notably, some experts specialize in certain metrics (for instance, expert 0 on R-2 and expert 4 on R-L).</p><p>Then, we apply SummaReranker on XSum and Reddit TIFU, as shown in <ref type="table" target="#tab_12">Table 6</ref>. We train SummaReranker using the three ROUGE metrics M = {R-1, R-2, R-L} as objective, and D = {beam search, diverse beam search} to generate the candidates. On XSum, SummaReranker improves a base PEGASUS with beam search candidates by 1.31%, setting a new state-of-the-art of 48.12/24.95/40.00 R-1/2/L. On Reddit TIFU, we improve a base PEGASUS with beam search and diverse beam search (30 candidates) by 9.34%, reaching 29.83/9.50/23.47 R-1/2/L, and a base BART with beam search by 4.22%, reaching 28.99/9.82/22.96 R-1/2/L. Across datasets, training on a combination of beam search and diverse beam search candidates is consistently effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ranking Evaluation</head><p>Beyond summary properties, we investigate the performance of re-ranking itself with rank-based evaluation measures. A perfect re-ranker should always single out the best summary from the rest, yielding oracle results. To evaluate how SummaReranker ranks the best summary, we compute the best summary candidate recall at different thresholds. Since several candidates might get the same metric scores (Appendix C), the best candidate recall at threshold k for the random uniform ranking baseline is not the standard R@k = k m anymore    <ref type="formula">(6)</ref> where m best is the number of best candidates.</p><p>Following <ref type="figure" target="#fig_1">Fig. 3</ref>, a PEGASUS with diverse beam search ranking of summary candidates (dashed lines) is not significantly better than the corresponding random baseline from eq. (6) (dotted lines) on CNN/DM and Reddit TIFU. However, it improves on it on XSum, confirming the observation made in <ref type="table" target="#tab_12">Table 6</ref> that it is harder to train a re-ranker on this dataset. On all three datasets, SummaReranker (solid lines) significantly pushes the recall at all thresholds. We note +14.90 absolute recall@5 improvement on CNN/DM (50.84 versus 35.94, indicated by the black arrow), +9.54 on XSum and +5.23 on Reddit TIFU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Evaluation</head><p>Lastly, we demonstrate that re-ranking improvements in quantitative metrics also translate to qualitatively better summaries. <ref type="figure" target="#fig_2">Fig. 4</ref> shows an example of summary selected by SummaReranker, alongside its source document, ground-truth (reference) summary and output from the base model. Sum-maReranker is able to include a whole sentence which was missed by the base summary. We refer to Appendix K for full re-ranking demonstrations on each of the three datasets.</p><p>We also conduct a human evaluation. We asked three different humans to evaluate 50 randomly sampled test summaries for each dataset. Human raters were graduate students with professional English proficiency (TOEFL scores above 100 out of 120). Humans were shown the source document alongside the top beam search summary from PEGASUS, and the corresponding summary candidate selected by SummaReranker. They were asked to choose which one they believe is more faithful. They could choose a tie, because in some cases the base summary and the re-ranked one are very similar, or even identical (Appendix I). In <ref type="figure" target="#fig_3">Fig. 5</ref>, we see that on average, humans are more likely to pick the SummaReranker candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Abstractiveness Given that we are not modifying the base model nor its training procedure, we analyze whether our re-ranking system favors more abstractive candidates. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we display the percentage of novel n-grams for n in {1,2,3,4}, for a base PEGASUS with beam search (blue) and diverse beam search (purple) decoding, and when adding SummaReranker in both cases (green and red, respectively). As first raised in <ref type="bibr">(See et al., 2017)</ref>, summary candidates are much less abstractive than ground truth summaries on CNN/DM. Yet, our re-ranker selects more abstractive candidates according to all n-grams metrics, even more so with diverse beam search, which is already more abstractive than beam search. This observation also holds on Reddit TIFU and XSum (other than 1-grams). XSum summary candidates are already almost as abstractive as the ground truth and it is harder to obtain significant abstractiveness gains through our re-ranking.</p><p>Speed/Performance trade-off On top of base model training and candidate generation, SummaReranker inference cost is linear in the number of candidates. A single candidate takes on average 38ms to be scored. As seen in <ref type="table" target="#tab_10">Table 5</ref> and <ref type="table" target="#tab_12">Table 6</ref>, the performance gains from mixing several decoding methods to generate summary candidates are not scaling consistently (all four decoding methods are not better than just beam search and diverse beam search). To provide more insights on the speed/performance trade-off, we show in Appendix J SummaReranker performance when randomly sub-sampling k ? {1, . . . , 15} candidates. On CNN/DM, re-ranking as few as two candidates is sufficient to improve on the baseline PEGASUS. On XSum, it needs three to eight, and on Reddit TIFU three to four. As a rule of thumb, it is better to score all candidates when possible, but six to eight candidates provide a good trade-off between speed and performance across datasets.</p><p>Further Work To encode the source jointly with the summary candidate, we need to truncate the source to a fixed number of tokens. Thus, we are limited by the maximum context window of the language model encoder (512 in the case of RoBERTa-large). Applying SummaReranker to long-document summarization, such as scientific articles summarization <ref type="bibr">(Cohan et al., 2018)</ref> would need better long-range modeling. In ?3, we weighted metric-dependent losses uniformly. We leave to further work the exploration of more complex weight balancing or multi-task learning objectives (Lin et al., 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced SummaReranker, the first multi-task re-ranking framework for abstractive summarization. Encoding the source with the candidate, our model predicts whether the summary candidate maximizes each of the metrics optimized for. Sum-maReranker works well across diverse datasets, models, decoding methods and summarization evaluation metrics. Summaries selected by SummaReranker improve the ROUGE state-of-the-art on CNN/DM and XSum. In addition, we also show that they are more abstractive and more likely to be preferred by human evaluators over base model outputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyper Parameters &amp; Packages</head><p>For evaluation metrics, we used the following packages:</p><p>? For ROUGE metrics <ref type="bibr">(Lin and Hovy, 2003)</ref>, we used the public rouge-score package from     Observations from <ref type="table" target="#tab_17">Table 9</ref> and <ref type="table" target="#tab_1">Table 10</ref> are consistent with the ones made in  In <ref type="table" target="#tab_1">Table 11</ref>, BERTScore (BS) and BARTScore (BaS) have results closer to 15, indicating that it is unlikely that two summary candidates share the exact metric score. This is understandable given that both these metrics are based on embeddings from pre-trained language models (BERT and BART, respectively), and embeddings values will vary whenever the input text is different, making it unlikely to have two candidates collude on the same score. In contrast, ROUGE measures n-gram overlaps, and two different summary candidates might get the same ROUGE score with the target summary (for instance if they only differ by n-grams not present in the target).  We note that cases where all scores are identical are a small minority. ROUGE-2 is more likely than other metrics to lead to such a scenario of all identical scores.     Tables <ref type="table" target="#tab_1">Table 15 and Table 16</ref> complement the base setup results exposed in <ref type="table" target="#tab_8">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Identical Candidates Scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Metrics Correlation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Recall Curves</head><p>Threshold k k=1 k=2 k=3 k=4 k=5   <ref type="table" target="#tab_1">Table 19</ref>: Re-ranking overlap with base and best candidates. Fraction of time that the re-ranked summary coincides with the base model one (left), and one of the best ones (oracle scores) among generated candidates (right). SR is SummaReranker.</p><p>In <ref type="table" target="#tab_1">Table 19</ref>, we observe that SummaReranker is more likely to stick to the base model candidate with diverse beam search. Results in bold represent the most ideal scenario: SummaReranker differs the most from the base setup (lowest scores of the left column), and matches the most one of the best candidates (highest scores of the right column). In <ref type="figure" target="#fig_6">Fig. 8</ref>, we observe a failure mode of SummaReranker: on XSum and with PEGASUS when training the re-ranking with beam search candidates, performance decreases. However, the problem vanishes when SummaReranker is trained on a mixture of beam search and diverse beam search candidates. <ref type="figure" target="#fig_7">Fig. 9</ref> top left (PEGASUS with beam search) represents a curious case: re-ranking a single candidate is better than the top beam baseline. Since re-ranking a single candidate is equivalent to randomly sampling one candidate, this means that the top beam baseline is on average lower than sampling a random candidate. We observed that such cases are rare and usually the top beam baseline is better than the random baseline. When the top beam baseline is lower, it is of utmost importance to keep all candidate and use a second-stage method to identify a better one.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Speed/Performance Trade-off</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Expert utilization for a base PEGASUS with SummaReranker optimized with {R-1, R-2, R-L, BS, BaS} on CNN/DM, with 10 experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Best summary candidate recall with 15 diverse beam search candidates for PEGASUS on all three datasets. SR denotes SummaReranker. Dotted lines are random baselines, and dashed lines correspond to the base PEGASUS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example of a summary generated by SummaReranker trained for {R-1, R-2, R-L} on CNN/DM. The sentence in green is included in the SummaReranker summary, while the one in red is discarded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Human evaluation results on all three datasets. Black vertical bars are standard deviation across human raters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Novel n-grams with PEGASUS, across all datasets and with beam search and diverse beam search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>ROUGE-1 on CNN/DM for k sampled candidates at inference time, with k ? {1, . . . , 15}. SR stands for SummaReranker, BS and DBS refer to beam search and diverse beam search, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>ROUGE-1 on XSum for k sampled candidates at inference time, with k ? {1, . . . , 15}. SR stands for SummaReranker, BS and DBS refer to beam search and diverse beam search, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>ROUGE-1 on Reddit TIFU for k sampled candidates at inference time, with k ? {1, . . . , 15}. SR stands for SummaReranker, BS and DBS refer to beam search and diverse beam search, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Oracle scores (maximum scores over all generated candidates) for four popular decoding methods and five summarization evaluation measures for a base PEGASUS model on CNN/DM. R-1/2/L denotes ROUGE-1/2/L, BS and BaS denote BERTScore and BARTScore, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Pearson correlation coefficient between the</cell></row><row><cell>five evaluation metrics {R-1, R-2, R-L, BS, BaS} for</cell></row><row><cell>a base PEGASUS with beam search on CNN/DM.</cell></row><row><cell>R-1/2/L denotes ROUGE-1/2/L, BS and BaS denote</cell></row><row><cell>BERTScore and BARTScore.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the three datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Specifically, during training, we rank candidates by decreasing sum of normalized scores for the evaluation metrics and keep the top m top and bottom m</figDesc><table><row><cell>Model</cell><cell>Model stage</cell><cell>Decoding methods (D)</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>Gain (%)</cell></row><row><cell>PEGASUS -1st half</cell><cell>1</cell><cell>{1}</cell><cell cols="3">42.23 19.62 38.90</cell><cell>_</cell></row><row><cell>PEGASUS -1st half</cell><cell>1</cell><cell>{2}</cell><cell cols="3">42.50 19.75 39.55</cell><cell>_</cell></row><row><cell>PEGASUS -2nd half</cell><cell>1</cell><cell>{1}</cell><cell cols="3">42.46 19.95 39.19</cell><cell>_</cell></row><row><cell>PEGASUS -2nd half</cell><cell>1</cell><cell>{2}</cell><cell cols="3">42.75 19.93 39.86</cell><cell>_</cell></row><row><cell>BART -1st half</cell><cell>1</cell><cell>{1}</cell><cell cols="3">42.79 20.25 39.66</cell><cell>_</cell></row><row><cell>BART -1st half</cell><cell>1</cell><cell>{2}</cell><cell cols="3">40.70 18.99 37.88</cell><cell>_</cell></row><row><cell>BART -2nd half</cell><cell>1</cell><cell>{1}</cell><cell cols="3">42.93 20.36 39.73</cell><cell>_</cell></row><row><cell>BART -2nd half</cell><cell>1</cell><cell>{2}</cell><cell cols="3">41.93 19.79 39.06</cell><cell>_</cell></row><row><cell>PEGASUS -1st half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="4">44.02 20.97 40.68 5.23</cell></row><row><cell>PEGASUS -1st half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="4">45.66 21.31 42.51 7.61</cell></row><row><cell>PEGASUS -2nd half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="4">44.11 21.08 40.82 4.57</cell></row><row><cell>PEGASUS -2nd half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="4">45.73 21.31 42.62 6.94</cell></row><row><cell>BART -1st half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="4">44.23 21.23 41.09 3.94</cell></row><row><cell>BART -1st half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="4">45.05 21.47 42.12 11.65</cell></row><row><cell>BART -2nd half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="4">44.51 21.52 41.29 4.44</cell></row><row><cell>BART -2nd half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="4">45.61 21.78 42.62 9.32</cell></row><row><cell>PEGASUS -1st half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="4">46.12 21.97 42.84 9.36</cell></row><row><cell>PEGASUS -2nd half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="4">46.19 22.02 42.92 8.70</cell></row><row><cell>BART -1st half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="4">45.76 22.14 42.71 7.99</cell></row><row><cell>BART -2nd half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="4">45.96 22.18 42.88 7.98</cell></row></table><note>bottom candidates. Thus, training time varies in O(m top + m bottom ), while inference is in O(m) as we need to score each candidate. In practice, we found that taking m top = 1 and m bottom = 1 per- formed well, on top of decreasing the training time.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Base setup results for SummaReranker applied to PEGASUS and BART on the CNN/DM</figDesc><table /><note>dataset. SR refers to SummaReranker. Decoding method {1} is beam search, {2} is diverse beam search. Best scores for each type of model are in bold. Gain represents the mean relative gain over {R-1, R-2, R-L} compared to the best decoding method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>? 22.23 ? 42.46 ? 87.60 ? -2.74 ? 3.18 PEGASUS ? 22.01 ? 43.59 ? 87.66 ? -2.73 ? 5.10 PEGASUS ? 22.61 ? 42.94 ? 87.67 ? -2.72 ? 4.59 PEGASUS ? 22.55 ? 43.87 ? 87.74 ? -2.71 ? 5.44 BaS} 46.59 ? 22.41 ? 43.45 ? 87.77 ? -2.58 ? 4.39 BART BaS} 45.96 ? 21.79 ? 43.01 ? 88.44 ? -2.09 ? 4.03 PEGASUS ? 22.32 ? 43.72 ? 87.69 ? -2.74 ? _</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Decoding methods</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Evaluation metrics</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Model stage</cell><cell>Dtrain</cell><cell>Dtest</cell><cell>m</cell><cell>Optimized Metrics (M)</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell><cell>BaS</cell><cell>Gain (%)</cell></row><row><cell>PEGASUS (Zhang et al., 2020)</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>8</cell><cell>_</cell><cell>44.16</cell><cell>21.56</cell><cell>41.30</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>PEGASUS -our setup</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>15</cell><cell>_</cell><cell>44.23</cell><cell>21.48</cell><cell>41.21</cell><cell>87.39</cell><cell>-2.78</cell><cell>_</cell></row><row><cell>PEGASUS -our setup</cell><cell>1</cell><cell>{2}</cell><cell>{2}</cell><cell>15</cell><cell>_</cell><cell>44.56</cell><cell>20.90</cell><cell>41.58</cell><cell>87.36</cell><cell>-2.81</cell><cell>_</cell></row><row><cell>BART (Lewis et al., 2020)</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>5</cell><cell>_</cell><cell>44.16</cell><cell>21.28</cell><cell>40.90</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>BART -our setup</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>15</cell><cell>_</cell><cell>43.28</cell><cell>20.44</cell><cell>40.06</cell><cell>87.78</cell><cell>-2.48</cell><cell>_</cell></row><row><cell>BART -our setup</cell><cell>1</cell><cell>{2}</cell><cell>{2}</cell><cell>15</cell><cell>_</cell><cell>44.48</cell><cell>21.21</cell><cell>41.60</cell><cell>88.11</cell><cell>-2.33</cell><cell>_</cell></row><row><cell>BART + R3F (Aghajanyan et al., 2020)</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>5</cell><cell>_</cell><cell>44.38</cell><cell>21.53</cell><cell>41.17</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>GSum (Dou et al., 2021)</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>4</cell><cell>_</cell><cell>45.94</cell><cell>22.32</cell><cell>42.48</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>GSum + RefSum (Liu et al., 2021)</cell><cell>2</cell><cell>{1}</cell><cell>{1}</cell><cell>4</cell><cell>_</cell><cell>46.18</cell><cell>22.36</cell><cell>42.91</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>BART + SimCLS (Liu and Liu, 2021)</cell><cell>2</cell><cell>{2}</cell><cell>{2}</cell><cell>16</cell><cell>_</cell><cell>46.67</cell><cell>22.15</cell><cell>43.54</cell><cell>66.14</cell><cell>_</cell><cell>_</cell></row><row><cell cols="12">PEGASUS + SR 45.56  + SR 2 {1} {1} 15 {R-1, R-2, R-L} 2 {2} {2} 15 {R-1, R-2, R-L} 46.86  + SR 2 {1, 2} {1} 15 {R-1, R-2, R-L} 46.13  + SR 2 {1, 2} {2} 15 {R-1, R-2, R-L} 46.83  ? 21.88  ? 43.55  ? 87.63  ? -2.74  ? 4.84</cell></row><row><cell>BART + SR</cell><cell>2</cell><cell>{1}</cell><cell>{1}</cell><cell>15</cell><cell>{R-1, R-2, R-L}</cell><cell cols="6">44.60  ? 21.38  ? 41.36  ? 88.03  ? -2.40  ? 3.63</cell></row><row><cell>BART + SR</cell><cell>2</cell><cell>{2}</cell><cell>{2}</cell><cell>15</cell><cell>{R-1, R-2, R-L}</cell><cell cols="6">46.47  ? 22.17  ? 43.45  ? 88.43  ? -2.19  ? 4.48</cell></row><row><cell>BART + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell>{1}</cell><cell>15</cell><cell>{R-1, R-2, R-L}</cell><cell cols="6">45.08  ? 21.79  ? 41.85  ? 88.13  ? -2.37  ? 5.08</cell></row><row><cell>BART + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell>{2}</cell><cell>15</cell><cell>{R-1, R-2, R-L}</cell><cell cols="6">46.50  ? 22.15  ? 43.50  ? 88.45  ? -2.18  ? 4.51</cell></row><row><cell cols="7">PEGASUS + SR (new SOTA) 47.16  PEGASUS + SR 2 {1, 2} {1, 2} 30 {R-1, R-2, R-L} 2 {1, 2} {1, 2} 30 {BS, BaS} 45.00  ?</cell><cell>20.90</cell><cell>41.93  ?</cell><cell cols="3">87.56  ? -2.55  ? 4.23</cell></row><row><cell cols="6">PEGASUS + SR 30 {R-1, R-2, R-L, BS, + SR 2 {1, 2} {1, 2} 2 {1, 2} {1, 2} 30 {R-1, R-2, R-L}</cell><cell cols="6">46.62  ? 22.39  ? 43.59  ? 88.47  ? -2.18  ? 5.05</cell></row><row><cell>BART + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell>{1, 2}</cell><cell>30</cell><cell>{BS, BaS}</cell><cell>44.90  ?</cell><cell>20.85</cell><cell>42.03  ?</cell><cell cols="3">88.28  ? -2.05  ? 6.11</cell></row><row><cell cols="6">BART + SR 30 {R-1, R-2, R-L, BS, + SR 2 {1, 2} {1, 2} 2 {1, 2, 3, 4} {1, 2, 3, 4} 60 {R-1, R-2, R-L}</cell><cell>47.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Transfer setup results on CNN/DM. SR refers to SummaReranker, m refers to the number of summary candidates, BS and BaS to BERTScore and BARTScore, respectively. Best scores for each type of model (single stage, second-stage) are in bold. ? marks are results significantly better than the base model counterpart among metrics that SummaReranker was optimized for. Results for optimized metrics are shaded. Gain represents the mean relative gain over optimized metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>? 24.95 40.00 ? 92.14 ? -1.90 ? 1.31 29.57 ? 9.70 ? 23.29 ? 87.63 ? -3.34 ? 9.47 PEGASUS .65 28.71 ? 8.73 ? 22.79 ? 87.84 ? -3.42 ? 9.57 BART 92.10 ? -1.94 -0.53 29.83 ? 9.50 ? 23.47 ? 87.81 ? -3.33 ? 9.34</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Decoding methods</cell><cell></cell><cell></cell><cell></cell><cell cols="2">XSum</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reddit TIFU</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Model stage</cell><cell>Dtrain</cell><cell>Dtest</cell><cell>m</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell><cell>BaS</cell><cell>Gain (%)</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell><cell>BaS</cell><cell>Gain (%)</cell></row><row><cell>PEGASUS (Zhang et al., 2020)</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>8</cell><cell>47.21</cell><cell>24.56</cell><cell>39.25</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>26.63</cell><cell>9.01</cell><cell>21.60</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>PEGASUS -our setup</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell cols="2">15 47.33</cell><cell>24.75</cell><cell>39.43</cell><cell cols="2">92.01 -1.92</cell><cell>_</cell><cell>26.28</cell><cell>9.01</cell><cell>21.52</cell><cell cols="2">87.34 -3.46</cell><cell>_</cell></row><row><cell>PEGASUS -our setup</cell><cell>1</cell><cell>{2}</cell><cell>{2}</cell><cell cols="2">15 46.78</cell><cell>23.77</cell><cell>38.70</cell><cell cols="2">91.94 -2.00</cell><cell>_</cell><cell>25.67</cell><cell>8.07</cell><cell>20.97</cell><cell cols="2">87.47 -3.48</cell><cell>_</cell></row><row><cell>BART (Lewis et al., 2020)</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>5</cell><cell>45.14</cell><cell>22.27</cell><cell>37.25</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>BART -our setup</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell cols="2">15 45.24</cell><cell>22.28</cell><cell>37.21</cell><cell cols="2">91.58 -1.97</cell><cell>_</cell><cell>27.42</cell><cell>9.53</cell><cell>22.10</cell><cell cols="2">87.43 -3.78</cell><cell>_</cell></row><row><cell>BART -our setup</cell><cell>1</cell><cell>{2}</cell><cell>{2}</cell><cell cols="2">15 44.15</cell><cell>20.84</cell><cell>35.88</cell><cell cols="2">91.51 -2.08</cell><cell>_</cell><cell>25.43</cell><cell>8.27</cell><cell>20.79</cell><cell cols="2">87.48 -4.19</cell><cell>_</cell></row><row><cell>BART + R3F (Aghajanyan et al., 2020)</cell><cell>1</cell><cell>{1}</cell><cell>{1}</cell><cell>5</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell cols="3">30.31 10.98 24.74</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>GSum + RefSum (Liu et al., 2021)</cell><cell>2</cell><cell>{1}</cell><cell>{1}</cell><cell>4</cell><cell>47.45</cell><cell>24.55</cell><cell>39.41</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>PEGASUS + SimCLS (Liu and Liu, 2021)</cell><cell>2</cell><cell>{2}</cell><cell>{2}</cell><cell cols="2">16 47.61</cell><cell>24.57</cell><cell>39.44</cell><cell>69.81</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell cols="11">PEGASUS + SR (new XSum SOTA) 15 48.12  + SR 2 {1, 2} {1} 2 {1, 2} {2} 15 47.04 38.55 91.98 -2.01 -0+ SR 23.27 2 {1, 2} {1} 15 45.79  ? 22.17 37.31 91.69  ? -1.97 0.33</cell><cell>28.99  ?</cell><cell>9.82</cell><cell cols="4">22.96  ? 87.53 -3.78 4.22</cell></row><row><cell>BART + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell>{2}</cell><cell>15</cell><cell>44.39</cell><cell>20.35</cell><cell cols="5">35.66 91.51 -2.16 -0.81 28.04  ?</cell><cell>8.66</cell><cell cols="4">22.41  ? 87.73  ? -3.91  ? 7.59</cell></row><row><cell cols="12">PEGASUS + SR (best Reddit TIFU score) 39.42 BART + SR 2 {1, 2} {1, 2} 30 47.72 24.16 2 {1, 2} {1, 2} 30 45.32 21.46 36.64 91.64 -2.04 -1.68 28.92  ?</cell><cell>9.16</cell><cell>22.87</cell><cell></cell><cell></cell><cell></cell></row></table><note>? 87.70 ? -3.83 ? 1.69</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Transfer setup results on XSum and Reddit TIFU. SR refers to SummaReranker, m refers to the number of summary candidates, BS and BaS to BERTScore and BARTScore, respectively. Best scores for each type of model (single stage, second-stage) are in bold. ? marks are results significantly better than the base model counterpart among metrics that SummaReranker was optimized for. Results for optimized metrics are shaded. Gain represents the mean relative gain over optimized metrics. Reddit TIFU results in italic are not directly comparable due to a different data split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR. Shichao Sun and Wenjie Li. 2021. Alleviating exposure bias via contrastive learning for abstractive text summarization. arXiv preprint arXiv:2108.11846.</figDesc><table><row><cell>Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-</cell><cell>Alexander R Fabbri, Wojciech Kry?ci?ski, Bryan Ramesh Nallapati. 2004. Discriminative models for in-Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh</cell></row><row><cell>quence prediction with recurrent neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems -Vol-ume 1, NIPS'15, page 1171-1179, Cambridge, MA, USA. MIT Press. tional Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Mar-jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-quence models. arXiv preprint arXiv:1610.02424. search: Decoding diverse solutions from neural se-Crandall, and Dhruv Batra. 2016. Diverse beam prasath R Selvaraju, Qing Sun, Stefan Lee, David Ashwin K Vijayakumar, Michael Cogswell, Ram-Noam Shazeer. 2015. Scheduled sampling for se-551, Hong Kong, China. Association for Computa-Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Cann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 540-</cell><cell>of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Association for Computational Linguistics. guage Learning, pages 280-290, Berlin, Germany. SIGNLL Conference on Computational Natural Lan-RNNs and beyond. In Proceedings of The 20th tive text summarization using sequence-to-sequence erarchical neural story generation. In Proceedings ?aglar Gul?ehre, and Bing Xiang. 2016. Abstrac-formation retrieval. In Proceedings of the 27th an-nual international ACM SIGIR conference on Re-search and development in information retrieval, pages 64-71. Nath, Shawn Andrews, Aditee Kumthekar, Mah-eswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. 2019b. Recommending what video to watch next: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender Systems, McCann, Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, pages 43-51.</cell></row><row><cell>Sumanta Bhattacharyya, Amirmohammad Rooshenas, Subhajit Naskar, Simeng Sun, Mohit Iyyer, and An-drew McCallum. 2021. Energy-based reranking: Improving neural machine translation using energy-based models. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4528-4537, Online. Association for Computational Linguistics. Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative rerank-ing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 173-180. training for natural language generation, translation, and comprehension. In Proceedings of the 58th An-nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association Ronald J Williams and David Zipser. 1989. A learn-ing algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280. for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: A package for auto-Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. Chin-Yew Lin and Eduard Hovy. 2003. ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Auto-matic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Hu-man Language Technology Conference of the North American Chapter of the Association for Computa-tional Linguistics, pages 150-157. Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language process-ing. In Proceedings of the 2020 Conference on Em-pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso-ciation for Computational Linguistics. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Na-zli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long docu-Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. and Sam Kwong. 2019. Pareto multi-task learning. Advances in neural information processing systems, Bartscore: Evaluating generated text as text genera-tion. arXiv preprint arXiv:2106.11520. 32:12060-12070. ments. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-nologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computa-tional Linguistics. Xiang Lin, Simeng Han, and Shafiq Joty. 2021. Straight to the gradient: Learning to use novel to-kens for neural text generation. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-ter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In In-ternational Conference on Machine Learning, pages 11328-11339. PMLR.</cell><cell>2011. Deep sparse rectifier neural networks. In Pro-ceedings of the fourteenth international conference on artificial intelligence and statistics, pages 315-323. JMLR Workshop and Conference Proceedings. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28:1693-1701. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751. Srinivasan Iyer, Sewon Min, Yashar Mehdad, and Wen-tau Yih. 2021. RECONSIDER: Improved re-ranking using span-focused cross-attention for open guage Processing, pages 1797-1807, Brussels, Bel-gium. Association for Computational Linguistics. Rodrigo Nogueira and Kyunghyun Cho. 2019. Pas-sage re-ranking with bert. arXiv preprint arXiv:1901.04085. Vinay Pandramish and Dipti Misra Sharma. 2020. Checkpoint reranking: An approach to select better hypothesis for neural machine translation systems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 286-291, Online. Associ-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, pages 1280-1287, On-ation for Computational Linguistics. Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao, Bartuer Zhou, Biao Cheng, Daxin Jiang, Jiusheng line. Association for Computational Linguistics. Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. 2019. Abstractive summarization of Reddit posts with multi-level memory networks. In Proceed-ings of the 2019 Conference of the North American Chen, Ruofei Zhang, Houqiang Li, and Nan Duan. 2021. ProphetNet-X: Large-scale pre-training mod-els for English, Chinese, multi-lingual, dialog, and code generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference</cell></row><row><cell>Learning Research, pages 6642-6653. PMLR. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Michael Collins and Terry Koo. 2005. Discriminative Weinberger, and Yoav Artzi. 2019a. Bertscore: Eval-reranking for natural language parsing. Computa-Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-uating text generation with bert. arXiv preprint tional Linguistics, 31(1):25-70. dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, arXiv:1904.09675.</cell><cell>Chapter of the Association for Computational Lin-on Natural Language Processing: System Demon-guistics: Human Language Technologies, Volume 1 strations, pages 232-239, Online. Association for (Long and Short Papers), pages 2519-2531, Min-Computational Linguistics. neapolis, Minnesota. Association for Computational</cell></row><row><cell>Luke Zettlemoyer, and Veselin Stoyanov. 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Roberta: A robustly optimized bert pretraining ap-Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,</cell><cell>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Linguistics. Lee, Sharan Narang, Michael Matena, Yanqi Zhou,</cell></row><row><cell>Kristina Toutanova. 2019. BERT: Pre-training of proach. arXiv preprint arXiv:1907.11692. Maosong Sun, and Qun Liu. 2019b. ERNIE: En-</cell><cell>Fajri Koto, Jey Han Lau, and Timothy Baldwin. 2021. Wei Li, and Peter J Liu. 2019. Exploring the limits</cell></row><row><cell>deep bidirectional transformers for language under-hanced language representation with informative en-</cell><cell>Evaluating the efficacy of summarization evaluation of transfer learning with a unified text-to-text trans-</cell></row><row><cell>standing. In Proceedings of the 2019 Conference Yixin Liu, Zi-Yi Dou, and Pengfei Liu. 2021. RefSum: tities. In Proceedings of the 57th Annual Meet-</cell><cell>across languages. arXiv preprint arXiv:2106.01478. former. arXiv preprint arXiv:1910.10683.</cell></row><row><cell>of the North American Chapter of the Association Refactoring neural summarization. In Proceedings ing of the Association for Computational Linguis-</cell><cell></cell></row><row><cell>for Computational Linguistics: Human Language of the 2021 Conference of the North American Chap-tics, pages 1441-1451, Florence, Italy. Association</cell><cell>Bernhard Kratzwald, Anna Eigenmann, and Stefan Abigail See, Peter J. Liu, and Christopher D. Manning.</cell></row><row><cell>Technologies, Volume 1 (Long and Short Papers), ter of the Association for Computational Linguistics: for Computational Linguistics.</cell><cell>Feuerriegel. 2019. RankQA: Neural question an-2017. Get to the point: Summarization with pointer-</cell></row><row><cell>pages 4171-4186, Minneapolis, Minnesota. Associ-Human Language Technologies, pages 1437-1448,</cell><cell>swering with answer re-ranking. In Proceedings of generator networks. In Proceedings of the 55th An-</cell></row><row><cell>ation for Computational Linguistics. Online. Association for Computational Linguistics. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-</cell><cell>the 57th Annual Meeting of the Association for Com-nual Meeting of the Association for Computational</cell></row><row><cell>tian M. Meyer, and Steffen Eger. 2019a. Mover-</cell><cell>putational Linguistics, pages 6076-6085, Florence, Linguistics (Volume 1: Long Papers), pages 1073-</cell></row><row><cell>Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Yixin Liu and Pengfei Liu. 2021. SimCLS: A sim-Score: Text generation evaluating with contextu-</cell><cell>Italy. Association for Computational Linguistics. 1083, Vancouver, Canada. Association for Computa-</cell></row><row><cell>Jiang, and Graham Neubig. 2021. GSum: A gen-ple framework for contrastive learning of abstractive alized embeddings and earth mover distance. In</cell><cell>tional Linguistics.</cell></row><row><cell>eral framework for guided neural abstractive summa-summarization. In Proceedings of the 59th Annual Proceedings of the 2019 Conference on Empirical</cell><cell>Bernhard Kratzwald and Stefan Feuerriegel. 2018.</cell></row><row><cell>rization. In Proceedings of the 2021 Conference of Meeting of the Association for Computational Lin-Methods in Natural Language Processing and the</cell><cell>Adaptive document retrieval for deep question an-Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,</cell></row><row><cell>the North American Chapter of the Association for guistics and the 11th International Joint Conference 9th International Joint Conference on Natural Lan-</cell><cell>swering. In Proceedings of the 2018 Conference on Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff</cell></row><row><cell>Computational Linguistics: Human Language Tech-on Natural Language Processing (Volume 2: Short guage Processing (EMNLP-IJCNLP), pages 563-</cell><cell>Empirical Methods in Natural Language Processing, Dean. 2017. Outrageously large neural networks:</cell></row><row><cell>nologies, pages 4830-4842, Online. Association for Papers), pages 1065-1072, Online. Association for 578, Hong Kong, China. Association for Computa-</cell><cell>pages 576-581, Brussels, Belgium. Association for The sparsely-gated mixture-of-experts layer. arXiv</cell></row><row><cell>Computational Linguistics. Computational Linguistics. tional Linguistics.</cell><cell>Computational Linguistics. preprint arXiv:1701.06538.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters for fine-tuning the base models. LR designates the learning rate, Epochs is the number of epochs, Opt. is the optimizer, BS is the batch size, LS means label smoothing, and MP means mixed precision. Source tokens is the maximum size of the input document, Summary tokens the maximum size of the output summary.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Source tokens</cell><cell>Summary tokens</cell><cell>Length penalty</cell><cell>Repetition penalty</cell><cell>Trigram blocking</cell></row><row><cell>CNN/DM</cell><cell>PEGASUS BART</cell><cell>1024 1024</cell><cell>128 128</cell><cell>0.8 0.8</cell><cell>1.0 1.0</cell><cell>No No</cell></row><row><cell>XSum</cell><cell>PEGASUS BART</cell><cell>512 512</cell><cell>64 64</cell><cell>0.8 0.8</cell><cell>1.0 1.0</cell><cell>Yes Yes</cell></row><row><cell>Reddit TIFU</cell><cell>PEGASUS BART</cell><cell>512 512</cell><cell>128 128</cell><cell>0.6 1.0</cell><cell>1.0 1.0</cell><cell>Yes Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameters for the summary candidates generation with the base models.</figDesc><table><row><cell cols="2">B Oracle Scores</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Decoding methods</cell><cell># Summary candidates</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell><cell>BaS</cell></row><row><cell>Beam search (top beam)</cell><cell>1</cell><cell cols="5">47.33 24.75 39.43 92.01 -1.92</cell></row><row><cell>Beam search</cell><cell>15</cell><cell cols="5">56.07 33.80 48.33 93.19 -1.82</cell></row><row><cell>Diverse beam search</cell><cell>15</cell><cell cols="5">57.82 35.28 50.95 93.65 -1.63</cell></row><row><cell>Top-k sampling</cell><cell>15</cell><cell cols="5">55.57 32.54 48.35 93.18 -1.86</cell></row><row><cell>Top-p sampling</cell><cell>15</cell><cell cols="5">56.74 33.94 49.60 93.40 -1.77</cell></row><row><cell>All four above</cell><cell>60</cell><cell cols="5">62.30 40.84 55.92 94.24 -1.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Oracle scores for four popular decoding methods and five summarization evaluation measures for a base PEGASUS model on XSum.</figDesc><table><row><cell>Decoding methods</cell><cell># Summary candidates</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell><cell>BaS</cell></row><row><cell>Beam search (top beam)</cell><cell>1</cell><cell cols="5">26.28 9.01 21.52 87.34 -3.46</cell></row><row><cell>Beam search</cell><cell>15</cell><cell cols="5">36.08 14.93 29.70 88.64 -2.89</cell></row><row><cell>Diverse beam search</cell><cell>15</cell><cell cols="5">36.70 15.22 30.88 89.08 -2.81</cell></row><row><cell>Top-k sampling</cell><cell>15</cell><cell cols="5">36.76 14.37 29.49 88.53 -3.14</cell></row><row><cell>Top-p sampling</cell><cell>15</cell><cell cols="5">37.54 15.24 30.50 88.69 -3.03</cell></row><row><cell>All four above</cell><cell>60</cell><cell cols="5">43.25 20.70 36.41 89.71 -2.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Oracle scores for four popular decoding methods and five summarization evaluation measures for a base PEGASUS model on Reddit TIFU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 1</head><label>1</label><figDesc>: oracle scores are widely above the top beam baseline, and keep increasing when mixing several decoding methods.</figDesc><table><row><cell cols="5">C Unique Candidates Scores</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Scoring metric</cell><cell></cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>Generation method</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell><cell>BaS</cell></row><row><cell></cell><cell></cell><cell>{1}</cell><cell cols="5">11.51 10.87 11.54 14.96 14.96</cell></row><row><cell>CNN/DM</cell><cell>PEGASUS</cell><cell>{2} {3} {4}</cell><cell cols="5">14.34 14.09 14.34 14.99 14.99 14.65 14.40 14.65 14.99 14.99 14.68 14.41 14.69 15.00 15.00</cell></row><row><cell></cell><cell>BART</cell><cell>{1} {2}</cell><cell cols="5">11.51 10.90 11.54 14.93 14.95 13.89 13.71 13.89 14.80 14.79</cell></row><row><cell>XSum</cell><cell>PEGASUS BART</cell><cell>{1} {2} {1} {2}</cell><cell cols="5">8.90 12.05 10.92 12.11 14.97 14.98 7.91 8.56 14.99 14.99 8.70 7.57 8.33 14.99 15.00 7.37 6.63 7.37 14.59 14.99</cell></row><row><cell>Reddit TIFU</cell><cell>PEGASUS BART</cell><cell>{1} {2} {1} {2}</cell><cell>9.19 7.84 7.73 7.42</cell><cell>6.31 5.06 5.15 3.92</cell><cell cols="3">8.85 14.99 14.99 7.77 14.89 14.97 7.56 14.99 14.99 7.38 14.89 14.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Number of unique scores among pools of 15</figDesc><table><row><cell>candidates generated on different datasets (CNN/DM,</cell></row><row><cell>XSum, Reddit TIFU) with different base models (PE-</cell></row><row><cell>GASUS, BART) and different decoding methods ({1}</cell></row><row><cell>stands for beam search, {2} is diverse beam search, {3}</cell></row><row><cell>is top-p sampling and {4} top-k sampling). The lowest</cell></row><row><cell>possible score of 1 indicates that all 15 candidates are</cell></row><row><cell>assigned the same score under the metric being consid-</cell></row><row><cell>ered, while the highest of 15 means that all candidates</cell></row><row><cell>are assigned a different score.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Fraction</figDesc><table><row><cell>of sets of candidates with all iden-</cell></row><row><cell>tical scores (%) for pools of 15 candidates generated</cell></row><row><cell>on different datasets (CNN/DM, XSum, Reddit TIFU)</cell></row><row><cell>with different base models (PEGASUS, BART) and dif-</cell></row><row><cell>ferent decoding methods ({1} stands for beam search,</cell></row><row><cell>{2} is diverse beam search, {3} is top-p sampling and</cell></row><row><cell>{4} top-k sampling.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 13 :</head><label>13</label><figDesc>Pearson</figDesc><table><row><cell></cell><cell cols="4">correlation coefficient between the</cell></row><row><cell cols="5">five evaluation metrics {R-1, R-2, R-L, BS, BaS} for a</cell></row><row><cell cols="5">base PEGASUS decoded with beam search on XSum.</cell></row><row><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell><cell>BaS</cell></row><row><cell cols="5">R-1 1.000 0.806 0.927 0.766 0.600</cell></row><row><cell cols="5">R-2 0.806 1.000 0.856 0.679 0.524</cell></row><row><cell cols="5">R-L 0.927 0.856 1.000 0.768 0.564</cell></row><row><cell cols="5">BS 0.766 0.679 0.768 1.000 0.646</cell></row><row><cell cols="5">BaS 0.600 0.524 0.564 0.656 1.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 14 :</head><label>14</label><figDesc>PearsonMetrics correlation fromTable 13 and Table 14follow the same pattern as inTable 2.</figDesc><table><row><cell>correlation coefficient between the</cell></row><row><cell>five evaluation metrics {R-1, R-2, R-L, BS, BaS} for a</cell></row><row><cell>base PEGASUS decoded with beam search on Reddit</cell></row><row><cell>TIFU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 15 :</head><label>15</label><figDesc>Base setup results for SummaReranker applied to PEGASUS and BART on the XSum dataset. SR refers to SummaReranker. Decoding method {1} is beam search, {2} is diverse beam search. Best scores for each type of model are in bold. Gain represents the mean relative gain over {R-1, R-2, R-L} compared to the best decoding method.</figDesc><table><row><cell>Model</cell><cell>Model stage</cell><cell>Decoding methods (D)</cell><cell>R-1</cell><cell>R-2 R-L</cell><cell>Gain (%)</cell></row><row><cell>PEGASUS -1st half</cell><cell>1</cell><cell>{1}</cell><cell cols="2">24.83 8.29 20.38</cell><cell>_</cell></row><row><cell>PEGASUS -1st half</cell><cell>1</cell><cell>{2}</cell><cell cols="2">23.77 7.38 19.37</cell><cell>_</cell></row><row><cell>PEGASUS -2nd half</cell><cell>1</cell><cell>{1}</cell><cell cols="2">25.16 8.42 20.53</cell><cell>_</cell></row><row><cell>PEGASUS -2nd half</cell><cell>1</cell><cell>{2}</cell><cell cols="2">24.18 7.53 19.68</cell><cell>_</cell></row><row><cell>BART -1st half</cell><cell>1</cell><cell>{1}</cell><cell cols="2">28.38 9.60 22.44</cell><cell>_</cell></row><row><cell>BART -1st half</cell><cell>1</cell><cell>{2}</cell><cell cols="2">28.60 8.96 22.49</cell><cell>_</cell></row><row><cell>BART -2nd half</cell><cell>1</cell><cell>{1}</cell><cell cols="2">26.94 9.13 21.65</cell><cell>_</cell></row><row><cell>BART -2nd half</cell><cell>1</cell><cell>{2}</cell><cell cols="2">25.83 8.38 20.97</cell><cell>_</cell></row><row><cell>PEGASUS -1st half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="3">28.78 9.20 22.74 12.83</cell></row><row><cell>PEGASUS -1st half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="3">28.63 8.71 22.71 18.53</cell></row><row><cell>PEGASUS -2nd half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="3">28.87 9.24 22.73 11.70</cell></row><row><cell>PEGASUS -2nd half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="3">28.41 8.46 22.44 14.63</cell></row><row><cell>BART -1st half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="3">28.98 9.62 22.96 1.53</cell></row><row><cell>BART -1st half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="3">28.89 8.70 22.40 -0.77</cell></row><row><cell>BART -2nd half + SR</cell><cell>2</cell><cell>{1}</cell><cell cols="3">27.93 9.48 22.45 3.73</cell></row><row><cell>BART -2nd half + SR</cell><cell>2</cell><cell>{2}</cell><cell cols="3">28.24 8.77 22.43 6.98</cell></row><row><cell>PEGASUS -1st half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="3">29.93 9.40 23.50 16.37</cell></row><row><cell>PEGASUS -2nd half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="3">29.65 9.24 23.22 13.53</cell></row><row><cell>BART -1st half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="3">29.00 8.78 22.32 -2.63</cell></row><row><cell>BART -2nd half + SR</cell><cell>2</cell><cell>{1, 2}</cell><cell cols="3">29.15 9.11 22.96 4.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 16 :</head><label>16</label><figDesc>Base setup results for SummaReranker applied to PEGASUS and BART on the Reddit TIFU dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 17 :</head><label>17</label><figDesc>Values of recall curves plotted inFig. 3.</figDesc><table><row><cell cols="4">H Human Evaluation</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Tie</cell><cell></cell><cell cols="4">Base model SummaReranker</cell></row><row><cell></cell><cell>Mean</cell><cell>Std</cell><cell>Mean</cell><cell>Std</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>CNN/DM</cell><cell cols="5">18.67 9.50 32.00 6.00 49.33</cell><cell>12.20</cell></row><row><cell>XSum</cell><cell cols="5">42.00 16.33 28.00 10.20 30.00</cell><cell>7.12</cell></row><row><cell cols="6">Reddit TIFU 16.00 4.32 28.00 2.82 58.00</cell><cell>4.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 18 :</head><label>18</label><figDesc>Numbers of the human evaluation in Fig. 5.</figDesc><table><row><cell cols="3">I Candidate Selection</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>Generation method</cell><cell>SR pick the base candidate (%)</cell><cell>SR pick the best candidate (%)</cell></row><row><cell>CNN/DM</cell><cell>PEGASUS BART</cell><cell>{1} {2} {1} {2}</cell><cell>3.57 11.11 2.75 6.67</cell><cell>14.81 15.00 15.51 13.54</cell></row><row><cell>XSum</cell><cell>PEGASUS BART</cell><cell>{1} {2} {1} {2}</cell><cell>4.86 20.73 8.01 22.23</cell><cell>9.97 16.57 18.19 23.80</cell></row><row><cell>Reddit TIFU</cell><cell>PEGASUS BART</cell><cell>{1} {2} {1} {2}</cell><cell>6.16 16.82 3.22 3.32</cell><cell>18.21 23.09 24.04 32.88</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table 22: Beam search summary candidates of a base PEGASUS and their ground truth and SummaReranker re-ranking scores on Reddit TIFU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the SINGA scholarship and partially supported by the National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. We would like to thank anonymous reviewers for their insightful feedback on how to improve the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03156</idno>
		<title level="m">Better fine-tuning by reducing representational collapse</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Angel Di Maria (left) has a new No 7 tattoo which stands out among others on his left arm . Di Maria wears the No 7 shirt at Manchester United following his ?60million from Real Madrid last summer. A new picture has been revealed on Twitter of Di Maria&apos;s latest piece of body art -the number seven which stands out strongly among others on his left arm. United&apos;s club record ?60million signing of course adorns the No 7 shirt at the Red Devils -so could his latest tattoo suggest he&apos;s committed to Louis van Gaal&apos;s side for the long haul? However, before United fans get too carried away it must be noted that the former Real Madrid star does also wear the No 7 jersey for Argentina too</title>
	</analytic>
	<monogr>
		<title level="m">this confirmation that Angel Di Maria is happy as a Manchester United player? The 27-year-old has endured a mixed start to his United career on-and-off the pitch since joining the club last summer -which has included an attempted burglary at his family home in Cheshire back in February</title>
		<imprint/>
	</monogr>
	<note>The midfielder has been linked with a move away from Old Trafford as a result, but speculation about his future could be squashed following his latest tattoo. As well as adorning the No 7 shirt at United, 27-year-old (right) also wears that number for Argentina too</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The 27-year-old has the No 7 shirt at Manchester United on his left arm. The Argentine has endured a mixed start to his United career. He has been linked with a move away from Old Trafford as a result</title>
	</analytic>
	<monogr>
		<title level="m">Beam #1 Summary Angel Di Maria has revealed his latest tattoo on Twitter</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 38.6364, R-2: 18.6047, R-L: 34.0909 // Rank: 15 Re-ranking SummaReranker score: 0.1577 // SummaReranker score rank</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">shirt at Manchester United. The 27-year-old has endured a mixed start to his United career. The midfielder has been linked with a move away from Old Trafford. Di Maria also wears the No 7shirt for Argentina too</title>
		<editor>Beam #2 Summary Angel Di Maria</editor>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 59.0909, R-2: 34.8837, R-L: 56.8182 // Rank: 5 Re-ranking SummaReranker score: 0.3905 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The 27-year-old joined Manchester United for a club record ?60million last summer. Di Maria also wears the No 7 shirt for Argentina</title>
		<idno>scores R-1: 61.1765, R-2: 33.7349, R-L: 58.8235 // Rank: 4 Re-ranking SummaReranker score: 0.4447 // SummaReranker rank: 5</idno>
	</analytic>
	<monogr>
		<title level="m">The number stands out strongly among others on his arm</title>
		<editor>Beam #3 Summary Angel Di Maria</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Angel Di Maria wears the number seven shirt at Old Trafford. The 27-year-old has endured a mixed start to his United career. He has been linked with a move away from Old Trafford as a result</title>
	</analytic>
	<monogr>
		<title level="m">Beam #4 Summary Manchester United&apos;s record signing has a new No 7 tattoo on his left arm</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 37.7778, R-2: 15.9091, R-L: 37.7778 // Rank: 14 Re-ranking SummaReranker score: 0.2528 // SummaReranker score</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The 27-year-old has endured a mixed start to his United career. The midfielder has been linked with a move away from Old Trafford. Di Maria joined United for a club record ?60million from Real Madrid. Reference scores R-1: 53</title>
		<idno>R-2: 27.2727, R-L: 48.8889 // Rank: 10 Re-ranking SummaReranker score: 0.2377 // SummaReranker rank: 12</idno>
	</analytic>
	<monogr>
		<title level="m">Beam #5 Summary Angel di Maria&apos;s latest tattoo shows him with the No 7 shirt at Manchester United</title>
		<imprint>
			<biblScope unit="volume">3333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The number stands out strongly among others on his arm. Di Maria joined Manchester United for a club record ?60million last summe$. The 27-year-old does also wear the No 7 shirt for Argentina too</title>
		<editor>Beam #6 Summary Argentina star Angel Di Maria</editor>
		<imprint/>
	</monogr>
	<note>Reference scores R-1: 61.3636, R-2: 37.2093, R-L: 59.0909 // Rank: 2 Re-ranking SummaReranker score: 0.3058 // SummaReranker rank: 8</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Beam #7 Summary Manchester United&apos;s Angel Di Maria has had a new No 7 tattoo on his left arm. The 27-year-old&apos;s latest body art was revealed on Twitter</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Di Maria wears the No 7 shirt at Old Trafford following his ?60million move from Real Madrid last summer. Reference scores R-1: 56</title>
		<idno>R-2: 31.3253, R-L: 47.0588 // Rank: 8 Re-ranking SummaReranker score: 0.8853 // SummaReranker rank: 2</idno>
		<imprint>
			<biblScope unit="volume">4706</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Angel Di Maria has been linked with a move away from Old Trafford in recent weeks. Di Maria wears the No 7 shirt at United following his ?60million move from Real Madrid last summer</title>
	</analytic>
	<monogr>
		<title level="m">Beam #8 Summary The Manchester United star has revealed his latest tattoo on Twitter</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 48.7805, R-2: 25.0000, R-L: 43.9024 // Rank: 12 Re-ranking SummaReranker score: 0.2473 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Manchester United&apos;s Angel Di Maria has had a new No 7 tattoo on his left arm. The 27-year-old&apos;s latest body art was revealed on Twitter</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Di Maria wears the No 7 shirt at Old Trafford following his ?60million move from Real Madrid last summer. The Argentine also wears the number for Argentina too</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 61.7021, R-2: 34.7826, R-L: 53.1915 // Rank: 6 Re-ranking SummaReranker score: 0.9135 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Beam #10 Summary The Manchester United star has revealed his latest tattoo on Twitter. Angel Di Maria has been linked with a move away from Old Trafford in recent weeks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Di Maria wears the No 7 shirt at United following his ?60million move from Real Madrid last summer. The Argentine also wears the number for Argentina too</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 54.9451, R-2: 29.2135, R-L: 50.5495 // Rank: 9 Re-ranking SummaReranker score: 0.1829 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Beam #11 Summary Man Utd star Angel Di Maria has revealed his latest tattoo on Twitter. The 27-year-old has the No 7 shirt at Manchester United on his left arm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Di Maria joined United for a club record ?60million from Real Madrid last summer. The Argentine also wears the No 7 shirt for Argentina too</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 68.1319, R-2: 40.4494, R-L: 61.5385 // Rank: 1 Re-ranking SummaReranker score: 0.3383 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">The number stands out strongly among others on his left arm</title>
		<editor>Beam #12 Summary Manchester United star Angel Di Maria</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Di Maria wears the No 7 shirt at Old Trafford following his ?60million move. The Argentine also wears the number for his country too</title>
		<imprint/>
	</monogr>
	<note>Reference scores R-1: 54.1176, R-2: 24.0964, R-L: 42.3529 // Rank: 11 Re-ranking SummaReranker score: 0.2172 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">shirt at the club on his left arm. The 27-year-old has endured a mixed start to his United career on-and-off the pitch since joining the club last summer. Di Maria has been linked with a move away from Old Trafford as a result</title>
		<idno>R-L: 36.7347 // Rank: 13 Re-ranking SummaReranker score: 0.2782 // SummaReranker rank: 9</idno>
		<editor>Beam #13 Summary Manchester United midfielder Angel Di Maria</editor>
		<imprint>
			<biblScope unit="volume">8333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The 27-year-old has the number seven inked on his left arm. Di Maria joined Manchester United for a club record ?60million last summer. The Argentine also wears the No 7 shirt for Argentina. Reference scores R-1: 58</title>
		<idno>R-2: 35.0000, R-L: 56.0976 // Rank: 7 Re-ranking SummaReranker score: 0.7447 // SummaReranker rank: 3</idno>
	</analytic>
	<monogr>
		<title level="m">Beam #14 Summary Angel Di Maria has revealed his latest tattoo on Twitter</title>
		<imprint>
			<biblScope unit="volume">5366</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The number seven is among others on his left arm. The 27-year-old wears the No 7 shirt at Manchester United. Di Maria joined United for a club record ?60million from Real Madrid last summer. Reference scores R-1: 62</title>
		<idno>R-2: 33.3333, R-L: 58.1395 // Rank: 3 Re-ranking SummaReranker score: 0.4988 // SummaReranker rank: 4</idno>
		<editor>Beam #15 Summary Angel di Maria</editor>
		<imprint>
			<biblScope unit="volume">7907</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reference Angel di Maria joined Manchester United from Real Madrid for ?60million. Di Maria took the No 7 shirt upon his arrival at the English giants. 27-year-old also wears the No 7 jersey for Argentina too</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Since the 1920s, Turkey has had a secular constitution with no state religion. The opposition have accused President Recep Tayyip Erdogan and his Islamist-rooted Justice and Development Party (AKP) of trying to reinterpret secularism. However, public debate has also evolved to accept the hijab as an expression of individual liberties, correspondents say. No strong opposition has been voiced against this latest move. President Erdogan has long embraced Turks&apos; right to express their religious beliefs openly, but he says he is committed to secularism. In 2010, the country&apos;s universities abandoned an official ban on Muslim headscarves</title>
	</analytic>
	<monogr>
		<title level="m">Diverse beam search summary candidates of a base PEGASUS and their ground truth and SummaReranker re-ranking scores on CNN/DM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note>XSum Source Female officers will be able to wear a headscarf under their caps or berets, provided it is plain and is the same colour as the uniform. Headscarf bans on university campuses and state institutions -except for the judiciary, military and police -have also been lifted in recent years. The garment has been controversial in Turkey for years. Secularists regard it as a symbol of religious conservatism. Three years later, women were allowed to wear headscarves in state institutions -with the exception of the judiciary, military and police. That year, four MPs wore headscarves in parliament. Most people in Turkey are Sunni Muslims</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">R-L: 61.5385 // Rank: 2 Re-ranking SummaReranker score: 0.8562 // SummaReranker rank: 2 Beam #3 Summary The Turkish authorities have lifted a ban on female police officers wearing headscarves, state media report. Reference scores R-1: 53</title>
		<idno>scores R-1: 50.0000, R-2: 18.1818, R-L: 50.0000 // Rank: 12 Re-ranking SummaReranker score: 0.6919 // SummaReranker rank: 10 Beam #9</idno>
	</analytic>
	<monogr>
		<title level="m">Reference scores R-1: 58.3333, R-2: 36.3636, R-L: 50.0000 // Rank: 5 Re-ranking SummaReranker score: 0.7104 // SummaReranker rank: 8 Beam #6 Summary The Turkish authorities have lifted a ban on police officers wearing headscarves</title>
		<imprint>
			<biblScope unit="volume">6667</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Re-ranking SummaReranker score: 0.9019 // SummaReranker rank: 1 Beam #8 Summary Turkey&apos;s police force has lifted its ban on female officers wearing headscarves. Reference. Summary Turkey&apos;s police force has lifted a ban on female officers wearing headscarves. Reference scores R-1: 58.3333, R-2: 36.3636, R-L: 58.3333 // Rank: 4 Re-ranking SummaReranker score: 0.8103 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Beam #10 Summary Turkey&apos;s police force has lifted its ban on female officers wearing headscarves, officials say</title>
		<idno>R-L: 46.1538 // Rank: 13 Re-ranking SummaReranker score: 0.5066 // SummaReranker rank: 15</idno>
		<imprint>
			<biblScope unit="volume">6667</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 46.1538, R-2: 16</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Beam #11 Summary The Turkish government has lifted a ban on female police officers wearing headscarves, state media report. Reference scores R-1: 51</title>
		<idno>R-2: 32.0000, R-L: 44.4444 // Rank: 9 Re-ranking SummaReranker score: 0.6522 // SummaReranker rank: 13</idno>
		<imprint>
			<biblScope unit="volume">8519</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Beam #12 Summary Turkey&apos;s police force has lifted a ban on female officers wearing headscarves, state media report. Reference scores R-1: 51</title>
		<idno>R-2: 32.0000, R-L: 51.8519 // Rank: 7 Re-ranking SummaReranker score: 0.7819 // SummaReranker rank: 6</idno>
		<imprint>
			<biblScope unit="volume">8519</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reference scores R-1: 54</title>
		<idno>R-2: 30.0000, R-L: 54.5455 // Rank: 6 Re-ranking SummaReranker score: 0.8140 // SummaReranker rank: 4</idno>
	</analytic>
	<monogr>
		<title level="m">Beam #13 Summary Turkey has lifted its ban on female police officers wearing headscarves</title>
		<imprint>
			<biblScope unit="volume">5455</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Beam #14 Summary Turkey has lifted a ban on female police officers wearing headscarves, the interior ministry has said</title>
		<idno>Reference scores R-1: 59.2593, R-2: 40.0000, R-L: 59.2593 // Rank: 3 Re-ranking SummaReranker score: 0.8298 // SummaReranker rank: 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Beam #15 Summary Turkey&apos;s police force has lifted its ban on female officers wearing headscarves, state media report. Reference scores R-1: 44</title>
		<idno>R-2: 16.0000, R-L: 44.4444 // Rank: 15 Re-ranking SummaReranker score: 0.6728 // SummaReranker rank: 11</idno>
		<imprint>
			<biblScope unit="volume">4444</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reference Turkey has lifted a ban on police women wearing the Islamic headscarf</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">pay another 2000aud and my wife submits her application. 3 days after my submission, i get an e-mail saying a decision has been made, yay? more like nay, refused because we don&apos;t accept pearson&apos;s. 2 days later, wife gets refuses as well because we refused your husband. 2000 down the drain, luckily they are refunding the healthcare extras. turns out i failed to go back to that webpage and click on the link to get access to the most ridiculous list of approved english tests. there&apos;s not a mention of pearson, what&apos;s more, it doesn&apos;t have idp melbourne so my ielts exams are useless as well. on the plus side, i learn there are other ways to meet the requirement. if you have a diploma from an australian uni, just submit your diploma and you&apos;re good to go? why didn&apos;t you give me a call or send me an email and say &quot;hey, you made a mistake silly&quot; and i&apos;d be like here&apos;s my diploma or something, wtf is refusing? this will stick to our international travel cv like a fucking bug. plus, i had to ask my future employer for another certificate, which takes another two weeks and makes me look like an idiot. then i&apos;ll have to submit another application, pay another 4000 and yada yada. A bit of background: we are turkish citizens living in australia as permanent residents obtained by using the same pearson&apos;s test. i&apos;m working as a post-doctoral researcher and hopefully starting another post-doctoral position in scotland, not giving up! Beam #1 Summary i didn&apos;t go back to the page that says you have to score a cefr level of b1 to meet the english requirement</title>
		<idno>scores R-1: 7.5472, R-2: 0.0000, R-L: 7.5472 // Rank: 8 Re-ranking SummaReranker score: 0.8890 // SummaReranker rank: 8</idno>
	</analytic>
	<monogr>
		<title level="m">Beam search summary candidates of a base PEGASUS and their ground truth and SummaReranker re-ranking scores on XSum. Reddit TIFU Source here&apos;s my reconstruction of the fuck-up: during the visa application, i&apos;m sifting through pages and pages of documentation with 15 tabs open on my browser and i arrive at a page with the title english requirement. it says something like &quot;here&apos;s a list of approved test providers and you have to score a minimum cefr level of b1 to meet the english requirement</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Beam #8 Summary i didn&apos;t go back to the page that says you have to score a minimum cefr level of b1 to meet the english requirement and now i&apos;m stuck in a foreign country. Reference scores R-1: 7.2727, R-2: 0.0000, R-L: 7.2727 // Rank: 10 Re-ranking SummaReranker score: 0.8889 // SummaReranker rank: 9 Beam #9 Summary i didn&apos;t go back to the page that says you have to score a cefr level of b1 to meet the english requirement on my visa application and now i&apos;m stuck in australia. Reference scores R-1: 7.2727, R-2: 0.0000, R-L: 7.2727 // Rank: 10 Re-ranking SummaReranker score: 0.8941 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Beam #10 Summary i didn&apos;t go back to the page that says you have to score a minimum cefr level of b1 to meet the english requirement on my visa application and now i&apos;m stuck in australia</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 21.4286, R-2: 3.7037, R-L: 17.8571 // Rank: 1 Re-ranking SummaReranker score: 0.9052 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Beam #11 Summary i didn&apos;t go back to the page that says you have to score a minimum cefr level of b1 to meet the english requirement on my visa application and now i&apos;m stuck in australia</title>
		<imprint/>
	</monogr>
	<note>Reference scores R-1: 21.0526, R-2: 3.6364, R-L: 17.5439 // Rank: 2 Re-ranking SummaReranker score: 0.9045 // SummaReranker rank: 2</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Beam #12 Summary i didn&apos;t go back to the page that says you have to score a minimum cefr level of b1 to meet the english requirement and now i&apos;ll have to submit another application, pay 4000 and look like an idiot</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 12.9032, R-2: 0.0000, R-L: 12.9032 // Rank: 6 Re-ranking SummaReranker score: 0.8861 // SummaReranker ran</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beam #13 Summary i didn&apos;t go back to the page that says you have to score a cefr level of b1 to meet the english requirement and now i&apos;m going to have to submit another application and pay 4000</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 13.5593, R-2: 0.0000, R-L: 13.5593 // Rank: 3 Re-ranking SummaReranker score: 0.8994 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Beam #14 Summary i didn&apos;t go back to the page that says you have to score a minimum cefr level of b1 to meet the english requirement and now i&apos;m going to have to submit another application and pay 4000</title>
		<idno>R-2: 0.0000, R-L: 13.3333 // Rank: 4 Re-ranking SummaReranker score: 0.8947 // SummaReranker rank: 6</idno>
		<imprint>
			<biblScope unit="volume">3333</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 13</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Beam #15 Summary i didn&apos;t go back to the page that says you have to score a minimum cefr level of b1 to meet the english requirement and now i&apos;m going to have to submit another application and pay 4000 dollars</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Reference scores R-1: 13.1148, R-2: 0.0000, R-L: 13.1148 // Rank: 5 Re-ranking SummaReranker score: 0.8964 // SummaReranker rank</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reference made a silly mistake and got refused on 2x tier 2 uk visa applications for me and my partner costing</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
