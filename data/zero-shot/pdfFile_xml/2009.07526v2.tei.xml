<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Chai</surname></persName>
							<email>chaiyuan@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of ASEE</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Wang</surname></persName>
							<email>yujwang@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Perception</orgName>
								<orgName type="department" key="dep2">MOE</orgName>
								<orgName type="department" key="dep3">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
							<email>huyue@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of balancing data distribution or learning unbiased models and representations, ignoring the correlations among the biased classes. In this work, we analyze this problem from a novel cognition perspective: automatically building a hierarchical cognitive structure from the biased predictions and navigating that hierarchy to locate the relationships, making the tail relationships receive more attention in a coarse-to-fine mode. To this end, we propose a novel debiasing Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small portion of easily confused ones. Then, we propose a debiasing loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships. The loss is modelagnostic and consistently boosting the performance of several state-of-the-art models. The code is available at: https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of balancing data distribution or learning unbiased models and representations, ignoring the correlations among the biased classes. In this work, we analyze this problem from a novel cognition perspective: automatically building a hierarchical cognitive structure from the biased predictions and navigating that hierarchy to locate the relationships, making the tail relationships receive more attention in a coarse-to-fine mode. To this end, we propose a novel debiasing Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small portion of easily confused ones. Then, we propose a debiasing loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships. The loss is modelagnostic and consistently boosting the performance of several state-of-the-art models. The code is available at: https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Making abstraction from an image into high-level semantics is one of the most remarkable capabilities of humans. Scene Graph Generation (SGG) <ref type="bibr" target="#b8">[Xu et al., 2017</ref>] ? a task of extracting objects and their relationships in an image to form a graphical representation ? aims to achieve the abstraction capability and bridge the gap between vision and language. SGG has greatly benefited the down-stream tasks of question answering <ref type="bibr" target="#b5">[Norcliffe-Brown et al., 2018</ref>    and visual understanding <ref type="bibr" target="#b2">Jiang et al., 2020]</ref>. Some works <ref type="bibr" target="#b2">Jiang et al., 2020]</ref> feed the scene graphs into graph neural networks for relation-aware object representation. Some others <ref type="bibr" target="#b2">[Hudson and Manning, 2019]</ref> perform sequential reasoning by traversing the relational graphs. Compared with independent objects, the rich relationships benefit explainable reasoning. Although much effort has been made with high accuracy in object detection, the detected relationships are far from satisfaction due to the long-tailed data distribution. Only a small portion of the relationships have abundant samples (head) while most ones contain just a few (tail). This heavily biased training data causes biased relationship prediction. Tail relationships will be mostly predicted into head classes, which are not discriminative enough for high-level reasoning, such as falsely predicting on instead of looking at, and coarsely predicting on instead of walking on. Besides, tail relationships like standing on, sitting on and lying on can be even harder to distinguish from each other due to their visual similarity and scarce training data.</p><p>To tackle this problem, most research focuses on learning unbiased models by re-weighting losses <ref type="bibr" target="#b8">[Zareian et al., 2020]</ref> or disentangling unbiased representations <ref type="bibr" target="#b8">[Tang et al., 2020]</ref>. However, humans can effectively infer the correct relationships even when some relationships appear more frequently than others. The essential difference between human and AI systems that has been ignored lies neither in the learning strategy nor feature representation, but in the way that the concepts are organized. To illustrate this discrepancy, we show an example truck parked on street in <ref type="figure" target="#fig_1">Figure 1</ref>. Existing models treat all the relationships independently for flat classification <ref type="figure" target="#fig_1">(Figure 1(a)</ref>). Because of the data bias, the head relationships, e.g. on and near, obtain high predicted probability. In contrast, the cognition theory [Sarafyazd and Jazayeri, 2019] supports that humans process information hierarchically in the prefrontal cortex (PFC). We intuitively start from making a rough distinction between remarkably different relationships. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b), relationships belonging to the concept "on", e.g. on and parked on, will firstly be distinguished from the ones about "near", e.g. near and behind; then we move to distinguish the slight discrepancy among easily confused ones in one concept, e.g. parked on, standing on and walking on. Inspired by the hierarchical reasoning mechanism in PFC, we propose a novel loss function, Cognition Tree (CogTree) loss, for unbiased scene graph generation. We first propose to build a hierarchy of the relationships, imitating the knowledge structure built from the independent relationships in human mind. The CogTree is derived from the prediction of a biased SGG model that satisfies the aforementioned thinking principles: distinguishing remarkably different relationships at first and then focusing on a small portion of easily confused ones. Then we design a CogTree-based loss to train the SGG network from scratch. This loss enables the network to surpass the noises from inter-concept relationships and then intra-concept relationships progressively. It frees the SGG models from the burden of distinguishing detailed discrepancy among all the relationships at one time, resulting in more accurate prediction via this coarse-to-fine strategy.</p><p>The main contributions are summarized as follows: (1) We propose a debiasing loss based on the inductive hierarchical structure, which supports coarse-to-fine distinction for the correct relationships while progressively eliminating the interference of irrelevant ones. (2) We automatically build the cognitive structure of the relationships from the biased SGG predictions, which reveals the inherent hierarchy that the relationships are organized after biased training; (3) The proposed loss is model-agnostic and can be applied to several representative SGG models. The results of extensive experiments indicate that the CogTree loss remarkably alleviates the bias in SGG and our loss achieves a new state-of-the-art performance compared with existing debiasing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Scene Graph Generation. SGG <ref type="bibr" target="#b8">[Xu et al., 2017]</ref> products graphical abstraction of an image and encourages visual relational reasoning and understanding in various down-stream tasks . Early works focus on object detection and relationship detection via independent networks <ref type="bibr" target="#b5">[Lu et al., 2016;</ref><ref type="bibr" target="#b8">Zhang et al., 2017]</ref>, ignoring the rich contextual information. To incorporate the global visual context, recent works leverage message passing mechanism <ref type="bibr" target="#b8">[Xu et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2019a]</ref>, recurrent sequential architectures <ref type="bibr" target="#b8">[Zellers et al., 2018;</ref> and contrastive learning  for more discriminative object and relationship representations. Although the accuracy is high in object detection, the relationship detection is far from satisfaction due to the heavily biased data. <ref type="bibr" target="#b1">[Chen et al., 2019b;</ref> consider the biased SGG problem and propose mean Recall as the unbiased metric without corresponding debiasing solutions. The recent work <ref type="bibr" target="#b1">[Liang et al., 2019]</ref> prunes the predominant relationships and keeps the tail but informative ones in the dataset. <ref type="bibr" target="#b8">[Tang et al., 2020]</ref> proposes the first solution for unbiased SGG by counterfactual surgeries on causal graphs. We rethink SGG task from the cognition view and novelly solve the debiasing problem based on the coarse-to-fine structure of the relationships. Biased Classification. Classification on highly-biased training data has been extensively studied in previous work, which can be divided into three categories: (1) balancing data distribution by data augmentation or re-sampling <ref type="bibr" target="#b0">[Burnaev et al., 2015;</ref><ref type="bibr" target="#b3">Li and Vasconcelos, 2019]</ref>; (2) debiasing learning strategies by re-weighting losses or training curriculums <ref type="bibr" target="#b5">[Lin et al., 2017b;</ref><ref type="bibr" target="#b2">Cui et al., 2019]</ref>; (3) separating biased representations from the unbiased for prediction <ref type="bibr" target="#b1">[Cadene et al., 2019;</ref><ref type="bibr" target="#b8">Tang et al., 2020]</ref>. Our CogTree loss belongs to the second category but differs from existing methods in that we are the first to leverage the hierarchical structure inherent in the relationships for re-weighting, which enables more discriminative representation learning by a coarse-to-fine mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We first introduce the automatic CogTree building process. Then a novel debiasing loss based on the tree structure is proposed to train the SGG models. Since the CogTree loss is model-agnostic and applicable to various SGG models, we case-study two representative models, i.e. the widely compared MOTIFS <ref type="bibr" target="#b8">[Zellers et al., 2018]</ref> and the state-of-the-art VCTree , and a newly proposed model, which first applies the transformer <ref type="bibr" target="#b8">[Vaswani et al., 2017]</ref> architecture to the SGG scenario and achieves superior performance. We name it as SG-Transformer and take it as a strong baseline. The framework is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic CogTree Building</head><p>Once the SGG model has been trained on the long-tailed data, the model tends to be biased on predicting "head" relationships. When different samples are predicted with the same relationship, they mostly share similar properties on either visual appearance (e.g. walking on and standing on) or high-level semantics (e.g. has and with). We regard these relationships sharing common properties as in one concept. We automatically induce concepts from the biased predictions and re-organize the relationships based on the concepts by a tree structure, denoted as Cognition Tree (CogTree). As shown on the top of <ref type="figure">Figure 2</ref>, this automatic process contains the following three steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Generation Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias-Based CogTree Building</head><p>Step 1: Bias-Adaptive Concept Induction</p><p>Step 2: Concept-Centered Subtree Building</p><p>Step <ref type="formula">3</ref>  <ref type="figure">Figure 2</ref>: The overview of CogTree loss applied to SGG models. It contains three parts: Scene Graph Generation Network summarizes the framework of biased SGG models; Bias-Based CogTree Building organizes relationships by a coarse-to-fine tree structure based on biased predictions; Debiasing CogTree Loss supports network to distinguish relationships hierarchically.</p><p>Step 1: Bias-Based Concept Induction. For all the samples in the ground-truth class d i , we predict their labels via a biased model and calculate the distribution of predicted label frequency, denoted as P i . The most frequently predicted class d j is regarded as the concept relationship of the ground-truth class d i . As shown in <ref type="figure">Figure 2(</ref>Step 1), on is the concept relationship of itself, standing on and walking on. The above operation induces all the relationships into C concepts with corresponding concept relationships {c i } C .</p><p>Step 2: Concept-Centered Subtree Building. We reorganize all the relationships in each concept by a Concept-Centered Subtree. For the i th subtree, the root is c i while the leaves are the fine-grained relationships induced in c i . Note that, if a subtree only contains a root c i , e.g. parked on in <ref type="figure">Figure 2</ref> (Step 2(A)), then c i is considered not representative enough to summarize common properties. We further induce it to the most approximate concept relationship, which has the second highest frequency in P i , e.g. induce parked on to on in <ref type="figure">Figure 2 (Step 2(B)</ref>). This process outputs T subtrees.</p><p>Step 3: Cognition-Based Subtree Aggregation. We aggregate the T subtrees into one hierarchical CogTree. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, CogTree contains four functional layers and each layer induces relationships into coarser groups than the layer that comes after it. Specifically, the Root Layer (y 0 ) contains a virtual node. The Concept Layer (y 1 ) distinguishes which concept the input belongs to. It contains T virtual nodes, each representing a subtree induced in Step 2. The following Coarse-fine Layer (y 2 ) tells whether the input can be described by a coarse-grained or fine-grained relationship in a certain concept. To this end, y 2 splits each node in y 1 into two nodes: one leaf node indicating the concept relationship while the other virtual node representing the cluster of fine-grained relationships in that concept. Each virtual node in y 2 links to its fine-grained relationships of the corresponding concept in the Fine-grained Layer (y 3 ). y 3 focuses on distinguishing the slight discrepancy among easily confused relationships, e.g. standing on and walking on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Debiasing CogTree Loss</head><p>We propose a CogTree loss for training the SGG model for unbiased predictions. This loss encourages the model to classify relationships from coarse to fine according to the CogTree structure (see the bottom right of <ref type="figure">Figure 2</ref>). To better make the loss functions <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref> understood, several notations are provided in advance as below: Ground-Truth CogTree Path (GCP). It denotes the groundtruth classification path in CogTree for a training sample. Given the ground-truth label of a sample, we track the path from the root to the leaf with that label in CogTree and denote this path as the ground-truth path L path = {l 0 , l 1 , ...l M }, where the k th value l m is the ground-truth node at layer y m . Predicted CogTree Probability (PCP). It means the predicted probability of each node in CogTree for a given sample by a biased SGG model. We denote the predicted probability set of all the classes as P pred and denote the probability of class k as p k . The PCP of node i is defined as below:</p><formula xml:id="formula_0">z i = p k if i is a leaf and class(i) = k 1 |C(i)| j?C(i) z j if i is not a leaf</formula><p>(1) where C(i) means the children of node i. class(i) denotes the corresponding class of node i. Class-Balanced Weight (CBW). It denotes the balance weight of each node in CogTree. Since the success of reweighting strategy for debiasing, we adopt a well performed weighting factor <ref type="bibr" target="#b2">[Cui et al., 2019]</ref> to compute CBW of each leaf and uniformly define CBW of each node as below:</p><formula xml:id="formula_1">w i = (1 ? ?)/(1 ? ? ni ) if i is a leaf 1 |C(i)| j?C(i) w j if i is not a leaf (2)</formula><p>where ? is a hyper-parameter in the range of [0, 1) and n i is the sample number of the corresponding class of leaf i. benefits: CogTree-based class-balanced (TCB) loss for debiasing by coarse-to-fine relationship classification and classbalanced (CB) loss for debiasing by re-weighting on softmax cross-entropy loss. The two loss terms are defined below. Given a sample with the ground-truth path L path , as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, we compute the class-balanced softmax cross-entropy loss on each layer of CogTree, and average the results to obtain the TCB loss as below:</p><formula xml:id="formula_2">L T CB = 1 |L path | i?L path ?w i log( exp(z i ) zj ?B(i) exp(z j ) ) (3)</formula><p>where B(i) means the brothers of node i. TCB loss forces the network to surpass the noises from inter-concept relationships and learn concept-specific embeddings first, and then surpass the noises within one concept to refine relationship-specific embeddings, resulting in more fine-grained predictions. Given a sample with the ground-truth class k, CB loss computes the class-balanced softmax cross-entropy based on the biased predicted probabilities P pred :</p><formula xml:id="formula_3">L CB = ?w k log( exp(p k ) pj ?P pred exp(p j ) )<label>(4)</label></formula><p>where w k is the weighting factor <ref type="bibr" target="#b2">[Cui et al., 2019]</ref> for class k. Empirically, we found it the best to combine TCB with CB. In conclusion, our CogTree loss is:</p><formula xml:id="formula_4">L = L CB + ?L T CB<label>(5)</label></formula><p>where ? = 1 is the balancing weight. The SGG model is trained from scratch by the CogTree loss and predicting results via flat multi-class classifier without modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scene Graph Generation Models</head><p>We case-study on three SGG models, i.e. MOTIFS, VCTree and our SG-Transformer. As shown in the bottom left of <ref type="figure">Figure</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head><p>Multi-Head Attention </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset: We evaluate our models on the widely compared Visual Genome (VG) split <ref type="bibr" target="#b8">[Xu et al., 2017]</ref>, with the 150 most frequent object classes and 50 most frequent relationship classes in VG . The VG split only contains training set and test set and we follow <ref type="bibr" target="#b8">[Zellers et al., 2018]</ref> to sample a 5K validation set from the training set. Tasks and Evaluation: The SGG task contains three subtasks <ref type="bibr" target="#b8">[Zellers et al., 2018]</ref>: (1) Predicate Classification (PredCls) takes the ground-truth object labels and bounding boxes for relationship prediction; (2) Scene Graph Classification (SGCls) takes ground-truth bounding boxes for object and relationship prediction; (3) Scene Graph Detection (SGDet) predict SGs from scratch. The conventional metric Recall@K (R@K) is reported but not considered as the main evaluation due to its bias. We adopt the recently proposed balanced metric mean Recall@K (mR@K) <ref type="bibr" target="#b1">[Chen et al., 2019b;</ref>. mR@K calculates R@K for each class independently, and then average the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11.8</head><p>PredCls mR@100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGCls mR@100</head><p>SGDet mR@100 <ref type="figure">Figure 5</ref>: The pie chart shows the relationships that are correctly predicted by the baseline but regarded as incorrect by CogTree. The predictions by CogTree are shown under the pie chart. The increase of mR@K bringing a drop of R@K is commonly existed (see <ref type="table" target="#tab_2">Table  1</ref>). Combining with the qualitative analysis, we believe that the contradiction of R@K and mR@K is caused by two reasons: (1) annotators prefer simple and vague labels due to bounded rationality <ref type="bibr" target="#b7">[Simon, 1990</ref>];</p><p>(2) CogTree predicts more fine-grained relationships. Our analysis is consistent with the recent study <ref type="bibr" target="#b8">[Tang et al., 2020]</ref>.</p><p>Implementation: The object detector is the pre-trained Faster R-CNN <ref type="bibr">[Ren et al., 2015]</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b4">[Lin et al., 2017a]</ref>. ? is set to 1 and ? is set to 0.999. SG-Transformer contains 3 O2O blocks and 2 R2O blocks with 12 attention heads. Models are trained by SGD optimizer with 5 epochs. The mini-batch size is 12 and the learning rate is 1.2 ? 10 ?3 . Experiments are implemented with PyTorch and conducted with NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">State-of-the-Art Comparison</head><p>We evaluate the CogTree loss on three models: MOTIFS, VCTree and SG-Transformer, and compare the performance with the state-of-the-art debiasing approach TDE <ref type="bibr" target="#b8">[Tang et al., 2020]</ref>. We also compare with the biased models,   sistently outperforms conventional debiasing methods: focal loss, reweight and resample, and the SOTA debiasing method TDE. Wherein, mR@20 achieves superior improvement compared with mR@50/100, which indicates that more accurate predictions have higher rank by CogTree. It is worth highlighting that CogTree outperforms TDE on both mR@K and most metrics of R@K on the strongest baseline VCTree. Analysis of Labeling Bias. We observe a drop of R@K with the increase of mR@K from baselines to CogTree. Further analysis in <ref type="figure">Figure 5</ref> reveals that annotators prefer simple and vague labels, which form the head classes. The baselines are biased on these head classes and result in high R@K.</p><p>CogTree prefers fine-grained and semantic-rich tail labels, causing the low R@K by "incorrectly" classifying the head classes into reasonable tail ones. To fairly compare with baselines on the conventional metric R@K, we compute R@K of the tail 45 classes from the predictions of all the classes. We observe a significant improvement on R@K from baselines to CogTree in <ref type="table" target="#tab_4">Table 2</ref>, which proves the strong debiasing ability of CogTree without sacrificing the performance of R@K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We investigate each loss terms, the weighting method, two tree building strategies and two methods for computing PCP and CBW: 1) L TCB : tree-based classficition loss term in Eq. 3; 2) L CB : flat classification loss term in Eq. 4; 3) L TCE : we remove the weights {w i } from Eq. 3; 4) L CE : we remove the weight w k from Eq. 4; 5) Fuse-subtree: we mix relationships belonging to different concepts in one subtree; 6) Fuse-layer: we mix the fine-grained and coarse relationships in the same layer of each subtree; 7) L(MAX): we use MAX instead of AVERAGE to compute z i and w i for non-leaf nodes in Eq.1 and Eq.2; 8) L(SUM): we use SUM instead of AVERAGE. The results in <ref type="table" target="#tab_5">Table 3</ref> on the SG-Transformer baseline indicate that: 1) L T CB and L CB have complementary benefits for SGG, where L T CB has obvious greater influence. 2) Both of the loss terms benefit from the weighting strategy. 3) The CogTree is built based on two principles: a) Relationships belonging to the same concept are organized in one subtree and b) relationships in one subtree are organized in different layers from coarse to fine. A significant performance decrease is observed when the tree violates either of the principles. 4) AVERAGE performs the best because that both MAX and SUM increase the predicted probabilities and decrease the incorrect prediction penalty in Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>We visualize several PredCls samples in <ref type="figure" target="#fig_4">Figure 6</ref>. SG-Transformer achieves obvious improvement when equipped with CogTree: (1) CogTree predicts more fine-grained relationships. As shown in the first two rows, the baseline mostly predicts the vague head classes on and near while CogTree predicts actions and locations like parked on and in front of. (2) CogTree accurately distinguishes visually and semantically similar relationships as shown in the last row. However, the baseline falsely predicts walking on as standing on and predicts in front of as behind, since flat classification cannot capture the detailed discrepancy. In summary, CogTree preferences towards fine-grained relationships instead of the vaguely biased ones.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Analysis</head><p>In <ref type="table" target="#tab_7">Table 4</ref>, we assess the effect of ? in Eq.5. ? = 1 achieves the best performance on most metrics. The performance drops slightly in the range of [0.7, 1.3]. We also vary the number of O2O and R2O blocks from 2 to 4 in SG-Transformer and obtain the highest mR@K with 3 O2O blocks and 2 R2O blocks. We use the above settings in our full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a CogTree loss to generate unbiased scene graphs with highly biased data. We novelly leverage the biased prediction from SGG models to organize the independent relationships by a tree structure, which contains multiple layers corresponding to the relationships from coarse to fine. We propose a CogTree loss specially for the above tree structure that supports hierarchical distinction for the correct relationships while progressively eliminating the irrelevant ones. The loss is model-agnostic and consistently boosting the performance of various SGG models with remarkable improvement. How to incorporate commonsense knowledge to optimize the CogTree structure will be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>; * Equal contribution. ? Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) SGG model with conventional flat loss. (b) SGG model with our proposed cognition tree loss. The word size in the top-left box is proportional to the relationship frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CogTree Loss. It contains two parts for complementary Illustration of calculating the TCB loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The encoder structure in SG-Transformer. of features are concatenated and fed into Encoder o to obtain the refined object embedding m i . m i is then decoded by Decoder o to predict its final object label l o i . MOTIFS implemented Encoder o as a Bi-LSTM and adopts an LSTM as Decoder o . The relationship encoder Encoder r first adopts another Bi-LSTM to capture the contextual information and then fuses the pairwise object features with the union region feature for decoding. The relationship decoder Decoder r is a fully connected layer followed by a softmax layer. VCTree. Different from MOTIFS, VCTree utilizes two independent TreeLSTMs as Encoder o and Encoder r , respectively. It additionally takes the bounding box pair feature as another input of Decoder r . SG-Transformer. It adopts the transformer architecture as Encoder o and Encoder r by adjusting inputs for the SGG scenario. As shown in Figure 4, Encoder o contains N object-to-object (O2O) transformer blocks with the input of object embeddings {v i } K . Encoder r consists of M relationto-object (R2O) transformer blocks with the input of relationship emebddings {r ij } K?K and the object embeddings {m i } K . r ij is a combination of three kinds of features from objects o i and o j : spatial features, union region feature, word embeddings [Pennington et al., 2014] of the predicted object labels. The output from Encoder r is concatenated with m i and m j for decoding. Decoder o and Decoder r contain a fully connected layer followed by a softmax layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of scene graphs generated by SG-Transformer (blue) and SG-Transformer+CogTree (green). Compared with the ground-truth, the quality of predicted relationships are marked in three colors: red (false), blue (correct), purple (better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>: Cognition-Based Subtree Aggregation Input Image Debiasing CogTree Loss On Parked on Standing on Walking on Near In front of Behind A B ? ? ? ? Relation Encoder Relation Decoder Object Encoder Object Decoder Object and Label Embeddings on parked on parked on on on in on walking on on near on near near behind v</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :standing on ?? near in front of behind ?? under between has for ??</head><label>1</label><figDesc>State-of-the-art comparison on R@K and mR@K. Our re-implemented SGG models are denoted by the superscript * .</figDesc><table><row><cell>67.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">near others</cell><cell>29.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">has</cell><cell></cell></row><row><cell>36.8</cell><cell>39.9</cell><cell>36.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>on</cell><cell>15.8</cell><cell>16.1</cell></row><row><cell></cell><cell>22.2</cell><cell cols="2">22.1</cell><cell>on</cell><cell cols="2">sitting on walking on with</cell><cell>laying on</cell><cell>8.5</cell><cell>6.8</cell></row><row><cell>PredCls R@100</cell><cell>SGCls R@100</cell><cell>SGDet R@100</cell><cell cols="2">others</cell><cell cols="2">?? holding</cell><cell>watching</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MOTIFS*</cell><cell cols="2">MOTIFS + CogTree</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>R@K of the tail 45 classes on predicate classification.</figDesc><table><row><cell></cell><cell></cell><cell>PredCls</cell><cell>SGCls</cell><cell>SGDet</cell></row><row><cell cols="2">Method</cell><cell>mR@20 / 50 / 100</cell><cell>mR@20 / 50 / 100</cell><cell>mR@20 / 50 / 100</cell></row><row><cell cols="2">Full loss</cell><cell>22.89 / 28.38 / 30.97</cell><cell>12.96 / 15.68 / 16.72</cell><cell>7.92 / 11.05 / 12.70</cell></row><row><cell>1</cell><cell>LT CB</cell><cell>21.08 / 27.08 / 29.41</cell><cell>12.15 / 15.07 / 16.15</cell><cell>7.70 / 10.39 / 12.07</cell></row><row><cell>2</cell><cell>LCB</cell><cell>18.02 / 23.40 / 25.25</cell><cell>10.76 / 13.13 / 13.88</cell><cell>6.74 / 9.56 / 11.29</cell></row><row><cell>3</cell><cell>LT CE</cell><cell>21.16 / 26.14 / 28.32</cell><cell>12.14 / 14.42 / 15.29</cell><cell>7.57 / 10.53 / 11.86</cell></row><row><cell>4</cell><cell>LCE</cell><cell>14.35 / 18.48 / 20.21</cell><cell>8.57 / 11.46 / 12.27</cell><cell>5.55 / 7.74 / 8.98</cell></row><row><cell>5</cell><cell>Fuse-subtree</cell><cell>16.20 / 20.17 / 22.12</cell><cell>8.71 / 10.66 / 11.61</cell><cell>5.36 / 7.19 / 8.28</cell></row><row><cell>6</cell><cell>Fuse-layer</cell><cell>13.77 / 18.87 / 20.77</cell><cell>8.17 / 10.39 / 11.32</cell><cell>5.86 / 8.02 / 9.05</cell></row><row><cell>7</cell><cell>L(MAX)</cell><cell>15.48 / 19.93 / 21.87</cell><cell>8.97 / 10.85 / 11.83</cell><cell>5.38 / 7.16 / 8.16</cell></row><row><cell>8</cell><cell>L(SUM)</cell><cell>11.31 / 15.67 / 17.98</cell><cell>6.58 / 8.82 / 9.86</cell><cell>1.86 / 3.09 / 3.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of key components in CogTree loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance with different balancing weights.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (Grant No. 62006222).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evgeny Burnaev, Pavel Erofeev, and Artem Papanov. Influence of resampling on accuracy of imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="987521" to="987521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counterfactual critic multi-agent training for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cadene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dualvd: an adaptive dual encoding model for deep visual understanding in visual dialogue</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>Hudson and Manning</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11125" to="11132" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual genome: connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Liang et al., 2019] Yuanzhi Liang, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, and Tao Mei</editor>
		<imprint>
			<publisher>Li and Vasconcelos</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="10403" to="10412" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page">8911</biblScope>
		</imprint>
	</monogr>
	<note>NeurIPS. Sarafyazd and Jazayeri, 2019] Morteza Sarafyazd and Mehrdad Jazayeri. Hierarchical reasoning by neural circuits in the frontal cortex. Science</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Utility and probability</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural motifs: scene graph parsing with global context</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Zhang et al., 2017] Ji Zhang, Mohamed Elhoseiny, Scott Cohen, Walter Chang, and Ahmed Elgammal</editor>
		<meeting><address><addrLine>Andrew Tao, and Bryan Catanzaro</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11535" to="11543" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mucko: multi-layer crossmodal knowledge reasoning for fact-based visual question answering</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1097" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
