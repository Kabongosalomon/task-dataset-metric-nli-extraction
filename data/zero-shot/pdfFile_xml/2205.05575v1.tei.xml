<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DoubleMatch: Improving Semi-Supervised Learning with Self-Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wallin</surname></persName>
							<email>walline@chalmers.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Saab AB</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
							<email>lennart.svensson@chalmers.se</email>
							<affiliation key="aff1">
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
							<email>fredrik.kahl@chalmers.se</email>
							<affiliation key="aff1">
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hammarstrand</surname></persName>
							<email>lars.hammarstrand@chalmers.se</email>
							<affiliation key="aff1">
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DoubleMatch: Improving Semi-Supervised Learning with Self-Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the success of supervised learning, semisupervised learning (SSL) is now becoming increasingly popular. SSL is a family of methods, which in addition to a labeled training set, also use a sizable collection of unlabeled data for fitting a model. Most of the recent successful SSL methods are based on pseudo-labeling approaches: letting confident model predictions act as training labels. While these methods have shown impressive results on many benchmark datasets, a drawback of this approach is that not all unlabeled data are used during training. We propose a new SSL algorithm, DoubleMatch, which combines the pseudo-labeling technique with a self-supervised loss, enabling the model to utilize all unlabeled data in the training process. We show that this method achieves state-of-the-art accuracies on multiple benchmark datasets while also reducing training times compared to existing SSL methods. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Supervised learning has gained much attention in recent years because of remarkable achievements in fields such as image classification <ref type="bibr" target="#b0">[1]</ref>, object detection <ref type="bibr" target="#b1">[2]</ref>, and natural language processing <ref type="bibr" target="#b2">[3]</ref>. The impressive results are typically fueled by vast amounts of labeled data with datasets such as ImageNet <ref type="bibr" target="#b3">[4]</ref> and COCO <ref type="bibr" target="#b4">[5]</ref>. In many practical applications, however, labeled data might be scarce or require expert domain knowledge to attain. In contrast, unlabeled data are often much easier to acquire, e.g., through web scraping or unsupervised sensor recordings, thus creating a natural demand for methods that can successfully learn from data without prior labels.</p><p>Semi-supervised learning is the procedure of combining unlabeled data with a (typically) much smaller set of labeled data for fitting a model. There are two main ideas behind the most well-performing methods for semi-supervised learning: consistency regularization and pseudo-labeling. The former means encouraging consistent predictions on unlabeled data across different views of the same sample, e.g., through domain-specific data augmentation. Pseudo-labeling, on the other hand, involves using confident model predictions on unlabeled data as de facto training labels.</p><p>UDA <ref type="bibr" target="#b5">[6]</ref> and FixMatch <ref type="bibr" target="#b6">[7]</ref> recently gained wide recognition because of their simple yet powerful frameworks for combining consistency regularization and pseudo-labeling in semi-supervised learning. Among their reported results is, e.g., an impressive classification accuracy of 94.93% on CIFAR-10 <ref type="bibr" target="#b7">[8]</ref> using only 250 labeled images for training <ref type="bibr" target="#b6">[7]</ref>. However, <ref type="figure">Fig. 1</ref>. A unification of semi-and self-supervised frameworks. While many existing methods for semi-supervised learning operate in the label space, DoubleMatch operates in both the label and feature spaces for a more efficient use of unlabeled data. Red arrows mark the flow of gradients. despite their excellent performance, these methods have the drawback of only enforcing consistency regularization on unlabeled data with confident model predictions while harder data samples are essentially discarded. For challenging datasets, in particular, this means the model only uses a smaller part of unlabeled data during training (the relatively easier part). We show that this inefficient and incomplete use of unlabeled data leads to unnecessarily long training times and, for some datasets, reductions in classification accuracy.</p><p>An alternative to semi-supervised training is to instead train fully without label information. This practice is referred to as self-supervised learning, and the goal is not to predict classes but instead to learn feature representations of the data that can be used for downstream tasks <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. By focusing on learning the fundamental representations of the data, these methods can more fully exploit the unlabeled training data and are in this respect more efficient than many semi-supervised methods. However, if the end task is classification, these methods do not extend the training data with task-specific information as in the case of, e.g., UDA and FixMatch.</p><p>Analogously to semi-supervised learning, self-supervised learning is heavily based on consistency regularization, which makes the two fields closely related. Generally, methods for both semi-and self-supervised learning employ a setting where a teacher model predicts optimization targets for a student model, making gradients flow only through the student predictions. The teacher and student models often share parameters, either by making the teacher a moving average of the student or by using the same parameters.</p><p>We propose DoubleMatch, an extension of FixMatch, which solves its inefficient use of unlabeled data by taking inspiration from research on self-supervised learning. <ref type="figure">Fig. 1</ref> illustrates how DoubleMatch unifies the typical setups for semi-and selfsupervised learning by operating both in the label and feature space for unlabeled data. In the label space, as with FixMatch for confident data, the model is evaluated based on predicted class distributions (pseudo-label loss). Whereas, in the feature space, the model is assessed based on the similarity of the predicted feature representations (self-supervised loss). More specifically, with motivation from methods such as <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we suggest adding a self-supervised feature loss to the Fix-Match framework by enforcing consistency regularization on all unlabeled data in the feature space. Moreover, we can add this term with minimal computational overhead because we do not require additional augmentations or model predictions.</p><p>Our main contributions of this work are: 1) We propose adding a self-supervised loss to the Fix-Match framework. 2) We demonstrate that our method leads to faster training times and increased classification accuracy than previous SOTA across multiple datasets. 3) We analyze the importance of a pseudo-labeling loss for different sizes of the labeled training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Research in semi-supervised learning dates back to the 1960s <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. Over the last couple of years, research in semi-supervised learning has been dominated by methods combining the use of consistency regularization and pseudolabeling <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. One of these methods is FixMatch <ref type="bibr" target="#b6">[7]</ref>, which this work proposes an extension of.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FixMatch</head><p>FixMatch <ref type="bibr" target="#b6">[7]</ref> made a big impact on the field of semisupervised learning because of its well-performing yet simple method for combining consistency regularization and pseudolabeling. FixMatch is similar to the marginally earlier UDA <ref type="bibr" target="#b5">[6]</ref> although slightly less complex.</p><p>As proposed by ReMixMatch <ref type="bibr" target="#b15">[16]</ref>, FixMatch employs consistency regularization through a setup with weak and strong data augmentations. The weak augmentation consists of a random horizontal flip followed by a random image translation, while the strong augmentation utilizes sharp imagetransformations such as RandAugment <ref type="bibr" target="#b16">[17]</ref>, CTAugment <ref type="bibr" target="#b15">[16]</ref>, and CutOut <ref type="bibr" target="#b17">[18]</ref>. The strong domain-specific augmentations for consistency regularization are highlighted as a crucial component in the performance gain compared to many previous methods, which use, e.g., domain-agnostic perturbations <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref> or MixUp-augmentations <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>.</p><p>FixMatch also makes use of pseudo-labeling, which can be compactly described as the practice of using model predictions as training labels. While there are many variations of this technique <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b28">[29]</ref>, FixMatch follows the confidencebased pseudo-labels introduced by UDA. The confidencebased pseudo-labels in FixMatch are designed such that only predictions on unlabeled data above a predetermined confidence-threshold are used for training. The artificial label used in the cross-entropy loss is then the argmax of the confident prediction.</p><p>The proposed method in this article is an extension of the FixMatch framework, following both its augmentation scheme and pseudo-labeling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extensions of FixMatch</head><p>As a consequence of the simplicity and high performance of FixMatch, there have been several subsequent studies trying to extend and improve on this framework <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>. For example, FlexMatch <ref type="bibr" target="#b29">[30]</ref> and Dash <ref type="bibr" target="#b30">[31]</ref> propose two different ways of introducing dynamic confidence-thresholds, replacing the constant threshold of FixMatch. DP-SSL <ref type="bibr" target="#b31">[32]</ref> generates pseudolabels using a data programming scheme with an ensemble of labeling functions, each specialized in a subset of the classes in the classification problem. In contrast to our work, methods such as <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref> aim to improve FixMatch through alternative strategies for pseudo-label selections. Our proposed method instead focuses on improving the consistency regularization of FixMatch and thus could potentially be further improved by using pseudo-labeling strategies proposed by <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-supervised learning</head><p>This section gives a very brief overview of research in selfsupervised learning that has inspired our work. Fully without labels, the goal of self-supervised learning is not to predict classes, but instead to learn representations of data that can be used for downstream tasks. Similarly to semi-supervised learning, recent advances in self-supervised learning have been largely reliant on consistency regularization through heavy use of domain-specific data augmentation <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p><p>Influential works such as <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> make use of the computeheavy contrastive loss for self-supervised learning, not only maximizing similarities between augmentations of the same data but also minimizing the agreement between different data. However, more recent works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref> show that a contrastive loss is not required to achieve competitive self-learning performance. Instead, they show that it is sufficient to, as in, e.g., FixMatch, focus on maximizing similarities between augmentations of the same data (consistency regularization). This means that self-supervised learning losses similar to those in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref> can be integrated into, e.g., FixMatch with a minimal additional computational load. For this work, we take particular inspiration from SimSiam <ref type="bibr" target="#b10">[11]</ref> by incorporating a similar self-supervised loss into our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Self-supervision in semi-supervised learning</head><p>Lastly, similarly to this paper, there has been some other work covering self-supervision techniques in semi-supervised learning <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b38">[39]</ref>. For example, EnAET <ref type="bibr" target="#b35">[36]</ref>, ReMix-Match <ref type="bibr" target="#b15">[16]</ref>, and S4L <ref type="bibr" target="#b36">[37]</ref> incorporate a self-supervised task that involves predicting the parameters of some stochastic image transformation that is applied to an unlabeled image. USADTM <ref type="bibr" target="#b37">[38]</ref> proposes a self-supervised loss that involves evaluating similarities between three different views of unlabeled data. Contrary to our proposed method, the selfsupervision in these methods requires introducing additional data augmentations and, as in case of USADTM, a more complex pseudo-label selection, all of which increases the computational load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>This section describes DoubleMatch, our proposed method. Many existing methods for semi-supervised learning have two central terms in their loss functions: a supervised loss term for labeled data and a pseudo-label loss term for unlabeled data <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[27]</ref>. We propose to add a third, self-supervised loss term, to fully utilize unlabeled data for faster training and increased accuracy. This term acts as a natural extension with minimal computational overhead to methods that utilize consistency regularization through data augmentation. We choose to apply our idea to the FixMatch <ref type="bibr" target="#b6">[7]</ref> framework but the method proposed can easily be applied to similar frameworks such as UDA <ref type="bibr" target="#b5">[6]</ref>. The training algorithm of DoubleMatch for a single batch is summarized in Algorithm 1 and detailed below.</p><p>For a C-class image classification problem, the supervised loss in FixMatch is the standard cross-entropy loss given by</p><formula xml:id="formula_0">l l = 1 B B i=1 H(y i , p i ),<label>(1)</label></formula><p>where B is the batch size, y i ? R C?1 is the ground-truth onehot label for image i, p i ? R C?1 is the predicted probability distribution for image i, and H(x, y) = ? x log y is the cross-entropy between two probability distributions.</p><p>The supervised loss is supplemented with a pseudo-label loss on unlabeled data:</p><formula xml:id="formula_1">l p = 1 ?B ?B i=1 1{max(w i ) &gt; ? }H(argmax(w i ), q i ).<label>(2)</label></formula><p>Here, ? ? Z + and ? ? [0, 1] are hyperparameters where ? determines the ratio of labeled to unlabeled data in a batch and ? sets the confidence threshold for assigning pseudo-labels. The probability distributions w i and q i are the predictions for a weak and a strong augmentation, respectively, of the unlabeled sample i. We let argmax : R C?1 ? {0, 1} C?1 so that it returns a one-hot vector, and 1{?} is the indicator function. Finally, we let the prediction on the weakly augmented image act as the teacher, meaning we consider w i to be constant when back-propagating through this loss term. The two loss terms from <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> constitute the key components of the FixMatch method. In our method, we add an additional consistency regularization between the two different augmentations of unlabeled data. This consistency regularization is enforced not on the predicted probability distributions but on the feature vectors from the penultimate layer of the classification network. This added loss improves the data efficiency of the method by operating on all unlabeled data, and not only for data with confident predictions as in <ref type="bibr" target="#b1">(2)</ref>. Inspired by the SimSiam <ref type="bibr" target="#b10">[11]</ref> method for self-supervised learning, we use the negative cosine similarity for this loss. Similarly to <ref type="bibr" target="#b10">[11]</ref>, in order to allow for differences between the feature representations of weakly and strongly augmented versions of the same image, we feed the feature representation of the strongly augmented image through a trainable linear projection head before evaluating the cosine similarity. We end up with our proposed additional loss term:</p><formula xml:id="formula_2">l s = ? 1 ?B ?B i=1 h(v i ) ? z i h(v i ) z i = ? 1 ?B ?B i=1 cos(h(v i ), z i ). (3)</formula><p>In this expression, z i ? R d?1 is the output from the penultimate layer of the classification network for the weakly augmented version of image i. The dimension of this vector, d, is determined by the network architecture. Similarly, v i ? R d?1 is the output from the penultimate layer for the strongly augmented version of image i. However, v i is also fed through the trainable linear projection head, h : R d?1 ? R d?1 . Again, the prediction on the weakly augmented image acts as the teacher, so we consider z i as constant when evaluating the gradient w.r.t. this loss term. Finally, ? is the l 2 norm. For each batch, we let the full loss be</p><formula xml:id="formula_3">l = l l + l p + w s l s ,<label>(4)</label></formula><p>where w s ? R + is a hyperparameter determining the weight of the self-supervised loss term. We empirically find that it is important to obtain a good balance between l l and w s l s in this loss, meaning that a well-tuned value for w s will be largely correlated with the number of labeled training data (see Section IV-D). Note that our loss function is identical to that used in FixMatch when w s = 0. A graph showing the loss evaluation for unlabeled images is displayed in <ref type="figure" target="#fig_0">Fig. 2</ref> where we divide the network into three parts:</p><p>? f , backbone: the network up to the final layer, ? g, prediction head: the final layer of the classification network, ? h, projection head: a single dimension-preserving linear layer for transforming features of strongly augmented unlabeled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data augmentation</head><p>Data augmentation has proved to be a central component of self-and semi-supervised learning <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We follow one of the augmentation schemes used in FixMatch <ref type="bibr" target="#b6">[7]</ref> where the weak augmentation is a horizontal flip with probability 0.5 followed by a random translation with maximum distance 0.125 of the image height. The strong augmentation stacks the weak augmentation, CTA <ref type="bibr" target="#b15">[16]</ref>, and Cutout <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimizer and regularization</head><p>For optimization, we stay close to the FixMatch settings and use SGD with Nesterov momentum <ref type="bibr" target="#b39">[40]</ref>. The learning rate, ?, is set to follow a cosine scheme <ref type="bibr" target="#b40">[41]</ref> given by</p><formula xml:id="formula_4">? = ? 0 cos ? ?k 2K<label>(5)</label></formula><p>where ? 0 is the initial learning rate, k is the current training step and K is the total number of training steps. We define one training step as one gradient update in the SGD optimization. The decay rate is determined by the hyperparameter ? ? (0, 1). Contrary to FixMatch which uses a constant ?, we suggest tuning ? for different datasets in order to minimize overfitting. Finally, we add weight-decay regularization to the loss as</p><formula xml:id="formula_5">l w = w d 1 2 ? f 2 + ? g 2 + ? h 2 ,<label>(6)</label></formula><p>where ? f , ? g and ? h are the vectors of parameters in the backbone, prediction head and projection head, respectively, and w d is a hyperparameter controlling the weight of this regularization term. The weight-decay is identical to FixMatch with the exception that DoubleMatch has the additional parameters from the projection head, ? h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 DoubleMatch algorithm</head><p>Require: Strong augmentation ?, weak augmentation ?, labeled batch {(x 1 , y 1 ), ? ? ? , (x B , y B )}, unlabeled batch {x 1 , ? ? ? ,x ?B }, unsupervised loss weight ws, weight decay parameter w d , confidence threshold ? , backbone model f , prediction layer g, projection layer h 1: Cross-entropy loss for (weakly augmented) labeled data 2: for i = 1, ? ? ? , B do 3:</p><formula xml:id="formula_6">p i = g ? f (?(x i )) 4: end for 5: l l = 1 B B i=1 H(y i , p i )</formula><p>6: Predictions on unlabeled data 7: for i = 1, ? ? ? , ?B do 8:</p><p>z i = f (?(x i )) Weak augmentation 9:</p><p>v i = f (?(x i )) Strong augmentation 10:</p><formula xml:id="formula_7">q i = g(v i ) 11:</formula><p>w i = stopgrad(g(z i )) 12: end for 13: Self-supervised cosine similarity 14: ls = ? 1 ?B ?B i=1 cos(h(v i ), stopgrad(z i )) 15: Cross-entropy with pseudo-labels</p><formula xml:id="formula_8">16: lp = 1 ?B ?B i=1 1{max(w i ) &gt; ? }H(argmax(w i ), q i ) return l l + lp + wsls + w d 1 2 ? f 2 + ?g 2 + ? h 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS/RESULTS</head><p>In this section we evaluate our method on a set of benchmark datasets for image classification. We use the datasets CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b7">[8]</ref>, SVHN <ref type="bibr" target="#b41">[42]</ref> and STL-10 <ref type="bibr" target="#b42">[43]</ref> with different sizes of the labeled training set. We compare our results with reported error rates from similar works. For fair comparisons, we choose works that state using similar experimental setups as us with the same data folds and the same architectures <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The results are shown in <ref type="table" target="#tab_0">Table I</ref> where we present mean and standard deviation of the error rate on the test set using five different data folds. We choose to report both the minimum error rate for the full training run, and the median for the 20 last evaluations. FixMatch <ref type="bibr" target="#b6">[7]</ref> uses the median of the last evaluations for their results, while others are not clear on this point.</p><p>We use an exponential moving average of the model parameters (with momentum 0.999) to evaluate the performance on the test set. We train DoubleMatch using 352, 000 training steps with batch size B = 64 in all our experiments. The method is implemented in TensorFlow and is based on the FixMatch codebase. All experiments are carried out on Nvidia A100 GPUs. The full list of hyperparameters can be found in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classification results</head><p>1) CIFAR-10 and SVHN: The datasets CIFAR-10 and SVHN both consist of color images of size 32 ? 32. Both datasets contain ten classes where CIFAR-10 has classes such as dog, horse and ship while the classes in SVHN are the ten digits. CIFAR-10 has a test set of 10,000 images and a training set of 50,000 images. SVHN has 26,032 images for testing and 73,257 for training. For these datasets we use a Wide ResNet-28-2 [44] with 1.5M parameters. This architecture makes the dimension of our feature vectors d = 128. Even though we obtain competitive results on many of the splits, we do perform worse than SOTA, especially in the very-low label regime.</p><p>2) CIFAR-100: CIFAR-100 is similar to CIFAR-10 in that it consists of color images of size 32 ? 32 with training and test sets of size 50,000 and 10,000 respectively. However, CIFAR-100 contains 100 classes, making it a much more challenging classification problem. For this dataset, we use the Wide ResNet-28-8 architecture with 24M parameters, resulting in d = 512. On this dataset, we achieve SOTA results across all splits, not only beating FixMatch and ReMixMatch, but also the more recent methods DP-SSL and Dash.</p><p>3) STL-10: The dataset STL-10 comprises color images of size 96 ? 96 belonging to ten classes. It has a labeled training set of 5,000 images and a unlabeled training set of 100,000 images. Its distribution of unlabeled data is wider than the labeled distribution, meaning that the unlabeled set contains classes that are not present in the labeled set. Here we use a Wide ResNet-37-2 with 6M parameters, making d = 256. On this dataset, we achieve a very competitive error rate, surpassed only by the result reported by Dash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training speed</head><p>Running FixMatch for its full training duration on CIFAR-100 using a single A100 GPU takes 5 days of wall-time. DoubleMatch reduces these long training times by more efficiently making use of unlabeled data through the added self-supervised loss. While the methods we use as comparison in <ref type="table" target="#tab_0">Table I</ref> run for little more than 1M training steps, we run DoubleMatch for only roughly a third of that. A clear illustration of our increase in training speed is seen in <ref type="figure" target="#fig_1">Fig. 3</ref> where we compare DoubleMatch to FixMatch during training runs on CIFAR-100 with 10,000 labeled training data and on STL-10 with 1,000 labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>We present an extension of FixMatch, which outperforms the original work in terms of prediction accuracy and training efficiency on CIFAR-100, STL-10, and 2/3 splits of SVHN. We note a particularly high performance on CIFAR-100 (new SOTA) and STL-10. Our hypothesis for the results on these two datasets is that the self-supervised loss contributes to the biggest improvement when it is difficult to reach high classification accuracies on the unlabeled training set. Low accuracies on the unlabeled set could be either due to 1) the difficulty of the classification problem, as in the case with CIFAR-100, or 2) due to the unlabeled data coming from a wider distribution than the labeled training set, as in the case with STL-10. For CIFAR-10 and SVHN, it is generally possible to reach high accuracies on the unlabeled training set so the quality of the pseudo-labels can be very high, making a self-supervised loss less important.</p><p>In <ref type="table" target="#tab_0">Table I</ref>, we include Dash <ref type="bibr" target="#b30">[31]</ref> and DP-SSL <ref type="bibr" target="#b31">[32]</ref>. These are recent methods that, similarly to us, aim to improve the FixMatch framework. However, contrary to DoubleMatch, these methods focus on improving the pseudo-labeling aspect: Dash through a dynamic confidence threshold and DP-SSL by an ensemble of labeling functions. Both of these methods report improvements compared to FixMatch. A future line of work could be combining these improved methods for pseudo-label selections with the self-supervised loss from DoubleMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameters</head><p>For CIFAR-10, SVHN and STL-10 we use ? = 7/8 and w d = 0.0005. For CIFAR-100 we use ? = 5/8 and w d = 0.001. For w s , we use 10, 5, 2 for CIFAR-100 with 10,000, 2,500 and 400 labels respectively; 5, 1, 0.5 for CIFAR-10 with 4,000, 250 and 40 labels; 0.05, 0.05, 0.001 for SVHN with 1,000, 250 and 40 labels; and 1.0 for STL-10 with 1,000 labels. We use ? 0 = 0.3, ? = 7, B = 64, ? = 0.95 and SGD momentum 0.9 for all datasets. We run all experiments for 352,000 training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION</head><p>In this section we cover two ablation studies related to our method. First, we show results from experiments where the cosine similarity in our self-supervised loss is replaced by other similarity functions. The second ablation study regards the importance of the pseudo-labeling loss for different sizes of the labeled training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-supervised loss functions</head><p>In DoubleMatch we use a cosine similarity for the selfsupervised loss term according to <ref type="bibr" target="#b2">(3)</ref>. We have conducted experiments with other loss functions. One alternative to the cosine similarity is a simple mean squared error, as used in, e.g., <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_9">l s = 1 ?Bd ?B i=1 (h(v i ) ? z i ) T (h(v i ) ? z i ).<label>(7)</label></formula><p>Another alternative, as done in, e.g., <ref type="bibr" target="#b34">[35]</ref> is to apply the softmax function to the feature vectors and then calculate the cross-entropy loss as</p><formula xml:id="formula_10">l s = 1 ?B ?B i=1 H(?(h(v i )), ?(z i /?)).<label>(8)</label></formula><p>Here, ? is a parameter that can be used to sharpen the resulting distribution for the prediction of weakly augmented data. The standard softmax function, ?(?), is given by</p><formula xml:id="formula_11">? k (x) = e x k D j=1 e xj for x ? R D?1 ,<label>(9)</label></formula><p>where ? k and x k are the k:th elements of ? and x respectively. We have evaluated the different loss function on CIFAR-100 with 10,000 labels, where we made training runs using 1) MSE 2) Softmax with ? = 1 and 3) Softmax with ? = 0.1. The self-supervised weight, w s , is re-tuned for every loss function. The results are shown in <ref type="table" target="#tab_0">Table II</ref>. When comparing to the other functions, we note that a considerably lower error rate is reached using the cosine similarity, indicating that this is indeed the correct choice for DoubleMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Importance of pseudo-labels</head><p>Lastly, we analyze the importance of pseudo-labels in DoubleMatch for different sizes of the labeled training set. These experiments are conducted on CIFAR-100. Here, we evaluate the difference in accuracy between DoubleMatch with and without the pseudo-labeling loss, (l p in (2)). The weight for the self-supervised loss, w s , is re-tuned for each split after removing l p . The drop in accuracy by removing l p for  <ref type="table" target="#tab_0">Table  III</ref>. We note that there is nearly no loss in performance by removing l p when using 10,000 labels. However, when moving towards fewer labels, this reduction in accuracy increases monotonously to roughly 8.5 for 400 labels. While DoubleMatch still reaches SOTA results for CIFAR-100 with 400 labels, these findings seem to be in line with our poor results on CIFAR-10 and SVHN with 40 labels, indicating that our added self-supervised loss does not do well on its own in the low-label regime. It is also consistent with results reported from Dash <ref type="bibr" target="#b30">[31]</ref> and DP-SSL <ref type="bibr" target="#b31">[32]</ref> indicating that improved pseudo-label selections seem to be more important in the low-label regime. However, with enough labels, the pseudo-label loss can be completely replaced by a selfsupervised loss with little to no loss in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper shows that using a self-supervised loss in semisupervised learning can lead to reduced training times and increased test accuracies for multiple datasets. In particular, we present new SOTA results on CIFAR-100 using different sizes of the labeled training set while using fewer training steps than existing methods. Additionally, we present interesting findings showing that, with enough labeled training data, the pseudolabeling loss can be removed with no performance reduction in the presence of a self-supervised loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Graph showing loss evaluation for unlabeled images. Double slash marks a stop-gradient operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Comparing FixMatch to DoubleMatch on training runs on CIFAR-100 and STL-10. We see that DoubleMatch reaches higher accuracies with roughly one third of the training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ERROR</head><label>I</label><figDesc>RATES ON DIFFERENT DATASETS USING DIFFERENT SIZES FOR THE LABELED TRAINING SET. DOUBLEMATCH ACHIEVES STATE-OF-THE-ART RESULTS ON MANY COMBINATIONS. Method 40 labels 250 labels 4000 labels 400 labels 2500 labels 10000 labels 40 labels 250 labels 1000 labels 1000 labels MixMatch [7], [24] 47.54?11.50 11.05?0.86 6.42?0.10 67.61?1.32 39.94?0.37 28.31?0.33 42.55?14.53 3.98?0.23 3.50?0.28 10.41?0.61 UDA [6], [7] 29.05?5.93 8.82?1.08 4.88?0.18 59.28?0.88 33.13?0.22 24.50?0.25 52.63?20.51 5.69?2.76 2.46?0.24 7.66?0.56 ReMixMatch [7], [16] 19.10?9.64 5.44?0.05 4.72?0.13 44.28?2.06 27.43?0.31 23.03?0.56 3.34?0.20 2.92?0.48 2.65?0.08 5.23?0.45 FixMatch (CTA) [7] 11.39?3.35 5.07?0.33 4.31?0.15 49.95?3.01 28.64?0.24 23.18?0.11 7.65?7.65 2.64?0.64 2.36?0.19 5.17?0.63 DP-SSL [32] 6.54?0.98 4.78?0.26 4.23?0.20 43.17?1.29 28.00?0.79 22.24?0.31 2.98?0.86 2.16?0.36 1.99?0.18 4.97?0.42 Dash (CTA) [31] 9.16?4.31 4.78?0.12 4.13?0.06 44.83?1.36 27.85?0.19 22.77?0.21 3.14?1.60 2.38?0.29 2.14?0.09 3.96?0.25 DoubleMatch (last 20) 14.02?5.71 5.72?0.51 4.83?0.17 42.61?1.15 27.48?0.19 21.69?0.26 16.50?13.73 2.58?0.53 2.25?0.09 4.46?0.20 DoubleMatch (min) 13.59?5.60 5.56?0.42 4.65?0.17 41.83?1.22 27.07?0.26 21.22?0.17 15.37?11.81 2.37?0.35 2.10?0.07 4.35?0.20</figDesc><table><row><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>SVHN</cell><cell>STL-10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EVALUATIONS</head><label>II</label><figDesc>OF DIFFERENT FUNCTIONS FOR THE SELF-SUPERVISED LOSS IN DOUBLEMATCH ON CIFAR-100 WITH 10,000 LABELS.TABLE III REDUCTION IN TEST ACCURACY ON CIFAR-100 BY REMOVING THE PSEUDO-LABEL LOSS FROM DOUBLEMATCH FOR DIFFERENT SIZES OF THE LABELED TRAINING SET.</figDesc><table><row><cell>Loss function</cell><cell cols="2">Error rate</cell><cell>ws</cell></row><row><cell>Cosine</cell><cell cols="3">21.22?0.17 10.0</cell></row><row><cell>MSE</cell><cell></cell><cell>23.91</cell><cell>0.25</cell></row><row><cell>Softmax (? = 1)</cell><cell></cell><cell>23.23</cell><cell>1.0</cell></row><row><cell cols="2">Softmax (? = 0.1)</cell><cell>23.57</cell><cell>0.5</cell></row><row><cell cols="4">Nr. of labeled training data</cell></row><row><cell>400</cell><cell cols="3">1,000 2,500 10,000</cell></row><row><cell>Reduction 8.46</cell><cell>3.81</cell><cell>2.20</cell><cell>0.39</cell></row><row><cell cols="4">different numbers of labeled training data is shown in</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by Saab AB, the Swedish Foundation for Strategic Research, and Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The experiments were enabled by resources provided by the Swedish National Infrastructure for Computing (SNIC) at Chalmers Centre for Computational Science and Engineering (C3SE), and National Supercomputer Centre (NSC) at Link?ping University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FixMatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to recognize patterns without a teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning with a probabilistic teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="373" to="379" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ReMixMatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RandAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MixMatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pseudo-Label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dash: Semi-supervised learning with dynamic thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DP-SSL: Towards robust semisupervised learning with a few labeled samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SimPLE: Similar pseudo label exploitation for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">All labels are not created equal: Enhancing semi-supervision via label grouping and co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">EnAET: A self-trained framework for semi-supervised and supervised learning with ensemble transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1639" to="1647" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">S4L: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised semantic aggregation and deformable template matching for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. akad. nauk Sssr</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fine-grained segmentation networks: Self-supervised segmentation for improved long-term visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
