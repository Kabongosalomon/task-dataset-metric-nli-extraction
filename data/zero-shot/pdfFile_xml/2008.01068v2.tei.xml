<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Qi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Fang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an unsupervised method for learning a generic and efficient shape encoding network for different shape analysis tasks. Our key idea is to jointly encode and learn shape and point features from unlabeled 3D point clouds. For this purpose, we adapt HRNet to octree-based convolutional neural networks for jointly encoding shape and point features with fused multiresolution subnetworks and design a simple-yetefficient Multiresolution Instance Discrimination (MID) loss for jointly learning the shape and point features. Our network takes a 3D point cloud as input and output both shape and point features. After training, Our network is concatenated with simple task-specific back-ends and fine-tuned for different shape analysis tasks. We evaluate the efficacy and generality of our method with a set of shape analysis tasks, including shape classification, semantic shape segmentation, as well as shape registration tasks. With simple back-ends, our network demonstrates the best performance among all unsupervised methods and achieves competitive performance to supervised methods. For fine-grained shape segmentation on the PartNet dataset, our method even surpasses existing supervised methods by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D shape analysis plays an important role in many graphics and vision applications. A key step in all shape analysis tasks is to extract representative features (or called descriptors) in different levels from 3D shapes. In particular, distinguishable shape instance features are preferred for shape classification, while per-point features are essential to fine-level analysis tasks, like semantic shape segmentation and registration.</p><p>Early methods compute handcrafted features of 3D shapes. Although these manually-designed features can preserve some good properties such as transformation invariant, they are difficult to be tailored to specific shape analysis applications. State-of-the-art methods integrate the feature extraction with specific shape analysis task and learn an end-to-end deep neural network with the supervision of labeled shape analysis results. The success of these supervised learning methods is built upon large-scale labeled datasets, and the networks optimized for one task are difficult to adapt to others.</p><p>Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>Unsupervised pre-training methods first learn a feature extraction backbone network from an unlabeled dataset via carefully designed unsupervised pretext task losses. After that, the pre-trained backbone network is concatenated with taskspecific back-end networks and refined for different downstream tasks via transfer learning. In computer vision and natural language processing tasks, unsupervised pre-training have demonstrated their advantages for reducing the workload of data labeling and network design <ref type="bibr" target="#b53">Yang et al. 2018;</ref><ref type="bibr" target="#b8">Deng, Birdal, and Ilic 2018;</ref><ref type="bibr" target="#b60">Zhao et al. 2019;</ref><ref type="bibr" target="#b15">Hassani and Haley 2019)</ref>. However, these networks and training schemes cannot be easily adapted for 3D shape analysis due to irregular representation of 3D point clouds and multi-level shape features required by different shape analysis tasks. A set of unsupervised 3D learning methods <ref type="bibr" target="#b53">Yang et al. 2018;</ref><ref type="bibr" target="#b8">Deng, Birdal, and Ilic 2018;</ref><ref type="bibr" target="#b60">Zhao et al. 2019</ref>) have been proposed for extracting shape features from 3D point clouds, none of them offers a generic backbone network for different shape analysis tasks with competitive performance to the supervised methods.</p><p>In this paper, we present an unsupervised pre-training method for learning a generic 3D shape encoding network for 3D shape analysis. Our key observation is that a 3D shape is composed of its local parts and thus the feature for shape and points are coherent and should be encoded and trained jointly. Based on this observation, our shape encoding backbone network adapts HRNet <ref type="bibr" target="#b41">(Wang et al. 2019a)</ref> to an octree-based convolutional network <ref type="bibr">(Wang et al. 2017)</ref> for extracting and fusing features from both points and shapes via parallel multiresolution subnetworks and connections across subnetworks. It takes 3D point cloud as input and outputs an instance-wise feature of the whole 3D shape as well as point-wise features. Inspired by the instance discrimination designed for 2D image classification , we design a simple-yet-efficient Multi-resolution Instance Discrimination (MID) losses for supervision of extracted shape and point features, in which a shape instance discrimination loss classifies augmented copies of each shape instance of a 3D dataset in one class, while a point instance discrimination loss classifies the same points on the augmented copies of a shape instance in a class.</p><p>We trained our backbone shape encoding network (denoted as MID-Net) with ShapeNetCore55  and evaluated its performance with simple back-ends in vari-ous shape analysis tasks, including shape classification, two shape segmentation tasks, and 3D shape registration. Our experiments demonstrate that in all these tasks, our pre-trained backbone offers better performance than the same network trained with the labeled data of downstream tasks, especially as the amount of labeled data in the downstream tasks becomes small. Among all unsupervised 3D learning methods, our method achieves the best performance in all shape analysis tasks. Moreover, it achieves competitive performance to the state-of-the-art supervised methods in all tasks. In fine-grained PartNet segmentation, our method surpasses state-of-the-art supervised methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Supervised 3D feature learning. Discriminative shape features can be learned with the supervision of the labeled data in a task-specific manner. Supervised deep learning approaches often achieve the best performance on the datasets of shape classification and segmentation <ref type="bibr" target="#b55">(Yi et al. 2017a;</ref><ref type="bibr" target="#b26">Mo et al. 2019;</ref><ref type="bibr" target="#b57">Yu et al. 2019)</ref>. Existing 3D deep learning methods can be classified according to shape representations: multi-view based CNNs <ref type="bibr" target="#b25">(Maturana and Scherer 2015;</ref><ref type="bibr" target="#b5">Su et al. 2015;</ref><ref type="bibr">Choy et al. 2016;</ref><ref type="bibr" target="#b17">Kalogerakis et al. 2017)</ref>, volumetric and sparse-voxel-based CNNs <ref type="bibr" target="#b47">(Wu et al. 2015;</ref><ref type="bibr" target="#b10">Graham 2015;</ref><ref type="bibr">Wang et al. 2017;</ref><ref type="bibr" target="#b31">Riegler, Ulusoy, and Geiger 2017;</ref><ref type="bibr" target="#b11">Graham, Engelcke, and van der Maaten 2018)</ref>, point-based networks <ref type="bibr">(Qi et al. 2017b,a;</ref><ref type="bibr" target="#b19">Li, Chen, and Lee 2018)</ref>, manifold-based CNNs <ref type="bibr" target="#b2">(Boscaini et al. 2015</ref><ref type="bibr" target="#b3">(Boscaini et al. , 2016</ref><ref type="bibr" target="#b14">Hanocka et al. 2019</ref>) and graph-based approaches <ref type="bibr" target="#b45">Wang et al. 2019b)</ref>. Despite the good performance, the taskspecific learned features are difficult to adapt to other tasks and preparing labeled 3D data is tedious and costly. Unsupervised 3D feature learning. unsupervised learning methods aim to learn generic 3D shape representations from unlabeled data via carefully designed pretext tasks.</p><p>3D-GANs ) train a generative adversarial network (GAN) on volumetric data, and its discriminator is used for extracting shape-level features. L-GANs learn deep shape representations by combining an autoencoder network and a GAN <ref type="bibr" target="#b0">(Achlioptas et al. 2018)</ref>. FoldingNets  and AtlasNets <ref type="bibr" target="#b12">(Groueix et al. 2018</ref>) optimize an autoencoder by reconstructing shapes from deformed point clouds. <ref type="bibr" target="#b58">Zhang and Zhu (2019)</ref> cascade two pretext taskspart contrasting and object clustering to learn the shape feature space. The unsupervised shape-level features are mainly used for shape classification and retrieval. <ref type="bibr" target="#b8">Deng et al. (2018)</ref> and <ref type="bibr" target="#b35">Shu et al. (2016)</ref> first extract handcrafted point-pair features from geometric patches, then uses an autoencoder to compress the features. SO-Nets (Li, Chen, and Lee 2018) extract hierarchical features from individual points and nodes of a self-organizing map. PointCap-sNets <ref type="bibr" target="#b60">(Zhao et al. 2019</ref>) extend capsule networks <ref type="bibr" target="#b33">(Sabour, Frosst, and Hinton 2017)</ref> to 3D point clouds and are trained by the shape reconstruction loss. <ref type="bibr" target="#b15">Hassani and Haley (2019)</ref> propose to train a multiscale graph-based encoder with multiple pretext tasks. <ref type="bibr" target="#b20">Li et al. (2020)</ref> propose an unsupervised clustering task to learn point features for detecting distinctive shape regions. The work (Sauder and Sievers 2019) proposes a novel pretext task that reconstructs the voxel indices of randomly arranged points. Concurrently, <ref type="bibr" target="#b49">Xie et al. (2020)</ref> use shape registration as the pretext task and adopts the point contrastive loss to learning point features; <ref type="bibr" target="#b52">Yang et al. (2020)</ref> design a pretext task of finding correspondences between shapes belonging the same category based on the cycle-consistency.</p><p>Although the above works can generate both point-level and shape-level features, their pretext tasks do not impose explicit self-supervision on both levels. We fill this gap by discriminating different-level features simultaneously and achieve significant improvements. Unsupervised pre-training. Unsupervised pre-training is actively studied due to its success in various fields. In the natural language processing field, BERT models <ref type="bibr" target="#b9">(Devlin et al. 2018</ref>) use masked context prediction and next sentence prediction as the pretext tasks, and GPT models <ref type="bibr" target="#b30">(Radford et al. 2018</ref>) prefer the language modeling task. In computer vision field, various pretext tasks or dedicated loss functions like colorization <ref type="bibr" target="#b59">(Zhang, Isola, and Efros 2016)</ref>, context prediction <ref type="bibr" target="#b9">(Doersch, Gupta, and Efros 2015)</ref>, motion segmentation <ref type="bibr" target="#b27">(Pathak et al. 2017)</ref>, iterative feature clustering <ref type="bibr" target="#b4">(Caron et al. 2018)</ref>, image instance discrimination  and contrastive losses <ref type="bibr" target="#b13">(Hadsell, Chopra, and LeCun 2006;</ref><ref type="bibr" target="#b16">He et al. 2020)</ref>, have also shown their strengths in learning effective image features.</p><p>Inspired by instance discrimination ) which operates on a single image level, we propose to discriminate multiresolution instances of shapes and develop effective training schemes to reduce the computational and memory cost caused by the large number of instances which cannot be handled by <ref type="bibr">Wu et al. 's approach (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given a large unlabeled 3D shape collection that consists of N 3D models, we assume each 3D shape X i is represented with a point cloud with M i points, and denote the j-th point of X i by p i,j . Here the term "unlabeled" means that the data has no shape category information or other human-annotated labels. Our goal is to train a shape encoder network f ? that takes the point cloud of X i as input and generate the representative and shape-level feature s i and point-wise features {v i,j , j = 1, . . . , M i } for X i :</p><formula xml:id="formula_0">f ? : X i ?? [s i , v i,1 , ? ? ? , v i,Mi ],<label>(1)</label></formula><p>where ? denotes the set of network parameters.</p><p>To train the above feature space in an unsupervised manner, we first augment shapes via various transformations and create multiresolution class labels for later training (see Section 3.1), then feed them into a deep neural network which maintains and fuses multi-scale resolution feature maps efficiently (see Section 3.2). We use the multi-resolution instance discrimination pretext task (see Section 3.3) to selfsupervise the feature learning progress (see Section 3.4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Data Processing</head><p>Preprocessing and data augmentation. We pre-process the input point cloud to assign point normals via principal component analysis if the accurate normal information is not available. For data augmentation, we first normalize each point cloud inside a unit sphere, then generate shape instances with a transformation composed by random rotations, random translations within <ref type="bibr">[-0.25, 0.25]</ref>, and random scaling along each coordinate axis with the ratio within [0.75,1.25]. For the dataset in which the up-right directions of models are aligned, our random rotation is restricted on the rotation along the upright axis. Multiresolution instance class creation. We label the transformed instances of the same shape with its index in the input dataset. Also, each point on the generated instance is labeled by the same index of the corresponding point in the input shape in the dataset. So in total, We create N shape-instance classes and M i point-instance classes for each shape-instance class. These multiresolution class labels serve as self-supervision signals for our network training. As the total number of point-level classes of a large-scale shape dataset could be huge: N i=1 M i , it will lead to huge memory consumption in network training. To overcome this issue, we introduce the concept of patch-instance class. On each shape X i in the dataset, we over-segment it into K i patches (K i M i ), and on each shape-instance class, we can create K i patch-instance classes, similar to the construction of pointinstance classes. In this way, the point-instance classes can be approximated by the patch-instance classes, which have a less total number. For simplicity, we use the K-Means algorithm to compute over-segmented patches and set K i = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Design</head><p>We adopt HRNet <ref type="bibr" target="#b41">(Wang et al. 2019a</ref>) to extract multiresolution instance features. Different from the conventional U-Net <ref type="bibr" target="#b32">(Ronneberger, Fischer, and Brox 2015)</ref> architecture that cascades sequential convolutional layers from high to low resolutions and then recovers high resolution features from low-resolution ones in a reversed order, HRNet maintains parallel multiresolution subnetworks and simultaneously outputs multi-resolution features. The features extracted by subnetworks in different resolutions are fused in different intermediate stages of HRNet. For 3D shape encoding, HRNet can simultaneously output low-resolution shape-level features and high-resolution point-wise features with one network. This property well matches our observation about multiresolution instance features as introduced in Section 1. Also, we can easily apply loss functions for outputs in different resolutions and each loss function will contribute to the training of all subnetworks. We built HRNet upon an octree-based CNN framework <ref type="bibr">(Wang et al. 2017)</ref> due to its efficiency in both computational cost and memory consumption and its natural multiresolution representation for building multiresolution subnetworks. A 3D point cloud is first converted to an octree representation, by default, in 64 3 resolution.</p><p>The details of the HRNet structure used in our method are illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. The numbers of feature channels are listed on the top of each feature map. The convolution kernel size is 3 ? 3 ? 3. ResBlk(3) represents three cascaded ResNet blocks with a bottleneck structure <ref type="bibr" target="#b16">(He et al. 2016)</ref>. The Downsample operation is implemented by max-pooling, and the Upsample operation is the tri-linear up-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Resolution Instance Discrimination</head><p>Our network training is supervised by two loss functions: shape-instance discrimination loss and point-instance discrimination loss. Both loss functions are based on input 3D models and their multiresolution instance class labels created in Section 3.1. Shape-instance discrimination loss. The shape-level feature is supervised by a linear classifier that classifies shape instances into N classes, where each 3D shape X i and its augmented instances are classified into the i-th class. The cross-entropy loss for the i-th class is defined as:</p><formula xml:id="formula_1">L s (s i ) = ? log exp((s i ?s i )/? s ) N k=1 exp((s i ?s k )/? s ) ,<label>(2)</label></formula><p>wheres i is the weight vector in the linear classifier for the i-th class. We follow the approach of <ref type="bibr" target="#b48">Wu et al. 2018</ref>) to normalize s i ands i to be unit-length and measure their difference by the cosine distance. ? s is a parameter controlling the concentration level of the extracted features and is set to 0.1 empirically.</p><p>To obtain a good classification, an optimals i expects to be the average center of features of all shape instances in this class . By optimizing the loss function Equation <ref type="formula" target="#formula_1">(2)</ref>, the features of shape instances under different transformations converge tos i and thus they are invariant to the imposed transformations. Meanwhile, the loss function of Equation (2) tends to maximize the distance between s i and others k (k = i), which makes the shape feature s i of shape X i is discriminative against the features of other 3D models. Point-instance discrimination loss. To optimize point-wise features, we can also use a linear classifier to classify the points of a shape and its augmented copies into M j classes, with the created supervision signals (Section 3.1). For points in the j-th point class, the cross-entropy loss is defined as:</p><formula xml:id="formula_2">L p (v i,j ) = ? log exp((v i,j ?? i,j )/? p ) Mi k=1 exp((v i,j ?? i,k )/? p ) ,<label>(3)</label></formula><p>where v i,j is the point-wise feature of j th point of an augmented shape instance of X i ,? i,j is the weight vector in the linear classifier for the j-th point class. All the point feature vectors are also unit-length and the control parameter ? p is set to 0.1. As discussed in Section 3.1, treating each point of a 3D shape as an individual class is impractical as it leads to a large number of classes for each object and results in huge memory consumption and computational cost for storing and updating the weights of linear classifiers. Therefore, we propose to approximate point-instance discrimination by patch-instance discrimination, and revise the loss function as:</p><formula xml:id="formula_3">L p (v i,j ) = ? log exp((v i,j ?? i,c(i,j) )/? p ) K c=1 exp((v i,j ?? i,c )/? p ) ,<label>(4)</label></formula><p>where c(i, j) is the index of the patch containing the j th point on shape X i . MID Loss. By combining the above two multiresolution instance discrimination loss functions, we define the MID loss for an instance of shape X i as:</p><formula xml:id="formula_4">L(X i ) = L s (s i ) + Mi j=1 L p (v i,j )/M i .<label>(5)</label></formula><p>Note that although our method applies different linear classifiers for points of different 3D shapes, the shape encoder is shared by all 3D shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Training</head><p>Given the MID loss function defined above, we optimize the network parameters of f ? with an input 3D shape collection by a stochastic gradient descent (SGD) algorithm. In each mini-batch, we randomly pick a set of 3D shapes from the dataset and generate an augmented shape for each original shape in runtime.</p><p>A na?ve approach to train the network is to update the weights of the shape encoder network and the linear classifier together according to the back-propagated gradient of the MID loss function. However, we found that updating the linear classifiers in each mini-batch often makes the classifier training unstable and hinders the optimization of the shape encoder. Inspired by the temporal ensembling scheme in <ref type="bibr" target="#b18">(Laine and Aila 2017)</ref>, we update the weights of the linear shape-instance classifier slowly b?</p><formula xml:id="formula_5">s i = (1 ? ? s ) ?s i + ? s ? s i ,<label>(6)</label></formula><p>where s i is the shape-level feature of the current shape instance for the i-th shape-instance class, ? s is a momentum parameter and is set to 0.5 in our implementation. For pointinstance classifier, we use a similar update rule:</p><formula xml:id="formula_6">v i,c = (1 ? ? p ) ?? i,c + ? p ? v i,c ,<label>(7)</label></formula><p>where v i,c is the average of the point-wise features of all points that belong to the patch class c, ? p is a momentum parameter and is set to 0.5 too. A detailed training procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MID-Nets for Shape Analysis</head><p>In this section, we first present our back-end network design and its training scheme and then discuss the performance of our method in each downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Backend Design and Training</head><p>For each downstream task, we concatenate MID-Net with simple back-end layers. We use a one-layer fully-connected (FC) network for shape classification and two-layer FC for shape segmentation. We denote the MID-Net with the FC back-end as MID-FC and optimize the concatenated networks with two training schemes:</p><p>-MID-FC(Fix): we fix the pre-trained MID-Net and only train the back-end with the labeled training data in each shape analysis task. -MID-FC(Finetune): we fine-tune both MID-Net and the FC back-end with the labeled training data in each shape analysis task. The MID-Net is initialized with the pretrained weights, the FC back-end is randomly initialized. To evaluate the impact of pre-training, we also randomly initialize both MID-Net and FC layers and train the network from scratch, denoted as MID-FC(NoPre).</p><p>We trained our MID-Net on ShapeNet dataset  that consists of 57,449 3D shapes. The overall network parameter size is 1.5M, which is comparable to PointNet++ <ref type="formula">(</ref> Algorithm 1: Network training procedure</p><formula xml:id="formula_7">Input: A set of shapes {Xi} N i=1 ; Output: Network f ? , {si} N i=1 and {{?i,c} K c=1 } N i=1 ; 1 Assign multiresolution instance labels to {Xi} N i=1 according to Section 3.1 2 for l in [0, max iteration] do 3 Randomly sample a batch of B samples {X b i } B i=1 ; 4 Compute loss L = 1 B B i=1 L(X b i ) (Equation (5)); 5</formula><p>Compute gradient ?L(?) and update ? with SGD;</p><formula xml:id="formula_8">6 Update {s b i } B i=1 and {{? b i ,c } K c=1 } B i=1</formula><p>with Equation <ref type="formula" target="#formula_5">(6)</ref> &amp; Equation <ref type="formula" target="#formula_6">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Method</head><p>Accu. Supervised Accu. Unsupervised Accu.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MID-FC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shape Classification</head><p>Dataset. We use the ModelNet40 <ref type="bibr" target="#b47">(Wu et al. 2015)</ref>, which contains 13,834 3D models across 40 categories: 9,843 models are used for training and 3,991 models for testing. We use the classification accuracy as the evaluation metric. Results and comparisons. <ref type="table" target="#tab_0">Table 1</ref> lists the testing accuracy on the ModelNet40. For a fair comparison, we do not use the voting or orientation pooling strategy <ref type="bibr" target="#b29">(Qi et al. 2017b)</ref> to improve the results. As shown in the first column, our pre-trained MID-Net provides a good initialization to MID-FC(Finetune) and thus results in better accuracy (93.1%) than MID-FC(NoPre) (92.9%).  <ref type="bibr" target="#b34">(Sauder and Sievers 2019)</ref>. Here all methods are pretrained with the ShapeNet dataset and use one FC layer, i.e. , a linear classifier. Our MID-FC(Fix) achieves the secondbest performance (90.3%) among all listed unsupervised approaches, and is slightly worse than ReconSpace.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we evaluate the accuracy of MID-FC learned from different ratios of labeled data, where our method attains good performance and is better than FoldingNet . MID-FC(Finetune) further improves the classification accuracy and verifies the effectiveness of our pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine-Grained PartNet Segmentation</head><p>Dataset. We evaluate our method with fine-grained segmentation on the PartNet <ref type="bibr" target="#b26">(Mo et al. 2019)</ref>, which is a challenging dataset that consists of 24,506 3D shapes in 17 categories with fine-grained (i.e. , Level-3) labels. Each shape contains 10,000 points and the part numbers in each shape category   vary from 3 to 50. We follow the data split setup in <ref type="bibr" target="#b26">(Mo et al. 2019)</ref>. We use the mean IOU across all 17 categories as the evaluation metric. Note that the ShapeNet dataset used for our pre-training does not contain Door, Fridge, and Storage categories of PartNet. Results and comparisons. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of MID-FC and other state-of-the-art supervised approaches, including PointNet++ <ref type="bibr" target="#b29">(Qi et al. 2017b</ref>), SpiderCNN <ref type="bibr" target="#b50">(Xu et al. 2018)</ref>, and PointCNN . For fair comparisons, we trained each model with the code provided by the original authors on the same dataset. And during the testing phase, we did not use the voting strategy to improve the result. Our MID-FC(Fix) outperforms most existing supervised approaches and is only slightly worse than PointNet++ by 0.2, which is significant considering the fact that MID-FC(Fix) only contains 2 trainable FC layers and optimizes less than 0.3M parameters. With fine-tuning, MID-FC(Finetune) achieves more improvements and the accuracy increases 11.2 points at least compared to other supervised approaches, and 2 point improvement over MID-FC(NoPre). The results clearly show the benefit of our pre-training. For the three categories that are only contained in PartNet while not in ShapeNet, MID-FC(Finetune) and MID-FC(Fix) still achieve better performance (see the supplemental material), which demonstrates the good generality of our pre-trained features.</p><p>The segmentation performance of all our three networks learned from different ratios of training data is reported in <ref type="table" target="#tab_4">Table 4</ref>. MID-FC(Finetune) and MID-FC(Fix) have better performance than the supervised MID-FC(NoPre), and the accuracy of MID-FC(Fix) and MID-FC(Finetune) trained with 20% labeled data is comparable to other supervised methods trained with 100% labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ShapeNet Part Segmentation</head><p>Dataset. We use the ShapeNet Part dataset <ref type="bibr" target="#b55">(Yi et al. 2017a)</ref> which contains 16,881 3D point clouds, collected from 16 categories of ShapeNet. Each point cloud has 2 to 6 part labels. The number of points of a shape varies from 1000 to 3000. We follow <ref type="bibr" target="#b55">(Yi et al. 2017a</ref>) to split the data. We use the mean IoU across all categories (C.mIoU) and the mean IoU across all instances (I.mIoU) as the evaluation metrics. Results and Comparisons. <ref type="table" target="#tab_6">Table 5</ref> shows the IoU of our methods and other state-of-the-art methods, including super-   vised methods such as PointNet++ <ref type="bibr" target="#b29">(Qi et al. 2017b</ref>), SynSpec-CNN ) and DGCNN <ref type="bibr" target="#b45">(Wang et al. 2019b</ref>) and SPLATNet <ref type="bibr" target="#b36">(Su et al. 2018)</ref>, Submanifold <ref type="bibr" target="#b11">(Graham, Engelcke, and van der Maaten 2018)</ref>, KPConv <ref type="bibr" target="#b39">(Thomas et al. 2019)</ref>, PointCNN , RS-CNN ). On the left panel of <ref type="table" target="#tab_6">Table 5</ref>, the methods did not use the voting strategy during the testing stage, and on the right panel, the methods used voting strategy during the testing stage. Compared with these supervised methods, MID-FC(Finetune) have a better performance. In <ref type="table" target="#tab_7">Table 6</ref>, we also compare our method with fine-tuned unsupervised methods like Multi-Task (Hassani and Haley 2019) and PointCapsNet <ref type="bibr" target="#b60">(Zhao et al. 2019)</ref> with 1% and 5% training data. Our MID-FC(Fix) and MID-FC(Finetune) also achieves better performance. And the improvements of our MID-FC(Finetune) over MID-FC(NoPre), especially when the training data is limited, clearly demonstrates the advantage of our pre-trained MID-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Shape Registration</head><p>The process of point cloud registration is to align a point cloud with its transformed version. To handle arbitrary rotations of shapes, we trained a new MID-Net with shapes augmented with arbitrary rotations. We regard the point-wise features extracted from our pre-trained MID backbone network as the point descriptors and use them to find closet point pairs from two input point clouds for computing the initial rigid transformation for the standard ICP algorithm. Dataset. We conduct a comparison on the test set of Mod-elNet40 <ref type="bibr" target="#b47">(Wu et al. 2015)</ref>, which contains 2,468 man-made models across 40 categories. We normalize each shape inside a unit sphere and apply a random rigid transformation to the shape. In particular, the rotation angle along each coordinate axis is randomly sampled from [0?, 360?], while the translation along each coordinate axis is randomly sampled from [?0.25, 0.25]. We evaluate the registration results by computing the Hausdorff distance between two registered shapes by different algorithms with the same ICP refinement. Results and comparisons. We compare our method with four state-of-the-art methods, including Go-ICP <ref type="bibr" target="#b51">(Yang et al. 2015)</ref>, FGR <ref type="bibr" target="#b61">(Zhou, Park, and Koltun 2016)</ref>, PointLK <ref type="bibr" target="#b1">(Aoki et al. 2019)</ref>, and DCP <ref type="bibr" target="#b44">(Wang and Solomon 2019)</ref>. DCP and PointLK were originally trained with rotation angles sampled within [0?, 45?]. We re-trained their models with the code provided by the authors. However, we found that the trained networks perform poorly in dealing with large rotations. For all the methods except FGR which performs nonlinear optimization, we run the ICP algorithm to refine the initial registration. <ref type="figure" target="#fig_4">Figure 2</ref> shows the accuracy curve within a given Hausdorff distance bound. The ground-truth curve and the curve by applying the standard ICP algorithm serve as the upper and lower bound of all the algorithms. Our method outperforms the other four methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Method Validation</head><p>In this section, we first evaluate the contributions of our octree-based HRNet and the MID loss, as well as the generality of our MID scheme for other network architectures. After that, we conduct a set of ablation studies to validate design decisions and parameter setting of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Contributions of Our Network and MID</head><p>Contribution of our network. We compare the performance of MID-FC(NoPre) with the existing supervised methods that can achieve the best performance in each downstream task. Because the existing supervised methods and MID-FC(NoPre) have different network architectures but share similar supervised training schemes with random initializations, the difference between their performance can illustrate the contribution of our network. As shown in <ref type="table">Table 7</ref>, our octree-based HRNet achieves very similar performance to the best supervised methods in shape classification and ShapeNet-Part segmentation (with 0.0 and +0.5 differences, respectively). For fine-grained shape segmentation, the performance of MID-FC(NoPre) is significantly better (with +11.9 difference). The good performance on different downstream tasks illustrates that our octree-based HRNet offers an efficient network architecture for our generic pre-trained model.  <ref type="table">Table 7</ref>: Evaluation of our network and the MID scheme.</p><p>Contribution of MID. For the contribution of the MID loss, we evaluate its contribution in each task by comparing the performance of MID-FC(NoPre) and MID-FC(Finetune). The two models share the same network architecture but are trained with different learning schemes. The MID-FC(NoPre) is trained from scratch with the task-specific labeled data by a supervised scheme, while the MID-FC(Finetune) is initialized with the weights of MID-Net pre-trained via our MID scheme and then refined with the task-specific labeled data. As shown in <ref type="table">Table 7</ref>, the performance gain of MID-FC(Finetune) over the supervised MID-FC(NoPre) counterparts in all three tasks (+0.2, +0.6, +2.4, respectively) clearly demonstrates the contribution of our MID scheme for different downstream tasks. Generality of the MID scheme. To validate the generality and advantage of the MID scheme for pre-training, we apply the MID scheme to PointNet++ with the default network in <ref type="bibr" target="#b29">(Qi et al. 2017b</ref>). Similar to MID-Net, we concatenate PointNet++ with a two-layered FC layer and fine-tune the networks (denoted as PointNet-FC(Finetune)) for all the tasks. For comparison, we also train the same concatenated Point-Net++ networks from scratch with the task-specific training data (denote as PointNet-FC(NoPre)). <ref type="table">Table 7</ref> shows that the performance gain between PointNet-FC(Finetune) and PointNet-FC(NoPre) in all three tasks exhibits the efficiency of our MID pre-training scheme to other network architectures, and the inferior performance of PointNet-FC(Finetune) to MID-FC(Finetune) also illustrates the importance of network architecture to the pre-trained model and the advantage of our octree-based HRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>We pre-train our networks with one modified component and keep all other settings unchanged. After pre-training, we test the modified models with shape classification and ShapeNet-Part segmentation tasks described in the last section with MID-FC(Fix). To better evaluate the efficacy of our pre-training in ablation studies, we use a denser version of point clouds provided by <ref type="bibr">(Wang et al. 2017)</ref> for the ShapeNet-Part segmentation task which has a similar point density to the data used in the pre-training. HRNet versus other network structures. Compared with the U-Net, the key advantages of HRNet are the parallel multi-resolution subnetworks and feature fusion in different resolutions. We develop two alternative networks that gradually remove the fusion layers between subnetworks. The MID-1Fusion removes the first fusion layer from HRNet, while the MID-NoFusion removes all fusion layers in HRNet.  <ref type="table">Table 8</ref>: Ablation study of different design choices.</p><p>As shown in <ref type="table">Table 8</ref>, the accuracy of the network decreases as we drop more fusion layers (e.g. , decrease 0.4% for MID-1Fusion and 2.1% for MID-NoFusion in shape classification), which clearly demonstrates the importance of the feature fusion. And the accuracy of U-Net drops 1.4% in shape classification and 0.5% in shape segmentation, which validates the advantage of HRNet in 3D pre-training. MID Loss versus single-level Loss. To demonstrate the advantages of MID loss, we train two networks, each of which is trained with one loss function only. Compared with our original MID loss, the performance of these two networks (denoted as Shape-Loss and Point-Loss in <ref type="table">Table 8</ref>) drops, which indicates that each loss function makes its own contribution to the full network training and affects the performance of shape encoding in both shape and point levels.</p><p>When the training data is limited, the performance gap is further enlarged: with only 1% of the training data, the testing accuracy on the classification task and category IoU on the segmentation task of our network trained with two loss functions are 61.8 and 74.6, respectively; with only the shape loss, the accuracy and IoU drop to 60.5 and 61.6; with only the point loss, the accuracy and IoU drop to 54.4 and 72.2. Augmentation scheme. We train the network with three different augmentation schemes, each of which drops one kind of transformation from the original augmentation scheme, respectively. Compared with the MID-Net trained with full augmentation, the performances of three networks trained with new augmentation schemes (denoted as NoRot, NoScale, NoTrans, and NoAug) shown in <ref type="table">Table 8</ref> decrease. Patch number. We train the networks with different K. As shown in <ref type="table">Table 8</ref>, as the number of clusters increases from 100 to 400, the performance increases less than 0.2% in both shape segmentation and classification tests. We thus set K as 100 in our current implementation to achieve a good balance between training cost and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose an unsupervised pre-training method for learning a generic backbone network from unlabeled 3D shape collections for shape analysis. We design an octree-based HRNet as backbone network architecture and a simple-yet-efficient MID loss for pre-training. The ablation study validates the advantages of joint shape and point feature encoding and training enabled by our design in the unsupervised pre-training and downstream shape analysis tasks. Our MID-Net offers state-of-the-art performance for various downstream shape analysis tasks, especially for tasks with small training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training details</head><p>In this section, we provide the training details in our pretraining, and downstream tasks. Hyperparameters of the pretraining. In the pretraining, we set the batch size as 32, momentum as 0.9, and weight decay as 0.0005. The initial learning rate is 0.03 and it decays by a factor of 10 after 200 epochs and 300 epochs, respectively. The whole training process is finished after 400 epochs, which takes about 60 hours on an NVIDIA 2080 Ti graphics card. Hyperparameters for the classification task. For the classification on ModelNet40 dataset <ref type="bibr" target="#b47">(Wu et al. 2015)</ref>, we optimize the networks via SGD with 240 epochs. The batch size is 32. The initial learning rate is set as 0.1 for MID-FC(NoPre) and the FC back-end, and 0.01 for MID-FC(Finetune). The learning rate decays by a factor of 1/10 after 120 and 180 epochs. Hyperparameters for the semantic segmentation task. In the ShapeNet part segmentation <ref type="bibr" target="#b54">(Yi et al. 2016)</ref> and Part-Net segmentation <ref type="bibr" target="#b26">(Mo et al. 2019</ref>), we used the same set of hyperparameters for training and finetuning. Specifically, we optimize our networks via SGD with batch size 32. We train one network for each category seperately. When the number of training data is large than 2k, we train the network for about 200 epochs; when the number of training data is less than 1k and 0.5k, we train the network by about 400 epochs and 600 epochs. The initial learning rate is set as 0.1 for MID-FC(NoPre) and the FC back-end, and 0.01 for MID-FC(Finetune). The learning rate decays by a factor of 1/10 after 05% and 75% of the total epochs. Input signal. The input to our pre-training network is the point cloud of a shape. We pre-process the point cloud to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape Classification</head><p>Shape Segmentation Point-wise Feature Instance-wise Shape Feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Back-end</head><p>Back-end Shape Encoder <ref type="figure">Figure 3</ref>: Summary of our multiresolution-instance-discrimination-based learning approach. Our MID encoder is learned from a collection of unlabeled 3D shapes. It takes 3D point cloud as inputs and generates both instance-wise shape features and point-wise features (shown in the middle), which can be used in different downstream shape analysis tasks, via concatenated simple back-ends with or without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>ICP Go-ICP FGR PointLK+ICP DCP+ICP Ours+ICP <ref type="figure">Figure 4</ref>: Two examples of point cloud registration. The blue shape is fixed, the green one is transformed by different methods and its transformed version is rendered in orange color. The stair model is not in the ShapeNet dataset. For both examples, only our method registers the inputs correctly. <ref type="table">Table 9</ref>: Category-specific fine-grained semantic segmentation IoU on PartNet. mIoU is the averaged IoU over categories. The categories Door, Frid., and Stora. do not exists in the ShapeNetCore55 . Our network is not trained on these categories even in the unsupervised training stage, and in the finetuning stage our network still get improvements.    assign a normal vector for each point via principal component analysis if the accurate normal information is not available in the dataset. For each non-empty octant at the finest level, we fit a plane to the points inside it, where the plane normal is fixed to the average normal of these points. We take the unit normal of the plane and the plane offset to the octant center as the raw feature vector. To deal with the possible inconsistent normal orientation issue across the shape, we convert each normal component to its absolute value, and find this trick does not hurt network performance. Additional details for the point cloud registration. We uniformly sample 2,048 points from the original shape and the same number of points from the transformed shapes and take these two point clouds as input for the shape registration task. We use the ShapeNet as training set, and the testing set of ModelNet40 as the testing set. As the transformation is known during the data creation, we can compute the groundtruth registration error by transforming the two paired shapes together and computing the Hausdorff distance of these two point clouds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1 shows the overview of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our multiresolution-instance-discrimination (MID) unsupervised pre-training pipeline. An augmented input point cloud via transformations is fed into the deep neural network which maintains and fuses multi-scale resolution feature maps. The shape-level features and point-wise features are extracted from the network, and they are encouraged to be discriminative and transformation-invariant under the supervision of the MID loss on both shape instance and point levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Qi et al. 2017b) (1.4 M), DGCNN (Wang et al. 2019b) (1.5 M), and much smaller than PointCNN (Li et al. 2018) (8.2 M). The detailed training hyper-parameters are provided in the supplemental material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Compared to the state-of-the-art supervised methods including PointNet++ (Qi et al. 2017b), PointCNN (Li et al. 2018), DGCNN (Wang et al. 2019b), KPConv (Thomas et al. 2019), and RS-CNN (Liu et al. 2019), our MID-FC(Finetune) also exhibits superior accuracy.We also compare our method with other unsupervised methods including L-GAN<ref type="bibr" target="#b0">(Achlioptas et al. 2018)</ref>, Fold-ingNet, Multi-Task<ref type="bibr" target="#b15">(Hassani and Haley 2019)</ref>, PointCapsNet<ref type="bibr" target="#b60">(Zhao et al. 2019)</ref>, and Recon-Space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Point cloud registration results. The curve represents the ratio of registration results under a specific Hausdorff distance. Our method achieves the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of t-SNE mapping of shape-level features and point-wise features of 3D shapes. The left figure displays the T-SNE map of shape-level features 100 3D models selected from 10 shape categories. The right figure exhibits the T-SNE map of point-wise features of 6 shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of patch-instance classes. An airplane model under different transformations is shown here, the over-segmented patches are color-coded according to their corresponding IDs. These transformed planes also belong to one shape-instance class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ModelNet40 shape classification. We list the performance of the state-of-the-art supervised methods (2nd column) and unsupervised methods (3rd column).</figDesc><table><row><cell>(NoPre)</cell><cell cols="3">92.9 PointNet++ 90.7 L-GAN</cell><cell>84.5</cell></row><row><cell>MID-FC(Fix)</cell><cell cols="3">90.3 PointCNN 92.2 FoldingNet</cell><cell>88.4</cell></row><row><cell cols="3">MID-FC(Finetune) 93.0 DGCNN</cell><cell>92.9 Multi-Task</cell><cell>89.1</cell></row><row><cell></cell><cell></cell><cell>KPConv</cell><cell>92.9 PointCapsNets 89.3</cell></row><row><cell></cell><cell></cell><cell>RS-CNN</cell><cell>92.9 ReconSpace</cell><cell>90.6</cell></row><row><cell>Data</cell><cell></cell><cell cols="2">1% 2% 5% 10% 20%</cell></row><row><cell>FoldingNet</cell><cell></cell><cell cols="2">56.4 66.9 75.6 81.2 83.6</cell></row><row><cell cols="2">MID-FC(Fix)</cell><cell cols="2">61.5 73.1 80.2 84.2 86.9</cell></row><row><cell cols="2">MID-FC(NoPre)</cell><cell cols="2">58.5 71.2 80.1 85.4 88.7</cell></row><row><cell cols="4">MID-FC(Finetune) 67.3 76.5 83.6 88.4 90.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The comparison of MID-FC(Fix) and FoldingNet for shape classification with a linear classifier as the back-end.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The semantic segmentation mIoU on PartNet.</figDesc><table><row><cell>Model</cell><cell>1% 2% 5% 10% 20%</cell></row><row><cell>MID-FC(NoPre)</cell><cell>14.5 16.1 27.7 29.5 41.6</cell></row><row><cell>MID-FC(Fix)</cell><cell>21.7 25.2 31.3 34.4 38.9</cell></row><row><cell cols="2">MID-FC(Finetune) 21.8 26.8 35.4 40.2 46.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The semantic segmentation mIoU on PartNet with different ratios of labeled data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The segmentation mean IoU across all categories (C.mIoU) and all instances (I.mIoU) on the ShapeNet Part.</figDesc><table><row><cell>Model</cell><cell cols="2">C.mIoU</cell><cell cols="2">I.mIoU</cell></row><row><cell></cell><cell>1%</cell><cell>5%</cell><cell>1%</cell><cell>5%</cell></row><row><cell>PointCapsNets</cell><cell>-</cell><cell cols="3">-67.0 70.0</cell></row><row><cell>Multi-Task</cell><cell cols="4">-73.0 68.2 80.7</cell></row><row><cell>MID-FC(NoPre)</cell><cell cols="4">46.3 70.9 65.1 80.3</cell></row><row><cell>MID-FC(Fix)</cell><cell cols="4">66.2 76.5 72.4 80.9</cell></row><row><cell cols="5">MID-FC(Finetune) 67.6 77.8 76.2 82.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The segmentation mIoU on ShapeNet Part with different ratios of labeled data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Model %train C.mIOU I.mIOU Aero Bag Cap Car Chair Ear. Guitar Knife Lamp Laptop Motor Mug Pistol Rocket Skate. Table</figDesc><table><row><cell>MID-FC(NoPre)</cell><cell></cell><cell>84.1</cell><cell>85.2</cell><cell>83.4 84.7 88.1 82.0 90.3 79.1</cell><cell>91.1</cell><cell>86.9</cell><cell>79.6</cell><cell>95.4</cell><cell>78.0</cell><cell>95.9</cell><cell>83.3</cell><cell>62.9</cell><cell>82.7</cell><cell>82.5</cell></row><row><cell>MID-FC(Fix)</cell><cell>100%</cell><cell>82.9</cell><cell>84.1</cell><cell>80.1 83.8 90.1 78.7 89.7 80.6</cell><cell>90.8</cell><cell>85.8</cell><cell>79.2</cell><cell>95.7</cell><cell>71.3</cell><cell>93.8</cell><cell>80.4</cell><cell>62.1</cell><cell>81.7</cell><cell>82.2</cell></row><row><cell>MID-FC(Finetune)</cell><cell></cell><cell>84.3</cell><cell>85.5</cell><cell>83.7 82.2 89.6 81.6 90.1 80.9</cell><cell>91.5</cell><cell>86.4</cell><cell>80.5</cell><cell>95.8</cell><cell>78.5</cell><cell>95.9</cell><cell>83.0</cell><cell>64.5</cell><cell>81.3</cell><cell>83.2</cell></row><row><cell>MID-FC(Fix) MID-FC(Finetune)</cell><cell>1%</cell><cell>66.2 67.8</cell><cell>76.6 76.2</cell><cell>71.4 44.9 66.7 68.1 86.2 58.8 73.3 60.9 80.8 71.9 86.4 53.5</cell><cell>88.6 89.3</cell><cell>76.9 72.1</cell><cell>61.9 57.4</cell><cell>93.6 93.0</cell><cell>34.4 21.4</cell><cell>88.6 80.9</cell><cell>66.3 67.9</cell><cell>31.2 42.3</cell><cell>45.1 56.5</cell><cell>77.4 77.9</cell></row><row><cell>MID-FC(Fix) MID-FC(Finetune)</cell><cell>5%</cell><cell>76.5 77.8</cell><cell>80.9 82.1</cell><cell>76.0 74.5 82.1 74.8 87.4 62.4 79.1 72.4 82.8 77.1 87.9 66.0</cell><cell>89.7 90.5</cell><cell>84.1 85.6</cell><cell>73.0 74.8</cell><cell>95.0 94.9</cell><cell>61.4 67.7</cell><cell>92.5 93.4</cell><cell>72.2 73.3</cell><cell>39.7 38.9</cell><cell>79.3 80.5</cell><cell>80.2 80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Category-specific semantic segmentation on the ShapeNet-Part dataset.</figDesc><table><row><cell>Airplane</cell><cell></cell></row><row><cell>Bed</cell><cell></cell></row><row><cell>Boat</cell><cell></cell></row><row><cell>Chair Pistol Mug Laptop Guitar Keyboard Car</cell><cell>Mug 2 Mug 1 Chair 2 Chair 1 Airplane 1 Airplane 2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visualization</head><p>Feature visualization. The quality of learned shape-level features and point-wise features can be assessed intuitively via visualization. In <ref type="figure">Figure 5</ref> we show the t-SNE mapping <ref type="bibr" target="#b24">(Maaten and Hinton 2008)</ref> of shape-level features of 1000 3D shapes randomly sampled from 10 shape categories, as well as the t-SNE mapping of point-wise features of 6 shapes (2 airplanes, 2 chairs, and 2 mugs). Note that the shape-level features and point-wise features of 3D shapes have similar distributions in their feature space, where the features of 3D models with similar overall shapes are clustered together and can be easily discriminated from the features of different 3D models. Data visualization. In <ref type="figure">Figure 6</ref>, we visualize the segmented patches of an airplane shape and its augmented copies under different transformations and colorize patches according to their patch-instance IDs. Network structure. In the ablation study of our paper, we designed two alternative networks (MID-1Fusion and MID-NoFusion) that gradually remove the fusion layers between subnetworks. Their network structures, as well as the U-Net, are shown in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More results</head><p>We provide more detailed results in this section. Specifically, in <ref type="table">Table 9</ref> we show the category-specific segmentation mIoU on the PartNet dataset; in <ref type="table">Table 10,</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PointNetLK: Robust &amp; Efficient Point Cloud Registration Using PointNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multiview 3D object reconstruction</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PPF-FoldNet: Unsupervised learning of rotation invariant 3D local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Unsupervised visual representation learning by context prediction</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Sparse 3D convolutional neural networks. In BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-M?ch? approach to learning 3D surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MeshCNN: A network with an edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised multi-task feature learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">; K</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. He</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D shape segmentation with projective convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Averkiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SO-Net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised detection of distinctive regions on 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transductive centroid projection for semi-supervised largescale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Point-Net: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sievers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised 3D shape segmentation and cosegmentation via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Aided Geom. Des</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07919</idno>
		<title level="m">Deep high-resolution representation learning for visual recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep Closest Point: Learning Representations for Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spi-derCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Go-ICP: A globally optimal solution to 3D ICP point-set registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Mapping in a cycle: Sinkhorn regularized unsupervised learning for point cloud shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">FoldingNet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH ASIA)</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Large-scale 3D shape reconstruction and segmentation from ShapeNet Core55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06104</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning for point cloud understanding by contrasting and clustering using graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">3D point capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. Model mIoU Bed Bottle Chair Clock Dish Display Door Ear Faucet Knife Lamp Micro Frid. Stora. Table Trash Vase #Train</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
