<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Ruan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ostendorff</surname></persName>
							<email>malte.ostendorff@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Rehm</surname></persName>
							<email>georg.rehm@dfki.de</email>
							<affiliation key="aff2">
								<orgName type="institution">DFKI GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i. e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i. e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model's SOTA performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Texts, especially long documents, contain internal hierarchical structure like sections, paragraphs, sentences, and tokens. When we manually summarize a text, the hierarchical text structure usually plays a key role. Taking a scientific paper as an example, we might focus more on the sections with the titles of "methodology", "discussion", and "conclusion" while paying less attention to the sections like "background". Furthermore, the sentences within one section could have closer relationship with each other, than the ones outside this section. Understanding not only the sequential relations between the sentences but also the internal hierarchical text structure helps us better determine the important sentences within a document. Similarly, a neural summarization model could benefit from these hierarchical structure information.</p><p>In this paper, we focus on extractive text summarization of single documents, which is the task of binary sentence classification with labels indicating whether a sentence should be included in a summary. Recently, pre-trained language models based on Transformer <ref type="bibr" target="#b18">(Vaswani et al., 2017)</ref>, such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, have been widely used to extract contextual representations from texts. The pre-trained Transformer language models (TLMs) can be easily reused for finetuning on the downstream tasks, so that the representations already learned from the large pretraining corpora are preserved. <ref type="bibr" target="#b9">Liu and Lapata (2019)</ref> have achieved the state-of-the-art (SOTA) performance by fine-tuning BERT for extractive summarization on short document datasets including CNN/DailyMail. However, the TLMs consider merely the sequential-context-dependency by adding a linear positional encoding to each input token embeddings. The hierarchical text structure information is not taken into account explicitly.</p><p>We propose a novel approach to formulate, extract, encode and inject the hierarchical structure (HiStruct) information explicitly into an extractive summarization model (HiStruct+ model), which consists of a TLM for sentence encoding and two stacked inter-sentence Transformer layers for hierarchical learning and extractive summarization (see <ref type="figure" target="#fig_0">Figure 1</ref>). We experiment with BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, RoBERTa , and Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> as underlying TLMs. The HiStruct information utilized in our work includes the section titles and the hierarchical positions of sentences, which are encoded using our proposed novel methods. The resulting embeddings can be injected into the TLM sentence representations to provide the HiStruct information for the summarization task. Section Title Embeddings <ref type="bibr">[BOS]</ref> 1st sent.</p><p>[EOS] <ref type="bibr">[BOS]</ref> 2nd sent.</p><p>[EOS] <ref type="bibr">[BOS]</ref> 3rd sent. <ref type="bibr">[</ref>  The HiStruct+ models are evaluated on short documents (i. e., CNN/DailyMail <ref type="bibr" target="#b17">(See et al., 2017)</ref>) and long documents (i. e., PubMed and arXiv <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref>) with various hierarchical characteristics. Our models produce competitive results on CNN/DailyMail and set the SOTA ROUGEs for extractive summarization on PubMed and arXiv to a new level. We also compare the HiStruct+ models with the corresponding strong baselines, which differ from our models only in that the HiStruct information is not injected. Using various experimental settings, our models collectively outperform the baselines on the three datasets, indicating the effectiveness of the proposed HiStruct encoding methods. The improvements are especially substantial on PubMed and arXiv, which contain longer scientific papers with conspicuous hierarchical structures. Ablation studies suggest that the performance gains are mainly contributed by the hierarchical position information of sentences.</p><p>Our contributions in this work are four-folds:</p><p>(1) We conceptualize novel measures to compare the internal hierarchical structure of the datasets.</p><p>(2) We propose novel methods to formulate the HiStruct information and implement data preprocessing to extract them from the raw datasets. <ref type="formula" target="#formula_1">(3)</ref> We propose novel methods to encode and inject the HiStruct information into an extractive summarization model explicitly. The effects of different encoding settings and injection settings are systematically investigated. (4) The data containing the extracted HiStruct information, the best HiStruct+ models, as well as the scripts for preprocessing, training and evaluation are available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Summarization</head><p>Extractive Text Summarization (ETS) is to classify sentences within a document with labels indicating whether a sentence should be included in the summary. <ref type="bibr" target="#b9">Liu and Lapata (2019)</ref> fine-tune BERT with stacked Transformer layers and a sigmoid classifier (BERTSUMEXT). Instead of directly utilizing the existing Transformer encoder for document encoding, <ref type="bibr" target="#b25">Zhang et al. (2019)</ref> pre-train a hierarchical Transformer encoder consisting of a sentence encoder and a document encoder (HIBERT) and fine-tune it for ETS. For long documents, <ref type="bibr" target="#b20">Xiao and Carenini (2019)</ref> propose a RNN-based ETS model incorporating both the global and the local context (ExtSum-LG). To address the problem of redundancy in extractive summaries, the authors fur-ther improve their work by introducing redundancy reduction <ref type="bibr" target="#b21">(Xiao and Carenini, 2020)</ref>. They systematically explore and compare different methods including Trigram Blocking <ref type="bibr" target="#b14">(Paulus et al., 2018)</ref>, RdLoss, MMR-Select and MMR-Select+ <ref type="bibr" target="#b21">(Xiao and Carenini, 2020)</ref>. Trigram Blocking is a traditional redundancy reduction method that avoids adding a candidate sentence to the summary if it has trigram overlap with the previously selected sentences. Their previous model combined with the redundancy reduction methods produce SOTA performance for ETS on PubMed and arXiv <ref type="bibr" target="#b21">(Xiao and Carenini, 2020)</ref>.</p><p>Previous works on extractive summarization model hierarchical structure of documents by introducing a hierarchical attention, where they first learn contextual token representations based on the linear dependencies between tokens and then add additional CNN <ref type="bibr" target="#b2">(Cheng and Lapata, 2016)</ref> or RNN <ref type="bibr" target="#b12">(Nallapati et al., 2017)</ref> or Transformer <ref type="bibr" target="#b25">(Zhang et al., 2019;</ref><ref type="bibr">Liu and Lapata, 2019) layer(s)</ref> to learn document-level representations for each sentence based on the linear dependencies between sentences. However, they learn hierarchical representations of sentences in an implicit way. The models are like black boxes, lacking interpretability. In contrast, our proposed approach enriches sentence representations in an explicit way by using section titles and hierarchical positions of sentences as additional HiStruct information, which is more intuitive and interpretable.</p><p>Abstractive text summarization (ATS) is to generate summaries with new sentences which are not present in the source text. BERTSUMABS <ref type="bibr" target="#b9">(Liu and Lapata, 2019)</ref> uses the pre-trained BERT as the encoder in its encoder-decoder architecture. Instead of simply using the pre-trained BERT, recent works, including T5 <ref type="bibr" target="#b16">(Raffel et al., 2020)</ref>, BART <ref type="bibr" target="#b7">(Lewis et al., 2020)</ref> and PEGAUSUS <ref type="bibr" target="#b24">(Zhang et al., 2020)</ref> pre-train encoder-decoder models specifically for seq2seq tasks. The first attempt at addressing neural abstractive summarization of long documents is undertaken by <ref type="bibr" target="#b3">Cohan et al. (2018)</ref>. <ref type="bibr" target="#b0">Aksenov et al. (2020)</ref> overcome the length limitations of BERT by a new method of BERT-windowing, allowing it to deal with longer documents. Gidiotis and Tsoumakas (2020) propose a divide-andconquer approach to train a model to summarize each part of the document separately. To address the essential issue of the quadratic full attention operation of TLMs, <ref type="bibr" target="#b23">Zaheer et al. (2020)</ref> propose BigBird with a sparse attention mechanism.</p><p>Hybrid text summarization combines extractive summarization, abstractive summarization, or other techniques as a two-stage hybrid system. MatchSum <ref type="bibr" target="#b26">(Zhong et al., 2020)</ref> is a recent work that first selects sentences from a document using an extractive model and builds a set of candidate summaries based on them. The summarization task is then formulated as a semantic text matching problem between the source document and the candidate summaries. <ref type="bibr" target="#b15">Pilault et al. (2020)</ref> presents a hybrid system that consists of an extractive model and a Transformer language model. The Transformer language model employs an encoder-decoder architecture for abstractive summarization, conditioned on the sentences extracted by the extractive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Injection of Additional Information</head><p>The idea of injecting additional information to TLM is inspired by two former works, LAMBERT <ref type="bibr" target="#b5">(Garncarek et al., 2020)</ref> and LayoutLM , where the visual layout information is injected into BERT by adjusting its input embeddings. These models were not proposed for text summarization and they cannot be applied to plain texts since the layout positions have to be obtained from scanned document images. In contrast, our approach makes use of the internal HiStruct information, which can be found in most types of textual data. Moreover, we enrich the output representations from the TLM instead of adjusting the input embeddings. This saves compute resources since TLM pre-training is not required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Structure Information</head><p>Hierarchical position of a sentence is represented in the proposed method as a vector of its positions at each hierarchy-level.</p><formula xml:id="formula_0">SSV s = (a s , b s )<label>(1)</label></formula><p>Given the s-th sentence within a document, its hierarchical position is formulated as a 2-dimensional vector (a s , b s ), denoted as the sentence structure vector SSV s , where a s represents the linear position of the section containing the sentence and b s is the linear position of the sentence within the section. All sentences within the same section have the same value in the first dimension of the SSV, indicating the close relationships between them. The second dimension indicates more precisely their linear relations within the section. By this very simple numerical formulation, hierarchical relations between sentences are clearly identified. Section titles exist in particular in long documents like scientific papers. They usually imply the section content and describe the common topic for its sub-sentences <ref type="bibr" target="#b13">(Ostendorff et al., 2020)</ref>. In our work, we propose to utilize the corresponding section title as an additional HiStruct information when encoding its sub-sentences. There exist typical section titles in scientific papers. Similar section titles like "Conclusion", "Conclusions" and "Concluding remarks" have the same semantic meaning and can be grouped into one typical section title class of "Conclusions". This is also taken into consideration when encoding the section titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Structure Encoding</head><p>Hierarchical position embedding is based on the existing linear position encoding methods (PE), including the sinusoidal method (sin) used by Transformer <ref type="bibr" target="#b18">(Vaswani et al., 2017)</ref> and the learnable method (la) used by BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. We use one of the PEs to encode the two dimensions of a SSV respectively, resulting in two embeddings. Using the la PE, the embeddings are initialized randomly and trained with the entire summarization model. Using the sin PE, the two embeddings are calculated simply by Equations 2 and 3 as described by <ref type="bibr" target="#b18">Vaswani et al. (2017)</ref>. P E (pos,2i) = sin(pos/10000 2i/d model )</p><p>(2) P E (pos,2i+1) = cos(pos/10000 2i/d model )</p><p>where pos is the value in one dimension of the SSV and i is the i-th dimension of the resulting embedding.</p><p>Given the s-th sentence with the hierarchical position of (a s , b s ), and the desired size of the output embeddings d, the Sentence Hierarchical Position Embedding (sHE) can be generated by Equations 4, 5, 6, using different combination modes. </p><p>where the symbol | denotes vector concatenation.</p><p>Using one of the PEs (i. e., sin or la) associated with one of the combination modes (i. e., sum, mean or concat), it totals six different settings of the hierarchical position encoding method: sinsum, sin-mean, sin-concat, la-sum, la-mean and la-concat.</p><p>(Classified) section title embedding is generated by the same pre-trained TLM, which is involved in the summarization model. We have two options to encode section titles: section title embedding (STE) and classified section title embedding (classified STE). A section title embedding is generated by feeding the tokenized section title into the TLM and summing up the last hidden states at each token position as a single embedding. Similar section titles consisting of similar tokens lead to embeddings that are already similar to each other in some way. We also manually pre-define typical section title classes and the corresponding intraclass section titles depending on the datasets and the domains. Using the classified STE, all intraclass STEs are replaced with the embedding of its corresponding class. In the case that a section title does not belong to any class or it falls into more than one class, the original STE is used. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the overview architecture of the proposed HiStruct+ model. The model consists of a base TLM for sentence encoding and two stacked inter-sentence Transformer layers for hierarchical learning and extractive summarization. The sequence on top is the input document, tokenized by the corresponding tokenizer of the involved TLM. The input embeddings to the TLM are the same as in the original TLM. In order to represent individual sentences, we insert a BOS token at the start of every sentence. Only the BOS token embeddings are preserved as the initial sentence representations (S s ). Each sentence representation is first enriched with a Sentence Linear Position Embedding, which encodes its linear position within the whole document. An additional Sentence Hierarchical Position Embedding (sHE s ) can be added, which is generated by encoding the hierarchical position of the sentence using the proposed hierarchical position encoding method. If section titles are available, we can further enrich the sentence representation by adding a STE or classified STE (ST E s ). The sentence representations with the injected HiStruct information are fed to the two stacked Transformer encoder layers to learn inter-sentence documentlevel hierarchical contextual features. The result is a set of Hierarchical Contextual Sentence Embed-dings (HS s ). The final output layer is a sigmoid classifier, which calculates the confidence score? s of including the s-th sentence in the extractive summary based on the HS s . The loss of the summarization model is the binary classification entropy of the prediction? s against the gold label y s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture</head><p>The two HiStruct injection components shaded in light-green are optional. Removing these from the HiStruct+ model based on BERT, the architecture is identical to BERTSUMEXT <ref type="bibr" target="#b9">(Liu and Lapata, 2019)</ref>, which is a strong baseline against our models on CNN/DailyMail. When using RoBERTa and Longformer as the base TLM, we also construct a baseline model without the two components. The comparison baselines are named as TransformerETS in this paper. The effectiveness of injecting HiStruct information using the proposed methods can be systematically investigated by comparing our HiStruct+ model to the corresponding TransformerETS baseline which uses the same base TLM and the same input length, but is unaware of the HiStruct information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Our models are evaluated on three benchmark datasets for single document summarization, including CNN/DailyMail <ref type="bibr" target="#b17">(See et al., 2017)</ref>, PubMed and arXiv <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref>. <ref type="table" target="#tab_8">Table 4</ref> presents detailed statistics of the datasets.</p><p>The three datasets represent different document types ranging from short news articles to long scientific papers. To emphasize the difference in the hierarchical structure among different datasets, we define the concepts of hierarchical depth (hi-depth) and hierarchical width (hi-width). The hi-depth refers to the number of the hierarchy-levels within the document. Scientific papers have a deeper hierarchy consisting of sections, paragraphs, sentences and tokens (i. e., hi-depth = 4). In news articles, paragraphs are not further grouped into sections (i. e., hi-depth = 3). In this case, we use paragraphs instead of sections as the highest hierarchy level when representing the hierarchical position of sentences (i. e., the first dimension of the SSVs). The hierarchical width, hi-width = N s N hsh , is the ratio of total number of sentences N s and the number of the text-units regarding the highest structure hierarchy N hsh . It indicates how many sentences are there on average in every paragraph/section. The more sentences are there, the second dimension of the SSVs has a more wide range of values, and the values in the first dimension of the SSVs differ a lot from the linear sentence positions. Larger hi-depth and larger hi-width indicate that the hierarchical structure of the dataset is more conspicuous. We hypothesize that the proposed method works better on datasets with more conspicuous hierarchical structures, where hi-depth and hi-width are larger. This will be proved by comparing the performance improvements on the three datasets with different hierarchical characteristics.</p><p>CNN/DailyMail is included as an exemplary dataset with less conspicuous hierarchical structure compared to PubMed and arXiv. The average hi-width over all documents is 1.33, which is much smaller than those in PubMed and arXiv. The dataset contains more than 310k news articles. We use the standard splits given by <ref type="bibr" target="#b17">See et al. (2017)</ref> for training, validation, and testing.</p><p>During data preprocessing, we first split documents into sentences and paragraphs respectively with the Stanford CoreNLP toolkit <ref type="bibr" target="#b11">(Manning et al., 2014)</ref>. The sentences and paragraphs are tokenized, resulting in the lists of sentence tokens and the lists of paragraph tokens. SSVs corresponding to each sentence can be obtained by comparing those lists side by side. For all three datasets, we use a greedy selection algorithm similar to <ref type="bibr" target="#b12">Nallapati et al. (2017)</ref> and <ref type="bibr" target="#b9">Liu and Lapata (2019)</ref> to select sentences from documents as the gold extractive summaries (OR-ACLE). Sentences in the ORACLE summaries are assigned with the gold label 1.</p><p>PubMed and arXiv contain longer scientific papers. PubMed contains papers in the bio-medical domain, while arXiv contains papers in various domains. The average hi-width over all PubMed documents is 15.79, in arXiv it is 37.33. We use the original splits given by <ref type="bibr" target="#b3">Cohan et al. (2018)</ref> for training, validation, and testing. SSVs are obtained by tokenizing the sentences and sections of every document respectively. The details on the generation of section title embeddings and classified section title embeddings can be found in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement our model based on BERTSUMEXT <ref type="bibr" target="#b9">(Liu and Lapata, 2019)</ref> and use HuggingFace Transformers <ref type="bibr" target="#b19">(Wolf et al., 2020)</ref> to make use of the pretrained instances of BERT, RoBERTa and Longformer. On CNN/DailyMail, we select 3 sentences with Trigram Blocking. On PubMed and arXiv, 7 sentences are extracted while Trigram Blocking is not applied (see more details with regard to implementation in Appendix A.3 and A.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We evaluate the performance of our summarization models automatically using ROUGE metrics <ref type="bibr" target="#b8">(Lin, 2004)</ref> including F1 ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL). Tables 1, 2 and 3 summarize the performance of our models in comparison to the baselines and the previously reported SOTA results on CNN/DailyMail, PubMed and arXiv respectively. On all three datasets, ablation studies are systematically conducted to investigate the contributions of different experimental settings. To analyze the output summaries from an overall perspective, we plot the distribution of the extracted sentences on each dataset and compare it to the ORACLE summaries and those outputted by the comparison baseline (see <ref type="figure" target="#fig_2">Figure 2</ref>). Appendix A.6 demonstrates human evaluation of extracted summaries for a more intuitive understanding about the superiority of the proposed system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on CNN/DailyMail</head><p>ROUGE results on CNN/DailyMail are summarized in <ref type="table" target="#tab_2">Table 1</ref>. The first three blocks highlight the results reported by the corresponding papers of abstractive, extractive, and hybrid summarization systems. The best results regarding the respective type of the summarization system are underlined. In the baselines block, the first two lines highlight the ORACLE results that build the upper bounds for extractive systems taking the same number of input tokens. The LEAD-n baselines simply select the first n sentences in a document as its extractive summary. Despite its simplicity, the LEAD-3 baseline already achieves relatively competitive performance. The three TransformerETS models are the corresponding comparison baselines that use the same model architecture and experimental settings as our models but without injected HiStruct information. The following block presents the results of our HiStruct+ models based on different TLMs with various input lengths. To make the evaluation results comparable to the SOTA extractive model BERTSUMEXT, we follow their approach and report the averaged results of three best checkpoints.</p><p>Regardless the base TLM and input length, our HiStruct+ models collectively outperform the corre- sponding TransformerETS baselines by merely injecting the hierarchical position information of sentences. However, the performance improvements gained by our models on CNN/DailyMail are small. One of the reasons might be that we merely inject the hierarchical position information of sentences, section titles are not available. Furthermore, as discussed in Section 4, the hierarchical structure of the CNN/DailyMail documents is not so obvious as those in PubMed and arXiv. Compared to the SOTA extractive model, our best HiStruct+ model produces competitive results. The R2 and RL scores are improved slightly. The model can be reused in many hybrid approaches. When we apply MatchSum based on our best model, the ROUGE results are further increased.</p><p>Ablation studies on CNN/DailyMail (see the results and detailed discussions in Appendix A.5) suggest that the setting la-sum works best for hierarchical position encoding. Two stacked Transformer layers in the summarization model perform better than one or three Transformer layers. When taking longer inputs than the length limit of the TLM, substantial improvements are achieved by using the copied token position embeddings for initialization instead of random initialization.</p><p>The extracted summaries are analyzed in more detail by plotting the proportions of the extracted sentences at each linear position within the whole document as shown in <ref type="figure" target="#fig_2">Figure 2a</ref>. The model in green is our best-performed HiStruct+ model on CNN/DailyMail. The model in orange is the corresponding comparison baseline without injected HiStruct information. The model in blue is the OR-ACLE system, which produces the gold extractive summaries. We can observe that the ORACLE summary sentences are distributed across documents more smoothly, while our HiStruct+ model and the baseline model tend to select the first sentences and fail to select sentences that appear at later positions within the documents. Compared to the baseline, the HiStruct+ model leads to more similar proportions as the ORACLE summaries at the most sentence indices.  ROUGE results on PubMed are summarized in <ref type="table" target="#tab_4">Table 2</ref>. As shown in the baselines block, the ORACLE upper bounds for extractive summarization are increased significantly by increasing the input length, which makes it possible to exploit potential gains from modeling longer input. The LEAD-n baselines do not produce competitive results on PubMed. It indicates that the first sentences in PubMed are not so informative as those in CNN/DailyMail. The last two TransformerETS models in the block are the comparison baselines that are unaware of HiStruct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on PubMed</head><formula xml:id="formula_3">Model ? / Metric ?<label>R1</label></formula><p>The last block in <ref type="table" target="#tab_4">Table 2</ref> presents the results of two groups of HiStruct+ models, grouped by the base TLM used in the summarization model. In PubMed, we can choose to inject the sentence hierarchical position embeddings (sHEs) with or without the section title embeddings (STEs). STEs can be replaced by classified STEs. This can result in three different injection settings for a model group, namely sHE, sHE+STE, and sHE+STE(classified). For each model setting, we report the results of the best-performed checkpoint. Our best HiStruct+ model on PubMed is a model based on Longformer-base taking 15,000 input tokens, which injects the sHEs and the classified STEs into the extractive model. It achieves ROUGE results of 46.59/20.39/42.11, which beat the SOTA extractive model ExtSum-LG+MMR-Select+ collectively on all three ROUGE metrics with improvements of 1.2/0.02/1.12. Taking the SOTA abstractive and hybrid approaches into account, our results are still very competitive.</p><p>All HiStruct+ models produce the competitive results that are better than or very close to the former SOTA results for extractive summarization. They also collectively outperform the TransformerETS baselines by a large margin on all evaluation metrics. The overperformance is much more substantial than that on CNN/DailyMail, even if only the hierarchical position information is injected. This supports our hypothesis that the proposed model works better on datasets with more conspicuous hierarchical structures.</p><p>Ablation studies on PubMed suggest that the largest improvement of our models against the baseline is contributed by the hierarchical position information of sentences. This is observed when we compare the three models in the first group of HiStruct+ models with the first TransformerETS baseline. Injecting merely sHE, the results are already increased by 4.07/3.88/3.86. When the section title embedding (STE) is included additionally, the results are further increased by 0.73/0.65/0.68. When using classified STE instead, the ROUGEs are increased by a small margin of 0.1/0.1/0.09. Comparing the second group of HiStruct+ models to the second TransformerETS baseline, it is also observed that injecting the sHE leads to the largest performance gain. <ref type="figure" target="#fig_2">Figure 2b</ref>. The model in green is our best-performed HiStruct+ model on PubMed, the model in orange is the corresponding TransformerETS baseline, the model in blue is the ORACLE system. The ORACLE summaries are distributed across documents evenly. The Trans-formerETS baseline favors the first 5 sentences and ignores the sentences appearing at later positions. In contrast, our HiStruct+ model overcomes the problem of focusing merely on the first sentences. The outputs of the HiStruct+ model are close to the ORACLE summaries. It indicates that by injecting HiStruct information explicitly using our method, the model successfully learns the deeper internal hierarchical structure of the PubMed documents and relies less on the linear sentence positions.  ROUGE results on arXiv are summarized in <ref type="table" target="#tab_6">Table 3</ref>. The results of the HiStruct+ models are presented in two groups. The first group takes 15k input tokens, while the second group increases the input length to 28k. In the groups, different injection settings are compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The extracted summaries analysis on PubMed test set is demonstrated in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on arXiv</head><p>Our best-performed HiStruct+ model on arXiv is an extractive model based on Longformer-base with 28k input tokens, injecting the sHEs with the original STEs. This model beats the results achieved by ExtSum-LG+RLoss and sets the new SOTA ROUGEs for extractive summarization on arXiv to 45.22/17.67/40.16.</p><p>All HiStruct+ models collectively outperform the corresponding TransformerETS baselines (i. e., the last two models in the baselines block) by a large margin on all ROUGE scores. On this dataset, the HiStruct+ improvement is much more significant than those on both CNN/DailyMail and PubMed. The arXiv dataset has the largest hi-width among the three datasets and the hierarchical structure is most conspicuous, which might be the reason why the HiStruct+ models yield the largest performance improvements on arXiv.</p><p>Ablation studies in the first HiStruct+ group also suggest that the largest improvement of our HiStruct+ model against the TransformerETS baseline is contributed by injecting the sentence hierarchical position information, which is encoded as sHEs. The effect of using the classified STE on arXiv is opposite to that on PubMed. The summarization performance declines slightly when we replace the STE with the classified STE. This outcome occurs in the second group of HiStruct+ models as well. We notice the fact that there are 500k unique section titles in arXiv, while PubMed contains 164k unique section titles. Accordingly, it becomes much more difficult to group a large number of section titles correctly into several section classes. Furthermore, the PubMed dataset contains papers mostly in the bio-medical domain. The structure of those papers tends to follow specific writing conventions in the bio-medical sciences. The arXiv dataset, in contrast, contains scientific papers that are not limited to a specific domain. As consequence, the document structure and the writing styles are more diverse.</p><p>The extracted summaries analysis on arXiv is demonstrated in <ref type="figure" target="#fig_2">Figure 2c</ref>. The baseline (in orange) tends to select the first sentence and the sentences indexed between 10 and 20, while it excludes sentences at later positions. It is clearly observed that the summary sentences extracted by the HiStruct+ model are evenly distributed, the informative sentences appearing at later positions are not ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This work addresses hierarchical modeling for extractive text summarization by explicitly leveraging hierarchical structure information, including section titles, as well as hierarchical position information of the sentences. We propose an intuitive and interpretable approach to formulate, extract, encode and inject the hierarchical structure information into an extractive summarization model.</p><p>The proposed HiStruct+ models are systematically evaluated on CNN/DailyMail, PubMed, and arXiv. On PubMed, our model increases the former SOTA ROUGEs for extractive summarization by 1.2/0.02/1.12. On arXiv, the new SOTA results for extractive summarization are set to <ref type="bibr">45.22/17.67/40.16</ref>. Our ablation studies suggest that the SOTA performance are mostly gained by providing the hierarchical position information of sentences to the summarization model. When comparing the HiStruct+ models with the baselines that are unaware of the HiStruct information, improvements are consistently observed on all three datasets under various experimental settings, indicating the effectiveness of the proposed method. Moreover, our experiments show that the more conspicuous hierarchical structure the dataset has, the larger the improvements of our method are. The proposed metrics of hi-depth and hi-width determine whether it is worth using our method by comparing the metrics of any dataset to those of the three involved datasets.</p><p>Utilizing the HiStruct information also for abstractive summarization is subject of future work. Similarly, we see great potential in an encoderdecoder architecture with the proposed HiStruct injection components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.  The CNN/DailyMail 2 , PubMed and arXiv 3 datasets are used in experiments. We use the original splits provided by <ref type="bibr" target="#b17">See et al. (2017)</ref> and <ref type="bibr" target="#b3">Cohan et al. (2018)</ref> for training, validation and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pre-defined Section Title Classes</head><p>The pre-defined dictionaries of the typical section title classes and the corresponding in-class section titles are released in our GitHub project (see Section 1). There are 164,195 unique section titles in PubMed, and 500,015 in arXiv, which are encoded as section title embeddings (STE) respectively using the proposed encoding method.</p><p>For PubMed, we define 8 section title classes: introduction, background (i. e., background, review and related work), case (i. e., case reports), method, result, discussion, conclusion and additional information (i. e., additional information such as conflicts of interest, financial support and acknowledgments). For arXiv, we define 10 classes: introduction, background, case, theory (i. e., problem formulation and proof of theorem), method, result, discussion, conclusion, reference and additional information. Classified STEs are prepared accordingly by replacing the original STEs of the intraclass section titles with the encoding of the section title class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details</head><p>The learning rate schedule follows Liu and Lapata (2019) with warm-up. On CNN/DailyMail, we train the HiStruct+ models and the Trans-formerETS baselines 50,000 steps with 10,000 warm-up steps. On PubMed and arXiv, the models are trained 70,000 steps with 10,000 warm-up steps when taking 15,000 tokens as input. When training models on arXiv with 28,000 input tokens, we train 100,000 steps with 10,000 warm-up steps.</p><p>The number of the extracted sentences depends on the dataset. On CNN/DailyMail, we follow <ref type="bibr" target="#b9">Liu and Lapata (2019)</ref> to select 3 sentences for each document as its extractive summary and apply Trigram Blocking <ref type="bibr" target="#b14">(Paulus et al., 2018)</ref> to reduce the redundancy of the selected sentences. On PubMed and arXiv, 7 sentences are extracted without Trigram Blocking.</p><p>The length limit of the original TLM is overcome by adding extra token linear position embeddings (tPE) to cover the desired length. The additional tPE are then trained with the whole summarization model. Instead of initializing them randomly, we copy the original tPE of the TLM multiple times until the desired length is covered.</p><p>The HiStruct+ models and the TransformerETS baselines are trained on 3 GPUs (NVIDIA? Quadro RTX? 6000 GPUs with 24GB memory) with gradient accumulation every two steps. Checkpoints are saved and evaluated on the validation set every 1,000 steps. The top-3 checkpoints based on the validation loss are kept. The batch size varies with the base TLM and the input length. On CNN/DailyMail, the base TLM is fine-tuned with the whole summarization model. Due to resource limitation, the TLM (i. e., Longformer) is not finetuned when training the summarization model on PubMed and arXiv with longer inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Model Architectures and Experimental Settings</head><p>The detailed model architectures and experimental settings for the models trained on CNN/DailyMail, PubMed and arXiv are summarized in <ref type="table" target="#tab_10">Table 5</ref>, <ref type="table" target="#tab_11">Table 6</ref> and <ref type="table" target="#tab_13">Table 7</ref> respectively. The detailed model architectures and experimental settings include:</p><p>Base TLM: the Transformer language model used for sentence encoding in the summarization system. Input length: how many tokens are taken as input. Extra tPE: how to initialize the extra input token linear position embeddings when taking longer input. We can choose to randomly initialize them or copy the original ones. FT: whether the base TLM is fine-tuned with the entire summarization model. TL: the number of the Transformer layers stacked upon the base TLM for extractive summarization. WS: warmup steps, how many steps are used for warming-up of the learning rate. TS: the total training steps. BS: batch size, how many documents are used as one batch during training. AC: accumulation count, gradient accumulation every k steps. GPU: the number of GPUs used for training, we use NVIDIA? Quadro RTX? 6000 GPUs with 24GB memory. HiStruct: the injection setting. Hierarchical structure information that can be injected into the summarization model are: sHE (i. e., sentence hierarchical position embeddings), STE (i. e., section title embeddings), or STE(classified) (i. e., classified section title embeddings) HPE: the hierarchical position encoding method used in the model. The method is based on the sinusoidal (sin) or the learnable (la) linear position encoding method associated with a combination mode (i. e., sum/mean/concat) #PE: the numbers of the learned position embeddings for each hierarchy-level of the hierarchical positions and the linear sentence positions, when using the learnable position encoding method. We set them to a same value during training. SS: saving steps, save checkpoints every k steps. n: select n sentences as the extractive summary for each document. TB: trigram blocking, whether to apply Trigram Blocking during sentence selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Ablation Studies on CNN/DailyMail</head><p>The effect of token-level hierarchical position embeddings is investigated in experiments. The hierarchical position embeddings of tokens are generated as followings: Given the t-th token within the document, its hierarchical position is represented by Equation 7:</p><formula xml:id="formula_4">T SV t = (a t , b t , c t )<label>(7)</label></formula><p>where a t represents the linear position of the section which contains the token, b t is the sentence's position within the section and c t is the linear position of the token within the sentence. Given the t-th token and the desired size of the output embeddings d, its token hierarchical position embeddings (tHE) is encoded by Equations 8, 9, 10, using different combination modes.</p><formula xml:id="formula_5">tHEsum(t, d) = P E(at, d) + P E(bt, d) + P E(ct, d) (8) tHEmean(t, d) = P E(at, d) + P E(bt, d) + P E(ct, d) 3 (9) tHE concat (t, d) = P E(at, d 3 )|P E(bt, d 3 )|P E(ct, d 3 )<label>(10)</label></formula><p>Initial experiments are conducted to assess the summarization performance of the HiStruct+ models with or without the tHE. For this purpose, we compare a HiStruct+ model merely injecting sentence hierarchical position embeddings (i. e., sHE) with a HiStruct+ model with both sentence and token hierarchical position embeddings (i. e., sHE&amp; tHE). That is, it adds the corresponding tHEs to the input embeddings at each input position, which are fed into the TLM. It also injects sHEs into the output sentence representations. <ref type="table" target="#tab_14">Table 8</ref> summarizes the evaluation results of three groups of HiStruct+ models based on different TLM with various input lengths. In each group, all experimental settings and parameters are the same, except for the injection setting of tHE. The experimental results suggest that the HiStruct+ models with merely sHE consistently outperform the HiStruct+ models with both sHE &amp; tHE under various circumstances. The reason might be that we directly fine-tune the TLM on the extractive summarization task. When adding extra tHE to the input embeddings to the TLM, we do not pre-train the TLM with the adjusted inputs. It is reasonable that the TLM has difficulties in understanding of the new inputs based on the knowledge learned from the original format of encoding. Previous works, such as LayoutLM , LamBERT <ref type="bibr" target="#b5">(Garncarek et al., 2020)</ref> and HIBERT <ref type="bibr" target="#b25">(Zhang et al., 2019)</ref>, which adjust the input embeddings or the encoder architecture of the pre-trained TLM, continue to pre-train the TLM on their own data. Continuing pre-training of the language models is a core part of these works and leads to significant improvements on downstream tasks. Due to lack of computing resources, we are not able to pre-train the language models. Furthermore, the key goal of our work is    <ref type="table" target="#tab_4">Table 2</ref>). The settings not included in the table are the same for all models. Input length: 15,000; Extra tPE: copied; FT: no; TL:2; WS:10,000; TS:70,000; AC:2; GPU:3; SS:1,000; n: 7; TB:no.</p><p>to experiment with various methods to make use of the internal hierarchical text structure information for extractive summarization. In this work, we conduct further experiments without token-level hierarchical position information and leave for future work the pre-training of language models with the adjusted input embeddings. The effect of different settings for hierarchical position encoding is investigated. As explained previously, based on different position encoding (PE) methods (i. e., sin or la) associated with various combination modes (i. e., sum, mean or concat), we have totally six different settings for hierarchical position encoding: sin-sum, sin-mean, sin-concat, la-sum, la-mean and la-concat. We investigate the effect of those 6 encoding settings systematically in experiments while keeping the rest settings and parameters the same, so that the evaluation results are comparable. <ref type="table">Table 9</ref> summarizes the evaluation results of six HiStruct+ models using the six encoding settings respectively, which are all trained on CNN/DailyMail based on BERT-base with 1,024 input tokens, injecting merely sHE. We observe that when using the la method, the combination mode sum leads to better results compared to the other modes (see the first three columns in <ref type="table">Table 9</ref>). When using the sin method, the various combination modes do not make a conspicuous difference in summarization performance. The sum and concat modes perform slightly better. When using the sum mode, the la and the sin methods produce similar results (see the first row in <ref type="table">Table 9</ref>).</p><p>The effect of the encoding settings la-sum vs. sin-sum is further investigated in experiments. As discussed above, the encoding settings la-sum and sin-sum lead to similar results. We conduct experiments to further investigate the effect of using these methods. We also compare our HiStruct+ models with the corresponding TransformerETS baseline which differs from our models only in that it does not take into account extra HiStruct information.    <ref type="table" target="#tab_2">Table 10</ref> includes the ROUGEs of three set of comparison models, which use different TLM with various input lengths. In each group, the first model is the baseline without HiStruct injection. The second model and the third model differ from each other only with regard to the encoding setting. The experimental results suggest that both of the settings improve the summarization performance of the baseline model. It is also observed that the la-sum method outperforms the sin-sum method slightly on CNN/DailyMail. The differences are not substantial.</p><p>The effect of the number of the stacked Transformer layers is investigated in our experiments. We fine-tune an extended BERT-base model with 1,024 input tokens for extractive summarization. We construct the HiStruct+ models with 1, 2,  <ref type="table">Table 9</ref>: Ablation study on CNN/DailyMail (b). Comparison of HiStruct+ models using various hierarchical position encoding methods based on the sinusoidal or the learnable PE method, associated with the combination modes of sum, mean and concat respectively. Underlined are the best ROUGEs in each block.</p><p>3 stacked Transformer layers respectively, while keeping all other settings the same. The results of those three HiStruct+ models are reported in the first block in <ref type="table" target="#tab_2">Table 11</ref>. It is suggested that two stacked Transformer layers perform best in our HiStruct+ models for extractive summarization.</p><p>The effect of different initialization strategies for the additional input Token Linear Position Embeddings is also investigated in experiments. When taking input texts longer than the original input length of the base TLM, we need to add extra Token Linear Position Embeddings (tPE) for each extended position. We can choose to randomly initialize the extra tPE or copy the original ones to cover the extended input length. To investigate the effect of different initialization strategies, we use the basic settings of the HiStruct+ model with two summarization layers, namely the second model in the first block in <ref type="table" target="#tab_2">Table 11</ref>. To build the comparison model, only the initialization strategy is changed to random. As shown in the second block in   11, substantial improvements are achieved by using the copied tPEs for initialization instead of random initialization. ROUGE-1, ROUGE-2 and ROUGE-L are increased by 2.84, 2.51 and 2.95 respectively. The effect of the Sentence Linear Position Embeddings is also investigated in experiments. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, besides the hierarchical positions of each sentence, we also take the linear position of each sentence within the whole document into account by adding a Sentence Linear Position Embedding (sPE) to each sentence representation. We assess the effect of the sPE by comparing two HiStruct+ models with or without the sPE. The second model in the first block in <ref type="table" target="#tab_2">Table 11</ref> is compared to a model that differs from it only in the injection of sPE. The results are shown in the third block in <ref type="table" target="#tab_2">Table 11</ref>. The HiStruct+ model with sPE outperforms the HiStruct+ model without sPE by a small margin regarding all ROUGE metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Human Evaluation of Extracted Summaries</head><p>To have a more intuitive understanding about the superiority of the proposed system, we showcase two samples in <ref type="figure">Figure 3</ref> for human evaluation and case analysis. The extractive summaries predicted by the HiStruct+ model and the baseline model are demonstrated respectively, in comparison with the gold summary (i.e., the abstract of the paper). To construct a final summary, top-7 sentences with the highest scores predicted by the model are extracted, and then combined in their original order. The first arXiv sample shows that the baseline simply selects the first sentences. The predicted summary focuses on detailed background knowledge and lacks an overview of the proposed work. In contrast, our HiStruct+ model selects sentences at later positions. The first five sentences introduce the main content from an overall perspective. The last two sentences draw conclusions and give an outlook to future work, which is indicated by the phrases highlighted in green.</p><p>The PubMed sample also indicates that the baseline favors the first sentences, which is consistent with our observations in <ref type="figure" target="#fig_2">Figure 2</ref>. Although the last two sentences highlight the same conclusion as in the gold summary that locally informed diagnosis and treatment strategies are needed, too much background information is unnecessarily included in the first five sentences. Our HiStruct+ model selects more informative sentences at later positions. The predicted summary covers all key parts of the gold summary: 1). the statistics are reported (i.e., 26% of primary tuberculosis (tb) was multidrug resistant (mdr)); 2). the novel strain s256 is mentioned; 3). the conclusion is highlighted. The overall topic of the work is especially highlighted by the sentence with the green-colored phrase. <ref type="figure">Figure 3</ref>: Two samples for human evaluation and case analysis of the extractive summaries predicted by the HiStruct+ model and the baseline model, in comparison with the gold summary (i.e., the abstract of the paper). The first sample is selected from the arXiv dataset, while the second sample is from PubMed. Top-7 sentences with the highest predicted scores are extracted, and then combined in their original order to construct a final summary. Their linear indices within the original document are shown in the second row of each table. The texts highlighted in yellow are the key words and the main content that appear in the gold summary. The phrases highlighted in green indicate typical parts of a scientific paper such as summary and future work. Sentences are split by '&lt;q&gt;'.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the HiStruct+ model. The model consists of a base TLM for sentence encoding and two stacked inter-sentence Transformer layers for hierarchical contextual learning with a sigmoid classifier for extractive summarization. The two blocks shaded in light-green are the HiStruct injection components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>sHEsum(s, d) = P E(as, d) + P E(bs, d) sHE concat (s, d) = P E(as, d 2 )|P E(bs, d 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Proportions of the extracted sentences at each linear position. The x-axis values are linear sentence indices, the y-axis values are percentages of the extracted sentences. In this figure, only the first 25 sentence indices are included due to space limitation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2203.09629v1 [cs.CL] 17 Mar 2022</figDesc><table><row><cell></cell><cell>STE1</cell><cell>STE2</cell><cell>STE3</cell></row><row><cell></cell><cell></cell><cell>Sigmoid classifier</cell><cell></cell></row><row><cell>Sentence Scores</cell><cell>?1</cell><cell>?2</cell><cell>?3</cell></row><row><cell>Gold Labels</cell><cell>y1</cell><cell>y2</cell><cell>y3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>F1 ROUGE results on CNN/DailyMail. Bold are the scores of the HiStruct+ models that are better than the corresponding TransformerETS baseline. The symbol * indicates an improvement over the corresponding SOTA ROUGE for extractive summarization.</figDesc><table><row><cell>Model ? / Metric ?</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell cols="2">Abstractive</cell><cell></cell><cell></cell></row><row><cell>BERTSUMABS (2019)</cell><cell cols="2">41.72 19.39</cell><cell>38.76</cell></row><row><cell>BART (2020)</cell><cell cols="2">44.16 21.28</cell><cell>40.90</cell></row><row><cell>PEGASUS (2020)</cell><cell cols="2">44.17 21.47</cell><cell>41.11</cell></row><row><cell>BigBird PEGASUS (2020)</cell><cell cols="2">43.84 21.11</cell><cell>40.74</cell></row><row><cell cols="2">Extractive</cell><cell></cell><cell></cell></row><row><cell>HIBERT (2019)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(BERT-base)</cell><cell cols="2">42.31 19.87</cell><cell>38.78</cell></row><row><cell>(BERT-large)</cell><cell cols="2">42.37 19.95</cell><cell>38.83</cell></row><row><cell>BERTSUMEXT (2019)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(BERT-base)</cell><cell cols="2">43.25 20.24</cell><cell>39.63</cell></row><row><cell>(BERT-large)</cell><cell cols="2">43.85 20.34</cell><cell>39.90</cell></row><row><cell>Hybrid</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MatchSum (2020)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(BERT-base)</cell><cell cols="2">44.22 20.62</cell><cell>40.38</cell></row><row><cell>(RoBERTa-base)</cell><cell cols="2">44.41 20.86</cell><cell>40.55</cell></row><row><cell cols="2">Reproduced baselines</cell><cell></cell><cell></cell></row><row><cell>ORACLE (512 tok.)</cell><cell cols="2">52.46 30.76</cell><cell>48.66</cell></row><row><cell>ORACLE (1,024 tok.)</cell><cell cols="2">55.45 32.78</cell><cell>51.59</cell></row><row><cell>LEAD-3</cell><cell cols="2">40.33 17.39</cell><cell>36.56</cell></row><row><cell>TransformerETS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base (1,024 tok.)</cell><cell cols="2">43.32 20.27</cell><cell>39.69</cell></row><row><cell>BERT-large (512 tok.)</cell><cell cols="2">43.45 20.36</cell><cell>39.83</cell></row><row><cell>RoBERTa-base (1,024 tok.)</cell><cell cols="2">43.62 20.53</cell><cell>39.99</cell></row><row><cell cols="2">Our models (Extractive)</cell><cell></cell><cell></cell></row><row><cell>HiStruct+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base (1,024 tok.)</cell><cell cols="2">43.38 20.33</cell><cell>39.78</cell></row><row><cell>BERT-large (512 tok.)</cell><cell cols="3">43.49 20.40* 39.90*</cell></row><row><cell>RoBERTa-base (1,024 tok.)</cell><cell cols="3">43.65 20.54* 40.03*</cell></row><row><cell cols="2">Our models (Hybrid)</cell><cell></cell><cell></cell></row><row><cell>HiStruct+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa-base (1,024 tok.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">&amp; MatchSum (RoBERTa-base) 44.31 20.73</cell><cell>40.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>F1 ROUGE results on PubMed. Bold are the scores of the HiStruct+ models that are better than the corresponding TransformerETS baseline. The symbol * indicates that the corresponding SOTA ROUGE for extractive summarization is improved by our model. The symbol ' indicates that the SOTA ROUGEs (incl. all types of summarization approaches) are outperformed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>F1 ROUGE results on arXiv. Bold are the scores of the HiStruct+ models that are better than the corresponding TransformerETS baseline. The symbol</figDesc><table /><note>* indicates that the corresponding SOTA ROUGE for extractive summarization is improved by our model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>1</head><label></label><figDesc>Statistics of the Datasets</figDesc><table><row><cell>Dataset</cell><cell cols="2">CNN/DailyMail PubMed</cell><cell>arXiv</cell></row><row><cell>Raw documents</cell><cell></cell><cell></cell><cell></cell></row><row><cell>avg. #words</cell><cell cols="3">792.24 2,967.22 5,825.68</cell></row><row><cell>avg. #sentences</cell><cell>40.31</cell><cell>86.37</cell><cell>206.3</cell></row><row><cell>avg. #sections*</cell><cell>31.2</cell><cell>5.91</cell><cell>5.55</cell></row><row><cell>avg. hi-width</cell><cell>1.33</cell><cell>15.79</cell><cell>37.33</cell></row><row><cell cols="2">Raw gold summaries</cell><cell></cell><cell></cell></row><row><cell>avg. #words</cell><cell>53.25</cell><cell>202.42</cell><cell>272</cell></row><row><cell>avg. #sentences</cell><cell>3.75</cell><cell>6.85</cell><cell>9.61</cell></row><row><cell cols="2">Novel n-grams in gold summaries</cell><cell></cell><cell></cell></row><row><cell>avg. % novel</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1grams</cell><cell>13.97</cell><cell>0.2</cell><cell>0.15</cell></row><row><cell>2grams</cell><cell>51.79</cell><cell>2.69</cell><cell>2.73</cell></row><row><cell>Nr. of documents</cell><cell></cell><cell></cell><cell></cell></row><row><cell>#train</cell><cell>287,227</cell><cell>119,924</cell><cell>203,037</cell></row><row><cell>#val</cell><cell>13,368</cell><cell>6,633</cell><cell>6,436</cell></row><row><cell>#test</cell><cell>11,490</cell><cell>6,658</cell><cell>6,440</cell></row><row><cell cols="3">Documents tokenized by the RoBERTa tokenizer</cell><cell></cell></row><row><cell>avg. doc length</cell><cell>964</cell><cell>4,252</cell><cell>8,991</cell></row><row><cell>75% doc length</cell><cell>1,219</cell><cell>5,382</cell><cell>11,289</cell></row><row><cell>85% doc length</cell><cell>1,448</cell><cell>6,709</cell><cell>14,294</cell></row><row><cell>99% doc length</cell><cell>2,345</cell><cell>15,277</cell><cell>35,559</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Statistics of the datasets. * avg. #paragraphs in CNN/DailyMail.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">: Detailed model architectures and experimental settings for models trained on CNN/DailyMail (also see</cell></row><row><cell cols="6">Table 1). The settings not included in the table are the same for all models. FT: yes, TL:2, WS:10,000, TS:50,000,</cell></row><row><cell>AC:2, GPU:3, SS:1,000, n: 3, TB:yes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models/Settings</cell><cell>Base TLM</cell><cell>BS</cell><cell>HiStruct</cell><cell>HPE</cell><cell>#PE</cell></row><row><cell></cell><cell cols="2">Reproduced baselines</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransformerETS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Longformer-base (15k tok.)</cell><cell>Longformer-base</cell><cell>500</cell><cell>none</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Longformer-large (15k tok.) Longformer-large 256</cell><cell>none</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Our models (Extractive)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HiStruct+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Longformer-base (15k tok.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sHE+STE(classified)</cell><cell>Longformer-base</cell><cell cols="4">500 sHE+STE(classified) la-sum 450</cell></row><row><cell>sHE+STE</cell><cell>Longformer-base</cell><cell>500</cell><cell>sHE+STE</cell><cell cols="2">la-sum 450</cell></row><row><cell>sHE</cell><cell>Longformer-base</cell><cell>500</cell><cell>sHE only</cell><cell cols="2">la-sum 450</cell></row><row><cell>Longformer-large (15k tok.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sHE+STE(classified)</cell><cell cols="5">Longformer-large 256 sHE+STE(classified) la-sum 450</cell></row><row><cell>sHE</cell><cell cols="2">Longformer-large 256</cell><cell>sHE only</cell><cell cols="2">la-sum 450</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Detailed model architectures and experimental settings for models trained on PubMed (also see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Detailed model architectures and experimental settings for models trained on arXiv (also seeTable 3). The settings not included in the table are the same for all models.</figDesc><table><row><cell>Extra tPE: copied; FT: no; TL:2; WS:10,000;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Ablation study on CNN/DailyMail (a). Com-</cell></row><row><cell>parison of HiStruct+ models with/without token-level</cell></row><row><cell>hierarchical position embeddings (tHE). The models in</cell></row><row><cell>different blocks are based on different TLMs with vari-</cell></row><row><cell>ous input lengths. Underlined are the best ROUGEs in</cell></row><row><cell>each block.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table</head><label></label><figDesc></figDesc><table><row><cell>Experimental Results</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell>BERT-base (1,024 tok.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransformerETS</cell><cell cols="3">43.32 20.27 39.69</cell></row><row><cell>HiStruct(la-sum)+</cell><cell cols="3">43.38 20.33 39.78</cell></row><row><cell>HiStruct(sin-sum)+</cell><cell cols="3">43.37 20.27 39.75</cell></row><row><cell>BERT-large (512 tok.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransformerETS</cell><cell cols="3">43.45 20.36 39.83</cell></row><row><cell>HiStruct(la-sum)+</cell><cell cols="2">43.49 20.4</cell><cell>39.9</cell></row><row><cell>HiStruct(sin-sum)+</cell><cell cols="2">43.46 20.4</cell><cell>39.85</cell></row><row><cell>RoBERTa-base (1,024 tok.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransformerETS</cell><cell cols="3">43.62 20.53 39.99</cell></row><row><cell>HiStruct(la-sum)+</cell><cell cols="3">43.65 20.54 40.03</cell></row><row><cell>HiStruct(sin-sum)+</cell><cell cols="3">43.64 20.56 40.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Ablation study onCNN/DailyMail (c). Comparison of the TransformerETS baseline and the HiStruct+ models using the la-sum and sin-sum settings for hierarchical position encoding respectively. The models in different blocks are based on different TLMs with various input lengths. Underlined are the best ROUGEs in each block.</figDesc><table><row><cell>Experimental Results</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell>HiStruct(sHE)+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base (1,024 tok.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>-#Transformer layers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>for summarization</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="3">43.29 20.25 39.69</cell></row><row><cell>2</cell><cell cols="3">43.37 20.27 39.75</cell></row><row><cell>3</cell><cell cols="3">43.16 20.15 39.56</cell></row><row><cell>-Extra Token Linear</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Position Embeddings (tPE)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Randomly initialized</cell><cell cols="3">40.53 17.76 36.8</cell></row><row><cell>Copied</cell><cell cols="3">43.37 20.27 39.75</cell></row><row><cell>-With/without Sentence Linear</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Position Embeddings (sPE)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>With sPE</cell><cell cols="3">43.37 20.27 39.75</cell></row><row><cell>Without sPE</cell><cell cols="3">43.31 20.25 39.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Ablation study on CNN/DailyMail (d). The first block deals with the variation of the numbers of inter-sentence Transformer layers stacked on top of the TLM. The second block deals with the different methods to initialize extra input Token Linear Position Embeddings when taking longer input. The third block deals with the effect of Sentence Linear Position Embeddings. Underlined are the best ROUGEs in each block.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/QianRuan/histruct</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://cs.nyu.edu/~kcho/DMQA/ 3 https://github.com/armancohan/long-summarization</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work presented in this article has received funding from the German Federal Ministry of Education and Research (BMBF) through the project QURA-TOR (no. 03WKDA1A).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abstractive text summarization based on language model conditioning and locality modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Aksenov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Moreno-Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bourgonje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schwarzenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6680" to="6689" />
		</imprint>
	</monogr>
	<note>Leonhard Hennig, and Georg Rehm. European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">LAMBERT: layout-aware language modeling using BERT for information extraction. ArXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Garncarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Stanislawek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Halama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Gralinski</surname></persName>
		</author>
		<idno>abs/2002.08087</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A divide-and-conquer approach to the summarization of academic articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexios</forename><surname>Gidiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<idno>abs/2004.06190</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aspect-based document similarity for research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ostendorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Till</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bela</forename><surname>Gipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Rehm</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.545</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6194" to="6206" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On extractive and abstractive neural document summarization with transformer language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9308" to="9319" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extractive summarization of long documents by combining global and local context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1298</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3011" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Systematically exploring redundancy reduction in summarizing long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="516" to="528" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Layoutlm: Pretraining of text and layout for document image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3394486.3403172</idno>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HI-BERT: Document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1499</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5059" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Extractive summarization as text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.552</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
