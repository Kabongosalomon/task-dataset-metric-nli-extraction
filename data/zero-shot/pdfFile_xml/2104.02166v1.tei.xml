<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Optical Flow from a Few Matches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Jiang</surname></persName>
							<email>shihao.jiang@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV 3 Data61</orgName>
								<address>
									<region>CSIRO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
							<email>yao.lu@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV 3 Data61</orgName>
								<address>
									<region>CSIRO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<email>hongdong.li@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV 3 Data61</orgName>
								<address>
									<region>CSIRO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
							<email>richard.hartley@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV 3 Data61</orgName>
								<address>
									<region>CSIRO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Optical Flow from a Few Matches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art neural network models for optical flow estimation require a dense correlation volume at high resolutions for representing per-pixel displacement. Although the dense correlation volume is informative for accurate estimation, its heavy computation and memory usage hinders the efficient training and deployment of the models. In this paper, we show that the dense correlation volume representation is redundant and accurate flow estimation can be achieved with only a fraction of elements in it. Based on this observation, we propose an alternative displacement representation, named Sparse Correlation Volume, which is constructed directly by computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experiments show that our method can reduce computational cost and memory use significantly, while maintaining high accuracy compared to previous approaches with dense correlation volumes. Code is available at https: //github.com/zacjiang/scv.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow estimation is a classic problem in computer vision <ref type="bibr" target="#b10">[11]</ref>. It aims at finding pixelwise correspondences between two images. Traditionally it has been formulated as an optimization problem solved by continuous <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref> or discrete <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> optimization. Since the development of deep learning, optical flow estimation has been formulated as a learning problem where direct regression from a neural network becomes a common approach <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>One popular representation in dense correspondence problems is the correlation (cost) volume, first introduced by Hosni et al. <ref type="bibr" target="#b11">[12]</ref>. Correlation volumes give an explicit representation of per-pixel displacements and have demonstrated their wide use in learning problems of stereo matching <ref type="bibr" target="#b17">[18]</ref> and optical flow <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. Contrary to stereo matching problems, where the search space is along a scanline, optical flow problems have a 2D search space, which leads to two challenges: large memory consumption and high computational cost when directly processing a 4D volume. To reduce the memory and computational cost, existing approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref> first build a feature pyramid and compute correlation volumes at coarse resolutions, then gradually warp upper-level feature maps based on upsampled flow and construct a local correlation volume over a limited search range. One notable problem observed in previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref>, was that coarse-to-fine frameworks fail to address the case when the flow displacement is larger than the flow structure, i.e. the famous small objects moving fast problem.   Recent approaches, Devon and RAFT <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref> proposed using direct search in the second image to remove the need for warping. RAFT especially demonstrated the benefit of first constructing an all-pairs correlation volume and directly processing it at a single resolution rather than in a coarse-to-fine manner. However, the all-pairs correlation volume requires pair-wise dot product between the two feature maps. Hence, both the time and the space complexity are O(N 2 ), where N is the number of pixels of an image. A small N is required to reduce the memory consumption and therefore RAFT can only use 1/8 resolution feature maps. A low-resolution feature map cannot fully represent the fine details of an image. We wonder if there is a way to construct a correlation volume with the all-pairs search range but without exceeding the maximum GPU memory. We thus question the necessity of storing all pairwise correlations and hypothesize that only storing the top-k correlations for each pixel might be sufficient.</p><p>Our intuition is that a feature vector in one image has only a few feature vectors in the other image with high correlation to match. Hence, there could be large redundancy in the dense correlation volume where the small correlations do not contribute to the prediction. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the comparison between a dense correlation volume and a sparse correlation volume.</p><p>We propose a Sparse Correlation Volume representation, where only the top-k correlations for each pixel are stored in a sparse data structure defined by a {value, coordinates} pair. In this paper, we demonstrate how a sparse correlation volume representation can be used to solve the optical flow problem. We propose an approach to construct and process such a sparse correlation volume in an optical flow learning framework. We demonstrate that even if only a small fraction of elements are stored, our results are still comparable to previous work <ref type="bibr" target="#b28">[29]</ref> which employs a dense correlation volume. We finally demonstrate that the sparse approach allows the construction of a high-resolution correlation volume, which can predict the motions of fine structures more accurately than previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Let I 1 , I 2 : Z 2 ? R 3 be two RGB images. The problem is to estimate a dense flow field f : Z 2 ? R 2 that maps each pixel coordinate x to a displacement vector f (x).</p><p>In modern deep learning optical flow approaches, a feature extraction network is first applied to extract feature maps from the image pair, F 1 , F 2 : Z 2 ? R c , where c is the number of channels. The correlation volume C : Z 4 ? R is formed by computing inner products between pairwise feature vectors,</p><formula xml:id="formula_0">C(x, d) = F 1 (x) ? F 2 (x + d).<label>(1)</label></formula><p>The output is a four-dimensional tensor which can be represented as a set</p><formula xml:id="formula_1">C = {C(x, d) | x ? X , d ? D}.<label>(2)</label></formula><p>Here, X = [0, h) ? [0, w) ? Z 2 is the domain of the feature map F 1 and |X | = hw, where h and w represent the height and width of F 1 respectively. The displacement set D is defined as D = [?d, d] 2 ? Z 2 where d represents the maximum displacement along the x or y direction and |D| = (2d + 1) 2 . Therefore, the correlation volume C contains hw(2d + 1) 2 elements.</p><p>To reduce the size of the correlation volume, previous approaches use coarse-to-fine and warping methods to constrain the size of d <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>. To handle large displacements accurately, RAFT <ref type="bibr" target="#b28">[29]</ref> constructs an all-pairs correlation volume where the displacement range contains the entire feature map. Excluding out-of-range matches, RAFT's all-pairs correlation volume contains N 2 elements where N = hw. Therefore, lower-resolution feature maps are required to constrain N . In this work, we show that the all-pairs correlation volume can in fact be a sparse tensor, where only a small fraction of the values are stored and processed. We show that we can effectively reduce the spatial complexity from O(N 2 ) to O(N k) with only a minor drop of performance, where k gives the number of matches we want to keep. The main idea is demonstrated in <ref type="figure" target="#fig_3">Figure 2</ref>.  <ref type="figure">Figure 3</ref>: Network architecture and residual flow prediction for a single iteration. <ref type="bibr" target="#b0">(1)</ref> F1 and F2 are feature maps extracted by a feature extraction network. We form the 4D sparse correlation volume by first computing a set of displacements d0 ? K with top-k correlations with KNN. We then take the dot product for each feature vector in F1 with its k corresponding feature vectors in F2. The dashed arrows denote paths that have no gradient flow while the solid arrows denote paths that do. <ref type="bibr" target="#b1">(2)</ref> In each iteration, the displacement vectors are updated by subtracting the residual flow di ? ?fi to update the 4D correlation volume. A multi-scale displacement encoder is applied to encode the 4D sparse correlation volume to a 2D dense motion tensor. (3) A GRU update block is applied to predict the residual flow ?fi+1 for the next iteration. The GRU block also takes input of hi, Fc, fi which represents the hidden state vector of current iteration, feature map extracted by the context network and current estimation of optical flow, and outputs the hidden state vector for the next iteration as well the residual flow.</p><formula xml:id="formula_2">F 1 F 2 h i+1 , ?f i+1 d 0 ? S (k) x d i ? ?f i C(x, d) F c , f i , h i Recurrent Decoder Gradient No Gradient</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sparse Correlation Volume</head><p>For each x ? X , we define a set</p><formula xml:id="formula_3">S (k) x = arg max S?D,|S|=k d?S C(x, d)<label>(3)</label></formula><p>containing the k displacements that give the maximum correlations. The correlation volume can now be represented as a four-dimensional sparse tensor</p><formula xml:id="formula_4">C = {C(x, d) | d ? S (k) x , x ? X }.<label>(4)</label></formula><p>This sparse correlation volume contains hwk elements as opposed to the original dense correlation volume with h 2 w 2 elements. The constant k is typically a small number (e.g. k = 8).</p><p>We now show how to construct the sparse correlation volume and estimate optical flow from it. Our network architecture is shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">k-Nearest Neighbours</head><p>We first use two weight-sharing feature extraction networks to extract 1/4 resolution feature maps from the input images. Our feature extraction networks consist of six residual blocks and the number of feature channels is 256. To construct the sparse correlation volume, we use a knearest neighbours (kNN) module <ref type="bibr" target="#b16">[17]</ref> to compute a set of indices with the k largest correlation scores for each feature vector in F 1 . The sparse correlation volume is computed by taking the dot product between each feature vector in F 1 with the top k feature vectors given by the indices in F 2 . During back-propagation, the gradients are only propagated to the k feature vectors that are selected by the kNN module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Displacements Update</head><p>We adopt an overall iterative residual refinement approach. As shown by previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>, estimating residual flows can effectively reduce the search space and predict better results than direct regression <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>. Rather than directly predicting optical flow f , a residual flow ?f i+1 is predicted at each step and used to update the current flow</p><formula xml:id="formula_5">estimation f i+1 = f i + ?f i+1 .</formula><p>At each step, a pixel x in F 1 is mapped to x i in F 2 according to the current estimate of flow x i = x + f i . Our sparse correlation volume C described in Section 2.1 can be regarded as an initial estimation f 0 = 0 at i = 0. When the coordinate x i is updated to x i+1 = x i + ?f i , the relative displacements in C should be updated accordingly as well. To do so, we shift the coordinates of the sparse correlation tensor by subtracting k-nearest neighbouring ?f i from d i in each step, <ref type="figure" target="#fig_5">Figure 4a</ref>. Note here we allow d i ? ?f i to be floatingpoint. It is also important to note that the inner products are computed only once at the start since in each step, only the correlation coordinates change while the correlation values remain the same.</p><formula xml:id="formula_6">C i (x, d i ) = C i+1 (x, d i ? ?f i ), as depicted in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multi-scale Displacement Encoder</head><p>One question that is often raised with any sparse approach is how to process a sparse tensor, since the regularity of a normal dense h ? w ? c tensor is lost. Sparse convolutions <ref type="bibr" target="#b7">[8]</ref> may be used, however, we will present a simpler and more efficient approach here.</p><p>A dense all-pairs correlation volume has dimension h ? w ? h ? w and we have reduced it to a sparse tensor with h ? w ? k elements where only the top-k correlations for each pixel are saved. We can see that the first two dimen-  sions are still dense and what has become sparse are the third and fourth dimensions. The goal here is to encode the k elements for each pixel and form a dense h?w ?c tensor, which can later be used to predict ?f i+1 . Following previous work <ref type="bibr" target="#b28">[29]</ref>, we propose creating multi-scale sparse tensors and sampling displacements locally with a fixed radius at different resolutions. Coarser resolutions give larger context while finer resolutions give more accurate displacements. We then convert the sparse tensors at each level to dense tensors and concatenate them to form a single 2D tensor. This is illustrated in <ref type="figure" target="#fig_5">Figure 4b</ref>.</p><formula xml:id="formula_7">F 1 F 2 x x i f i x i+1 ?f i x i + d i d i d i ? ?f i (a)</formula><p>At each iteration i, for each pixel x, we start with a set of the top k correlation positions S (k)</p><p>x . So, the set</p><formula xml:id="formula_8">{ d, C(x, d) | d ? S (k)</formula><p>x } records the top k correlation values for pixel x and their locations, obtained using a kNN algorithm.</p><p>We construct a five-level sparse correlation volume pyramid by dividing the coordinates by <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16)</ref> and denote the scaled displacements at level l, updated with the current ?f i by</p><formula xml:id="formula_9">d l = (d i ? ?f i ) / 2 l?1 .</formula><p>In addition, we denote the correlation values at level l by</p><formula xml:id="formula_10">C l (x, d l ) = C(x, d) for d ? S (k)</formula><p>x . At each level, we constrain the displacements by a constant radius r and define the windowed set of correlation values at level l,</p><formula xml:id="formula_11">d l , C l (x, d l ) d l ? ? r, d ? S (k) x .<label>(5)</label></formula><p>Since the coordinates d l are not necessarily integers, we need to resample to integer coordinates in order to densify the sparse tensor of correlations. We propose an approach which we call "bilinear splatting". The correlation values are bilinearly splatted to the four nearest integer grids. For instance, the correlation C l (x, d l ) at location d l is propagated to each of four neighbouring integer points, denoted by [d l ] = (d x , d y ), according to</p><formula xml:id="formula_12">C l (x, [d l ]) = 1 ? |d l x ? d x | 1 ? |d l y ? d y | C l (x, d l ).</formula><p>These values are then summed for the set of correlations <ref type="bibr" target="#b4">(5)</ref> and the sparse tensors of each level are converted to dense tensors, reshaped and concatenated to form a single 2D dense tensor of dimension 5(2r + 1) 2 , where 5 is the number of pyramid levels. The approach we introduce here does not require learning. It is merely a conversion between sparse and dense tensors hence is simpler than sparse convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">GRU Update Block</head><p>Each vector in this 2D dense tensor encodes position information as well as the correlation values of the k matches. We concatenate the 2D motion tensor with the context features and current estimate of flow and pass it through a gated recurrent units (GRU) update block. The GRU update block estimates the residual flow ?f i+1 which is used to shift the correlation volume coordinates in the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation details</head><p>Network details We first extract quarter-resolution feature maps with 256 channels. Our feature extraction network contains six residual blocks. When passing the feature maps to the kNN, we set k = 8. Namely, for each feature vector, return the indices of the the top-8 feature vectors that give the maximum inner products. The GRU update block takes the current estimate of optical flow as well as the context feature map as input. The context feature map is extracted by a separate network with 128 channels. The GRU update block also updates a 128-dimensional hiddenstate feature vector. During training time, the GRU iterates 8 times as opposed to 12 times in RAFT <ref type="bibr" target="#b28">[29]</ref>.</p><p>Training schedule Following previous work, we first pretrain our model on FlyingChairs <ref type="bibr" target="#b8">[9]</ref> for 120k iterations with batch size 6 and then on FlyingThings <ref type="bibr" target="#b20">[21]</ref> for another 120k iterations with batch size 4. We then fine-tune on a combination of Things, Sintel <ref type="bibr" target="#b5">[6]</ref>, KITTI 2015 <ref type="bibr" target="#b22">[23]</ref> and HD1K <ref type="bibr" target="#b18">[19]</ref> for 120k iterations for Sintel evaluation and 50k on KITTI 2015 <ref type="bibr" target="#b22">[23]</ref> for KITTI evaluation. We use a batch size of 4 for fine-tuning. We train our model on two 2080Ti GPUs. The ablation experiments are conducted on a single Tesla P100 GPU. We implemented with the PyTorch library <ref type="bibr" target="#b23">[24]</ref>.</p><p>Loss function Similar to RAFT <ref type="bibr" target="#b28">[29]</ref>, we employ a recurrent network architecture where a sequence of residual flows ?f i are predicted. The optical flow prediction in each step can be represented as f i+1 = f i + ?f i+1 and the initial values are f 0 = 0, ?f 0 = 0.</p><p>We apply the loss function on the sequence of optical flow predictions. Given the ground-truth optical flow f gt and predicted optical flow at each step f i , the loss function is defined as</p><formula xml:id="formula_13">L = N i=1 ? N ?i f i ? f gt 1 .</formula><p>The weight ? is set to 0.8 for pre-training on Chairs and Things and 0.85 for fine-tuning on Sintel and KITTI. The total number of steps N is set to 8.</p><p>kNN We use the faiss library <ref type="bibr" target="#b16">[17]</ref> to run kNN on gpu. Currently we are applying the brute-force exact search method given our problem size is still considered small. The faiss library provides optimized k-selection routines to speed up the computation and for more details we refer the readers to the original article <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>We show quantitative comparison with existing works in <ref type="table">Table 1</ref>. We have achieved state-of-the-art results on the Sintel clean dataset in the two-view case, obtaining 11.3% improvement (1.94 ? 1.72) over RAFT <ref type="bibr" target="#b28">[29]</ref>. We also tested the "warm-start" strategy in RAFT <ref type="bibr" target="#b28">[29]</ref>, which uses optical flow estimated in the previous frames to initialize current optical flow estimation. We found that it did not help our performance hence our result is still behind RAFT's "warm-start" results. On the Sintel final dataset our result is comparable to state-of-the-art results, currently behind RAFT <ref type="bibr" target="#b28">[29]</ref> and DICL <ref type="bibr" target="#b29">[30]</ref> while better than all the other methods. On the KITTI-15 dataset, our result is behind RAFT <ref type="bibr" target="#b28">[29]</ref> and MaskFlowNet <ref type="bibr" target="#b35">[36]</ref> and supersedes other approaches. We tested the generalization ability of our approach by evaluating the pre-trained model (C+T) on Sintel and KITTI-15. We have achieved the best results on Sintel clean and are second to RAFT <ref type="bibr" target="#b28">[29]</ref> on Sintel final and KITTI-15.</p><p>The improvements on Sintel clean can be attributed to the larger correlation volume (1/4 resolution vs 1/8 resolution). We provide qualitative results in <ref type="figure" target="#fig_6">Figure 5</ref>, which clearly demonstrates the advantage of building the correlation volume and predicting optical flow in high resolutions.</p><p>It can be seen that the motion of fine structures fails to be captured by RAFT but can be accurately predicted by our approach, with the use of a 1/4 resolution correlation volume. With Sintel final and KITTI-15, there exists significantly more motion blur and featureless regions. Therefore, setting k = 8 might be too small to reach the same performance as a dense correlation volume. We analyse the effect of k in Section 3.3 via ablation experiments. We  want to emphasize that even though we do not outperform RAFT <ref type="bibr" target="#b28">[29]</ref> in all datasets, it is surprising to see that a sparse approach can do almost as well given the few storage of correlation values. For each pixel, we store and process only k = 8 correlations whereas RAFT requires to store h ? w correlations, limiting its ability to scale up to higher resolutions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation Study</head><p>We conducted ablation experiments to validate our hypothesis that top-k correlations are sufficient to give a good representation of the full correlation volume. The main setting here is how large should k be. We show results on our model with different choices of k on 1/8 resolutions. It can be clearly seen that larger k gives better performance. Even when k = 1, the results are still reasonable and do not completely fail.</p><p>We also compare 1/4 resolution correlation volume with 1/8 resolution correlation volume and we can see that 1/4 resolution correlation volume gives better results in all datasets except the EPE in KITTI-15. Since large correlation volumes are constructed from higher-resolution feature maps, we believe that larger correlation volumes are more descriptive of the image details and the results agree with our hypothesis.</p><p>An additional experiment we conducted is with RAFT's original implementation. We keep the top-k elements in the correlation volume and set the rest to be zero. We vary k to be {1, 8, 32, 128}. In <ref type="table" target="#tab_3">Table 2</ref>, ReLU refers to setting all negative values to be zero and only keeping the positive correlations. We also trained with the original code which is denoted as "Dense". We can see that larger k gives better results and k = {32, 128} almost reach the same performance as the dense method. ReLU even outperforms the dense method on KITT-15. This again validates our hypothesis that there exists significant redundancy in the current dense approach and a sparse correlation volume with a large enough k could do just as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Memory Consumption</head><p>Our method of processing the sparse correlation volume does not introduce new learning parameters. The number of parameters in our network is 5.3 MB, which is the same as RAFT. Given an image pair of size 436 ? 1024, the size and memory of sparse correlation volumes and dense correlation volumes in 1/4 and 1/8 resolutions are listed in <ref type="table" target="#tab_5">Table 3a</ref>.</p><p>When correlation volumes are built from 1/8 resolution feature maps, our approach does not lead to a significant memory saving. This is due to the constant 2 GB memory overhead of the kNN search library and also the correlation volume is not a memory bottleneck (191 MB when batch size = 1) when resolutions are small.</p><p>However, our approach demonstrates a clear advantage when correlation volumes are built from 1/4 resolution feature maps for images of size 436 ? 1024. When training at 1/4 resolution, with a random crop of 400 ? 720 of the original image and batch size = 1 and 2, our approach consumes around 50% of total memory compared to RAFT. The results are demonstrated in <ref type="table" target="#tab_5">Table 3b</ref>. This showcases the effectiveness of our approach in saving memory when correlation volumes are scaled to higher resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Limitations</head><p>Increasing the resolution to 1/4, we have observed consistent improvements on fine-structure motions (e.g. the bamboo sequence in Sintel). However, the commonly used metric for overall evaluation, mean EPE, is defined to be biased towards large motions on large regions. One particular weakness of our approach is the handling of featureless or blurry regions. Such features typically have a large number of matches due to the ambiguity, top-k might not be sufficient to cover the correct match and could give misguided motion prediction. An example failure case is shown in Figure 6. One can see that the red hair contains significant motion blur, where our top-k correlations do not contain the correct matches hence lead to incorrect prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Optical flow was first formulated as a continuous optimization problem with variational approaches <ref type="bibr" target="#b10">[11]</ref>. Various subsequent papers have worked on on improving robustness <ref type="bibr" target="#b1">[2]</ref> and energy term <ref type="bibr" target="#b34">[35]</ref>, incorporating descriptor matching into energy minimization <ref type="bibr" target="#b3">[4]</ref> and improving regularizations <ref type="bibr" target="#b25">[26]</ref>. Accurate flow fields can be predicted when displacements are small. However, their performances are limited at large displacements, due to the use of first order Taylor approximation.</p><p>Pyramidal approaches were developed to handle large displacements in stereo and optical flow, pioneered by Quam et al. <ref type="bibr" target="#b24">[25]</ref>. Traditional methods build a Gaussian pyramid <ref type="bibr" target="#b4">[5]</ref> and predict optical flow or stereo in a coarseto-fine manner. In contrast, deep optical flow approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30]</ref> build a feature pyramid to extract more representative information on different levels through learning. Optical flows are then predicted in a coarse-to-fine manner. A local correlation volume with a limited search range is constructed in each level, based on featured maps warped with the upsampled flow from the previous level. This approach limits the search range of the correlation volume and effectively reduces the memory and computational cost for processing it. However, as pointed out in previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29]</ref>, pyramidal warping approaches can have ghosting effects at occlusions. They are also limited at handling the small object large motion problem and fine-level predictions often fail to recover errors made in coarser levels. By contrast, our method operates at a single resolution and does not suffer from such problems. Pyramidal approaches are designed to handle large motions while keeping memory cost small. However, they are not the only successful approach to dealing with large motions. Before the deep learning era, optical flow estimation was formulated as a discrete optimization problem by solving a Markov random field (MRF) <ref type="bibr" target="#b2">[3]</ref>. Chen et al. <ref type="bibr" target="#b6">[7]</ref> proposed a one-shot global discrete optimization approach at a single resolution with a distance transform, which is then refined using continuous optimization. Menze et al. <ref type="bibr" target="#b21">[22]</ref> proposed reducing the search space by limiting to a fixed number k matches per pixel via approximate nearest neighbour search. Our idea is similar in the sense that we both limit the search space to top-k, but rather than solving an MRF, we propose explicitly constructing a sparse correlation volume and using iterative refinement to predict optical flow. The main difference is that our final solution does not necessarily lie the top-k solution space, which gives better occlusion handling. Our idea is also inspired by the recent paper on learning to find sparse matches, which proposed to find top-k matches first then process with sparse convolutions. However, we do not use sparse convolutions but rather convert the sparse correlation volume to a dense tensor with our proposed multi-scale displacement encoder.</p><p>A recent breakthrough in deep optical flow estimation has been achieved by RAFT <ref type="bibr" target="#b28">[29]</ref>, which proposed constructing a dense all-pairs correlation volume on a single resolution and adopting a recurrent network to iteratively predict optical flow. Due to the memory cost of the correlation volume, the feature map resolutions are limit to 1/8 of the original image resolution. Our approach is different to RAFT in three major ways:</p><p>1. Rather than constructing a dense all-pairs correlation volume, we construct a sparse correlation volume where only the top k correlations are stored. This allows us to reduce the spatial complexity from O(N 2 ) to O(N ), where N refers to the number of pixels of an image.</p><p>2. Because of the savings in memory cost, we can build the correlation volume from higher resolution feature maps (1/4 vs. 1/8) without limiting the searching range. This allows our method to accurately predict the motion of the finer structures.</p><p>3. We propose a new way of iteratively decoding the sparse correlation volume. Rather than sampling in a dense correlation volume at different locations, we iteratively update the coordinates of the sparse correlation volume and apply "bilinear splatting" to splat the correlations onto the integer grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Since the publication of RAFT <ref type="bibr" target="#b28">[29]</ref>, the use of allpairs (large displacements) correlation volume is becoming a standard way of solving for optical flow due to its superior performance compared to pyramidal approaches. However, the memory consumption of an all-pairs correlation volume grows quadratically with the number of pixels, quickly limiting its ability to handle high-resolution images or capture fine-structure motion. We observed a rather surprising fact where storing just the top-k correlations provides almost as good results as storing the dense correlation volume. The sparse correlation volume method proposed in this paper provides an alternative approach to store the allpairs matching information which massively reduces memory consumption while gives accurate prediction of optical flow. Experiments validated the feasibility of using sparse correlation volume in optical flow estimation tasks. We believe our paper has paved a way for future optical flow research directions, where the memory requirement of correlation volumes is no longer a limiting factor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sparse correlation volume (e) Optical flow (RAFT [29]) (f) Optical flow (ours) Optical flow estimation with dense correlation volume and sparse correlation volume. (c) and (d) illustrate the correlation volumes for a single pixel (yellow dot) in the first image. The white crosses in (b) indicate the top-k matches. We show accurate optical flow can be estimated given only a few matching correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F 1 F 2 (</head><label>12</label><figDesc>a) A dense correlation volume requires saving all pairs of matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F 1 F 2 (</head><label>12</label><figDesc>b) A sparse correlation volume requires saving only top-k matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between dense correlation volume and top-k sparse correlation volume. In a sparse correlation volume, only the top-k matches are stored and the rest are discarded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of how to process a sparse correlation volume in an iterative fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on Sintel. We compare the results of the pre-training models (trained on Chairs + Things) on the Sintel training dataset. The results are compared against RAFT. We demonstrate cases where the quarter resolution correlation volume outperforms the eighth resolution correlation volume. Two noticable examples are the first two rows, where the motion of a thin bamboo cannot be captured by the eighth resolution correlation volume due to the large downsampling. Nevertheless, it can be accurately predicted by our method. Best viewed on screen when zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>An example failure case. When the scene contains significant featureless regions or motion blur, top-k correlations may not contain the correct matches and can lead to incorrect flow prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Displacements update. As a pixel's coordinates are updated by adding ?fi, the relative displacements are diminished by ?fi. Multi-scale displacement encoder. We first form a multi-level sparse correlation pyramid by scaling the coordinates by different constants. We then bilinearly splat the correlations onto the integer grids and extract correlation values within a local window. The extracted windows are converted to dense tensors and are reshaped and concatenated to form a single h ? w ? c tensor.</figDesc><table><row><cell>Bilinear</cell></row><row><cell>Splatting</cell></row><row><cell>Densify &amp;</cell></row><row><cell>Reshape</cell></row><row><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiment results. Settings used in our final model are underlined. The details are in Section 3.3. We also give results run on RAFT's original code but with varying sparsity levels of the correlation volume. We ran these experiments on 1/8 resolution.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>? 10 6 14.3 MB 8.8 ? 10 5 3.5 MB (a) Size and memory of a correlation volume based on a pair of images of size 436 ? 1024. Size refers to the number of elements and the correlation volumes are stored in 32-bit floats.</figDesc><table><row><cell>Sparsity</cell><cell cols="2">1/4 Resolution</cell><cell cols="2">1/8 Resolution</cell></row><row><cell></cell><cell>Size</cell><cell>Memory</cell><cell>Size</cell><cell>Memory</cell></row><row><cell cols="2">Dense k = 8 k = 32 k = 128 3.6 Method 7.8 ? 10 8 2.2 ? 10 5 8.9 ? 10 5</cell><cell cols="2">3.1 GB 0.9 MB 3.6 MB batch = 1 batch = 2 4.8 ? 10 7 5.5 ? 10 4 2.2 ? 10 5</cell><cell>191 MB 0.2 MB 0.9 MB</cell></row><row><cell></cell><cell>RAFT [29]</cell><cell>10.6 GB</cell><cell>20.0 GB</cell></row><row><cell></cell><cell>Ours</cell><cell>6.1 GB</cell><cell>9.3 GB</cell></row><row><cell cols="5">(b) Actual memory consumption when training on the Sintel</cell></row><row><cell cols="5">dataset. The correlation volumes are built from 1/4 resolution</cell></row><row><cell cols="5">feature maps. We use a random crop of 400 ? 720. The batch</cell></row><row><cell cols="2">size is set to 1 and 2.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results for memory consumption.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is funded in part by the ARC Centre of Excellence for Robotic Vision (CE140100016), ARC Discovery Project grant (DP200102274) and (DP190102261). We thank all reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scopeflow: Dynamic scene scoping for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviram</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for the robust estimation of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmanabhan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on communications</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Full flow: Optical flow estimation by global optimization over regular grids. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Minkowski convolutional neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>4d spatio-temporal convnets</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Flownet: Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
		<idno>2020. 8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Determining optical flow. Techniques and Applications of Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asmaa</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<title level="m">Liteflownet: A lightweight convolutional neural network for optical flow estimation. CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A lightweight optical flow cnn-revisiting data fidelity and regularization. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Iterative residual refinement for joint optical flow and occlusion estimation. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The hci benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Brenner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Devon: Deformable</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A large dataset to train convolutional networks for disparity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
			<affiliation>
				<orgName type="collaboration">ISA</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
			<affiliation>
				<orgName type="collaboration">ISA</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
			<affiliation>
				<orgName type="collaboration">ISA</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Workshop on Image Sequence Analysis</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hierarchical warp stereo. Readings in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlocal total generalized variation for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Models matter, so does training: An empirical study of cnns for optical flow estimation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Displacement-invariant matching cost learning for accurate optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Motion detail preserving optical flow estimation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Asymmetric feature matching with learnable occlusion mask. CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
