<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AGRNet: Adaptive Graph Representation Learning and Reasoning for Face Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Gusi</forename><surname>Te</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wei</forename><forename type="middle">Hu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yinglu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Hailin</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Tao</forename><surname>Mei</surname></persName>
						</author>
						<title level="a" type="main">AGRNet: Adaptive Graph Representation Learning and Reasoning for Face Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Face parsing</term>
					<term>graph representation</term>
					<term>attention mechanism</term>
					<term>graph reasoning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their success in face parsing, which however overlook the correlation among facial components. As a matter of fact, the component-wise relationship is a critical clue in discriminating ambiguous pixels in facial area. To address this issue, we propose adaptive graph representation learning and reasoning over facial components, aiming to learn representative vertices that describe each component, exploit the componentwise relationship and thereby produce accurate parsing results against ambiguity. In particular, we devise an adaptive and differentiable graph abstraction method to represent the components on a graph via pixel-to-vertex projection under the initial condition of a predicted parsing map, where pixel features within a certain facial region are aggregated onto a vertex. Further, we explicitly incorporate the image edge as a prior in the model, which helps to discriminate edge and non-edge pixels during the projection, thus leading to refined parsing results along the edges. Then, our model learns and reasons over the relations among components by propagating information across vertices on the graph. Finally, the refined vertex features are projected back to pixel grids for the prediction of the final parsing map. To train our model, we propose a discriminative loss to penalize small distances between vertices in the feature space, which leads to distinct vertices with strong semantics. Experimental results show the superior performance of the proposed model on multiple face parsing datasets, along with the validation on the human parsing task to demonstrate the generalizability of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their success in face parsing, which however overlook the correlation among facial components. As a matter of fact, the component-wise relationship is a critical clue in discriminating ambiguous pixels in facial area. To address this issue, we propose adaptive graph representation learning and reasoning over facial components, aiming to learn representative vertices that describe each component, exploit the componentwise relationship and thereby produce accurate parsing results against ambiguity. In particular, we devise an adaptive and differentiable graph abstraction method to represent the components on a graph via pixel-to-vertex projection under the initial condition of a predicted parsing map, where pixel features within a certain facial region are aggregated onto a vertex. Further, we explicitly incorporate the image edge as a prior in the model, which helps to discriminate edge and non-edge pixels during the projection, thus leading to refined parsing results along the edges. Then, our model learns and reasons over the relations among components by propagating information across vertices on the graph. Finally, the refined vertex features are projected back to pixel grids for the prediction of the final parsing map. To train our model, we propose a discriminative loss to penalize small distances between vertices in the feature space, which leads to distinct vertices with strong semantics. Experimental results show the superior performance of the proposed model on multiple face parsing datasets, along with the validation on the human parsing task to demonstrate the generalizability of our model.</p><p>Index Terms-Face parsing, graph representation, attention mechanism, graph reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face parsing is a particular task in semantic segmentation, which assigns a pixel-wise label to each semantic component, such as facial skin, eyes, mouth and nose. It enables more complex face analysis tasks, including face swapping <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, face editing <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and face completion <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>A plethora of methods have been proposed to fulfill this task with remarkable success, which can be classified into two categories: 1) Face-based parsing, which takes the holistic face image as input for segmentation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. This however may neglect the scale discrepancy in different facial components, sometimes resulting in the lack of details. 2) Region-based parsing, which addresses the above problem <ref type="figure">Fig. 1</ref>: Illustration of the proposed adaptive graph representation learning and reasoning for face parsing, which aims to capture the long range dependencies among facial components. Given an input image, we represent each facial component by a few vertices via adaptive pixel-to-vertex projection, learn and reason over the graph connectivity to infer the relations among facial components, and reproject the refined vertex features to the pixel grids for face parsing.</p><p>by first predicting the bounding box and then acquiring the parsing map of each facial component individually <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, there still exist limitations and challenges. Firstly, region-based parsing methods are based on the individual information within each component, while the correlation among components is not exploited yet to capture long range dependencies. In fact, facial components present themselves with abundant correlation between each other. For instance, eyes, mouth and eyebrows will generally become more curvy when people smile, as shown in <ref type="figure">Fig. 1</ref>; facial skin and other components will be dark when the lighting is weak, etc.. Secondly, it remains a challenge to segment pixels around the boundary between components, since boundaries tend to be ambiguous in real scenarios.</p><p>To this end, we explicitly model the component-wise correlations on a graph abstraction, with each vertex describing a facial component and each link capturing the correlation between a pair of components. To learn such a graph representation, we propose Adaptive Graph Representation learning and reasoning over facial components-AGRNet, which learns and reasons over non-local components to capture long range arXiv:2101.07034v3 [cs.CV] 17 Sep 2021 dependencies with the boundary information between components leveraged. In particular, to bridge the image pixels and graph vertices, AGRNet adaptively projects a collection of pixels with similar features to each vertex. To obtain representative vertices, instead of projecting pixels within an image block to a vertex by fixed spatial pooling as in our previous work <ref type="bibr" target="#b9">[10]</ref>, we adaptively select pixels with high responses to a distinct facial component as a graph vertex. Since facial components are unknown in the beginning, we employ a predicted parsing map as the initial condition. Further, to achieve accurate segmentation along the edges between components, AGRNet incorporates edge attention in the pixel-to-vertex projection, which assigns larger weights to the features of edge pixels during the feature aggregation.</p><p>Based on the projected vertices, AGRNet learns the correlations among facial components, i.e., the graph links between vertices, and reasons over the correlations by propagating information across all vertices on the graph via graph convolution <ref type="bibr" target="#b16">[17]</ref>. This enables characterizing long range correlations in the facial image for learning high-level semantic information. Finally, we project the learned graph representation back to the pixel grids, which is integrated with the original pixelwise feature map for face parsing.</p><p>To train the proposed model, we design a boundary-attention loss as well as a discriminative loss to reinforce edge pixels and vertex features respectively. The boundary-attention loss measures the error of the predicted parsing map only at edge pixels, which aims to obtain accurate segmentation along the boundary. Meanwhile, the discriminative loss is designed to penalize small distances among vertices in the feature space, thus leading to more representative and distinct vertices with strong semantics.</p><p>Compared with our previous work <ref type="bibr" target="#b9">[10]</ref>, the proposed AGRNet makes significant improvements in the following three aspects. 1) AGRNet projects pixels to vertices in an adaptive fashion conditioned on a predicted parsing map, rather than simply applying spatial pooling in <ref type="bibr" target="#b9">[10]</ref>. Specifically, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, <ref type="bibr" target="#b9">[10]</ref> projects pixels within an image patch to a vertex via spatial pooling, which results in ambiguous semantics in vertices. In contrast, AGRNet projects a collection of pixels to a vertex adaptively so as to represent each semantic facial component, which leads to more semantic and compact vertices for the subsequent efficient reasoning among components. This improvement is substantial and critical for capturing the long-range dependencies between facial components. 2) We design a discriminative loss, which encourages vertices to keep distant in the feature space. This is advantageous in learning distinct vertices for accurate segmentation of ambiguous pixels along the boundary. While the parsing map in our model provides hints in selecting semantic vertices, the similarity among different vertices is overlooked, thus the discriminative loss is critical in pushing pixels to its corresponding component. 3) We conduct more extensive experiments on large-scale datasets, including LaPa <ref type="bibr" target="#b8">[9]</ref> and CelebAMask-HQ <ref type="bibr" target="#b17">[18]</ref>. Also, we generalize the proposed model to the human parsing task on the LIP dataset <ref type="bibr" target="#b18">[19]</ref>, and the results demonstrate the effectiveness of our model.</p><p>Our main contributions are summarized as follows.</p><p>1) We propose a component-level adaptive graph representation learning and reasoning for face parsing-AGRNet, aiming to exploit the correlations among components for capturing long range dependencies. 2) The graph representation is adaptively learned by projecting a collection of pixels with similar features to each vertex in a differentiable manner along with edge attention. 3) To train the proposed model effectively, we propose a discriminative loss, which conduces to enlarging the feature distance among distinct vertices. 4) We conduct extensive experiments on the benchmarks of face parsing as well as human parsing to show the generalization. Results demonstrate that our model outperforms the state-of-the-art methods on almost every category for face parsing. The paper is organized as follows. We first review previous works in Section II. Then we elaborate on the proposed model in Section III. Next, we present the loss functions for training the proposed model and provide further analysis in Section IV. Finally, experiments and conclusions are presented in Section V and VI, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face Parsing</head><p>Previous methods can be classified into two categories: facebased parsing and region-based parsing.</p><p>Face-based parsing takes the whole image as input regardless of component properties <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Liu et al. import features learned from a convolutional neural network into the Conditional Random Field (CRF) framework to model individual pixel labels and neighborhood dependencies <ref type="bibr" target="#b6">[7]</ref>. Luo et al. propose a hierarchical deep neural network to extract multi-scale facial features <ref type="bibr" target="#b15">[16]</ref>. Zhou et al. adopt an adversarial learning approach to train the network and capture the high-order inconsistency <ref type="bibr" target="#b19">[20]</ref>. Liu et al. design a CNN-RNN hybrid model that benefits from both highquality features of CNNs and non-local properties of RNNs <ref type="bibr" target="#b11">[12]</ref>. To refine the segmentation along critical edges, Liu et al. introduce edge cues and propose a boundary-aware loss for face parsing, leading to improved performance <ref type="bibr" target="#b8">[9]</ref>. However, this class of methods neglect the discrepancy in scales of various facial components, sometimes resulting in the lack of details in some components.</p><p>Region-based parsing takes the scale discrepancy into account and predicts each component respectively, which is advantageous in capturing elaborate details <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Zhou et al. present an interlinked CNN that takes multi-scale images as input and allows bidirectional information passing <ref type="bibr" target="#b14">[15]</ref>. Lin et al. propose a novel RoI Tanh-Warping operator that preserves both central and peripheral information. It contains two branches with the local-based branch for inner facial components and the global-based branch for outer facial ones. This method demonstrates high performance especially for hair segmentation <ref type="bibr" target="#b12">[13]</ref>. Yin et al. introduce the Spatial Transformer Network and build a training connection between traditional interlinked CNNs, which makes the end-to-end joint training process possible <ref type="bibr" target="#b13">[14]</ref>. Nevertheless, this class of methods often neglect the correlation among components to characterize long range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>Limited by the locality of the convolution operator, CNNs lack the ability to model the global contextual information in a single layer. Therefore, attention mechanism has been proposed to capture long-range information <ref type="bibr" target="#b20">[21]</ref>, which has been successfully applied to many applications such as sentence encoding <ref type="bibr" target="#b21">[22]</ref> and image recognition <ref type="bibr" target="#b22">[23]</ref>.</p><p>Chen et al. propose a Double Attention Model that gathers information spatially and temporally to reduce the complexity of traditional non-local modules <ref type="bibr" target="#b23">[24]</ref>. Zhu et al. present an asymmetric module to reduce the computation and distill features <ref type="bibr" target="#b24">[25]</ref>. Fu et al. devise a dual attention module that applies both spatial and channel attention in feature maps <ref type="bibr" target="#b25">[26]</ref>. To research the underlying relationship among different regions, Chen et al. project the original features into an interactive space and utilize the GCN <ref type="bibr" target="#b16">[17]</ref> to exploit the highorder relationship <ref type="bibr" target="#b26">[27]</ref>. Li et al. devise a robust attention module that incorporates the Expectation-Maximization algorithm <ref type="bibr" target="#b27">[28]</ref>. Yin et al. propose the disentangled non-local module, aiming at disentangling pairwise and unary relations in the classical non-local network <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph Representation for Images</head><p>Images could be interpreted as a particular form of regular grid graphs, which thus enables studying images from the graph perspective. Chandra et al. propose a Conditional Random Field based method on image segmentation <ref type="bibr" target="#b29">[30]</ref>. Recently, graph convolution networks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> have been leveraged in image segmentation. Li et al. introduce the graph convolution to semantic segmentation, which projects features onto vertices in the graph domain and applies graph convolution afterwards <ref type="bibr" target="#b32">[33]</ref>. Furthermore, Lu et al. propose Graph-FCN where semantic segmentation is cast as vertex classification by directly transforming an image into regular grids <ref type="bibr" target="#b33">[34]</ref>. Pourian et al. propose a graph-based method for semi-supervised segmentation <ref type="bibr" target="#b34">[35]</ref>, where the image is divided into community graphs and labels are assigned to corresponding communities. Zhang et al. utilize the graph convolution both in the coordinate space and the feature space <ref type="bibr" target="#b35">[36]</ref>. Li et al. propose a spatial pyramid graph reasoning module based on an improved Laplacian matrix to learn a better distance metric for feature filtering <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ADAPTIVE GRAPH REPRESENTATION LEARNING AND REASONING</head><p>In this section, we first introduce the overall framework in Section III-A. Then, we elaborate on the proposed adaptive graph projection, graph reasoning, and graph reprojection in Section III-B, Section III-C and Section III-D, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, given an input face image I ? R H?W of height H and width W , we aim to predict a pixelwise label to each facial semantic component. In particular, we aim to model the long-range dependencies among distant components on a graph, which is critical for the description of the facial structure. The overall framework of the proposed AGRNet consists of three procedures as follows.</p><p>? Initial Feature and Edge Extraction. We take the ResNet-101 <ref type="bibr" target="#b37">[38]</ref> as the backbone and extract low-level and high-level features for the subsequent graph representation. The low-level features generally contain image details but often lack semantic information, while the high-level features provide rich semantics with global information at the cost of image details. To fully exploit the global information in high-level features, we employ spatial pyramid pooling to learn multi-scale contextual information. Further, we propose an edge perceiving module to acquire an edge map for the subsequent edge attention operation. ? Adaptive Graph Representation Learning and Reasoning. We project a collection of pixels to similar features to graph vertices in an adaptive and differentiable manner, and reason over the vertices to learn the longrange relations among facial components. This consists of three operations: adaptive graph projection, graph reasoning and graph reprojection, which extracts component features as vertices in a differentiable fashion, reasons the relations between vertices with a graph convolution network, and projects the learned graph representation back to pixel grids, leading to a refined feature map with rich local and non-local semantics. ? Semantic Decoding. Finally, we add the refined feature map to the original pixel-wise feature map and predict the parsing map via a convolution layer with kernel size of 1 ? 1 . Next, we will elaborate on the three operations in Adaptive Graph Representation Learning and Reasoning in order as follows. The specific architecture is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Graph Projection</head><p>We learn the graph representation of facial components in an adaptive and differentiable manner, which projects a collection of pixels that tend to reside in the same facial component to K (K ? 1) vertices in the graph. We choose K vertices to represent a facial component, because 1) the feature from K vertices provides abundant visual description of the component, and 2) both the intra-component relations and inter-component relations will be exploited during the graph reasoning.</p><p>Considering the facial components are unknown in the test, we first generate a preliminary raw parsing map that provides rough component information, which is supervised by the ground truth parsing map during the training. Based on the raw prediction, we select K pixels with the highest confidence with respect to each component. Then the corresponding indices are utilized to sample representative vertices on the original  feature map. These vertices will be fed into the subsequent graph reasoning module to extract the mutual relationship.</p><p>Specifically, the input to the proposed adaptive graph projection consists of a low-level feature map X 1 ? R H1W1?C1 and a high-level feature map X 2 ? R H2W2?C2 (H 1 W 1 &gt; H 2 W 2 ) as well as an edge map E ? R H1W1?1 from the first procedure of Initial Feature and Edge Extraction, where H i and W i denote the height and width of the ith feature map respectively while C i is the number of feature channels, i ? {1, 2}. Taking them as input, we aim to construct a set of vertices, each of which corresponds to a distinct facial component.</p><p>First, we upsample the feature map X 2 to the scale of X 1 with bilinear interpolation, and concatenate the upsampled version X 2 and X 1 followed by a 1 ? 1 convolution layer to reduce the channel dimension to C. Then we obtain the fused feature map X 0 ? R H1W1?C , with both detailed texture and abundant semantic information:</p><formula xml:id="formula_0">X 0 = conv([X 1 , X 2 ]),<label>(1)</label></formula><p>where [?, ?] denotes the concatenation operation, and conv(?) represents the 1 ? 1 convolution. To separate pixels into edge pixels and non-edge pixels, we employ the learned edge map E to mask the original feature map X 0 :</p><formula xml:id="formula_1">X e = X 0 ? E, X ne = X 0 ? (1 ? E),<label>(2)</label></formula><p>where ? denotes the Hadamard product and 1 is a matrix with all the elements as 1. X e and X ne denote the masked feature map of edge pixels and non-edge pixels, respectively. In order to generate vertices that capture semantic features of facial components, we propose to first predict a preliminary parsing map Z 0 ? R H1W1?Nc , where N c denotes the number of segmentation categories. The prediction of Z 0 is produced by a Multi-layer Perceptron (MLP) under the supervision of the ground truth parsing map, i.e., Z 0 = conv(X 0 ). The predicted parsing map Z 0 corresponds to the confidence in terms of each facial component.</p><p>As component vertices should be compact and representative, we choose top K pixels with highest confidence in Z 0 as component vertices. Specifically, we extract the top K pixels and their indices, along with the corresponding feature vectors from the masked feature map of non-edge pixels X ne . That is, only non-edge interior pixels are taken from X ne as vertices:</p><formula xml:id="formula_2">X G = X ne { Nc i=1 topk(Z 0 [:, i])} = X ne { Nc i=1 topk(conv(X 0 )[:, i])},<label>(3)</label></formula><p>where {?} denotes selection by the indices. The semantic vertices are then effectively extracted, each of which corresponds to a vertex in the graph. By the adaptive graph projection, we bridge the connection between pixels and each component via the selected vertices, leading to the features of the projected vertices on the graph</p><formula xml:id="formula_3">X G ? R KNc?C .</formula><p>The adaptive graph projection significantly improves the projection in our previous work <ref type="bibr" target="#b9">[10]</ref>. As in <ref type="figure" target="#fig_1">Fig. 3</ref>, we improve the traditional regular spatial pooling strategy by adaptively selecting representative vertices. Although facial components tend to locate in certain positions, regular spatial pooling fails to capture the representative information in each component and introduces irrelevant pixels. In contrast, our model characterizes each component with vertices adaptively, leveraging on a predicted parsing map that provides confident semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph Reasoning</head><p>The connectivity between vertices represents the relation between each pair of facial components. Hence, we reason over the relations by propagating information across vertices to learn higher-level semantic information. Instead of constructing a pre-defined graph, the proposed AGRNet learns the graph connectivity dynamically without the supervision of a ground truth graph. We propose to leverage a singlelayer Graph Convolution Network (GCN) [17]-a first-order approximation of spectral graph convolution-to aggregate the neighborhood information and learn local vertex features.</p><p>Specifically, we feed the input vertex features X G into the GCN. The output feature mapX G ? R KNc?C i?</p><formula xml:id="formula_4">X G = ReLU [(I ? A)X G W G ] ,<label>(4)</label></formula><p>where A denotes the adjacency matrix that encodes the graph connectivity to learn, W G ? R C?C denotes the weights of the GCN, and ReLU is the activation function. The feature? X G are acquired by the vertex-wise interaction (multiplication with (I?A)) and channel-wise interaction (multiplication with W G ). We set the same number of output channels as the input to keep consistency, allowing the module to be compatible with the subsequent process.</p><p>The main difference between previous GCNs <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b38">[39]</ref> and ours is that, instead of a hand-crafted graph in <ref type="bibr" target="#b16">[17]</ref> or a graph with available connectivities <ref type="bibr" target="#b38">[39]</ref>, we randomly initialize the graph A and then iteratively learn BOTH the connectivity and edge weights by a linear layer during the training, which adaptively captures the implicit relations among facial components. Besides, we add a residual connection to reserve the input features of vertices X G . The finally reasoned relations are acquired with the information propagation across all vertices based on the learned graph. After graph reasoning, the semantic information is greatly enhanced across different vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Graph Reprojection</head><p>Having learned vertex features that capture the semantic information of each facial component, we project vertex features in the graph domain back to the original pixel domain for face parsing. That is, we aim to compute a matrix P :X G ? X P that reprojects vertex featuresX G to pixel features X P .</p><p>In particular, we leverage vertex features X G prior to reasoning and edge-masked pixel features X e to construct the projection matrix P, which models the correlation between vertices and pixels. Specifically, we take the inner product of X G and X e to capture the similarity between vertices and each pixel. We then apply a softmax function for normalization. Formally, the projection matrix takes the form:</p><formula xml:id="formula_5">P = softmax X G ? X e ,<label>(5)</label></formula><p>where P ? R KNc?H1W1 . Having acquired the projection matrix P modeling the correlation between vertices and pixels, we obtain the final refined pixel-level feature map X P by taking the product of the projection matrix P and refined vertex featuresX G , i.e.,</p><formula xml:id="formula_6">X P = P ?X G .<label>(6)</label></formula><p>After reprojection, in order to take advantage of the original feature map X 0 that captures features of pixel-level texture information, we add X 0 to the refined feature map X P that captures edge-aware component-level semantic information by element-wise summation, leading to multi-level feature representations. Followed by a 1 ? 1 convolution, our model predicts the final parsing result Y as</p><formula xml:id="formula_7">Y = conv(X 0 + P ?X G ),<label>(7)</label></formula><p>where conv(?) denotes the 1 ? 1 convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TRAINING OBJECTIVES AND ANALYSIS</head><p>Having presented our model, we propose a boundaryattention loss and a discriminative loss tailored to the training of our model in Section IV-A. Further, we provide detailed analysis and discussion in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Loss Function</head><p>In addition to the commonly adopted parsing loss, the training objective of AGRNet further aims to promote the segmentation accuracy along the boundary as well as learning discriminative vertex features. Hence, we propose a boundaryattention loss and a discriminative loss respectively, which are detailed below.</p><p>1) The Proposed Boundary-Attention Loss: To improve the segmentation results along the boundary, we introduce the boundary-attention loss (BA-Loss) inspired by <ref type="bibr" target="#b8">[9]</ref> as in our previous work <ref type="bibr" target="#b9">[10]</ref>. The BA-loss measures the loss of the predicted parsing labels compared to the ground truth only at edge pixels, thus strengthening the model capacity for critical edge pixels that are challenging to distinguish. Mathematically, the BA-loss is formulated as</p><formula xml:id="formula_8">L BA = HW i=1 Nc j=1 [e i = 1] y ij log p ij ,<label>(8)</label></formula><p>where i is the pixel index, j is the class index and N c is the number of categories. e i is a binary scalar to indicate an edge pixel (e i = 1), y ij denotes the ground truth label of face parsing, and p ij denotes the predicted parsing label.</p><p>[?] is the Iverson bracket, which denotes a number that is 1 if the condition in the bracket is satisfied, and 0 otherwise.</p><p>2) The Proposed Discriminative Loss: In semantic segmentation, pixels belonging to different categories should be distant from each other in the feature space, while pixels belonging to the same category should be as similar as possible. In our model, while the parsing map provides hints in selecting semantic vertices, the similarity among different vertices is however overlooked. The separation of vertices is also critical in pushing pixels to its corresponding component.</p><p>Motivated by <ref type="bibr" target="#b39">[40]</ref> [41] <ref type="bibr" target="#b41">[42]</ref> [43], we propose a discriminative loss that penalizes small feature distances between vertices representing different components and encourages multiple vertices corresponding to the same component to be more diverse. Specifically, let x 1 and x 2 denote the features of two semantic vertices respectively, we formulate the penalty function as</p><formula xml:id="formula_9">?(x 1 , x 2 ) = (? ? ||x 1 ? x 2 || 2 ) 2 , ||x 1 ? x 2 || 2 &lt; ? 0, ||x 1 ? x 2 || 2 ? ? ,<label>(9)</label></formula><p>where ? is a pre-defined threshold to control the threshold of the feature distance between two semantic vertices. If the l 2 distance between the two features exceeds ?, the function does not impose any penalty. Otherwise, the penalty is a quadratic function that takes a larger value for the smaller distance.</p><p>Considering all the vertices with cardinality KN c , we formulate the complete discriminative loss L dis as follows:</p><formula xml:id="formula_10">L dis = 1 KN c (KN c ? 1) KNc i=1 KNc j=1,j =i ?(x i , x j ).<label>(10)</label></formula><p>3) The Total Loss: In addition to the above two loss functions, we have three more losses: 1) the prediction loss of the raw parsing map for adaptive graph projection L raw , which takes the cross entropy between the predicted raw parsing map and the ground truth parsing map; 2) the final parsing loss L final , which takes the cross entropy between each predicted label and the ground truth label; 3) the edge prediction loss L edge , which measures the estimation error of the image edges. The three loss functions take the following forms:</p><formula xml:id="formula_11">L raw = ? 1 HW HW i=1</formula><p>Nc j=1 y ij log(pred raw ij );</p><formula xml:id="formula_12">L final = ? 1 HW HW i=1 Nc j=1 y ij log(pred f inal ij ); L edge = ? 1 HW HW i=1 e i log(pred edge i ) + (1 ? e i ) log(1 ? pred edge i ).</formula><p>The total loss function is then defined as follows:</p><formula xml:id="formula_13">L = ? 1 L raw + ? 2 L edge + ? 3 L BA + ? 4 L final + ? 5 L dis ,<label>(11)</label></formula><p>where {? i } 5 i=1 are hyper-parameters to strike a balance among different loss functions.</p><p>Finally, we provide a detailed training pipeline in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Difference from Relevant Models</head><p>While the proposed graph projection is inspired by nonlocal modules and the entire model belongs to the family of graph-based methods, we analyze the prominent differences between previous works and our method.</p><p>Comparison with non-local modules. Typically, a traditional non-local module models pixel-wise correlations based on pixel-wise feature similarities, while neglecting the highorder relationship among regions. In contrast, we explicitly exploit the correlation among distinct components via the proposed pixel-to-vertex projection and graph reasoning over vertices. Each vertex embeds certain facial component, which models the most prominent characteristics towards semantics. We further learn and reason over the relations between vertices  <ref type="formula" target="#formula_0">(1)</ref> Learn an edge map E = EdgeM odule(X 0 ) Predict a preliminary parsing map Z 0 = P redict(X 0 ) // Adaptive Graph Projection</p><formula xml:id="formula_14">I i , E i , Y i from {I 1 , ..., I n }, {E 1 , ..., E n }, {Y 1 , ..., Y n } Acquire a low-level/high-level feature map X 1 , X 2 = Resnet(I i ) X 0 = F use([X 1 , X 2 ]) as in</formula><formula xml:id="formula_15">X e = X 0 ? E X ne = X 0 ? (1 ? E) X G = X ne { Nc i=1 topk(Z 0 [:, i])} // Graph Learning and Reasonin? X G = ReLU [(I ? A)X G W G ] // Graph Reprojection Compute the projection matrix P = softmax X G ? X e</formula><p>Obtain the final refined pixel-level feature map via graph reprojection X P = P ?X G Predict the final parsing result Y = conv(X 0 + X P ) // Loss Aggregation L = ? 1 L raw + ? 2 L edge + ? 3 L BA + ? 4 L final + ? 5 L dis as in (11) Update network parameters with the SGD optimizer and loss L by graph convolution, which captures high-order semantic relations among different facial components. Also, the computation complexity of non-local modules is expensive in general <ref type="bibr" target="#b22">[23]</ref>. The proposed edge-aware adaptive pooling addresses this issue by extracting a few significant vertices instead of redundant query points, which reduces the attention map size from O(N 2 ) to O(N K), where K N . Further, we separate and assign different weights to edge and non-edge pixels via the learned edge map, which emphasizes on challenging edge pixels, thus improving the parsing quality.</p><p>Comparison with graph-based models. In comparison with other graph-based models, such as <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we significantly improve the graph projection process by introducing semantics in an adaptive and differentiable manner. It is worth noting that, the vertex selection is critical in the graph construction. In previous works, each vertex is simply represented by a weighted sum of image pixels or acquired by spatial pooling, which may result in ambiguity in understanding vertices. Besides, given different inputs of feature maps, the pixel-wise features often vary greatly but the projection matrix is fixed after training. In contrast, we incorporate explicit semantics into the projection process to extract robust vertices and construct the projection matrix onthe-fly on the basis of the similarity between vertices and each pixel in the feature space.</p><p>Furthermore, in comparison with hand-crafted graph models <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, which are broadly used in human parsing, our model learns the implicit graph and the correlation between facial components. Different from the human structure which implies the graph connectivity, there is no conventional topology predefined on human faces. Therefore, we propose the dynamically learned graph reasoning module to infer the underlying graph adjacency matrix instead of the hand-crafted human structure in Graphonomy <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Theoretical analysis</head><p>In particular, our method has addressed existing major challenges of face parsing from the following two crucial aspects.</p><p>Firstly, the correlation between different facial components is critical in face parsing. However, existing methods, including face-based and region-based parsing, overlook such correlation among different parts. Face-based parsing may neglect the scale discrepancy in different facial components and region-based parsing does not exploit the inter-region relationship and long-range dependencies, while it would definitely enhance the parsing performance to explore such correlations. Similar theory has been validated on the task of human parsing <ref type="bibr" target="#b43">[44]</ref>. Further, facial structural variance is much less than human structure, leading to more stable structural correlations for effective learning. Graph representation is one of the most effective ways to model such correlations. Hence, we introduce a graph-based model for face representation and employ graph convolutional networks to construct and reason the component-wise relationship.</p><p>Secondly, it remains a challenge to segment pixels around the edges between components, since edges tend to be ambiguous in real-world scenarios. This issue is more severe in face parsing, as edge pixels cover a higher proportion in face images than that for other tasks such as scene parsing. Therefore, improving the segmentation accuracy of edge pixels is one effective way to enhance the performance of face parsing. In this paper, we achieve accurate segmentation along the edges from two aspects: 1) we incorporate edge attention in the pixel-to-vertex projection, which assigns larger weights to the features of edge pixels during the feature aggregation; 2) we design a boundary-attention loss to reinforce edge pixels, which measures the error of the predicted parsing map only at edge pixels.</p><p>In face of the above two challenges and based on the principles to address them, we explicitly model the componentwise correlations on a graph abstraction with edge pixels taken into consideration, where each vertex describes a facial component and each link captures the correlation between a pair of components. To learn such a graph representation, we propose adaptive graph representation learning and reasoning over facial components, which learns and reasons over nonlocal components to capture long range dependencies with the boundary information between components leveraged. To obtain representative vertices, we adaptively select pixels with high responses to a distinct facial component as a graph vertex. Since facial components are unknown in the beginning, we employ a predicted parsing map as the initial condition, and propose a discriminative loss to enhance the discrimination between vertices, leading to more representative and distinct vertices with strong semantics. Further, we integrate the component-wise correlation into the feature representation of edge pixels, so that it contains not only the local contextual information, but also the correlation with other components, thus enhancing the semantics. The above theoretical analysis provides the interpretability of our method for deep-learning based face analysis, which can be summarized as follows: we propose an effective network to address two great challenges in face parsing-insufficient use of component-wise correlations and ambiguities of edge pixels, and design several losses to guide the network to adaptively learn the intrinsic features of face images from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To validate the proposed AGRNet, we conduct extensive experiments on face parsing as well as on human parsing for generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Metrics</head><p>Face Parsing The Helen dataset <ref type="bibr" target="#b47">[48]</ref> includes 2,330 images with 11 categories: background, skin, left/right brow, left/right eye, upper/lower lip, inner mouth and hair. Specifically, we keep the same training/validation/test protocol as in <ref type="bibr" target="#b47">[48]</ref>. The number of the training, validation and test samples are 2,000, 230 and 100, respectively. The LaPa dataset <ref type="bibr" target="#b8">[9]</ref> is a large-scale face parsing dataset, consisting of more than 22,000 facial images with abundant variations in expression, pose and occlusion, and each image is provided with an 11category pixel-level label map and 106-point landmarks. The CelebAMask-HQ dataset <ref type="bibr" target="#b17">[18]</ref> is composed of 24,183 training images, 2,993 validation images and 2,824 test images. The number of categories in CelebAMask-HQ is <ref type="bibr" target="#b18">19</ref>. In addition to facial components, the accessories such as eyeglass, earring, necklace, neck, and cloth are also annotated.</p><p>Human Parsing The LIP dataset <ref type="bibr" target="#b18">[19]</ref> is a large-scale dataset focusing on semantic understanding of person, which contains 50,000 images with elaborated pixel-wise annotations of 19 semantic human part labels as well as 2D human poses with 16 key points. The images collected from the real-world scenarios contain human appearing with challenging poses and views, serious occlusions, various appearances and lowresolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics:</head><p>We employ three evaluation metrics to measure the performance of our model: pixel accuracy, intersection over union (IoU) and F1 score. The mean value is calculated by the average of total categories. Directly employing the pixel accuracy metric ignores the scale variance amid facial components, while the mean IoU and F1 score are better for evaluation. To keep consistent with the previous methods, we report the overall F1 score on the Helen dataset, which is computed over the merged facial components: brows (left + right), eyes (left + right), nose, mouth (upper lip + lower lip + inner mouth). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>During the training, we employ the random rotation and scale augmentation. The rotation angle is randomly selected from (?30 ? , 30 ? ) and the scale factor is randomly selected from (0.75, 1.25). The ground truth of the edge mask is extracted according to the semantic label map. If the label of a pixel is different from any of its four neighbors, it is regarded as an edge pixel. For the Helen dataset, we pre-process the face image by face alignment as in other works. For the LaPa and CelebAMask-HQ datasets, we directly utilize the original images without any pre-processing.</p><p>We take the ResNet-101 <ref type="bibr" target="#b37">[38]</ref> as the backbone and extract the output of conv 2 and conv 5 layers as the low-level and highlevel feature maps for multi-scale representations. To reduce the information loss in the spatial space, we utilize the dilation convolution in the last two blocks and the output size of the final feature map is 1/8 of the input image. To fully exploit the global information in high-level features, we employ a spatial pyramid pooling operation <ref type="bibr" target="#b46">[47]</ref> to learn multi-scale contextual information. In the edge prediction branch, we concatenate the output of conv 2, conv 3, conv 4 layers and apply a 1 ? 1 convolution to predict the edge map. In the differentiable graph projection, we set top-4 pixels as representative vertices for each face component, and thus the number of graph vertices is 4 times of the category number. We choose the hyperparameters in (11) as ? 1 = 1, ? 2 = 1, ? 3 = 1, ? 4 = 0.5, ? 5 = 0.1 by grid search and according to prior knowledge 1 of the scale of each loss for the initial setting of grid search. In the proposed discriminative loss in (9), we normalize the vertex features and set ? = 1.</p><p>All the experiments are implemented with 4 NVIDIA RTX 2080Ti GPUs. Stochastic Gradient Descent (SGD) is employed for optimization. We initialize the network with a pretrained model on ImageNet. The input size is 473 ? 473 and the batch size is set to 8. The learning rate starts at 0.001 with the weight decay of 0.0005. The batch normalization is implemented with In-Place Activated Batch Norm <ref type="bibr" target="#b48">[49]</ref>. For fair comparisons, these settings are adopted for all the compared methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Parsing</head><p>We perform comprehensive experiments on the commonly used three face parsing datasets and present the experimental results as follows.</p><p>1) Ablation study: The ablation study is conducted from multiple aspects on the LaPa dataset.</p><p>On composition of modules. We evaluate the improvement brought by different modules in AGRNet. Specifically, we remove some components and train the model from scratch under the same initialization. The quantitative results are reported in <ref type="table" target="#tab_1">Table I</ref>-(a), where ResNet denotes the baseline model trained with the network consisting of the backbone and the pyramid spatial pooling. It achieves 90.9% in Mean F1 score. Spatial and Adaptive refer to the respective schemes of graph construction. Spatial denotes uniform spatial pooling as in the previous work <ref type="bibr" target="#b9">[10]</ref> and Adaptive denotes the proposed adaptive pooling method in this paper, which improves the F1 score of Model 1 (ResNet) by 0.2% and 0.5%, respectively. This result validates the effectiveness of the proposed adaptive pixel-to-vertex projection. Edge and Graph represent the edge module and graph reasoning module, respectively. Compared with Model 3, the edge module achieves improvement of 0.2% in Mean F1 score, while the Graph module improves the Mean F1 score by 0.5%. This demonstrates the superiority of the proposed graph reasoning. By employing all the modules, we achieve the best performance of 92.3% in Mean F1 score.</p><p>On loss functions. We evaluate the effectiveness of different loss functions described in Section IV-A and show the comparison results in <ref type="table" target="#tab_1">Table I</ref> <ref type="bibr">-(b)</ref>. We observe that the Discriminative loss leads to 0.5% of improvement and the Boundary-Attention loss brings 0.9% of improvement in Mean F1 score compared with the traditional cross entropy loss. The best performance is obtained by utilizing all of these loss functions.</p><p>On vertex numbers. <ref type="table" target="#tab_1">Table I</ref>-(c) presents the performance with respect to different number of vertices. Our model achieves the best mIoU and overall F1 under the top 4 setting. We suppose that too many vertices result in redundant noisy vertices whereas fewer vertices fail to represent enough semantic information.</p><p>On FLOPs and parameters. We compute the complexity of different modules in FLOPs and Parameter size in <ref type="table" target="#tab_1">Table I</ref> <ref type="bibr">-(d)</ref>. All the numbers are computed with the input image size of 473 ? 473 when the batch size is set to 1. ResNet refers to the backbone network, which has 6.652GFLOPs and 14.10M of parameters. The Edge Perceiving module brings only about 0.25GFLOPs and 0.03M of parameters. The Graph Projection module further leads to only 0.471G and 0.02M of increase in FLOPs and parameters, respectively. After integrating the Graph Reasoning module, the network has 7.411GFLOPs and 14.15M of parameters. We see that our model improves the F1 score by 1.4% at the cost of mere 0.8GFLOPs and 0.05M of parameters compared with the ResNet backbone.</p><p>On hyper-parameters.</p><p>In the experiments, we set the parameters {? i } 4 i=1 in (11) based on the setting of the LaPa work <ref type="bibr" target="#b8">[9]</ref>, which achieves convincing performance on several face parsing datasets. As to the weighting parameter of the discriminative loss-? 5 , we assign its value according to the approximate proportion of loss scale. For more rigorous parameter settings, we additionally adopt the traditional way of performing hyper-parameter optimization-grid search or a parameter sweep, which is an exhaustive searching through a manually specified subset of the hyper-parameter space of a learning algorithm. In our case, the grid search algorithm is guided by the evaluation metric  </p><formula xml:id="formula_16">i } 5 i=1 .</formula><p>We change a single parameter at a time, and report the respective results in <ref type="table" target="#tab_1">Table III</ref>. We see that a choice of these parameters ? 1 = 1, ? 2 = 1, ? 3 = 1, ? 4 = 0.5, ? 5 = 0.1 is empirically optimal in this case, which is our setting. Thus, in general, our choice of the parameters is empirically optimal.</p><p>Further, we illustrate the grid search in <ref type="figure" target="#fig_4">Fig. 6</ref>. We observe that the results are insensitive to ? 1 , ? 2 , ? 3 , ? 4 , while they show sensitivity to ? 5 -the weighting parameter of the discriminative loss. This is mainly because the discriminative loss has less contribution to the final segmentation result than the cross entropy loss. Therefore, assigning a large weight to the discriminative loss will weaken the supervision of the ground truth parsing map and thus influence the overall performance.</p><p>2) Comparison with the state of the art: We compare our method with the state-of-the-art approaches on three datasets, including small-scale and large-scale ones, as presented in <ref type="table" target="#tab_1">Table II</ref>. The score of the eyes/brows is the sum of scores of the left and right ones, and the overall score is calculated by the average score of the mouth, eyes, nose, and brows. It is worth noting that, because the codes of some algorithms are not publicly available and the requirements of training data are different, it is hard to re-implement all the methods on every dataset. Therefore, we focus on implementing and testing the latest methods while dropping several previous methods, including Liu et al. <ref type="bibr" target="#b11">[12]</ref>, Lin et al. <ref type="bibr" target="#b12">[13]</ref> and Lee et al. <ref type="bibr" target="#b17">[18]</ref>. Specifically, the work by Lin et al. <ref type="bibr" target="#b12">[13]</ref> cannot be tested on the LaPa dataset and the CelebAMask-HQ dataset, because there is a lack of fine facial segmentation bounding boxes in both datasets, which is however required by <ref type="bibr" target="#b12">[13]</ref>. Liu et al. <ref type="bibr" target="#b11">[12]</ref> and Lee et al. <ref type="bibr" target="#b17">[18]</ref> are improved by follow-up works such as Wei et al. <ref type="bibr" target="#b45">[46]</ref> and Luo et al. <ref type="bibr" target="#b10">[11]</ref>, thus we show the better results of Wei et al. <ref type="bibr" target="#b45">[46]</ref> and Luo et al. <ref type="bibr" target="#b10">[11]</ref>.</p><p>On the small-scale Helen dataset, our method achieves comparable performance with the state-of-the-art approaches. On the two large-scale face parsing datasets-LaPa and CelebAMask-HQ, our method outperforms the previous methods by a large margin. Specifically, our model achieves improvement over Te et al. <ref type="bibr" target="#b9">[10]</ref> by 1.2% on the LaPa dataset and by 0.4% on the CelebAMask-HQ dataset, especially on brows and eyes.</p><p>We also visualize some parsing results of the CelebAMask-HQ dataset in comparison with competitive methods in <ref type="figure" target="#fig_3">Fig. 5</ref>. We see that our results exhibit accurate segmentation even over delicate details such as the categories of hair, earring and necklace. For example, in the first row, our model distinguishes the hand from the hair component accurately even if they are overlapped, while other methods assign the label of the hair to the hand. In the second row, our model generates the most consistent parsing result for the thin necklace while other results are fragmented. In the third row, our model separates the delicate earring from the hair, producing the finest parsing result.</p><p>3) Performance over domain gaps: Although there is no significant domain gap among multiple datasets, there are different aspects to discuss the domain gap <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. The first is to train/test on different datasets, while the second is to explore the diversity of face poses. The evaluation results are listed in <ref type="table" target="#tab_1">Table IV</ref>.</p><p>Firstly, we conduct the cross test with respect to different datasets, including the CelebAMask-HQ and Helen datasets. We divide the Helen dataset into the training set (denoted as "Helen-Train") and the testing set (denoted as "Helen-Test"). Specifically, we utilize the model trained on the CelebAMask-HQ dataset and test directly on "Helen-Test". However, these two datasets are labeled with different categories, so we select the common facial components for testing and mark the other labels as the background. Consequently, finer labels such as "Glasses" and "Ears" are categorized as the background, which affects the performance when compared with the result of evaluating on the Helen dataset as shown in <ref type="table" target="#tab_1">Table IV</ref>. Besides, among the common labels, the segmentation accuracy decreases mostly on "Brows" and "Mouth".</p><p>Secondly, we conduct the cross test between data with different face poses. Specifically, we detect the pitch angle of each input image, and divide the Helen dataset into two categories according to whether the pitch angle is within 0 ? ?40 ? . If the angle is between 0 ? and 40 ? , we refer to the images as the "Helen-Primary" dataset, and otherwise the "Helen-Reference" dataset. Specifically, "Helen-Reference" contains 798 images out of 2330 images in the Helen dataset, which occupies 34% of the complete dataset. As presented in <ref type="table" target="#tab_1">Table  IV</ref>, the performance only drops by 1.4% if we take "Helen-Primary" as the training dataset and "Helen-Reference" as the testing dataset. In comparison, when we swap the datasets, the parsing result is only 42.9% in Overall F1 score. This is because the "Helen-Reference" dataset with large pitch angles does not provide enough information for understanding complete faces, especially for subtle mouth components.</p><p>In addition, the cross test between data with different face   poses also demonstrates the performance of AGRNet when some parts of the face missing, e.g., due to the self-occlusion, where non-front faces are self-occluded in general. The good performance of training on "Helen-Primary" and testing on "Helen-Reference" shows our method generalizes to selfoccluded faces well. This generalization ability gives credits to the intrinsic structure embedded in the proposed graph representation. That is, even though self-occluded data may demonstrate rather different distribution characteristics, there may be an intrinsic structure embedded in the data. Such intrinsic structure may be regarded to be better maintained from seen datasets to unseen datasets. Consequently, the graph representation learned from the face structure provides extra insights from the structure domain, in addition to the data domain, that finally enhances the generalizability of the network. 4) Visualization of vertices and response: Further, we present some visualization examples of vertices and the pixelto-vertex projection matrix in order to provide intuitive interpretation. <ref type="figure" target="#fig_6">Fig. 8</ref> shows the selected vertices with respect to specific facial components, where keypoints are marked as yellow. We observe that vertices lie in the interior region of the corresponding facial component. Besides, symmetric components are well separated, such as left and right brows.</p><p>Also, we visualize the adjacency matrix trained on the Helen dataset in <ref type="figure" target="#fig_5">Fig. 7</ref>. As shown by the color bar, lighter colors indicate larger edge weights. We observe that the pair of left brow ("lb" in the figure) and the right brow ("rb" in the figure) has the strongest positive correlation, while the pair of inner mouth ("imouth" in the figure) and nose has the strongest negative correlation, which is reasonable to some extent.</p><p>Furthermore, we visualize the pixel-to-vertex projection matrix via response maps. As in <ref type="figure" target="#fig_7">Fig. 11</ref>, given a projection matrix P ? R KNc?H1W1 , we visualize the weight of each pixel that contributes to semantic vertices. Since there are 4 vertices corresponding to each component, we sum them up as a complete component response map. Brighter color in <ref type="figure" target="#fig_7">Fig. 11</ref> indicates higher response. We observe that pixels demonstrate high response to vertices in the corresponding face component in general, which validates the effectiveness of the proposed adaptive graph projection. In particular, edge pixels show higher response in each component, thanks to the proposed edge attention and Boundary-Aware loss. Note that, there exist outliers if some component is blocked. For example, while one ear is occluded in both images from row 3 and row 4, several pixels still exhibit high response to the other ear region in the response map. 5) Failure cases: We show several unsatisfactory examples in the CelebAMask-HQ dataset in <ref type="figure">Fig. 10</ref>, where most of incorrect labels lie along the boundaries or "accessories" categories (e.g., earring and necklace). The cause of failure cases is mostly the significant imbalance of pixel numbers in different categories or the disturbance of adjacent pixels along the boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Human Parsing</head><p>Human parsing is another segmentation task which predicts a pixel-wise label to each semantic human part. Unlike face parsing, human structure is hierarchical, where a large component could be decomposed into several fragments. Unlike The yellow points represent vertices acquired from the proposed adaptive graph projection, and each column represents a certain category. Note that, the vertices significant overlap with the mainstream facial landmark layout <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, which brings the advantages towards learning and reasoning the semantic representation for face parsing recent works which often construct a fixed graph topology based on hand-crafted human hierarchy <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, our model learns the graph connectivity adaptively. Following the standard training and validation split as described in <ref type="bibr" target="#b56">[57]</ref>, we evaluate the performance of our model on the validation set of the LIP dataset, and present the results in <ref type="table" target="#tab_5">Table V</ref>. Several comparison methods involve the attention module, including SS-NAN <ref type="bibr" target="#b55">[56]</ref> and Attention <ref type="bibr" target="#b52">[53]</ref>. The results in <ref type="table" target="#tab_5">Table V</ref> show that our model outperforms the competitive method CE2P <ref type="bibr" target="#b56">[57]</ref> by 0.7% in mean IoU, which validates that our model is generalizable to human parsing. Also, it is worth noting that our model achieves significant improvement in elaborate categories, such as socks, glove and j-suits.</p><p>Furthermore, we provide visualization examples in <ref type="figure">Fig. 9</ref>. The visualized results demonstrate that our model leads to accurate prediction of human components, especially around the boundaries between components, which gives credits to the proposed component-level modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose an adaptive graph representation learning and reasoning method (AGRNet) for the component-wise modeling of face images, aiming to exploit the component-wise relationship for accurate face parsing. In particular, we adaptively project the representation from the pixel space to the vertex space that represents semantic components as graph abstraction under the initial condition of a predicted parsing map. The image edge information (a) Image (b) Ground Truth (c) Ours Im <ref type="figure">Fig. 9</ref>: Human parsing results on the LIP dataset. Our parsing maps are clear and smooth along the boundaries, and even exhibit better results of shorts compared to the ground truth as depicted in the third row. <ref type="figure">Fig. 10</ref>: Failure cases. The earring is mislabeled in the first sample, the hair region is not continuous in the second sample, and some background is classified as nose in the third one. is incorporated to highlight ambiguous pixels during the projection for precise segmentation along the edges. Then, the model learns and reasons over the relationship among components by propagating features across vertices on the graph. Finally, the refined vertex features are projected back to pixel grids for the prediction of the final parsing map. Further, we propose a discriminative loss to learn vertices with distinct features for semantic description of facial components. Experimental results demonstrate that AGRNet sets the new state of the art on large-scale face parsing datasets, with accurate segmentation along the boundaries. Also, our model shows effectiveness on the human parsing task, which validates its generalizability.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The overview of the proposed face parsing framework.(b) Adaptive Projection (a) Spatial Projection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of the spatial graph projection in<ref type="bibr" target="#b9">[10]</ref> and the proposed adaptive graph projection. (a) Spatial graph projection, where pixels within an image patch are projected to a vertex via spatial pooling. (b) Adaptive graph projection, where a collection of pixels are projected to a vertex adaptively so as to represent each semantic facial component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Architecture of the proposed adaptive graph representation learning and reasoning for face parsing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Image (b) Ground Truth (e) Ours (c) Luo et al. [11] (d) Lee et al. [10] Visualization of parsing results from different methods on the CelebAMask-HQ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of the hyper-parameter search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization of the learned graph adjacency matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization of projected vertices with respect to facial components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Response maps with respect to different facial components. Brighter color indicates higher response. The response maps exhibit high response to ambiguous pixels along the inter-component boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Yinglu</head><label></label><figDesc>Liu Yinglu Liu is currently a senior researcher in JD AI Research, Beijing, China. She received the B.E. degree in Information Engineering from Xiamen Unversity in 2009, and the Ph.D. degree in Pattern Recognition and Intelligent System from Institute of Automation, Chinese Academy of Sciences in 2015. Before joining JD.com, she was a senior algorithm engineer in Samsung Research China-Beijing. Her research interest includes computer vision and machine learning, with a focus on image generation, semantic segmentation, etc. Hailin Shi Hailin Shi received his PhD from Institute of Automation, Chinese Academy of Sciences in 2017. He is currently a senior researcher at JD AI Research. His research interest includes face analysis and deep learning. He has authored or co-authored over 40 publications in the major conferences and journals of computer vision and pattern recognition, and a book chapter about deep metric learning, and has applied over 20 face-analysis-related patents.Tao Mei Tao Mei (M07-SM11-F19) is a Vice President with JD.COM and the Deputy Managing Director of JD AI Research, where he also serves as the Director of Computer Vision and Multimedia Lab. Prior to joining JD.COM in 2018, he was a Senior Research Manager with Microsoft Research Asia in Beijing, China. He has authored or coauthored over 200 publications (with 12 best paper awards) in journals and conferences, 10 book chapters, and edited five books. He holds over 25 US and international patents. He is or has been an Editorial Board Member of IEEE Trans. on Image Processing, IEEE Trans. on Circuits and Systems for Video Technology, IEEE Trans. on Multimedia, ACM Trans. on Multimedia Computing, Communications, and Applications, Pattern Recognition, etc. He is the General Co-chair of IEEE ICME 2019, the Program Co-chair of ACM Multimedia 2018, IEEE ICME 2015 and IEEE MMSP 2015. Tao received B.E. and Ph.D. degrees from the University of Science and Technology of China, Hefei, China, in 2001 and 2006, respectively. He is an adjunct professor of University of Science and Technology of China, The Chinese University of Hong Kong (Shenzhen), and Ryerson University. Tao is a Fellow of IEEE (2019), a Fellow of IAPR (2016), a Distinguished Scientist of ACM (2016), and a Distinguished Industry Speaker of IEEE Signal Processing Society (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Training for AGRNet Input: Image set {I 1 , ..., I n }, Edge set {E 1 , ..., E n }, Label set {Y 1 , ..., Y n }, Hyper parameters ? 1 , ..., ? 5 , Number of categories N c Output: Parsing Map Y, Model parameters ? while not converge do Sample</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Ablation study on the LaPa dataset. We conduct comparison experiments from multiple aspects, including composition of modules, loss functions, vertex numbers, FLOPs and parameters.</figDesc><table><row><cell>Model</cell><cell cols="5">ResNet Spatial Adaptive Edge Graph</cell><cell></cell><cell cols="2">Mean F1 (%) Mean IoU (%) Mean Accuracy (%)</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90.9</cell><cell>85.1</cell><cell>90.2</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.1</cell><cell>85.7</cell><cell>90.8</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.4</cell><cell>86.0</cell><cell>91.3</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.9</cell><cell>86.3</cell><cell>91.7</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.6</cell><cell>85.6</cell><cell>92.1</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.3</cell><cell>87.0</cell><cell>92.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) On composition of modules</cell></row><row><cell>Model</cell><cell cols="6">Cross Entropy Discriminative Boundary-Attention</cell><cell cols="2">Mean F1 (%) Mean IoU (%) Mean Accuracy (%)</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90.9</cell><cell>85.1</cell><cell>90.2</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.4</cell><cell>86.1</cell><cell>92.5</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.8</cell><cell>86.3</cell><cell>91.8</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.3</cell><cell>87.0</cell><cell>92.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) On loss functions</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Top K</cell><cell></cell><cell></cell><cell></cell><cell>FLOPs (G) Parameters (M)</cell></row><row><cell cols="2">Metric (%)</cell><cell>top 1</cell><cell>top 2</cell><cell>top 4</cell><cell>top 8</cell><cell></cell><cell>ResNet</cell><cell>6.652</cell><cell>14.10</cell></row><row><cell cols="2">Mean F1</cell><cell>91.6</cell><cell>91.9</cell><cell>92.3</cell><cell>91.1</cell><cell></cell><cell>Edge Perceiving</cell><cell>0.249</cell><cell>0.03</cell></row><row><cell cols="2">Mean IoU</cell><cell>86.0</cell><cell>86.3</cell><cell>87.0</cell><cell>86.2</cell><cell cols="2">Graph Projection</cell><cell>0.471</cell><cell>0.02</cell></row><row><cell cols="2">Mean Accuracy</cell><cell>92.0</cell><cell>93.5</cell><cell>92.7</cell><cell>92.3</cell><cell cols="2">Graph Reasoning</cell><cell>0.037</cell><cell>&lt;0.01</cell></row><row><cell></cell><cell></cell><cell cols="3">(c) On vertex numbers</cell><cell></cell><cell></cell><cell cols="2">(d) On FLOPs and parameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparison with the state-of-the-art methods on face parsing datasets (in F1 score).</figDesc><table><row><cell cols="2">Methods</cell><cell cols="9">Skin Nose U-lip I-mouth L-lip Eyes Brows Mouth</cell><cell>Overall</cell></row><row><cell cols="2">Liu et al. [12]</cell><cell>92.1</cell><cell cols="2">93.0</cell><cell>74.3</cell><cell>79.2</cell><cell>81.7</cell><cell>86.8</cell><cell>77.0</cell><cell>89.1</cell><cell>88.6</cell></row><row><cell cols="2">Lin et al. [13]</cell><cell>94.5</cell><cell cols="2">95.6</cell><cell>79.6</cell><cell>86.7</cell><cell>89.8</cell><cell>89.6</cell><cell>83.1</cell><cell>95.0</cell><cell>92.7</cell></row><row><cell cols="2">Wei et al. [46]</cell><cell>95.6</cell><cell cols="2">95.2</cell><cell>80.0</cell><cell>86.7</cell><cell>86.4</cell><cell>89.0</cell><cell>82.6</cell><cell>93.6</cell><cell>91.7</cell></row><row><cell cols="2">Yin et al. [14]</cell><cell>-</cell><cell cols="2">96.3</cell><cell>82.4</cell><cell>85.6</cell><cell>86.6</cell><cell>89.5</cell><cell>84.8</cell><cell>92.8</cell><cell>91.0</cell></row><row><cell cols="2">Liu et al. [9]</cell><cell>94.9</cell><cell cols="2">95.8</cell><cell>83.7</cell><cell>89.1</cell><cell>91.4</cell><cell>89.8</cell><cell>83.5</cell><cell>96.1</cell><cell>93.1</cell></row><row><cell cols="2">Te et al. [10]</cell><cell>94.6</cell><cell cols="2">96.1</cell><cell>83.6</cell><cell>89.8</cell><cell>91.0</cell><cell>90.2</cell><cell>84.9</cell><cell>95.5</cell><cell>93.2</cell></row><row><cell cols="2">Ours</cell><cell>95.1</cell><cell cols="2">95.9</cell><cell>83.2</cell><cell>90.0</cell><cell>90.9</cell><cell>90.1</cell><cell>85.0</cell><cell>96.2</cell><cell>93.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) The Helen dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="11">Skin Hair L-Eye R-Eye U-lip I-mouth L-lip Nose L-Brow R-Brow</cell><cell>Mean</cell></row><row><cell>Zhao et al. [47]</cell><cell>93.5</cell><cell>94.1</cell><cell>86.3</cell><cell></cell><cell>86.0</cell><cell>83.6</cell><cell>86.9</cell><cell>84.7</cell><cell>94.8</cell><cell>86.8</cell><cell>86.9</cell><cell>88.4</cell></row><row><cell>Liu et al. [9]</cell><cell>97.2</cell><cell>96.3</cell><cell>88.1</cell><cell></cell><cell>88.0</cell><cell>84.4</cell><cell>87.6</cell><cell>85.7</cell><cell>95.5</cell><cell>87.7</cell><cell>87.6</cell><cell>89.8</cell></row><row><cell>Te et al. [10]</cell><cell>97.3</cell><cell>96.2</cell><cell>89.5</cell><cell></cell><cell>90.0</cell><cell>88.1</cell><cell>90.0</cell><cell>89.0</cell><cell>97.1</cell><cell>86.5</cell><cell>87.0</cell><cell>91.1</cell></row><row><cell>Luo et al. [11]</cell><cell>95.8</cell><cell>94.3</cell><cell>87.0</cell><cell></cell><cell>89.1</cell><cell>85.3</cell><cell>85.6</cell><cell>88.8</cell><cell>94.3</cell><cell>85.9</cell><cell>86.1</cell><cell>89.2</cell></row><row><cell>Wei et al. [46]</cell><cell>96.1</cell><cell>95.1</cell><cell>88.9</cell><cell></cell><cell>87.5</cell><cell>83.1</cell><cell>89.2</cell><cell>83.8</cell><cell>96.1</cell><cell>86.0</cell><cell>87.8</cell><cell>89.4</cell></row><row><cell>Ours</cell><cell>97.7</cell><cell>96.5</cell><cell>91.6</cell><cell></cell><cell>91.1</cell><cell>88.5</cell><cell>90.7</cell><cell>90.1</cell><cell>97.3</cell><cell>89.9</cell><cell>90.0</cell><cell>92.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) The LaPa dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">Face I-Mouth U-Lip Nose</cell><cell cols="5">Glasses L-Eye R-Eye L-Brow L-Lip Hair Hat Earring</cell><cell>R-Brow Necklace</cell><cell cols="2">L-Ear R-Ear Neck Cloth</cell><cell>Mean</cell></row><row><cell>Zhao et al. [47]</cell><cell>94.8 89.8</cell><cell cols="2">90.3 87.1</cell><cell></cell><cell>75.8 88.8</cell><cell>79.9 90.4</cell><cell>80.1 58.2</cell><cell>77.3 65.7</cell><cell>78.0 19.4</cell><cell>75.6 82.7</cell><cell>73.1 64.2</cell><cell>76.2</cell></row><row><cell>Lee et al. [18]</cell><cell>95.5 63.4</cell><cell cols="2">85.6 88.9</cell><cell></cell><cell>92.9 90.1</cell><cell>84.3 86.6</cell><cell>85.2 91.3</cell><cell>81.4 63.2</cell><cell>81.2 26.1</cell><cell>84.9 92.8</cell><cell>83.1 68.3</cell><cell>80.3</cell></row><row><cell>Luo et al. [11]</cell><cell>96.0 93.8</cell><cell cols="2">93.7 88.6</cell><cell></cell><cell>90.6 90.3</cell><cell>86.2 93.9</cell><cell>86.5 85.9</cell><cell>83.2 67.8</cell><cell>83.1 30.1</cell><cell>86.5 88.8</cell><cell>84.1 83.5</cell><cell>84.0</cell></row><row><cell>Wei et al. [46]</cell><cell>96.4 90.6</cell><cell cols="2">91.9 87.9</cell><cell></cell><cell>89.5 91.0</cell><cell>87.1 91.1</cell><cell>85.0 83.9</cell><cell>80.8 65.4</cell><cell>82.5 17.8</cell><cell>84.1 88.1</cell><cell>83.3 80.6</cell><cell>82.1</cell></row><row><cell>Te et al. [10]</cell><cell>96.2 95.0</cell><cell cols="2">94 88.9</cell><cell></cell><cell>92.3 91.2</cell><cell>88.6 94.9</cell><cell>88.7 87.6</cell><cell>85.7 68.3</cell><cell>85.2 27.6</cell><cell>88 89.4</cell><cell>85.7 85.3</cell><cell>85.1</cell></row><row><cell>Ours</cell><cell>96.5 92.0</cell><cell cols="2">93.9 89.1</cell><cell></cell><cell>91.8 91.1</cell><cell>88.7 95.2</cell><cell>89.1 87.2</cell><cell>85.5 69.6</cell><cell>85.6 32.8</cell><cell>88.1 89.9</cell><cell>88.7 84.9</cell><cell>85.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) The CelebAMask-HQ dataset</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>The parsing results under different hyperparameters (measured by overall F1).</figDesc><table><row><cell>Parameter</cell><cell cols="2">=0.1 =0.5</cell><cell>=1</cell><cell>=2</cell><cell>=5</cell></row><row><cell>? 1</cell><cell>92.8</cell><cell>93.0</cell><cell>93.1</cell><cell cols="2">93.1 92.7</cell></row><row><cell>? 2</cell><cell>92.9</cell><cell>92.8</cell><cell>93.1</cell><cell cols="2">93.0 92.4</cell></row><row><cell>? 3</cell><cell>92.2</cell><cell>92.6</cell><cell>93.1</cell><cell cols="2">93.0 92.1</cell></row><row><cell>? 4</cell><cell>93.1</cell><cell>93.2</cell><cell cols="3">93.1 93.0 92.9</cell></row><row><cell>? 5</cell><cell>93.1</cell><cell>92.0</cell><cell cols="3">91.5 90.0 85.4</cell></row></table><note>of Overall F1 score on the test set of the Helen dataset. Specifically, we train our model under a finite set of reasonable settings of hyper-parameters {?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>The performance over domain gaps (measured in F1 score).</figDesc><table><row><cell>Train</cell><cell>Test</cell><cell cols="8">Skin Nose U-lip I-mouth L-lip Eyes Brows Mouth</cell><cell>Overall</cell></row><row><cell>Helen-Train</cell><cell>Helen-Test</cell><cell>95.0</cell><cell>96.0</cell><cell>83.1</cell><cell>90.0</cell><cell>90.7</cell><cell>89.9</cell><cell>85.1</cell><cell>96.2</cell><cell>93.1</cell></row><row><cell>CelebAMask-HQ</cell><cell>Helen-Test</cell><cell>93.2</cell><cell>94.7</cell><cell>82.7</cell><cell>88.5</cell><cell>90.1</cell><cell>89.1</cell><cell>79.6</cell><cell>92.4</cell><cell>91.8</cell></row><row><cell>Helen-Primary</cell><cell>Helen-Reference</cell><cell>92.6</cell><cell>92.0</cell><cell>74.1</cell><cell>91.0</cell><cell>89.2</cell><cell>81.2</cell><cell>83.1</cell><cell>95.6</cell><cell>90.9</cell></row><row><cell>Helen-Reference</cell><cell>Helen-Primary</cell><cell>61.3</cell><cell>34.8</cell><cell>4.6</cell><cell>0</cell><cell>45.1</cell><cell>69.4</cell><cell>30.7</cell><cell>33.2</cell><cell>42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Experimental comparison on the LIP dataset for human parsing (in IoU score).</figDesc><table><row><cell></cell><cell>background</cell><cell>hat</cell><cell>hair</cell><cell>glove</cell><cell cols="2">glasses u-cloth</cell><cell>dress</cell><cell>coat</cell><cell>socks</cell><cell>pants</cell><cell>j-suits</cell><cell>scarf</cell><cell>skirt</cell><cell>face</cell><cell>l-arm</cell><cell>r-arm</cell><cell>l-leg</cell><cell>r-leg</cell><cell cols="2">l-shoe r-shoe</cell><cell>mean</cell></row><row><cell>SegNet [50]</cell><cell>70.62</cell><cell>26.60</cell><cell>44.01</cell><cell>0.01</cell><cell>0.00</cell><cell>34.46</cell><cell>0.00</cell><cell>15.97</cell><cell>3.59</cell><cell>33.56</cell><cell>0.01</cell><cell>0.00</cell><cell>0.00</cell><cell>52.38</cell><cell>15.30</cell><cell cols="3">24.23 13.82 13.17</cell><cell>9.26</cell><cell>6.47</cell><cell>18.17</cell></row><row><cell>FCN-8s [51]</cell><cell>78.02</cell><cell>39.79</cell><cell>58.96</cell><cell>5.32</cell><cell>3.08</cell><cell>49.08</cell><cell cols="2">12.36 26.82</cell><cell>15.66</cell><cell>49.41</cell><cell>6.48</cell><cell>0.00</cell><cell>2.16</cell><cell>62.65</cell><cell>29.78</cell><cell cols="3">36.63 28.12 26.05</cell><cell>17.76</cell><cell>17.70</cell><cell>28.29</cell></row><row><cell>DeepLabV2 [52]</cell><cell>84.53</cell><cell>56.48</cell><cell>65.33</cell><cell>29.98</cell><cell>19.67</cell><cell>62.44</cell><cell cols="2">30.33 51.03</cell><cell>40.51</cell><cell>69.00</cell><cell>22.38</cell><cell cols="2">11.29 20.56</cell><cell>70.11</cell><cell>49.25</cell><cell cols="3">52.88 42.37 35.78</cell><cell>33.81</cell><cell>32.89</cell><cell>41.64</cell></row><row><cell>Attention [53]</cell><cell>84.00</cell><cell>58.87</cell><cell>66.78</cell><cell>23.32</cell><cell>19.48</cell><cell>63.20</cell><cell cols="2">29.63 49.70</cell><cell>35.23</cell><cell>66.04</cell><cell>24.73</cell><cell cols="2">12.84 20.41</cell><cell>70.58</cell><cell>50.17</cell><cell cols="3">54.03 38.35 37.70</cell><cell>26.20</cell><cell>27.09</cell><cell>42.92</cell></row><row><cell>Attention+SSL [19]</cell><cell>84.56</cell><cell>59.75</cell><cell>67.25</cell><cell>28.95</cell><cell>21.57</cell><cell>65.30</cell><cell cols="2">29.49 51.92</cell><cell>38.52</cell><cell>68.02</cell><cell>24.48</cell><cell cols="2">14.92 24.32</cell><cell>71.01</cell><cell>52.64</cell><cell cols="3">55.79 40.23 38.80</cell><cell>28.08</cell><cell>29.03</cell><cell>44.73</cell></row><row><cell>ASN [54]</cell><cell>84.01</cell><cell>56.92</cell><cell>64.34</cell><cell>28.07</cell><cell>17.78</cell><cell>64.90</cell><cell cols="2">30.85 51.90</cell><cell>39.75</cell><cell>71.78</cell><cell>25.57</cell><cell>7.97</cell><cell>17.63</cell><cell>70.77</cell><cell>53.53</cell><cell cols="3">56.70 49.58 48.21</cell><cell>34.57</cell><cell>33.31</cell><cell>45.41</cell></row><row><cell>SSL [19]</cell><cell>84.64</cell><cell>58.21</cell><cell>67.17</cell><cell>31.20</cell><cell>23.65</cell><cell>63.66</cell><cell cols="2">28.31 52.35</cell><cell>39.58</cell><cell>69.40</cell><cell>28.61</cell><cell cols="2">13.70 22.52</cell><cell>74.84</cell><cell>52.83</cell><cell cols="3">55.67 48.22 47.49</cell><cell>31.80</cell><cell>29.97</cell><cell>46.19</cell></row><row><cell>MMAN [55]</cell><cell>84.75</cell><cell>57.66</cell><cell>65.63</cell><cell>30.07</cell><cell>20.02</cell><cell>64.15</cell><cell cols="2">28.39 51.98</cell><cell>41.46</cell><cell>71.03</cell><cell>23.61</cell><cell>9.65</cell><cell>23.20</cell><cell>69.54</cell><cell>55.30</cell><cell cols="3">58.13 51.90 52.17</cell><cell>38.58</cell><cell>39.05</cell><cell>46.81</cell></row><row><cell>SS-NAN [56]</cell><cell>88.67</cell><cell>63.86</cell><cell>70.12</cell><cell>30.63</cell><cell>23.92</cell><cell>70.27</cell><cell>33.51</cell><cell>56.75</cell><cell>40.18</cell><cell>72.19</cell><cell>27.68</cell><cell>16.98</cell><cell>26.41</cell><cell cols="4">75.33 55.24 58.93 44.01</cell><cell>41.87</cell><cell>29.15</cell><cell>32.64</cell><cell>47.92</cell></row><row><cell>CE2P [57]</cell><cell>87.6</cell><cell cols="2">65.29 72.54</cell><cell>39.09</cell><cell>32.73</cell><cell>69.46</cell><cell cols="2">32.52 56.28</cell><cell>49.67</cell><cell>74.11</cell><cell>27.23</cell><cell>14.19</cell><cell>22.51</cell><cell>75.5</cell><cell cols="2">65.14 66.59</cell><cell>60.1</cell><cell>58.59</cell><cell>46.63</cell><cell>46.12</cell><cell>53.1</cell></row><row><cell>Ours</cell><cell>87.4</cell><cell cols="3">67.56 72.63 42.96</cell><cell>36.65</cell><cell>69.15</cell><cell>35.35</cell><cell>55.74</cell><cell>51.13</cell><cell>74.19</cell><cell>30.49</cell><cell>15.69</cell><cell>22.61</cell><cell cols="2">75.01 65.04</cell><cell>67.76</cell><cell cols="2">58.51 57.13</cell><cell>45.79</cell><cell>45.37</cell><cell>53.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Gusi Te Gusi Te is currently an MSc student with Wangxuan Institute of Computer Technology, Peking University. Before that, he acquired his bachelor degree in Peking University. His research interests include 3D computer vision and graph neural networks. Hu (Senior Member, IEEE) received the B.S. degree in Electrical Engineering from the University of Science and Technology of China in 2010, and the Ph.D. degree in Electronic and Computer Engineering from the Hong Kong University of Science and Technology in 2015. She was a Researcher with Technicolor, Rennes, France, from 2015 to 2017. She is currently an Assistant Professor with Wangxuan Institute of Computer Technology, Peking University. Her research interests are graph signal processing, graph-based machine learning and 3D visual computing. She has authored over 50 international journal and conference publications, with several paper awards including Best Student Paper Runner Up Award in ICME 2020 and Best Paper Candidate in CVPR 2021. She was awarded the 2021 IEEE Multimedia Rising Star Award-Honorable Mention. She serves as an Associate Editor for Signal Processing Magazine, IEEE Transactions on Signal and Information Processing over Networks, etc.</figDesc><table><row><cell>Wei</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The first four losses in<ref type="bibr" target="#b10">(11)</ref> are in the form of logarithm, while the last loss is in the form of the Euclidean distance. Hence, the weighting parameters of the first four losses should be larger than the parameter of the last loss.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transfiguring portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural face editing with intrinsic image disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5541" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesis of high-quality visible faces from polarimetric thermal faces using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Riggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning adaptive receptive fields for deep image parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2434" to="2442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new dataset and boundary-attention semantic segmentation for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="page" from="11" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Edge-aware graph representation learning and reasoning for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="258" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ehanet: An effective hierarchical aggregation network for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3135</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face parsing via recurrent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face parsing with roi tanh-warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5654" to="5663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end face parsing via interlinked convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04831</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interlinked convolutional neural networks for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2480" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5549" to="5558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="932" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A?2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Expectationmaximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dense and low-rank gaussian crfs using deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5103" to="5112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9225" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-fcn for image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised graph based semantic segmentation by learning communities of imageparts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1359" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dual graph convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2019. British Machine Vision Association</title>
		<meeting>the British Machine Vision Conference 2019. British Machine Vision Association</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial pyramid based graph reasoning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8950" to="8959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Every node counts: Self-ensembling graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107451</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic instance segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Categorylevel adversarial adaptation for semantic segmentation using purified features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6778" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graphonomy: Universal human parsing via graph transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7450" to="7459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grapy-ml: graph pyramid mutual learning for cross-dataset human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accurate facial image parsing at real-time speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4659" to="4670" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Macro-micro adversarial network for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-supervised neural aggregation networks for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR-workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Devil in the details: Towards accurate single and multiple human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4814" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Grand challenge of 106-point facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="613" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Braidnet: Braiding semantics and details for accurate human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350857</idno>
		<ptr target="https://doi.org/10.1145/3343031.3350857" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, ser. MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia, ser. MM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="338" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning compositional neural information fusion for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5703" to="5713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4116" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
