<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LONG-TERM SERIES FORECASTING WITH QUERY SELECTOR -EFFICIENT MODEL OF SPARSE ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-17">17 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Klimek</surname></persName>
							<email>jacek.klimek@morai.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">Nicholas Copernicus University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Klimek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nicholas Copernicus University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witold</forename><surname>Kra?kiewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nicholas Copernicus University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Topolewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nicholas Copernicus University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LONG-TERM SERIES FORECASTING WITH QUERY SELECTOR -EFFICIENT MODEL OF SPARSE ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-17">17 Aug 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Time series forecasting (TSF) is an old and important area of statistical research with vast practical applications to solving real life problems. It has been applied to solving problems arising from many areas of business activities such as finance, healthcare, business processes monitoring and others. On the other hand it has been used to model natural phenomena such as earthquakes, road traffic ect. At a first glance, it seems to be a very promising area for application of Machine Learning (ML) algorithms, especially those called Deep Learning (DL) methods. However, the situation in that matter is not clear yet and even some experts of a more traditional approach to TSF are expressing their general doubts whether DL methods are most appropriate for TSF <ref type="bibr" target="#b7">([Elsayed et al., 2021]</ref>). We believe that it is not the case and if remaining obstacles hindering progress in this area are overcome, then DL methods will prevail -similar to what happened in the Natural Language Processing (NLP) area. We think that the main remaining obstacles lay in building right DL models for TSF and finding software implementations able to deal with the heavy burden of computations associated with modeling TSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>As we mentioned above we define work to be related to our research when it can be placed in one of the following two categories. First category contains works that are concerned with finding appropriate general algorithmic approaches to modeling TSF using DL methods. The second category contains works that are dealing with computational efficiency of particular DL models. In the first category, we would like to mention works that use older neural networks architecture such as RNN, CNN, LSTM. ( <ref type="bibr" target="#b22">[Wen et al., 2018]</ref>, <ref type="bibr" target="#b25">[Yu et al., 2019]</ref>). However, since the emergence of Transformer models <ref type="bibr" target="#b20">([Vaswani et al., 2017])</ref> and their overwhelming success in ML research areas stretching from NLP to Computer Vision we observe the same phenomenon happening in the area of TSF. In our opinion the most interesting papers describing application of Transformer architecture to solving TSF problems are <ref type="bibr" target="#b26">[Zhou et al., 2021]</ref>, <ref type="bibr" target="#b14">[Li et al., 2020]</ref>, <ref type="bibr" target="#b23">[Wu et al., 2020a]</ref>. Most approaches to modeling TSF are based on ideas coming from NLP i.e. sequence to sequence framework ( <ref type="bibr" target="#b17">[Sutskever et al., 2014]</ref>, <ref type="bibr" target="#b22">[Wen et al., 2018]</ref>, <ref type="bibr" target="#b25">[Yu et al., 2019]</ref>). However, there are some other approaches utilizing different ideas -for instance <ref type="bibr" target="#b15">([Oreshkin et al., 2020]</ref>).</p><p>In the second category of works the most interesting, from our point of view, are papers dealing with the most fundamental Transformer inefficiency i.e. computational cost of attention mechanism. The most common solution to this problem comes down to introducing some sparsity to the attention matrix. One can mention works: <ref type="bibr" target="#b3">[Beltagy et al., 2020]</ref>, <ref type="bibr" target="#b5">[Child et al., 2019]</ref>. Additionally, there are some more technical enhancements that can be used to deal with this computational inefficiency by fully utilizing hardware capacity -DeepSeed <ref type="bibr" target="#b16">([Ren et al., 2021]</ref>)or FairScale (https://pypi.org/project/fairscale/)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND (Time series and forecasting)</head><p>In statistics, time series is a collection of random variables X = (X t ) indexed in time order. Usually time indeces are equally spaced. In practice we identify the time series with its sample path -a sequence of reals or real vectors of the same dimension. Time series forecasting is, roughly speaking, a problem of predicting future values of X based on previously observed values. Statistical approach to this problem is to find a conditional distribution of (X n+1 , X n+2 , . . . , X n+k ) given (X 1 , X 2 , . . . , X n ) and then predict future values using this distribution. There are many ways to solve this problem. If X is sufficiently regular, one can try to fit one of ARIMA-type models (involving autoregressive and moving average type of dependence). There is a wide literature concerning properties and methods of fitting the ARIMA-type models, see e.g. <ref type="bibr" target="#b4">[Box et al., 2015</ref><ref type="bibr" target="#b0">, Adebiyi et al., 2014</ref>. Another very popular (especially in econometrics) models of dependence are so-called ARCH (autoregressive conditional heteroscedasticity) models introduced by Robert F. Engle in <ref type="bibr" target="#b8">[Engle, 1982]</ref>. The main idea of ARCH models is that the variance of X t is random and depends on the past of X. More about properties and methods of fitting of ARCH models the reader can find in <ref type="bibr" target="#b10">[Kirchg?ssner et al., 2007]</ref>. Unfortunately, in most applications those very well described models are not achieving the best results. In general situation, one can try to decompose the series into 3 components: trend, seasonality and noise, i.e. rewrite the series X as a sum:</p><formula xml:id="formula_0">X = T + S + N,<label>(1)</label></formula><p>where T depends linearly on time (with some nonperiodic changes, usually related to "business cycle"), S is periodic and N is a white noise (usually Gaussian). Such decomposition provides a useful abstract model for thinking about time series generally. For details concerning construction and usage of the decomposition (1) we defer the reader to wide literature devoted to this problem (e.g. <ref type="bibr" target="#b9">[Hyndman and Athanasopoulos, 2018]</ref>). In machine learning, the time series forecasting problem is solved in many different ways. Some of them involve decomposition (1) and deep learning methods (see e.g. <ref type="bibr" target="#b15">[Oreshkin et al., 2020</ref><ref type="bibr" target="#b6">, Cleveland et al., 1990</ref>) and some use only classical ML methods like GBRT (e.g. <ref type="bibr" target="#b7">[Elsayed et al., 2021]</ref>). There is a wide literature devoted to sequence-to-sequence models for TSF (see e.g. <ref type="bibr" target="#b12">[Kondo et al., 2019</ref><ref type="bibr" target="#b24">, Wu et al., 2020b</ref><ref type="bibr" target="#b13">, Lai et al., 2018</ref>). Nowadays, the Transformer-based models are very popular in deep learning, in particular in sequence-to-sequence models. The main advantage of Transformer is that it is able to learn and model all dependencies between input data. This property is invaluable in NLP problems, but in time series case the decomposition <ref type="formula" target="#formula_0">(1)</ref> suggests that the number of dependencies is not so large, especially when n is much larger then the length of the period of S. Therefore it is reasonable to consider sparse versions of attention. There are many methods of construction of the sparse version of attention and a lot of them involve some random effects (see e.g. <ref type="bibr" target="#b26">[Zhou et al., 2021</ref><ref type="bibr" target="#b5">, Child et al., 2019</ref>). We believe that for validation and comparison reasons it is valuable to use a deterministic method to build a sparse version of attention. That is why we propose the method described in section 3. It is a result of a number of experiments with different ideas of sparse attention mechanisms in TSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>As we mentioned above, the problem of Time Series Forecasting gets more and more computationally challenging along with increasing length of input data and forecasting range. Thus, the computational hindrances are an inherent problem in the area of TSF research. In order to deal with them we researched several methods aiming to reduce this complexity by approximating the full attention matrix by some sparse matrices. We find that the following approaches to this problem are the most interesting -LogSparse <ref type="bibr" target="#b14">[Li et al., 2020]</ref>, Reformer <ref type="bibr" target="#b11">[Kitaev et al., 2020]</ref> and Informer <ref type="bibr" target="#b26">[Zhou et al., 2021]</ref>. Moreover, since Informer reports the best results in comparison to the other mentioned systems, we focused our analysis on this algorithm. The clue of Informer approach seems to be contained in so called Probability Sparse Attention. It relies on a clever choice of indexes that will be used in computing a matrix that approximates usual attention matrix in vanilla Transformers. In the process of analyzing and experimenting with this methodology we come to an impression that, in the case of long sequence TSF, Transformer based systems can converge to optimum using the whole variety of probabilistic choices of indexes for approximations of attention matrices. The one of main disadvantages of using probabilistic methods is the difficulty of comparing different experiments with changing parameters. Thus, we propose a simple but very effective, completely deterministic method for computing sparse approximation of attention matrix which yields excellent results on various time series datasets. Let us recall the basic notions of Transformer architecture and set necessary notation. Given the input representation X, the attention matrix can be calculated as follows. First, we compute the query, key and value matrices for each attention head through</p><formula xml:id="formula_1">Algorithm 1 Query Selector Require: K ? R L?D , Q ? R L?D , V ? R L?E , 0 &lt; f &lt; 1 1: set ? = ?(1 ? f ) ? L? 2: set I = {X ? P({1, 2, ..., L}) : |X| = ?} 3: setK = 1 ? ? (max I?I i?I K i,1 , max I?I i?I K i,2 , . . . , max I?I i?I K i,D ) 4: set S =K ? Q T 5: for i in 1 . . . ? do ? find indeces of ? greatest coordinates in S 6: c i = arg max S 7: S c i = ?? 8: end for 9: setQ = ? ? ? ? ? q c 1 ,1 q c 1 ,2 . . . q c 1 ,D q c 2 ,1 q c 2 ,2 . . . q c 2 ,D . . . . . . . . . . . . q c ? ,1 q c ? ,2 . . . q c ? ,D ? ? ? ? ? 10: setQK =Q ? K T 11: set A = softmax(QK/ ? D) ? V 12: set v = 1 L ? ( L i=1 V i,1 , L i=1 V i,2 , ..., L i=1 V i,E ) 13: set? = ? ? ? ? ? v v . . . v ? ? ? ? ? ? ? ? ? ? ? ? ? ? L rows 14: for i in 1 . . . ? do 15: for j in 1 . . . E do 16:? c i ,j = A i,j 17:</formula><p>end for 18: end for 19: return? linear projections, i. e. , Q = XW Q , K = XW K and V = XW V , where Q, K and V denote query, key and value matrices respectively, W Q , W K and W V are linear projections. Then, the attention matrix is computed by a scaled dot-product operation:</p><formula xml:id="formula_2">Attention(X) = softmax(QK T / ? d)V,</formula><p>were d denotes the hidden dimension. In order to incorporate sequential informa-tion into the model, we utilize (as usual) a positional encoding to the input representation. Here, we follow the original implementation of absolute positional encoding ( <ref type="bibr" target="#b20">[Vaswani et al., 2017]</ref>), it is added to the input representation X directly. It is worth mentioning here that Informer has its original proposition for positional encoding.</p><p>In our approach, we are mostly concerned with lowering computational complexity of the algorithm. We called our algorithm Query Selector because it's main insight relies on proper selection of some number of queries that we will use in our computations. Additionally, we would like to lower the amount of memory needed to store the whole QK T . To this end, we construct a matrix? which will play the analogous role to role of attention matrix in the Transformer algorithm.</p><p>Another insight for our algorithm comes from the idea of dropout that is successfully employed in many different ML algorithms. As in dropout we choose a number f , 0 &lt; f &lt; 1, that is one of the hyperparameters of our algorithm. Then, the construction of the matrix? is as follows. We choose ? = ?(1 ? f ) ? L? queries that give the biggest scalar products with keys. Next, we replace the usual matrix K with a column-constant matrix K of elements equal to the mean value of ? greatest elements in the column of K. Next, we construct Q by choosing ? rows of matrix Q with indices equal to indices of ? columns of K with highest common value of given column and remaining rows are set to zero. Matrix? is given by formul?</p><formula xml:id="formula_3">A = softmax(Q ? K T / ? D) ? V.</formula><p>It is worth mentioning that this formula implies that the rows of? corresponding to zero rows of Q can be computed by taking the arithmetic means of columns of V . The details of the above described algorithm are given in the pseudo code on page 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In our experiments we use the ETT (Electricity Transformer Temperature) dataset. In this dataset each data point consists of 8 variables, including the date of the point, the oil temperature and 6 other parameters measured at electric transformer stations in China. As we can see from this description, ETT dataset allows not only for univariate TSF but for multivariate as well. ETT dataset consists of three different datasets: ETT -small, ETT -large, ETT-full. So far, the only publicly available dataset is ETT -small so we perform our experiments using this set of data only. Moreover, Informer <ref type="bibr" target="#b26">[Zhou et al., 2021]</ref> reports the State of the Art (SOTA) results on this dataset, so in order to be able to compare our methodology to theirs, we closely follow their experimental setup. ETT contains data collected from two regions of a province of China. Datasets are named ETT-small-m1 and ETT-small-m2 and m denotes the fact that data has been measured every minute for 2 years. This gives 70,080 data records altogether. Additionally, the ETT dataset contains ETT-small-h1 and ETT-small-h2 subsets with hourly measurements of parameters of the same type. More details and access to data can be found at https://github.com/zhouhaoyi/ETDataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Baselines</head><p>As we already mentioned, Informer reports the SOTA results on ETT dataset and carried out comparison with other important TSF algorithms such that <ref type="bibr" target="#b1">[Ariyo et al., 2014]</ref>, Prophet <ref type="bibr" target="#b19">[Taylor and Letham, 2018]</ref>, LSTMa <ref type="bibr" target="#b2">[Bahdanau et al., 2015]</ref>, LSTnet <ref type="bibr" target="#b13">[Lai et al., 2018]</ref>, DeepAR <ref type="bibr" target="#b9">[Flunkert et al., 2017]</ref>, Reformer <ref type="bibr" target="#b11">[Kitaev et al., 2020]</ref>, and LogSparse self-attention <ref type="bibr" target="#b14">[Li et al., 2020]</ref>. So we compare our results on this dataset with only Informer's results and our implementation of full attention Transformer based on DeepSpeed framework. We use two evaluation metrics, MSE = 1 n (y i ?? i ) 2 and MAE = 1 n |y i ?? i |. For multivariate prediction, we average metrics over coordinates. They are the most popular measures of prediction accuracy. Unfortunately they are dependent on the scale of the data. While scale independent measures (like MAPE, sMAPE and others) gain their popularity in literature (see e.g. <ref type="bibr" target="#b21">[Wallstr?m and Segerstedt, 2010]</ref>), we avoid scale dependency by normalization of data.</p><p>The source code is available at https://github.com/moraieu/query-selector. All the models were trained and tested on a 2 Nvidia V100 16 GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Hyper-parameter tuning</head><p>We conduct a search over the hyper-parameters. Tuned parameters include number of encoder and decoder layers, number of heads, batch size, hidden dimension size, value dimension size, iterations number and dropout. For cases using Query Selector we tested different values of query selector factor f . Ranges of search are given in <ref type="table">Table 1</ref> and the final values can be found on github (https://github.com/moraieu/queryselector/tree/master/settings). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results summery</head><p>The results for Query Selector are summarized in <ref type="table" target="#tab_2">Table 2 and Table 3</ref>. They are compared to the results for Informer and for Transformer published in <ref type="bibr" target="#b26">[Zhou et al., 2021]</ref>. In prevail number of cases, prediction error of Query Selector is smaller or comparable to the error of two other models. In the last column of both tables, ratio of MSE for Query Selector to MSE for Informer is given.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">BUSINESS PROCESSES</head><p>Having investigated applications of Query Selector algorithms in the area of time series forecasting related to data points gathered by technical devices such as ETT dataset, we extended our research to other areas of time series forecasting. One of the most appealing and interesting areas is area related to predictive business process monitoring. These challenges are related to predicting the evolution of ongoing business processes using models built on historical data. The very compelling review of more traditional approaches (like: Naive Bayes Classification, Support Vector Regression) to this problem can be found in <ref type="bibr" target="#b16">[Polato et al., 2016]</ref>. As in so many other cases the deep learning methods and algorithms applied to this field of research turned out to yield the better results and using LSTM neural networks the authors of <ref type="bibr" target="#b18">[Tax et al., 2017]</ref> achieved the state of the art results on two popular datasets containing data related to business processes. Using QuerySelector we were able to outperform their results. Precise results are resumed in <ref type="table">Table 6</ref>. As structure of business processes is completely different from that of previous examples, we need to introduce some notation which will explain our theoretical setup. We follow <ref type="bibr" target="#b16">[Polato et al., 2016]</ref> and <ref type="bibr" target="#b18">[Tax et al., 2017]</ref> since we would like to precisely compare the effectiveness of both methods.</p><p>The central notion is trace which is a mathematical representation of a case (i.e. execution of a specific instance of a business process). Trace is a time-ordered finite sequence of events. Each event can have different properties but for us only its timestamp and type of activity are essential. They are given by two functions ? T : E ?? T and ? A : E ?? A where E is the universe of all possible events, T is time domain and A is a finite set of activity types.</p><p>If ? = (? 1 , ? 2 , . . . , ? n ), ? i ? E, is a trace, we assume that ? i = ? j for i = j, and ? T (? i ) ? ? T (? j ) for i &lt; j (time-ordering). Functions ? T and ? A have natural extensions to traces (in fact, to any sequences of events): ? T (?) = (? T (? 1 ), ? T (? 2 ), . . . , ? T (? n )) and similarly for ? A . A nonempty set of traces is called event log and it is a mathematical abstraction of dataset.</p><p>Our objective is to predict activity type of future events i.e. to find ? A (? k+1 ) given k ? n first events in a trace ? = (? 1 , ? 2 , . . . , ? n ).</p><p>The two datasets we dealed with are Helpdesk 1 and BPI Challenge 2012 2 . The data contained in those datasets is of the form of event logs of particular business processes. Usually, event logs are well structured and each event of a business process is recorded and it refers to an actual activity of a particular case. Additionally, it contains other information such as the originator of an activity or the timestamp. Since our main goal is to investigate the effectiveness of our Ouery Selector method we follow strictly the setup of <ref type="bibr" target="#b18">[Tax et al., 2017]</ref> in order to be able to compare the results of forecasting algorithms on those datasets.</p><p>The first dataset, Helpdesk contains data logs concerning the ticketing management process of the help desk of an Italian software company. In particular, this process consists of 9 activities: it starts with the insertion of a new ticket and then a seriousness level is applied. Then the ticket is managed by a resource and it is processed. When the problem is resolved it is closed and the process instance ends. This log contains around 3,804 cases and 13,710 events.</p><p>The BPI Challenge 2012 dataset contains real-world event logs from the 2012 BPI challenges (van Dongen 2012). The anonymized data from a Dutch financial institute's loan or overdraft application process was gathered in total of 262,000 events in 13,087 instances that come from three subprocesses: one that tracks the states of the application, another that tracks the states of the offer, and a third that tracks the states of work items associated with the application. Events from the third subprocess are classified further into type schedule (work items scheduled for execution), type start (work items on which work has started), or type complete (work items on which work has been completed). In the context of predicting the coming events and their timestamps we are not interested in events that are performed automatically. Thus, we narrow down our evaluation to the work items subprocess, which contains events that are manually executed. Further, we alter the log to retain only events of type complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Dataset LSTM QuerrySelector Helpdesk 0.7123 0.743 BPI'12 0.7600 0.790 <ref type="table">Table 4</ref>: Comparison of accuracy results. Results for LSTM are the best results reported in <ref type="bibr" target="#b18">[Tax et al., 2017]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Influence on MSE of Query Selector factor f for different input lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Forecasting results for univariate time-series.</figDesc><table><row><cell>Data</cell><cell>Pre-diction len</cell><cell>Informer MSE</cell><cell>Informer MAE</cell><cell>Trans former MSE</cell><cell>Trans former MAE</cell><cell>Query Selector MSE</cell><cell>Query Selector MAE</cell><cell>MSE ratio</cell></row><row><cell></cell><cell>24</cell><cell>0.5770</cell><cell>0.5490</cell><cell>0.4496</cell><cell cols="4">0.4788 0.4226 0.4627 0.732</cell></row><row><cell>ETTh1</cell><cell>48 168 336</cell><cell>0.6850 0.9310 1.1280</cell><cell cols="6">0.6250 0.7520 0.8730 0.8321 0.7041 0.4668 0.4968 0.4581 0.4878 0.669 0.7146 0.6325 0.6835 0.6088 0.734 0.8503 0.7039 0.738</cell></row><row><cell></cell><cell>720</cell><cell>1.2150</cell><cell cols="4">0.8960 1.1080 0.8399 1.1150</cell><cell cols="2">0.8428 0.912</cell></row><row><cell></cell><cell>24</cell><cell>0.7200</cell><cell>0.6650</cell><cell>0.4237</cell><cell cols="4">0.5013 0.4124 0.4864 0.573</cell></row><row><cell>ETTh2</cell><cell>48 168 336</cell><cell>1.4570 3.4890 2.7230</cell><cell cols="6">1.0010 1.5150 1.6225 0.9726 1.7385 1.5220 0.9488 1.4074 0.9317 0.966 1.0125 0.465 1.3400 2.6617 1.2189 2.3168 1.1859 0.851</cell></row><row><cell></cell><cell>720</cell><cell>3.4670</cell><cell>1.4730</cell><cell>3.1805</cell><cell cols="4">1.3668 3.0664 1.3084 0.884</cell></row><row><cell></cell><cell>24</cell><cell>0.3230</cell><cell cols="3">0.3690 0.3150 0.3886</cell><cell>0.3351</cell><cell cols="2">0.3875 0.975</cell></row><row><cell>ETTm1</cell><cell>48 96 288</cell><cell>0.4940 0.6780 1.0560</cell><cell cols="6">0.5030 0.4454 0.4620 0.4726 0.6140 0.4641 0.4823 0.4543 0.7860 0.6814 0.6312 0.6185 0.5991 0.586 0.4702 0.902 0.4831 0.670</cell></row><row><cell></cell><cell>672</cell><cell>1.1920</cell><cell>0.9260</cell><cell>1.1365</cell><cell cols="4">0.8572 1.1273 0.8412 0.946</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Forecasting results for multivariate time-series.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://data.mendeley.com/datasets/39bp3vv62t/1 2 doi:10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Special thanks goes to Grzegorz Klimek for help in dealing with datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparison of ARIMA and Artificial Neural Networks Models for Stock Price Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adebiyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Mathematics</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stock price prediction using the arima model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ariyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th International Conference on Computer Modelling and Simulation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="106" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bahdanau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Box</surname></persName>
		</author>
		<title level="m">Time Series Analysis: Forecasting and Control. Wiley Series in Probability and Statistics</title>
		<imprint>
			<publisher>Wiley &amp; Son</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Child</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">STL: A Seasonal-Trend Decomposition Procedure Based on Loess (with Discussion)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cleveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Official Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="73" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elsayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02118</idno>
		<title level="m">Do we really need deep learning models for time series forecasting</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Engle ; Engle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="987" to="1007" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepar: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Flunkert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04110</idno>
		<ptr target="https://otexts.com/fpp2/" />
	</analytic>
	<monogr>
		<title level="m">Forecasting: principles and practice</title>
		<imprint>
			<publisher>Hyndman and Athanasopoulos</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to Modern Time Series Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kirchg?ssner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kitaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sequence to sequence with attention for influenza prevalence prediction using google trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kondo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02786</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling longand short-term temporal patterns with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM</title>
		<imprint>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="2018" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00235</idno>
		<title level="m">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">N-beats: Neural basis expansion analysis for interpretable time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Oreshkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Time and activity sequence prediction of business process instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Polato</surname></persName>
		</author>
		<idno>ArXiv:2101.06840</idno>
	</analytic>
	<monogr>
		<title level="m">Zero-offload: Democratizing billion-scale model training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ren et al., 2021</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3215</idno>
		<title level="m">Sequence to sequence learning with neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictive business process monitoring with lstm neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="477" to="492" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Forecasting at scale. The American Statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Letham ;</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluation of forecasting error measurements and techniques for intermittent demand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wallstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Segerstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Production Economics</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="625" to="636" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Wallstr?m and Segerstedt. Supply Chain Forecasting Systems</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11053</idno>
		<title level="m">A multi-horizon quantile recurrent forecaster</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial sparse transformer for time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Connecting the dots: Multivariate time series forecasting with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
		<idno>ArXiv:2005.11650</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00073</idno>
		<title level="m">Long-term forecasting using higher order tensor rnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Informer: Beyond efficient transformer for long sequence timeseries forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07436</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
