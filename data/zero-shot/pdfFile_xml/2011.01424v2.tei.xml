<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling Knowledge by Mimicking Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Hua</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">Distilling Knowledge by Mimicking Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>TO APPEAR IN IEEE TRANS. PAMI 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Knowledge Distillation</term>
					<term>Image Classification</term>
					<term>Object Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation (KD) is a popular method to train efficient networks ("student") with the help of high-capacity networks ("teacher"). Traditional methods use the teacher's soft logits as extra supervision to train the student network. In this paper, we argue that it is more advantageous to make the student mimic the teacher's features in the penultimate layer. Not only the student can directly learn more effective information from the teacher feature, feature mimicking can also be applied for teachers trained without a softmax layer. Experiments show that it can achieve higher accuracy than traditional KD. To further facilitate feature mimicking, we decompose a feature vector into the magnitude and the direction. We argue that the teacher should give more freedom to the student feature's magnitude, and let the student pay more attention on mimicking the feature direction. To meet this requirement, we propose a loss term based on locality-sensitive hashing (LSH). With the help of this new loss, our method indeed mimics feature directions more accurately, relaxes constraints on feature magnitudes, and achieves state-of-the-art distillation accuracy. We provide theoretical analyses of how LSH facilitates feature direction mimicking, and further extend feature mimicking to multi-label recognition and object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ECENTLY, deep learning has achieved remarkable success in many visual recognition tasks. To deploy deep networks in devices with limited resources, more and more efficient networks have been proposed <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Knowledge distillation (KD) <ref type="bibr" target="#b2">[3]</ref> is a popular method to train these efficient networks (named "student") with the help of high-capacity networks (named "teacher").</p><p>Initial study of KD <ref type="bibr" target="#b2">[3]</ref> used the softmax output of the teacher network as the extra supervisory information for training the student network. However, the output of a high-capacity network is not significantly different from groundtruth labels. And, due to the existence of the classifier layer, the softmax output contains less information compared with the representation in the penultimate layer. These issues hinder the performance of a student model. In addition, it is difficult for KD to distill teacher models trained by unsupervised or self-supervised learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Feature distillation has received more and more attention in recent years <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, previous works only focused on distilling features in the middle layers <ref type="bibr" target="#b7">[8]</ref> or transforming the features <ref type="bibr" target="#b8">[9]</ref>. Few have addressed the problem of making the student directly mimic the teacher's feature in the penultimate layer. Distilling features in the middle layers suffers from the different architectures between teacher and student, while transforming the features may lose some information in the teacher. We believe it is a better way to directly mimic the feature for knowledge distillation, in which we only mimic the feature in the penultimate layer. Compared with KD, it does not need the student model to learn a classifier from the teacher. Feature mimicking can be applied to a teacher trained by unsupervised, metric or self-supervised learning, and can be easily used when the teacher and student have different architectures. Furthermore, if the student features are the same as the teacher's, the classification accuracy will surely be the same, too. Some reasons may explain why feature mimicking has not yet been popular in the literature. First, previous work used the mean squared loss ( 2 loss) to distill features. In this paper, we decompose a feature vector into the magnitude and the direction. The 2 loss focuses on both magnitude and direction. But due to the different capacities, the student cannot mimic the teacher in its entirety. In fact, only the direction affects the classification result while the magnitude mainly represents the confidence of prediction <ref type="bibr" target="#b11">[12]</ref>. We find that different networks often have different feature magnitudes (cf. <ref type="table" target="#tab_1">Table 2</ref>). That inspires us to give more freedom to the student feature's magnitude. One possible approach to tackle this problem is to distill the feature after 2 -normalization <ref type="bibr" target="#b8">[9]</ref>. However, it will lose all magnitude information about the teacher feature and make the optimization difficult <ref type="bibr" target="#b12">[13]</ref>. In this paper, we propose a loss term which focuses on the feature direction and gives more freedom to its magnitude, which alleviates the shortcomings of the 2 loss (cf. <ref type="figure">Figure 3</ref>).</p><p>Second, when teacher and student features have different dimensionalities, difficulty arises. To solve this problem, we split the final fully connected (FC) layer of the student network into two FC layers without non-linear activation in-between. The dimensionality of the first FC layer matches that of the teacher feature. The two FCs can be merged into one after training. Hence, no extra parameter or computation is added in the student's architecture during inference.</p><p>Third, even though the feature structure of the student is the same as that of the teacher, their feature space may misalign (cf. <ref type="figure" target="#fig_1">Figure 2</ref>). If we have the freedom to rotate and rescale the student's feature space, it will align to the teacher's feature space better. Thanks to our two FC structure in the proposed feature mimicking method, we demonstrate that the first FC layer can transform the student's feature space and make feature mimicking easier, which is particularly important when the student network is initialized using a pretrained model (i.e., the student has formed a basic feature space to finetune rather than a random feature space).</p><p>Our contributions are as follows. <ref type="bibr">?</ref> We argue that directly mimicking features in the penultimate layer is advantageous for knowledge distillation.</p><p>arXiv:2011.01424v2 [cs.CV] 14 Aug 2021</p><p>It produces better performance than distilling logits after log-softmax (as in <ref type="bibr" target="#b2">[3]</ref>). It can be applied when the teacher and student have different architectures, while distilling features in the middle layers cannot. <ref type="bibr">?</ref> We claim that the feature's direction contains more effective information than its magnitude, and we should allow more freedom to the student feature's magnitude. We propose a loss term based on Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b13">[14]</ref> to meet this requirement, and theoretically show why LSH fits this purpose. <ref type="bibr">?</ref> We propose a training strategy for mimicking features in transfer learning. With a pretrained student, we first transform its feature space to align to the teacher's, then finetune the student on the target dataset with our loss function. Our method is flexible and handles multi-label recognition well, while existing KD methods are difficult to apply to multi-label problems.</p><p>Our feature mimicking framework achieves state-of-the-art results on both single-label and multi-label recognition, and object detection tasks. The rest of this paper is organized as follows. First, we review the related work in Section 2. Then, we introduce our method for feature mimicking in Section 3, and mathematically analyze the effectiveness of it in Section 4. Experimental results are reported and analyzed in Section 5. Finally, Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Knowledge distillation was first introduced in <ref type="bibr" target="#b2">[3]</ref>, which proposed to use the teacher's soft logits after log-softmax as extra supervision to train the student. FitNet <ref type="bibr" target="#b7">[8]</ref> is the first work to distill the intermediate feature maps between teacher and student. Inspired by this, a variety of other feature-based knowledge distillation methods have been proposed. AT <ref type="bibr" target="#b14">[15]</ref> transfers the teacher knowledge to student by the spatial attention maps. AB <ref type="bibr" target="#b15">[16]</ref> proposes a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. FitNet, AT and AB focus on activation maps of the middle layers, and it is difficult to apply them on crossarchitecture settings. SP <ref type="bibr" target="#b16">[17]</ref> considers pairwise similarities of different features instead of mimicking the teacher's representation space. FSP <ref type="bibr" target="#b17">[18]</ref> computes the inner product between features from two layers and treats it as the extra information to teach student. FT <ref type="bibr" target="#b8">[9]</ref> introduces a paraphraser to compress the teacher feature and uses the translator located at the student network to extract the student factors, then teaches the student by making student factors mimic teacher's compressed features. These methods transform the teacher's feature into other forms, which will lose some information in teacher features. In contrast, feature mimicking in the penultimate layer can apply on arbitrary teacher/student combinations and carry all information from the teacher.</p><p>Recently, CRD <ref type="bibr" target="#b9">[10]</ref> and SSKD <ref type="bibr" target="#b10">[11]</ref> take advantage of contrastive learning and transfer the structural knowledge of the teacher network to the student. In this paper, we argue that we can also achieve state-of-the-art by only mimicking features without explicitly considering the structural knowledge.</p><p>Object detection is a fundamental task in computer vision. Several previous works study knowledge distillation on the object detection task. ROI-mimic <ref type="bibr" target="#b18">[19]</ref> mimics the features after ROI pooling. Fine-grained <ref type="bibr" target="#b19">[20]</ref> uses the ground truth bounding box to generate the foreground mask and distill the foreground features on the feature map. PAD <ref type="bibr" target="#b20">[21]</ref> introduces the adaptive sample weighting to improve these distillation methods. In this paper, we will show that mimicking features in the penultimate layer works better.</p><p>Locality-sensitive hashing (LSH) was first introduced in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. With the help of p-stable distributions, <ref type="bibr" target="#b13">[14]</ref> extended the algorithm to the 2 norm. With the rise of deep learning, hashing methods were widely used in image retrieval <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Most of them focused on how to learn good hash functions to transform images into compact codes. Different from that, we utilized LSH to help the student network to learn from the teacher network. To the best of our knowledge, we are the first to propose the use of LSH in distilling knowledge. <ref type="figure">Figure 1</ref> shows the pipeline of our method. Given an image x, the teacher backbone network extracts feature f t , in which f t ? R Dt is the penultimate layer feature (after the global average pooling and before the final classifier or detection head). The student backbone network extracts feature f s . To make the dimensionalities of f s and f t match, we add a linear embedding layer after the student backbone. Section 3.1 will introduce this module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURE MIMICKING FOR KNOWLEDGE DISTIL-LATION</head><p>Three losses are used. L c is the regular cross-entropy loss between the student output and the ground truth label of x. L mse and L lsh are used to make the student feature mimic the teacher's. More details about these two losses can be found in Section 3.2. More analyses are in Sections 3.3 and 3.4. During training, modules with green boxes (student backbone, linear embedding and classifier) in <ref type="figure">Figure 1</ref> need to be learned by back-propagation. Parameters in the teacher backbone and locality-sensitive hashing will not change after initialization. Finally, Section 3.5 discusses how to initialize our framework. We leave theoretical results for feature mimicking to Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The linear embedding layer</head><p>When the dimensionality of the student's feature is different from that of the teacher's, we add a linear embedding layer before the student's classifier layer. Assume the dimensionality of student's features and teacher's are D s and D t , respectively, the embedding layer is defined as</p><formula xml:id="formula_0">f c1 s (f ) = W T 1 f + b 1 ,<label>(1)</label></formula><p>where W 1 ? R Ds?Dt and b 1 ? R Dt . The main advantage of this approach is that the embedding layer can be merged into the classifier without adding parameters or computation post-training. Assume the classifier is defined as</p><formula xml:id="formula_1">f c2 s (f ) = W T 2 f + b 2 ,<label>(2)</label></formula><p>where W 2 ? R Dt?C and b 2 ? R C . Then, the final classifier for student can be computed by</p><formula xml:id="formula_2">f c s (f ) = f c2 s (f c1 s (f ))<label>(3)</label></formula><formula xml:id="formula_3">= (W 1 W 2 ) T f + (W T 2 b 1 + b 2 ) .<label>(4)</label></formula><p>f c1 s and f c2 s can be merged by setting the weights and bias for the final classifier as W 1 W 2 and W T 2 b 1 + b 2 , respectively. This linear embedding layer shares similar idea as FSKD <ref type="bibr" target="#b27">[28]</ref>. FSKD adds a 1 ? 1 conv at the end of each block of the student network and proves that the 1 ? 1 conv can be merged into the previous convolution layer. However, FSKD requires the teacher <ref type="figure">Figure 1</ref>. The pipeline of our method. We use a linear embedding layer to make sure the dimensionality of student's feature is the same as that of the teacher's. But, this embedding layer will be absorbed post-training. <ref type="figure">(</ref>  and student to share similar architectures, and adds more parameters during training. Our method is more efficient and can be applied with different teacher/student architectures.</p><formula xml:id="formula_4">????? &gt;????? ????????? ??1 ? ??? d?????? ???????? ????? ? ??? ?????? ???????? ????? ? ??? ??????? ??????? D^ ???? ? ? &gt;???????? ??????|? ,?????? ???? ? ? ????????? ??2 ? ??? ???? ?</formula><p>Even when the dimensionality of the student's feature is the same as that of the teacher's, the linear embedding layer may still be necessary. Because the teacher and the student may have significantly different network architectures, their feature spaces may be misaligned. The teacher and the student feature spaces, even when they encode the same semantic information, can still be subject to differences caused by transformations such as rotation and scaling. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the feature space misalignment issue. Assume the penultimate layer feature is denoted by f and the classifier's parameters are W and b, respectively. The prediction can be computed by</p><formula xml:id="formula_5">p = W T f + b .<label>(5)</label></formula><p>Given any orthogonal matrix R, we have</p><formula xml:id="formula_6">p = W T R T Rf + b (6) = W T * f * + b ,<label>(7)</label></formula><p>where W * and f * are RW and Rf , respectively. That is, the feature space can be rotated without changing the prediction.</p><p>Our linear embedding layer f c1 s can learn any linear transformation (such as the above rotation R) to align the student's feature space to that of the teacher's. If we let the student mimic the <ref type="figure">Figure 3</ref>. An illustration of the LSH loss. fs and ft denote a student and a teacher feature vector for the same input image, respectively. h represents the hash function constraints. These constraints form a small polyhedron (the shaded region) and ? is the maximum angle between any two vectors in this polyhedron. Magnitudes of these features, however, can alter with greater freedom.</p><formula xml:id="formula_7">? ? ? ? ? ? ?</formula><p>teacher's features directly without aligning their feature spaces, the performance will be lower, especially when the student has been pretrained. Experimental validation of the importance of feature space alignment can be found in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The LSH module</head><p>To mimic the teacher's feature, L mse and L lsh are used in our framework. L mse is defined as</p><formula xml:id="formula_8">L mse = 1 nD n i=1 f t (x i ) ? f s (x i ) 2 2 ,<label>(8)</label></formula><p>where f t (x i ) and f s (x i ) represent the teacher and student features for the i-th image in the training set, and D denotes the dimensionality of the feature (after the linear embedding f c1 s ). Note that L mse addresses both feature direction and magnitude. On the contrary, we propose to use locality-sensitive hashing (LSH) <ref type="bibr" target="#b13">[14]</ref> to give the student more freedom with regard to its magnitude, but let the student concentrate more on mimicking the feature direction. <ref type="figure">Figure 3</ref> shows an illustration for L lsh . In our LSH module, each hash function can be considered as a linear constraint. Many constraints will divide the feature space into a lot of polyhedra, and in general each polyhedron will be small. The LSH loss will encourage f s and f t to fall into the same polyhedron. Hence, more hash functions will result in smaller polyhedra, which in turn means that the angle between f t and f s will be small (upper bounded by ? in <ref type="figure">Figure 3</ref>, which is small itself because the polyhedron is small compared to the feature magnitudes.) In short, the LSH module encourages f t and f s to have similar directions, but relaxes constraints on their magnitudes. LSH aims at hashing the points into bins by several hash functions to ensure that, for each function, near points will fall into the same bin with high probability. In our framework, we use the hash family based on the Gaussian distribution which is a 2-stable distribution, defined as</p><formula xml:id="formula_9">h w,b (f ) = w T f + b r ,<label>(9)</label></formula><p>where f ? R D is the feature, w ? R D is a random vector whose entries are sampled from a Guassian distribution, b is a real number chosen uniformly from the range [0, r], r is the length of each bin, and ? is the floor function. Our loss term L lsh encourages the student feature to fall into the same bin as that of the teacher's. According to the theory of locality-sensitive hashing, for two vectors f 1 , f 2 , the probability of collision decreases monotonically with the distance between f 1 and f 2 . Therefore,</p><formula xml:id="formula_10">h W ,b (f t ) = h W ,b (f s ) (which will result in a low value of L lsh ) is a necessary condition for f t ? f s 2 = 0.</formula><p>Hence, it is reasonable to force the student to mimic the teacher by minimizing L lsh .</p><p>In our framework, we use N hash functions with the form in <ref type="figure">Equation 9</ref>. The locality-sensitive hashing module will generate N hash codes for each feature. 0 is used as the threshold to chop the real line. Therefore, the LSH module can be implemented by a FC layer and a signum function:</p><formula xml:id="formula_11">h W ,b (f ) = sign(W T f + b) ,<label>(10)</label></formula><formula xml:id="formula_12">sign(x) = 1, if x &gt; 0; 0, otherwise ,<label>(11)</label></formula><p>in which f ? R D is the feature, W ? R D?N is the weights whose entries are sampled from a Guassian distribution and b ? R N is the bias. Equation 10 generates N binary codes for teacher feature f t , and the hash code for student feature is expected to be the same as teacher's. We enforce this requirement by learning a classification problem, and the binary cross entropy loss is to be minimized, i.e.,</p><formula xml:id="formula_13">h = sign W T f t + b ,<label>(12)</label></formula><formula xml:id="formula_14">p = ? W T f s + b ,<label>(13)</label></formula><formula xml:id="formula_15">L lsh = ? 1 nN n i=1 N j=1 [h j log p j + (1 ? h j ) log(1 ? p j )] ,<label>(14)</label></formula><p>where ? is the sigmoid function ?(x) = 1 1+exp(?x) , h j and p j are the j-th entry of h and p, respectively.</p><p>Finally, inspired by <ref type="bibr" target="#b28">[29]</ref>, we only distill the features which the teacher classifies correctly. To reduce the effect of randomness in the locality-sensitive hashing module, the average of the last 10 epochs' models during training is used as our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental analysis</head><p>We use experiments to demonstrate the advantage of giving the student more freedom to the feature magnitude and making it focus on mimicking the feature direction. <ref type="table">Table 1</ref> shows the experimental results. The models vgg13 and vgg8 share similar architectures, while ResNet50 and MobileNetV2 have different architectures. "CE" denotes training the student by only the cross entropy loss without a teacher. We find that f s 2 <ref type="table">Table 1</ref> The difference between teacher features and student features. The statistics were estimated average values on the training and testing sets of CIFAR-100. ft 2 and fs 2 denote the 2-norm of teacher and student features, respectively. ? represents the average angle between them. Acc@1 is the accuracy (%) of the student model. is very different from f t 2 . More statistics on f 2 of different models can be found in <ref type="table" target="#tab_1">Table 2</ref>. When knowledge distillation is not used, teacher and student features have very different directions as there are large angles between them, especially when their architectures are different.</p><p>When the 2 loss ( 2 + CE) is used for feature mimicking, the student features are encouraged to be similar to the teacher features in both magnitudes and angles, and the student accuracy is higher.</p><p>The proposed LSH loss gives the student more freedom to its feature magnitude. With the LSH loss (LSH + CE), vgg8 gets a larger feature magnitude while MobileNetV2 gets a smaller feature magnitude than that of CE. For vgg8, although ? of LSH + CE is a little larger than that of 2 + CE, the accuracy of LSH + CE is higher, which shows the benefit of giving more freedom to the feature magnitude. For MobileNetV2, LSH + CE achieves both a smaller ? and better performance.</p><p>Finally, the LSH loss and the 2 loss can be combined to help each other, and result in both smaller ? (i.e., similar directions) and better accuracy rates.</p><p>In Section 4, we will analyze the LSH module theoretically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ensemble all losses</head><p>The final loss consists of two terms, the classification and the feature mimicking losses. The regular cross-entropy loss L c is used as the classification loss. We use both L mse and L lsh as the feature mimicking loss. Different from CRD <ref type="bibr" target="#b9">[10]</ref> and SSKD <ref type="bibr" target="#b10">[11]</ref>, our method does not need the knowledge distillation loss <ref type="bibr" target="#b2">[3]</ref> (KLdivergence between teacher and student logits with temperature). The final loss is</p><formula xml:id="formula_16">L = L c + ?(L mse + L lsh ) ,<label>(15)</label></formula><p>where ? is the balancing weight. Therefore, if the mean square loss is already used in other researches (e.g., detection, segmentation), our LSH module can be added directly without introducing extra hyperparameter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model initialization</head><p>The LSH module needs to be initialized before the end-to-end training. In the LSH module, the entries of W are sampled from a Guassian distribution. We always set 0 as its mean and treat the standard deviation (std hash ) as a hyperparameter. To find a good default value for std hash , we collect statistics about the standard deviation (std) of the final classifier's weight (W ) with vanilla training (cf. <ref type="table" target="#tab_1">Table 2</ref>)</p><formula xml:id="formula_17">. Assume W = [w 1 , w 2 , ? ? ? , w c ] T ,</formula><p>where w i ? R D and c is the number of categories, the expectation of w 2 can be roughly calculated by</p><formula xml:id="formula_18">E( w 2 ) = E( w T w ) ? std ? ? D ,<label>(16)</label></formula><p>where the last transition holds because we noticed that the mean of W is roughly zero. There is a tendency that E( w 2 ) does not change drastically, and std will become small when D is large. These phenomena inspire us to choose std hash according to D.</p><p>We also find that directly using the std of teacher's final classifier's weight is a good default value for std hash . b is the bias in the LSH module. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the bias can be initialized by 0, the median, or the mean of the teacher hash values. Because BCE loss is applied, to make the binary classification problem balanced, we use the median of the teacher hash values as the bias in our LSH module. We also tried to use the mean of the teacher hash values or simply set b = 0. Later we will exhibit in <ref type="table" target="#tab_6">Table 7</ref> and <ref type="table">Table 8</ref> the experimental results when using different initialization for the bias. These results show that our method is not sensitive to the initialization of b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL ANALYSES</head><p>Now we will analyze why the LSH loss is sensitive to the feature's direction but not to the feature's magnitude. First, the following Claim 1 says that if the teacher features are scaled, L lsh will not change, i.e., L lsh is not sensitive to the teacher feature's magnitude.</p><formula xml:id="formula_19">Claim 1. For a given scale s &gt; 0, L lsh (sf t , f s ) = L lsh (f t , f s )</formula><p>for arbitrary f s .</p><p>Next, the following Claim 2 states that when f s and f t have the same direction, L lsh will encourage f s to be longer. Claim 2. Assume the direction of f s is the same as that of f t , and b = 0 in LSH. For a given scale s &gt; 1, then</p><formula xml:id="formula_20">L lsh (f t , sf s ) ? L lsh (f t , f s ) always holds.</formula><p>Finally, the following Claim 3 and Claim 4 are the most important conclusions, which explain why our LSH loss can help the student to mimic the direction of teacher features. Claim 3 computes the probability of the LSH loss being small (less than log 2) when we are given ? (f t , f s ), the angle between teacher  <ref type="figure">Figure 5</ref>. The cumulative probability of the angle with D = 2048, where D and N denote the feature dimensionality and the number of hash functions, respectively. With N becoming larger, the angle between the student feature and the teacher feature will become smaller with high probability. This figure is best viewed in color and zoomed in. and student features. Hence, if the angle between f s and f t is smaller, the LSH loss will become small with higher probability.</p><p>Claim 4 gives the probability of ? (f t , f s ) &lt; under the constraint that L lsh is small. Using the probability formula in Claim 4, we can numerically calculate the cumulative probability of the angle when L lsh meets the condition (cf. <ref type="figure">Figure 5</ref>). From this figure, we can conclude that if more hashing functions are used, the direction of f s will approach that of f t with higher probability. Claim 3. Suppose b = 0 in LSH, and f s and f t follow the standard Gaussian distribution. Then,</p><formula xml:id="formula_21">Pr {l j &lt; log 2 | ? (f t , f s ) = ?} = 1 ? ? ?<label>(17)</label></formula><p>will hold, where ? (f t , f s ) denotes the angle between f t and f s , and</p><formula xml:id="formula_22">l j . = ?h j log (p j ) ? (1 ? h j ) log (1 ? p j ) .<label>(18)</label></formula><p>Claim 4. Suppose b = 0 in LSH, and f s and f t follow the standard Gaussian distribution. Then, for any 0 &lt; &lt; ?, the equation</p><formula xml:id="formula_23">Pr ? ? ? ? (f t , f s ) &lt; | N j=1 (l j &lt; log 2) ? ? ? = 0 1 ? ? ? N ? sin D?2 (?) d? ? 0 1 ? ? ? N ? sin D?2 (?) d?<label>(19)</label></formula><p>will hold.</p><p>The proof of Claim 3 and Claim 4 are provided in the appendix to this paper. Utilizing these results, we numerically calculate the probability in Claim 4 for different N values. As <ref type="figure">Figure 5</ref> shows, when the number of hash function N grows, the angle between teacher and student features indeed converge to 0. That is, our LSH loss is effective in mimicking the teacher feature's direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate the proposed feature mimicking framework on single-label classification, multi-label recognition, and object detection. For single-label classification, we use the CIFAR-100 <ref type="bibr" target="#b29">[30]</ref> and ImageNet <ref type="bibr" target="#b30">[31]</ref> datasets, which are usually used For CIFAR-100, we used the code provided by CRD <ref type="bibr" target="#b9">[10]</ref>. <ref type="bibr" target="#b0">1</ref> For a fair comparison, we used the same hyperparameters of CRD in our experiments, such as the learning rate, batch size and epoch. For ImageNet, we followed the standard PyTorch example code and trained 100 epochs (following CRD). 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation studies</head><p>We first study the effects of the loss functions, hyperparameters in LSH, and model initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">The loss functions</head><p>First, we conduct ablation studies on the loss functions. Our final loss contains L mse and L lsh . We will use only one of them to see their individual effects. <ref type="table">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref> summarize the results. We used "KD" <ref type="bibr" target="#b2">[3]</ref> as the baseline method. Note that all experiments used the classification loss L c . " 2 loss" denotes only using L mse , while "LSH loss" represent only using L lsh . " 2 loss + LSH loss" combines the L mse and L lsh as in Equation <ref type="bibr" target="#b14">15</ref>. To balance the feature mimicking loss and classification loss, ? was set as 6. In these tables, we also show the relative improvement as a percentage. Accuracy of the student and the teacher are treated as 0% and 100%, respectively. For example, in the last column of <ref type="table" target="#tab_3">Table 4</ref>, the student and teacher accuracy are 70.50 and 75.61, while the proposed " 2 loss + LSH loss" is 76.25, hence the relative improvement is When the teacher and student share similar architectures, only using the 2 loss can surpass the standard KD significantly, which 1. https://github.com/HobbitLong/RepDistiller 2. https://github.com/pytorch/examples/tree/master/imagenet demonstrates the advantage of feature mimicking for knowledge distillation. When only applying the LSH loss we proposed, the performance of most teacher/student combinations are better than that of the 2 loss, showing the benefit of giving the student more freedom to the feature magnitude and letting it focus on mimicking the feature direction. Combining 2 and LSH losses can boost the performance. We believe it is because the LSH loss can alleviate the shortcomings of the 2 loss, and the LSH loss can also benefit from the 2 loss.</p><p>When the teacher and student use different architectures, the difference of their accuracy is larger than that in the similararchitecture settings, and their features are more different. Due to the limited capacity of student networks, it is difficult for the student to mimic both features' directions and magnitudes. The experimental results in <ref type="table" target="#tab_3">Table 4</ref> show that only using L lsh outperforms using L mse in most cases, which justifies that feature directions have more effective information to boost the student performance, and that we should make the student pay more attention to the feature direction. Combining 2 and LSH losses is consistently better than only applying the 2 loss. It demonstrates that feature mimicking indeed benefits from giving more freedom to the student feature's magnitude.</p><p>Furthermore, by comparing the relative improvement numbers in <ref type="table">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref>, it is obvious that knowledge distillation across different network architectures is a more challenging task than distilling between similar-architecture networks. Hence, it is not surprising that differences among the 2 loss, the proposed LSH loss, and the " 2 loss + LSH loss" are relatively small in <ref type="table">Table 3</ref>. On the other hand, <ref type="table" target="#tab_3">Table 4</ref> confirms that the proposed LSH loss is supervisor to the 2 loss, which also shows that the combination of these two are complementary in feature mimicking. For example, when we distill knowledge from ResNet50 to MobileNetV2, the combined relative improvement (30%) is even higher than the sum of both (8% + 16%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Hyperparameters in the LSH module</head><p>Next, we study the effect of hyperparameters in the LSH loss. There are three hyperparameters in locality-sensitive hashing. N denotes the number of hashing functions. std hash represents the standard deviation of the Gaussian sampler. Note that we always use 0 as the mean of the Gaussian sampler. ? is the balancing weight for both L lsh and L mse . <ref type="table">Table 5</ref> and <ref type="table" target="#tab_5">Table 6</ref> summarize the results. First, when std hash = 1 and N = 2048, different teacher/student combinations achieve the best results with different ?. So it is better to use a validation set to tune this hyperparameter. Limited by computation resources, we simply used ? = 6 for all experiments on CIFAR-100. Second, the value of std hash also affect the performance. But we find that it is less sensitive than ?. Third, a larger N may reduce the randomness in LSH. Experiments show that setting N = 2048 is good enough. Overall, if applying our method to other problems, we suggest that N = 2048 or N = 4D t , std hash = 1 or std hash = std t , and finally using a validation set to tune ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Different model initialization</head><p>We study different initialization for bias in the LSH module. By default, the bias is initialized as the median of teacher hashing codes to balance the binary classification problem. We also tried to use the mean of teacher hashing codes or 0 to initialize the bias. <ref type="table" target="#tab_6">Table 7</ref> and <ref type="table">Table 8</ref> present the results. We find that knowledge <ref type="table">Table 3</ref> Test accuracy (%) of the student network on CIFAR-100. The teacher and the student share similar architectures.   <ref type="table">Table 5</ref> Test accuracy (%) of the student network on CIFAR-100 using different hyperparameters (?, std hash , N ). The teacher and the student share similar architectures.   distillation is not sensitive to the initialization of bias. When apply our method on large-scale datasets (like ImageNet), we used 0 to initialize the bias because it is difficult to compute the median. <ref type="table">Table 9</ref> and <ref type="table">Table 10</ref> compare our method with other knowledge distillation approaches on the CIFAR-100 benchmark. We simply set ? = 6, std hash = 1 and N = 2048 for all experiments. And for a fair comparison, we used the same teacher networks as CRD <ref type="bibr" target="#b9">[10]</ref>. Different from SSKD <ref type="bibr" target="#b10">[11]</ref>, we only used self-supervised learning <ref type="bibr" target="#b4">[5]</ref> to train student networks and got the backbone weights to initialize our framework. <ref type="table">Table 9</ref> presents the results when the teacher and student share similar architecture. Note that "Ours (1FC)" removed the linear embedding layer, which is possible because teacher and student features have the same dimensionality. Our method surpasses CRD+KD <ref type="bibr" target="#b9">[10]</ref> on most teacher/student combinations. Note that our method did not use the original KD <ref type="bibr" target="#b2">[3]</ref> loss, and is thus more flexible. Compared with SSKD <ref type="bibr" target="#b10">[11]</ref>, our method outperform on five teacher/student combinations. And our method can be combined with SSKD ("Ours + SSKD"), which consistently outperforms SSKD. We simply set ? = 0.01, std hash = 1, N = 2048 and added our loss terms into the SSKD framework. <ref type="table">Table 10</ref> summarizes the results when the architectures of teacher and student are different. Our method outperformed CRD+KD <ref type="bibr" target="#b9">[10]</ref> on the majority of teacher/student combinations, but slightly worse than SSKD <ref type="bibr" target="#b10">[11]</ref>. These results suggest that with different teacher/student architectures, self-supervised learning is critical for KD (because SSKD outperformed other methods). However, note that our method can be combined with SSKD, which consistently outperforms SSKD. Same as that on similar architecture, we simply set ? = 0.01, std hash = 1, N = 2048 and added our loss terms into the SSKD framework. <ref type="table" target="#tab_9">Table 11</ref> summarize the results on ImageNet. The hyperparameters in our method are ? = 5, std hash = std t and N = 2048. Note that different from CRD + KD and SSKD, we did not use the standard KD loss <ref type="bibr" target="#b2">[3]</ref> to boost the performance. Only using the 2 loss to force the student features to mimic the teacher features outperforms CRD, which once again supports the validity of our proposed feature mimicking. Combining the 2 and LSH losses further boosts the performance by a significant margin and achieves the state-of-the-art performance, which further supports the proposed LSH loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Single-label Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multi-label classification</head><p>We consider two typical multi-label classification tasks, i.e., VOC2007 <ref type="bibr" target="#b33">[34]</ref> and MS-COCO <ref type="bibr" target="#b34">[35]</ref>. VOC007 contains a train-val set of 5011 images and a test set of 4952 images. And MS-COCO contains 82081 images in the training set and 40137 images for validation. We resize all images into a fixed size (448 ? 448) to train the networks. And the data augmentation consist of random horizontal flips and color jittering. The backbone networks contain MobileNetV2, ResNet18, ResNet34, ResNet50 and ResNet101. The networks are all pre-trained on ImageNet and finetuned on the multi-label classification dataset with stochastic gradient descent (SGD) for 60 epochs in total. The binary cross entropy (BCE) loss is used to finetune the network. We employ the mean average precision (mAP) to evaluate all the methods. Note that multi-label recognition is not a typical application of KD because existing KD methods rely on the soft logits, which do not exist in multi-label scenarios. The proposed feature mimicking method, however, is flexible and handles multi-label distillation well.</p><p>First, we conduct experiments on VOC2007 using ResNet34 as teacher and ResNet18 as student to demonstrate that feature space alignment is necessary and important. The teacher ResNet34 is first trained on ImageNet and then finetuned on VOC2007. It achieves 91.69% mAP as in <ref type="table" target="#tab_1">Table 12</ref>. The student ResNet18 achieves 89.15% mAP. And in Section 5.2, we have trained ResNet18 supervised by ResNet34 on ImageNet. This model is denoted as "ResNet18 (pretrained by KD)" and achieves 89.88% mAP. When finetuned on VOC2007 supervised by the teacher with the 2 loss, ResNet18 achieves a worse performance (88.75%) than baseline, which we believe is because the feature spaces of the teacher and student do not align well. If we use ResNet18 pretrained by KD whose feature space aligns to the teacher's, the student can be improved to 90.89%. With the 2FC structure, the first linear layer can transform the student feature space to align to the teacher's. It alleviates the feature space misalignment issue and achieve a better <ref type="table">Table 9</ref> Test accuracy (%) of the student network on CIFAR-100. The teacher and the student share similar architectures. We denote by * methods where we re-run three times using author-provided code. And the results of our method were run by five times. Bold denotes the best results.  <ref type="table">Table 10</ref> Test accuracy (%) of the student network on CIFAR-100. The architectures of teacher and student are different. We denote by * methods where we re-run three times using author-provided code. And the results of our method were run by five times. Bold denotes the best results.   1FC. That demonstrates it does not need the first linear layer to transform the feature space. Although the backbone pretrained by KD on a large scale dataset will transfer better and easily mimic the teacher's features  during finetuning, it is expensive to pretrain the student on a large scale dataset in many cases. Hence, we propose a simple but effective approach to alleviate the feature space misalignment problem. We finetune the student by two stages. In the first stage, we fix the weights in the student backbone and only optimize the linear embedding layer with the feature mimicking loss functions. This stage aims at transform the student feature space to align to the teacher's. In the second stage, we add the classifier on top of the linear embedding layer and optimize all parameters in the student with the supervision of both the groundtruth labels and the teacher. <ref type="table" target="#tab_10">Table 13</ref> summarize the results. With this two-stage training, the student can be improved by a large margin, compared with 89.98% mAP when training the student by one stage. We find that the feature mimicking loss chosen in the first stage is important, and the LSHL2 (L mse + L lsh ) loss is consistently better than the 2 loss.</p><p>We conduct experiments on VOC2007 and MS-COCO and adopt two settings, i.e., using ResNet101 to teach ResNet50 and MobileNetV2, respectively. The student is finetuned with the twostage strategy, and the LSHL2 loss is used in the first stage based on the above findings. <ref type="table" target="#tab_3">Table 14</ref> presents the results on VOC2007. LSHL2 ? L2 denotes using the LSHL2 loss in the first stage and the 2 loss in the second stage. The hyperparameters are set as ? = 0.5, std hash = std t , and N = 4D t in all experiments. LSHL2 ? L2 achieves the best performance. ResNet50 is improved by 0.41% and MobileNetV2 is improved by 0.61%. Experimental results of MS-COCO are showed in <ref type="table" target="#tab_12">Table 15</ref>. And we use ? = 3, std hash = std t , and N = 4D t in all experiments. LSHL2 ? LSHL2 achieves the best performance.</p><p>A common trick in the multi-label classification task is replacing the global average pooling (GAP) with the global maximum pooling (GMP). So we evaluate the backbone network with GMP on the MS-COCO. <ref type="table" target="#tab_5">Table 16</ref> presents the results. As previously mentioned, we use ResNet101 (GMP) to teach MobileNetV2 (GMP) and ResNet50 (GMP). In addition, we also evaluate the 75.0 (3% ?) PAD-ROI-mimic <ref type="bibr" target="#b20">[21]</ref> 82.5 (31% ?) 75.8 (18% ?) Fine-grained <ref type="bibr" target="#b19">[20]</ref> 82.0 (0% ?) 74.6 (13% ?) PAD-Fine-grained <ref type="bibr" target="#b20">[21]</ref> 82.3 (19% ?) 75.2 (3% ?) Ours (L2) 83.0 (63% ?) 76.9 (46% ?)</p><formula xml:id="formula_24">Ours (LSHL2) 83.1 (69% ?) 77.2 (54% ?)</formula><p>performance of self-distillation, i.e., using ResNet101 (GMP) to teach ResNet101 (GMP). Our method achieves better performances than baselines. We compared our method with MCAR <ref type="bibr" target="#b35">[36]</ref>, which employs a complex training pipeline designed for multi-label classification and is the state-of-the-art method on multi-label classification. Our MobileNetV2 surprisingly surpassed that in MCAR, which demonstrates the advantage of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Detection</head><p>We evaluate our method on the object detection task. Following previous work <ref type="bibr" target="#b20">[21]</ref>, we conduct experiments on the Pascal VOC dataset <ref type="bibr" target="#b33">[34]</ref>. The training set consists of the VOC2007 trainval set and the VOC2012 trainval set, and in total 21K images. The testing set is the VOC2007 test set of 5K images. We use mAP@0.5 as the metric to compare the performance of different methods. The detection frameworks we adopted are both two-stage (Faster-RCNN <ref type="bibr" target="#b36">[37]</ref>) and one-stage (RetinaNet <ref type="bibr" target="#b37">[38]</ref>). And we use four networks (ResNet50, ResNet101, VGG11, VGG16) pretrained on ImageNet as the backbone. FPN <ref type="bibr" target="#b38">[39]</ref> layers are adopted in all experiments. All models are finetuned on VOC with 24 epochs. The hyperparameters in feature mimicking loss are set as ? = 7, std hash = std t , N = 4D t and bias = 0 in all experiments. We have released our code. 3 <ref type="figure" target="#fig_5">Figure 6a</ref> shows our feature mimicking framework with Faster-RCNN. As in classification, we want the student to mimic features in the penultimate layer. In the object detection framework, two linear layers are applied on the penultimate layer to generate the classification and bounding box predictions, respectively. Given one image, the backbone and FPN produce the feature pyramid, and the region proposal network (RPN) generates proposals to indicate the localities that objects may appear. Hence, many features are extracted according to the proposals. To make sure the student will mimic teacher's features in the same locations, the teacher   uses the proposals produced by the student. When training this framework, we only add the proposed loss to the original loss and apply the traditional training strategy. Our proposed loss is applied on the entire detection network, and it affects the optimization of the backbone network, FPN, RPN and MLP. The experimental results are presented in <ref type="table" target="#tab_6">Table 17</ref>. First, we use ResNet101 to teach ResNet50. The performances of these two baseline networks are 83.6% and 82.0%, respectively. The teacher is higher than student by 1.6%. All experimental results of ROI-mimic, PAD-ROI-mimic, Fine-grained and PAD-Fine-grained are cited from PAD <ref type="bibr" target="#b20">[21]</ref>. They improve the student by at most 0.5%. With our feature mimicking framework, i.e., mimicking the features in the penultimate layer, simply using the 2 loss as the feature mimicking loss can improve the student by 1%. That shows the benefit of feature mimicking on object detection. Combining the LSH loss and the 2 loss, the student is improved by 1.1%. When using VGG16 to teach VGG11, the 2 loss can improve the student by 1.8%. With the LSH loss, the student is improved by 2.1%. <ref type="figure" target="#fig_5">Figure 6b</ref> shows our feature mimicking framework with RetinaNet. Different from Faster-RCNN, RetinaNet produces features on all positions of the feature pyramid, and each position will consider several anchors. With the groundtruth bounding boxes, only a few of positions are considered as positive and sent to the classification loss. We force the student to mimic the features on these positive positions and ignore the features on negative positions. RetinaNet uses class subnet and box subnet to generate class feature and box feature, respectively. We find that it is better to only mimic the class feature and ignore the box feature. So our proposed feature mimicking loss affects the optimization of the backbone network, FPN and the class subnet. <ref type="table" target="#tab_15">Table 18</ref> shows the experimental results. Similar to Faster-RCNN, our feature mimicking framework can improve the student with a large margin. ResNet50 is improved by 0.5% whose performance is comparable to the teacher performance. And VGG11 is also improved by 2%.</p><p>Overall, these object detection experimental results demonstrate the advantages of our method. The LSHL2 loss is consistently better than the 2 loss in all experiments. Note that the difference between the teacher and the student is smaller when compared to the differences in recognition tasks. However, the high relative improvement numbers and the consistent improvements across different experiments both verifies our proposed method is effective. In this paper, we only focus on mimicking the final features and leave mimicking proposals as the future work. However, only using feature mimicking has already improved the student by a large margin, and the RetinaNet with ResNet50 backbone is even comparable to the teacher performance.</p><p>Compared with multi-label classification, we find it does not need the two stage training strategy on object detection. We guess it may be due to the MLP layer and the subnet in Faster-RCNN and RetinaNet, respectively. These layers are randomly initialized before finetuning on the detection dataset. The feature space alignment will be learned implicitly in these layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed a flexible and effective knowledge distillation method. We argued that mimicking feature in the penultimate layer is more advantageous than distilling the teacher's soft logits <ref type="bibr" target="#b2">[3]</ref>. And to make the student learn the more effective information from the teacher, it needs to give the student more freedom to its feature magnitude, but let it focus on mimicking the feature direction. We proposed a loss term based on Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b13">[14]</ref> to fulfill this objective. Our algorithm was evaluated on single-label classification, multi-label classification and object detection. Experiments showed the effectiveness of the proposed method.</p><p>Future work could explore how to improve our method, such as reducing the randomness in the LSH module, and aligning feature spaces efficiently and even if without training data. Applying our method to other problems is also interesting. It is promising to combine our method with self-supervised learning. And we will also consider how to deploy our method to knowledge distillation under a data free setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF CLAIM 1</head><p>Proof. We will prove that</p><formula xml:id="formula_25">h W ,b (sf t ) = h W ,b (f t ), where W = [w 1 , w 2 , ? ? ? , w N ] T , b = [b 1 , b 2 , ? ? ? , b N ] T and b = [b 1 , b 2 , ? ? ? , b N ] T . Assume there are m teacher features, that is f 1 , f 2 , ? ? ? , f m . For 0 ? j ? N , b j = median(f 1 , f 2 , ? ? ? , f m ).</formula><p>And it is easy to see b j = median(sf 1 , sf 2 , ? ? ? , sf m ) = sb j when s &gt; 0. Hence, sign(w T</p><formula xml:id="formula_26">j f i + b j ) = sign(w T j sf i + sb j ) for 0 ? i ? m when s &gt; 0. That implies h W ,b (sf t ) = h W ,b (f t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PROOF OF CLAIM 2</head><p>Proof. Note that</p><formula xml:id="formula_27">L lsh (f t , f s ) = ? 1 N N j=1 [h j log p j + (1 ? h j ) log(1 ? p j )] ,<label>(20)</label></formula><p>where h j and p j is the j-th entry of sign(W T f t ) and ?(W T f s ), respectively.</p><p>First, we discuss the situation when h j = 1, that is, the angle between w j and f t is less than 90 degrees, and cos w j , f t ? 0. Therefore,</p><formula xml:id="formula_28">? h j log ?(sw T j f s ) ? (1 ? h j ) log(1 ? ?(sw T j f s )) = ? log ?(s w j f s cos w j , f s ) (21) ? ? log ?( w j f s cos w j , f t ) (22) = ? h j log p j ? (1 ? h j ) log(1 ? p j ) .<label>(23)</label></formula><p>Then, when h j = 0, similar to equation 21, we can get</p><formula xml:id="formula_29">? h j log ?(sw T j f s ) ? (1 ? h j ) log(1 ? ?(sw T j f s )) = ? log 1 ? ?(sw T j f s )<label>(24)</label></formula><formula xml:id="formula_30">? ? log (1 ? ?( w j f s cos w j , f t )) (25) = ? h j log p j ? (1 ? h j ) log(1 ? p j ) .<label>(26)</label></formula><p>To sum up, L lsh (f t , sf s ) ? L lsh (f t , f s ) always holds when s &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C PROOF OF CLAIM 3 AND CLAIM 4</head><p>We define the notations and terminologies first. We assume that f s and f t follow the standard normal distribution:</p><formula xml:id="formula_31">? f t ? R D : f t ? N (0, I D ) ? f s ? R D : f t ? N (0, I D )</formula><p>In our LSH module, W ? R D?N can be alternatively written as [w 1 , w 2 , ? ? ? , w N ] T and entries of W are sampled from a Guassian distribution:</p><formula xml:id="formula_32">? w j ? R D : w j ? N (0, I D )</formula><p>A few derived variables are:</p><formula xml:id="formula_33">? h ? R N : h j . = sign w T j f t ? p ? R N : p j . = ? w T j f s ? l ? R N : l j . = ?h j log (p j ) ? (1 ? h j ) log (1 ? p j )</formula><p>We also use a few shorthand notations:</p><formula xml:id="formula_34">? unit (x) . = x x 2 if x 2 &gt; 0 , 0</formula><p>otherwise . </p><p>= ?(n/2) 2? n/2</p><formula xml:id="formula_36">1 A n?1 S n?1 f (u) d? n?1 (u) .<label>(33)</label></formula><p>Corollary 5.1. Let x ? R n (n ? N + ) be a random vector with each element x i ? N (0, 1) independently. Then for any Borel set B in S n?1 ,</p><p>Pr {unit (x) ? B} = ? n?1 (B) A n?1 .</p><p>Proof. For an open set O in S n?1 (rename it to make things clear), define</p><formula xml:id="formula_38">f O (u) = 0, for u = 0; ? O , for u ? S n?1 ,<label>(35)</label></formula><p>where ? O is the characteristic function of O and f (k) O (u) = 0 if u = 0; max (0, 1 ? k ? inf v?O d geo (u, v)) if u ? S n?1 <ref type="bibr" target="#b35">(36)</ref> for k ? N * . Then the conditions of Lebesgue's dominated convergence theorem are met, i.e., .</p><formula xml:id="formula_39">p{? (f t , f s ) = ?} = A D?2 A D?1 sin D?2 (?)<label>(46)</label></formula><p>for ? ? (0, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>Pr </p><p>Then</p><formula xml:id="formula_41">Pr {? (f t , f s ) ? ?} = R D (Pr {? (f t , f s ) ? ? | f t } ? p(f t )) d? D (f t ) (51) = R D A D?2 A D?1 ? 0 sin D?2 (?) d? ? p(f t ) d? D (f t ) (52) = R D (p(f t )) d? D (f t ) A D?2 A D?1 ? 0 sin D?2 (?) d? (53) = A D?2 A D?1 ? 0 sin D?2 (?) d? .<label>(54)</label></formula><p>Thus</p><formula xml:id="formula_42">p{? (f t , f s ) = ?} = d d? Pr {? (f t , f s ) ? ?} (55) = d d? A D?2 A D?1 ? 0 sin D?2 (?) d? (56) = A D?2 A D?1 d d? ? 0 sin D?2 (?) d? (57) = A D?2 A D?1 sin D?2 (?) .<label>(58)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Claim 3</head><p>Proof. First, let us inspect the properties of the l j 's, which can be rewritten as</p><formula xml:id="formula_43">l j = ? log (p j ) if h j = 1 , ? log (1 ? p j ) if h j = 0 .<label>(59)</label></formula><p>Note that log 2 = ? log 1 ? 1 2 . Thus l j &lt; log 2 if and only if</p><formula xml:id="formula_44">p[j] &gt; 1 2 if h[j] = 1 , &lt; 1 2 if h[j] = 0 ,<label>(60)</label></formula><p>which is equivalent to</p><formula xml:id="formula_45">w T j f s &gt; 0 if w T j f t &gt; 0 , &lt; 0 if w T j f t ? 0 .<label>(61)</label></formula><p>In other words, unit (w j ) ? L ft,fs ,</p><p>where</p><formula xml:id="formula_47">L ft,fs = x ? S D?1 | x T f s &gt; 0 ? x T f t &gt; 0 ? x ? S D?1 | x T f s &lt; 0 ? x T f t ? 0 (63)</formula><p>is the union of two lunes. </p><formula xml:id="formula_48">= ? ? ? (f t , f s ) ?A D?1 ? 2 0 A 1 cos (?) ? A D?3 sin D?3 (?) d? (66) = ? ? ? (f t , f s ) ?A D?1 A D?1 (67) =1 ? ? (f t , f s ) ? .<label>(68)</label></formula><p>Thus</p><formula xml:id="formula_49">Pr {unit (w j ) ? L ft,fs | ? (f t , f s ) = ?} = R D?D (Pr {unit (w j ) ? L ft,fs | f t , f s }? p(f t , f s | ? (f t , f s ) = ?))d? D?D (f t , f s ) (69) = R D?D 1 ? ? ? ? p(f t , f s | ? (f t , f s ) = ?)d? D?D (f t , f s ) (70) = 1 ? ? ? R D?D p(f t , f s | ? (f t , f s ) = ?) d? D?D (f t , f s ) (71) =1 ? ? ? .</formula><p>(72)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Claim 4</head><p>Proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pr</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This figure is best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An illustration of the feature space misalignment issue. The points denote the features, and different colors with different shapes represent different classes. The student's feature space needs to rotate to align to the teacher's. (This figure is best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of different initialization for the bias. Red lines denote the hash function constraints, while blue points represent teacher features. (a), (b), and (c) show the bias initialized by 0, the median, or the mean of the teacher hash values, respectively. This figure is best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>76.25?70.50 75.61?70.50 = 113%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The pipeline of our method on the object detection task. (a) and (b) show our feature mimicking framework with Faster-RCNN and RetinaNet, respectively. This figure is best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?? 1 ?Lemma 5 .f 1 feS n? 1 f</head><label>1511</label><figDesc>(x, y) . = arccos (unit (x) T unit (y)) ? S n?1 . = {x ? R n | x 2 = 1}: the n-dimensional unit hypersphere ? d geo : the geodesic distance, with which S n?1 forms a legitimate metric space ? ? n : the Lebesgue measure on R n ? ? n?1 : the surface area measure on S n?A n?1 . = S n?1 d? n?1 = 2? n/2 ?(n/2) : the surface area of S n?1 ? I: the indicator function ? p(x): the p.d.f. of x Let x ? R n (n ? N + ) be a random vector with each element x i ? N (0, 1) independently. Then for any functionf : S n?1 ? {0} ? R satisfying ? f is bounded, ? f is continuous on S n?1 , there holds E x [f (unit (x))] = 1 A n?1 S n?1 f (u) d? n?1 (u) . (27)Proof.E x [f (unit (x))] = R n f (unit (x)) ? p(x) d? n (x)+ {0}f (unit (x)) ? p(x) d? n (x) (unit (x)) ? p(x) d? n (x(u) d? n?1 (u) ? e ? r 2 2 r n?1 dr integration by substitution (from Cartesian to polar) ?r 2 /2 r n?1 dr (u) d? n?1 (u)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>OO</head><label></label><figDesc>'s converge pointwise to f O . Thus Pr {unit (x) ? O} = E x [f O (unit (x))] (u) d? n?1 (u) (40) = 1 A n?1 S n?1 f O (u) d? n?1 (u) equationholds for any open set O, it can be shown by induction that Pr {unit (x) ? B} = ? n?1 (B) A n?1 (45) for any Borel set B. Lemma 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>A</head><label></label><figDesc>{? (f t , f s ) ? ? | f t } =Pr unit (f s ) ? {x ? S D?1 | d geo (unit (f t ), x) ? ?} | f t (47) = 1 A D?1 ? D?1 x ? S D?1 | d geo (unit (f t ), x) ? ? due to Corollary 5.1 D?2 sin D?2 (?) d?(49)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Applying Corollary 5 2 0?</head><label>52</label><figDesc>.1, we havePr {unit (w j ) ? L ft,fs | f t , f s } = ? D?1 (L ft,fs ) ? ? (f t , f s ) 2? A 1 cos (?)A D?3 sin D?3 (?)d?S D?1 viewed as a union of tori (65)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Pr {l j &lt; log 2 |</head><label>2</label><figDesc>&lt; log 2) | ? (f t , f s ) = ? ? (f t , f s ) = ?} due to conditional independence of lj 's ' rule, the conditional probability density of ? (f t , f s ) can be derived as p ? ? ? ? (f t , f s ) = ? | N j=1 (l j &lt; log 2) ? ? ? = p N j=1 (l j &lt; log 2) | ? (f t , f s ) = ? ? p{? (f t , f s ) = ?} p N j=1 (l j &lt; log 2) (75) = p N j=1 (l j &lt; log 2)| ? (f t , f s ) = ? p{? (f t , f s ) = ?} ? 0 p N j=1 (l j &lt; log 2)| ? (f t , f s ) = ? p{? (f t , f s ) = ?}d? (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>12.64 11.83 15.52 15.03 CE fs 2 16.50 16.16 16.64 16.33 ? 69.49?68.00?90.09?90.07?A cc@1 99.19 70.72 90.31 64.36 2 + CE fs 2 12.53 12.06 13.82 13.59 ? 26.86?29.90?32.04?33.25?A cc@1 98.26 72.33 89.07 65.73</figDesc><table><row><cell></cell><cell>Teacher</cell><cell cols="2">vgg13</cell><cell>ResNet50</cell></row><row><cell></cell><cell>Student</cell><cell>vgg8</cell><cell></cell><cell>MobileNetV2</cell></row><row><cell></cell><cell cols="2">dataset train</cell><cell>test</cell><cell>train</cell><cell>test</cell></row><row><cell cols="4">fs 2 24.74 23.73 ft 2 LSH + CE ? 28.22?31.00?31.28?32.29?A</cell><cell>9.69</cell><cell>9.60</cell></row><row><cell></cell><cell cols="4">cc@1 97.60 72.69 84.11 67.02</cell></row><row><cell></cell><cell cols="3">fs 2 15.22 14.50</cell><cell>9.94</cell><cell>9.83</cell></row><row><cell>LSH+ 2 +CE</cell><cell>?</cell><cell>25.43?28.99?29.73?30.80?A</cell><cell></cell></row><row><cell></cell><cell cols="4">cc@1 97.72 73.68 85.76 68.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The networks used in our experiments. ResNet34 and ResNet18 were used on ImageNet, while other models were used on CIFAR-100. D denotes the dimensionality of the feature before the final classifier.std   represents the standard deviation of the final classifier's weight with vanilla training. f 2 is the mean of the 2-norm of features in the training set.</figDesc><table><row><cell>role</cell><cell>model</cell><cell>D</cell><cell>std</cell><cell>f 2</cell></row><row><cell></cell><cell>WRN-40-2</cell><cell>128</cell><cell>0.1713</cell><cell>13.64</cell></row><row><cell></cell><cell>resnet56</cell><cell>64</cell><cell>0.2415</cell><cell>18.08</cell></row><row><cell>Teacher</cell><cell>resnet110 resnet32x4</cell><cell>64 256</cell><cell>0.2262 0.1100</cell><cell>20.13 12.06</cell></row><row><cell></cell><cell>vgg13</cell><cell>512</cell><cell>0.0749</cell><cell>12.64</cell></row><row><cell></cell><cell>ResNet50</cell><cell cols="2">2048 0.0381</cell><cell>15.52</cell></row><row><cell></cell><cell>WRN-16-2</cell><cell>128</cell><cell>0.2035</cell><cell>15.07</cell></row><row><cell></cell><cell>WRN-40-1</cell><cell>64</cell><cell>0.2638</cell><cell>14.29</cell></row><row><cell></cell><cell>resnet20</cell><cell>64</cell><cell>0.2704</cell><cell>14.47</cell></row><row><cell></cell><cell>resnet32</cell><cell>64</cell><cell>0.2563</cell><cell>16.06</cell></row><row><cell>Student</cell><cell>resnet8x4</cell><cell>256</cell><cell>0.1573</cell><cell>16.92</cell></row><row><cell></cell><cell>vgg8</cell><cell>512</cell><cell>0.0980</cell><cell>16.50</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>640</cell><cell>0.0579</cell><cell>15.82</cell></row><row><cell></cell><cell>ShuffleNetV1</cell><cell>960</cell><cell>0.0642</cell><cell>17.26</cell></row><row><cell></cell><cell>ShuffleNetV2</cell><cell cols="2">1024 0.0625</cell><cell>15.70</cell></row><row><cell>Teacher</cell><cell>ResNet34</cell><cell>512</cell><cell>0.0640</cell><cell>30.75</cell></row><row><cell>Student</cell><cell>ResNet18</cell><cell>512</cell><cell>0.0695</cell><cell>29.58</cell></row></table><note>as benchmarks for knowledge distillation. CIFAR-100 contains 32 ? 32 natural images from 100 categories, which contains 50000 training images and 10000 testing images. ImageNet is a large-scale dataset with natural color images from 1000 categories. Each category typically has 1300 images for training and 50 for evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>43% ?) 70.66 (49% ?) 70.67 (31% ?) 73.08 (61% ?) 73.33 (12% ?) 72.98 (61% ?)</figDesc><table><row><cell>Teacher</cell><cell>WRN-40-2</cell><cell>WRN-40-2</cell><cell>resnet56</cell><cell>resnet110</cell><cell>resnet110</cell><cell>resnet32x4</cell><cell>vgg13</cell></row><row><cell>Student</cell><cell>WRN-16-2</cell><cell>WRN-40-1</cell><cell>resnet20</cell><cell>resnet20</cell><cell>resnet32</cell><cell>resnet8x4</cell><cell>vgg8</cell></row><row><cell>Teacher</cell><cell>75.61</cell><cell>75.61</cell><cell>72.34</cell><cell>74.31</cell><cell>74.31</cell><cell>79.42</cell><cell>74.64</cell></row><row><cell>Student</cell><cell>73.26</cell><cell>71.98</cell><cell>69.06</cell><cell>69.06</cell><cell>71.14</cell><cell>72.50</cell><cell>70.36</cell></row><row><cell cols="8">KD 73.54 (2 loss 74.92 (71% ?) 75.53 (97% ?) 74.33 (65% ?) 71.42 (72% ?) 71.30 (43% ?) 73.81 (84% ?) 74.01 (22% ?) 72.33 (46% ?)</cell></row><row><cell>LSH loss</cell><cell cols="7">75.61 (100% ?) 74.20 (61% ?) 71.51 (75% ?) 71.73 (51% ?) 73.69 (80% ?) 73.49 (14% ?) 72.69 (54% ?)</cell></row><row><cell>2 loss + LSH loss</cell><cell cols="7">75.62 (100% ?) 74.54 (71% ?) 71.65 (79% ?) 71.39 (44% ?) 73.99 (90% ?) 73.37 (13% ?) 73.68 (78% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Test accuracy (%) of the student network on CIFAR-100. The teacher and the student use different architectures. loss + LSH loss 67.16 (25% ?) 68.99 (30% ?) 74.89 (50% ?) 75.36 (54% ?) 76.70 (64% ?) 76.25 (113% ?)</figDesc><table><row><cell>Teacher</cell><cell>vgg13</cell><cell>ResNet50</cell><cell>ResNet50</cell><cell>resnet32x4</cell><cell>resnet32x4</cell><cell>WRN-40-2</cell></row><row><cell>Student</cell><cell>MobileNetV2</cell><cell>MobileNetV2</cell><cell>vgg8</cell><cell>ShuffleNetV1</cell><cell>ShuffleNetV2</cell><cell>ShuffleNetV1</cell></row><row><cell>Teacher</cell><cell>74.64</cell><cell>79.34</cell><cell>79.34</cell><cell>79.42</cell><cell>79.42</cell><cell>75.61</cell></row><row><cell>Student</cell><cell>64.60</cell><cell>64.60</cell><cell>70.36</cell><cell>70.50</cell><cell>71.82</cell><cell>70.50</cell></row><row><cell>KD</cell><cell cols="6">67.37 (28% ?) 67.35 (19% ?) 73.81 (38% ?) 74.07 (40% ?) 74.45 (35% ?) 74.83 (85% ?)</cell></row></table><note>2 loss 66.98 (24% ?) 65.73 (8% ?) 71.90 (17% ?) 74.65 (47% ?) 75.73 (51% ?) 75.37 (95% ?) LSH loss 67.48 (29% ?) 67.02 (16% ?) 74.15 (42% ?) 75.49 (56% ?) 75.56 (49% ?) 75.89 (105% ?)2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Test accuracy (%) of the student network on CIFAR-100 using different hyperparameters (?, std hash , N ). The teacher and the student use different architectures.</figDesc><table><row><cell>Teacher</cell><cell>vgg13</cell><cell>ResNet50</cell><cell>ResNet50</cell><cell>resnet32x4</cell><cell>resnet32x4</cell><cell>WRN-40-2</cell></row><row><cell>Student</cell><cell cols="2">MobileNetV2 MobileNetV2</cell><cell>vgg8</cell><cell cols="3">ShuffleNetV1 ShuffleNetV2 ShuffleNetV1</cell></row><row><cell>stdt</cell><cell>0.07</cell><cell>0.04</cell><cell>0.04</cell><cell>0.11</cell><cell>0.11</cell><cell>0.17</cell></row><row><cell>stds</cell><cell>0.06</cell><cell>0.06</cell><cell>0.10</cell><cell>0.06</cell><cell>0.06</cell><cell>0.06</cell></row><row><cell>Dt</cell><cell>512</cell><cell>2048</cell><cell>2048</cell><cell>256</cell><cell>256</cell><cell>128</cell></row><row><cell>Ds</cell><cell>640</cell><cell>640</cell><cell>512</cell><cell>960</cell><cell>1024</cell><cell>960</cell></row><row><cell>(1, 1, 2048)</cell><cell>66.82</cell><cell>65.79</cell><cell>72.12</cell><cell>75.23</cell><cell>75.42</cell><cell>74.98</cell></row><row><cell>(3, 1, 2048)</cell><cell>67.95</cell><cell>67.33</cell><cell>73.47</cell><cell>74.94</cell><cell>76.12</cell><cell>76.17</cell></row><row><cell>(5, 1, 2048)</cell><cell>68.01</cell><cell>67.60</cell><cell>74.64</cell><cell>75.38</cell><cell>75.56</cell><cell>76.06</cell></row><row><cell>(6, 1, 2048)</cell><cell>67.16</cell><cell>68.99</cell><cell>74.89</cell><cell>75.36</cell><cell>76.70</cell><cell>76.25</cell></row><row><cell>(7, 1, 2048)</cell><cell>67.88</cell><cell>69.20</cell><cell>74.43</cell><cell>75.25</cell><cell>76.70</cell><cell>76.35</cell></row><row><cell>(6, stdt, 2048)</cell><cell>68.12</cell><cell>67.57</cell><cell>72.89</cell><cell>75.22</cell><cell>76.52</cell><cell>75.63</cell></row><row><cell>(6, stds, 2048)</cell><cell>67.77</cell><cell>67.47</cell><cell>73.68</cell><cell>74.93</cell><cell>76.27</cell><cell>75.70</cell></row><row><cell>(6, stdt, 4 ? Dt)</cell><cell>68.12</cell><cell>67.95</cell><cell>72.80</cell><cell>75.02</cell><cell>76.46</cell><cell>76.36</cell></row><row><cell>(6, stdt, 32 ? Dt)</cell><cell>67.78</cell><cell>67.33</cell><cell>72.76</cell><cell>75.36</cell><cell>76.25</cell><cell>75.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Test accuracy (%) of the student network on CIFAR-100 with different initializations of bias in the LSH module. The teacher and the student share similar architectures. Bold denotes the best results. Test accuracy (%) of the student network on CIFAR-100 with different initializations of bias in the LSH module. The teacher and student use different architectures. Bold denotes the best results.</figDesc><table><row><cell cols="2">Teacher</cell><cell cols="7">WRN-40-2 WRN-40-2 resnet56 resnet110 resnet110 resnet32x4 vgg13</cell></row><row><cell>Student</cell><cell></cell><cell cols="3">WRN-16-2 WRN-40-1 resnet20</cell><cell>resnet20</cell><cell cols="2">resnet32</cell><cell>resnet8x4</cell><cell>vgg8</cell></row><row><cell>KD</cell><cell></cell><cell>74.92</cell><cell>73.54</cell><cell>70.66</cell><cell>70.67</cell><cell>73.08</cell><cell></cell><cell>73.33</cell><cell>72.98</cell></row><row><cell>0</cell><cell></cell><cell>76.04</cell><cell>74.46</cell><cell>71.16</cell><cell>71.79</cell><cell>74.18</cell><cell></cell><cell>73.70</cell><cell>73.92</cell></row><row><cell>mean</cell><cell></cell><cell>75.39</cell><cell>74.11</cell><cell>71.52</cell><cell>70.95</cell><cell>73.85</cell><cell></cell><cell>73.64</cell><cell>73.98</cell></row><row><cell>median</cell><cell></cell><cell>75.62</cell><cell>74.54</cell><cell>71.65</cell><cell>71.39</cell><cell>73.99</cell><cell></cell><cell>73.37</cell><cell>73.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Table 8</cell><cell></cell><cell></cell></row><row><cell>Teacher</cell><cell></cell><cell>vgg13</cell><cell>ResNet50</cell><cell>ResNet50</cell><cell cols="2">resnet32x4</cell><cell cols="2">resnet32x4</cell><cell>WRN-40-2</cell></row><row><cell>Student</cell><cell cols="3">MobileNetV2 MobileNetV2</cell><cell>vgg8</cell><cell cols="4">ShuffleNetV1 ShuffleNetV2 ShuffleNetV1</cell></row><row><cell>KD</cell><cell></cell><cell>67.37</cell><cell>67.35</cell><cell>73.81</cell><cell>74.07</cell><cell></cell><cell></cell><cell>74.45</cell><cell>74.83</cell></row><row><cell>0</cell><cell></cell><cell>67.14</cell><cell>68.64</cell><cell>74.25</cell><cell>75.57</cell><cell></cell><cell></cell><cell>76.71</cell><cell>75.76</cell></row><row><cell>mean</cell><cell></cell><cell>68.16</cell><cell>68.07</cell><cell>74.54</cell><cell>75.55</cell><cell></cell><cell></cell><cell>75.32</cell><cell>75.99</cell></row><row><cell>median</cell><cell></cell><cell>67.16</cell><cell>68.99</cell><cell>74.89</cell><cell>75.36</cell><cell></cell><cell></cell><cell>76.70</cell><cell>76.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 Top</head><label>11</label><figDesc>-1 and Top-5 error rates (%) on the ImageNet validation set. The teacher and student are ResNet-34 and ResNet-18, respectively. Bold denotes the best results.</figDesc><table><row><cell cols="12">Teacher Student CC [32] SP [17] Online-KD [33] KD [3] AT [15] CRD [10] CRD+KD SSKD [11] Ours ( 2 ) Ours ( 2 + LSH)</cell></row><row><cell>Top-1 26.70</cell><cell>30.25</cell><cell>30.04</cell><cell>29.38</cell><cell>29.45</cell><cell>29.34</cell><cell>29.30</cell><cell>28.83</cell><cell>28.62</cell><cell>28.38</cell><cell>28.61</cell><cell>28.28</cell></row><row><cell>Top-5 8.58</cell><cell>10.93</cell><cell>10.83</cell><cell>10.20</cell><cell>10.41</cell><cell>10.12</cell><cell>10.00</cell><cell>9.87</cell><cell>9.51</cell><cell>9.33</cell><cell>9.61</cell><cell>9.59</cell></row><row><cell></cell><cell></cell><cell>Table 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Test mAP (%) on Pascal VOC2007.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Teacher</cell><cell>ResNet34</cell><cell></cell><cell></cell><cell>ResNet34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Student</cell><cell>ResNet18</cell><cell cols="3">ResNet18 (pretrained by KD)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Teacher</cell><cell>91.69</cell><cell></cell><cell></cell><cell>91.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Student</cell><cell>89.15</cell><cell></cell><cell></cell><cell>89.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KD</cell><cell cols="2">89.26 (4% ?)</cell><cell cols="2">89.85 (2% ?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 (1FC)</cell><cell cols="2">88.75 (20% ?)</cell><cell cols="2">90.89 (56% ?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 (2FC)</cell><cell cols="2">89.98 (33% ?)</cell><cell cols="2">90.77 (49% ?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>performance (89.98%) than baseline. ResNet18 pretrained by KD with 2FC achieves a worse performance (90.77%) than that with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13</head><label>13</label><figDesc>Test mAP (%) of the student network on Pascal VOC07. Bold denotes the best results.</figDesc><table><row><cell>2nd stage</cell><cell>1st stage</cell><cell>L2</cell><cell>LSH</cell><cell>LSHL2</cell></row><row><cell>L2</cell><cell></cell><cell cols="3">90.40 (49% ?) 90.21 (42% ?) 90.59 (57% ?)</cell></row><row><cell cols="2">LSH</cell><cell cols="3">90.29 (45% ?) 90.11 (38% ?) 90.30 (45% ?)</cell></row><row><cell cols="2">LSHL2</cell><cell cols="3">90.57 (56% ?) 90.37 (48% ?) 90.59 (57% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 14</head><label>14</label><figDesc>Test mAP (%) of the student network on Pascal VOC2007. Bold denotes the best results.</figDesc><table><row><cell>Teacher</cell><cell>ResNet101</cell><cell>ResNet101</cell></row><row><cell>Student</cell><cell>ResNet50</cell><cell>MobileNetV2</cell></row><row><cell>Teacher</cell><cell>93.27</cell><cell>93.27</cell></row><row><cell>Student</cell><cell>92.76</cell><cell>89.53</cell></row><row><cell>LSHL2 ? KD</cell><cell>92.69 (14% ?)</cell><cell>89.64 (3% ?)</cell></row><row><cell>LSHL2 ? L2</cell><cell>93.17 (80% ?)</cell><cell>90.14 (16% ?)</cell></row><row><cell>LSHL2 ? LSH</cell><cell cols="2">92.40 (71% ?) 89.91 (10% ?)</cell></row><row><cell>LSHL2 ? LSHL2</cell><cell cols="2">92.85 (18% ?) 89.90 (10% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 15</head><label>15</label><figDesc>Test mAP (%) of the student network on MS-COCO. Bold denotes the best results.</figDesc><table><row><cell>Teacher</cell><cell>ResNet101</cell><cell>ResNet101</cell></row><row><cell>Student</cell><cell>ResNet50</cell><cell>MobileNetV2</cell></row><row><cell>Teacher</cell><cell>77.67</cell><cell>77.67</cell></row><row><cell>Student</cell><cell>75.54</cell><cell>71.06</cell></row><row><cell>LSHL2 ? KD</cell><cell>75.14 (19% ?)</cell><cell>71.47 (6% ?)</cell></row><row><cell>LSHL2 ? L2</cell><cell cols="2">77.04 (70% ?) 73.28 (34% ?)</cell></row><row><cell>LSHL2 ? LSH</cell><cell cols="2">76.59 (49% ?) 73.73 (40% ?)</cell></row><row><cell>LSHL2 ? LSHL2</cell><cell>77.16 (76% ?)</cell><cell>73.73 (40% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 16</head><label>16</label><figDesc>Test mAP (%) on MS-COCO. The backbone networks use the global maximum pooling (GMP) to aggregate features. Bold denotes the best results.</figDesc><table><row><cell>Model</cell><cell cols="3">MobileNetV2 ResNet50 ResNet101</cell></row><row><cell>Baseline</cell><cell>73.90</cell><cell>77.20</cell><cell>79.57</cell></row><row><cell>MCAR [36]</cell><cell>75.0</cell><cell>82.1</cell><cell>83.8</cell></row><row><cell>Ours</cell><cell>76.03</cell><cell>79.55</cell><cell>81.24</cell></row><row><cell></cell><cell></cell><cell>Table 17</cell><cell></cell></row><row><cell cols="4">Test mAP@0.5 (%) of the student network on Pascal VOC0712. The</cell></row><row><cell cols="4">detector is Faster R-CNN with different backbones. Bold denotes the</cell></row><row><cell></cell><cell></cell><cell>best results.</cell><cell></cell></row><row><cell>Teacher</cell><cell></cell><cell>ResNet101</cell><cell>VGG16</cell></row><row><cell>Student</cell><cell></cell><cell>ResNet50</cell><cell>VGG11</cell></row><row><cell>Teacher</cell><cell></cell><cell>83.6</cell><cell>79.0</cell></row><row><cell>Student</cell><cell></cell><cell>82.0</cell><cell>75.1</cell></row><row><cell cols="2">ROI-mimic [19]</cell><cell>82.3 (19% ?)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>3. https://git.nju.edu.cn/wanggh/detection.vision</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FODVV VXEQHW</cell><cell>FODVV IHDWXUH</cell></row><row><cell></cell><cell>52, SRRO</cell><cell>0/3</cell><cell>)HDWXUH</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ER[ VXEQHW</cell><cell>ER[ IHDWXUH</cell></row><row><cell>WHDFKHU</cell><cell></cell><cell></cell><cell>WHDFKHU</cell><cell>.' ORVV</cell></row><row><cell>531</cell><cell>3URSRVDOV</cell><cell></cell><cell>.' ORVV</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FODVV</cell><cell>FODVV VXEQHW</cell><cell>FODVV IHDWXUH</cell><cell>FODVV</cell></row><row><cell></cell><cell>52, SRRO</cell><cell>0/3</cell><cell>)HDWXUH</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EER[</cell><cell>ER[ VXEQHW</cell><cell>ER[ IHDWXUH</cell><cell>EER[</cell></row><row><cell>VWXGHQW</cell><cell></cell><cell></cell><cell>VWXGHQW</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 18</head><label>18</label><figDesc>Test mAP@0.5 (%) of the student network on Pascal VOC0712. The detector is RetinaNet with different backbones. Bold denotes the best results.</figDesc><table><row><cell>Teacher</cell><cell>ResNet101</cell><cell>VGG16</cell></row><row><cell>Student</cell><cell>ResNet50</cell><cell>VGG11</cell></row><row><cell>Teacher</cell><cell>83.0</cell><cell>76.6</cell></row><row><cell>Student</cell><cell>82.5</cell><cell>73.2</cell></row><row><cell>Fine-grained [20]</cell><cell cols="2">81.5 (200% ?) 72.0 (35% ?)</cell></row><row><cell>PAD-Fine-grained [21]</cell><cell>81.9 (120% ?)</cell><cell>73.2 (0% ?)</cell></row><row><cell>Ours (L2)</cell><cell>82.6 (20% ?)</cell><cell>74.8 (47% ?)</cell></row><row><cell>Ours (LSHL2)</cell><cell>83.0 (100% ?)</cell><cell>75.2 (59% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Yifan Ge received his BS degree in the Kuang Yaming Honors School from Nanjing University in 2019. He is currently a graduate student in the School of Artificial Intelligence at Nanjing University, China. His research interests include computer vision and machine learning. Wu received his BS and MS degrees from Nanjing University, and his PhD degree from the Georgia Institute of Technology, all in computer science. He is currently a professor in the Department of Computer Science and Technology and the School of Artificial Intelligence at Nanjing University, China, and is associated with the State Key Laboratory for Novel Software Technology, China. He has served as an (senior) area chair for CVPR, ICCV, ECCV, AAAI and IJCAI, and as an associate editor for the IEEE Transactions on Pattern Analysis and Machine Intelligence. His research interests are computer vision and machine learning.</figDesc><table><row><cell>Jianxin</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Guo-Hua Wang received his BS degree in the School of Management and Engineering from Nanjing University. He is currently a Ph.D. student in the Department of Computer Science and Technology in Nanjing University, China. His research interests are computer vision and machine learning.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), ser. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FitNets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge distillation meets selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), ser. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="588" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature normalized knowledge distillation for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), ser. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12370</biblScope>
			<biblScope unit="page" from="664" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NormFace: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM Symposium on Computational Geometry</title>
		<meeting>the 20th ACM Symposium on Computational Geometry</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3779" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7130" to="7138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7341" to="7349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distilling object detectors with fine-grained feature imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4928" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Primeaware adaptive distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), ser. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12364</biblScope>
			<biblScope unit="page" from="658" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 25th International Conference on Very Large Data Bases (VLDB)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual ACM symposium on Theory of computing</title>
		<meeting>the 30th annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HashNet: Deep learning to hash by continuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5608" to="5617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Cauchy hashing for Hamming space retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1229" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep supervised hashing for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2064" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2156" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few sample knowledge distillation for efficient network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Preparing lessons: Improve knowledge distillation with better supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07471</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5007" to="5016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7517" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), ser. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-label image recognition with multi-class attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01755</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
