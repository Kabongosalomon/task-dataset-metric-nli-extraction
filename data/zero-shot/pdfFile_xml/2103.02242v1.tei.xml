<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present FFB6D, a Full Flow Bidirectional fusion network designed for 6D pose estimation from a single RGBD image. Our key insight is that appearance information in the RGB image and geometry information from the depth image are two complementary data sources, and it still remains unknown how to fully leverage them. Towards this end, we propose FFB6D, which learns to combine appearance and geometry information for representation learning as well as output representation selection. Specifically, at the representation learning stage, we build bidirectional fusion modules in the full flow of the two networks, where fusion is applied to each encoding and decoding layer. In this way, the two networks can leverage local and global complementary information from the other one to obtain better representations. Moreover, at the output representation stage, we designed a simple but effective 3D keypoints selection algorithm considering the texture and geometry information of objects, which simplifies keypoint localization for precise pose estimation. Experimental results show that our method outperforms the state-of-the-art by large margins on several benchmarks. Code and video are available at https://github.com/ethnhe/FFB6D.git.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Encoder CNN Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud Encoder</head><p>(a) The DenseFusion [66] Network. The two networks extract features from different modalities of data separately without any communication, util the final layers of the encoding-decoding architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>6D Pose Estimation is an important component in lots of real-world applications, such as augmented reality <ref type="bibr" target="#b40">[41]</ref>, autonomous driving <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b70">70]</ref> and robotic grasping <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b16">17]</ref>. It has been proven a challenging problem due to sensor noise, varying lighting, and occlusion of scenes. Recently, the dramatic growth of deep learning techniques motivates several works to tackle this problem using convolution neural networks (CNNs) on RGB images <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b33">34]</ref>. However, the loss of geometry information caused by perspective projection limits the performance of these approaches in challenging scenarios, such as poor lighting conditions, low-contrast scenes, and textureless objects. The recent advent of inexpensive RGBD sensors provides extra depth information to ease the problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref> and also leads (b) The Proposed Full Flow Bidirectional Fusion Network. Bidirectional fusion modules are added as bridges for information communication in the full flow of the two networks, where fusion is applied on each encoding and decoding layers. Local and global supplementary information from each other is shared between the two networks for better appearance and geometry representation learning, which is crucial for 6D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Network Comparison</head><p>to an interesting research question: How to fully leverage the two data modalities effectively for better 6D pose estimation?</p><p>One line of existing works <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b31">32]</ref> leverage the advantage of the two data sources within cascaded designs. These works first estimate an initial pose from RGB images and then refine it on point clouds using either the Iterative Closest Point (ICP) algorithm or multi-view hypothesis verification. Such refinement procedures are time-consuming and can not be optimized with the pose from RGB images endto-end. On the other hand, works like <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b70">70]</ref> apply a point cloud network (PCN) and a CNN to extract dense features from the cropped RGB image and point cloud respectively and the extracted dense features are then concatenated for pose estimation <ref type="bibr" target="#b66">[66]</ref>. Recently, DenseFusion <ref type="bibr" target="#b66">[66]</ref> proposed a better fusion strategy which replaced the naive concatena-tion operation with a dense fusion module, shown in <ref type="figure">Figure  1</ref>(a), and delivered improved performance. However, both feature concatenation and DenseFusion suffers from performance degeneration due to the separation of CNN and PCN in several scenarios, including objects with similar appearance or with reflective surfaces. Such cases are challenging either for the isolated CNN or the PCN feature extraction.</p><p>In this work, we propose a full flow bidirectional fusion network that perform fusion on each encoding and decoding layers for representation learning from the RGBD image, shown in <ref type="figure">Figure 1(b)</ref>. Our key insight is that appearance information in RGB and geometry information in point cloud can serve as complementary information during their feature extraction procedure. Specifically, during the CNN encoding-decoding procedure, it's hard for CNN to learn a distinctive representation for similar objects from the RGB image, which, however, is obvious in the PCN's view. On the other hand, the miss of depth caused by reflective surfaces of objects challenges the point cloud only geometry reasoning. Whereas, these objects are visible by CNN from RGB images. Hence, it's necessary to get through the two separated feature extraction branches in the early encodingdecoding stages and the proposed full flow bidirectional fusion mechanism bridges this information gap.</p><p>We further leverage the learned rich appearance and geometry representation for the pose estimation stage. We follow the pipeline proposed in PVN3D <ref type="bibr" target="#b16">[17]</ref>, which opens up new opportunities for the 3D keypoint based 6D pose estimation. However, it only considers the distance between keypoints for 3D keypoint selection. Some selected keypoints might appear in non-salient regions like smooth surfaces without distinctive texture, making it hard to locate. Instead, we take both the object texture and geometry information into account and propose the SIFT-FPS algorithm for automatic 3D keypoint selections. Salient keypoints filtered in this way are easier for the network to locate and the pose estimation performance is facilitated.</p><p>To fully evaluate our method, we conduct experiments on three popular benchmark datasets, the YCB-Video, LineMOD, and Occlusion LineMOD datasets. Experimental results show that the proposed approach without any time-consuming post-refinement procedure outperforms the state-of-the-art by a large margin.</p><p>To summarize, the main contributions of this work are:</p><p>? A novel full flow bidirectional fusion network for representation learning from a single scene RGBD image, which can be generalized to more applications, such as 3D object detection.</p><p>? A simple but effective 3D keypoint selection algorithm that leverages texture and geometry information of object models.</p><p>? State-of-the-art 6D pose estimation performance on the YCB-Video, LineMOD, and Occlusion LineMOD datasets.</p><p>? In-depth analysis to understand various design choices of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pose Estimation with RGB Data</head><p>This line of works can be divided into three classes, holistic approaches, dense correspondence exploring, and 2D-keypoint-based. Holistic approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b55">55]</ref> directly output pose parameters from RGB images. The non-linearity of rotation space limit the generalization of these approaches. Instead, dense correspondence approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref> find the correspondence between image pixels and mesh vertexes and recover poses within Perspective-n-Point (PnP) manners. Though robust to occlusion, the large output space limits the prediction accuracy. Instead, 2Dkeypoint-based <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b37">38]</ref> detect 2D keypoints of objects to build the 2D-3D correspondence for pose estimation. However, the loss of geometry information due to perspective projections limit the performance of these RGB only methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pose Estimation with Point Clouds</head><p>The development of depth sensors and point cloud representation learning techniques <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b51">51]</ref> motivates several point clouds only approaches. These approaches either utilize 3D ConvNets <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b58">58]</ref> or point cloud network <ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b23">24]</ref> for feature extraction and 3D bounding box prediction. However, sparsity and non-texture of point cloud limit the performance of these approaches. Besides, objects with reflective surfaces can not be captured by depth sensors. Therefore, taking RGB images into account is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Pose Estimation with RGB-D Data</head><p>Traditional methods utilize hand-coded <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b52">52]</ref> templates or features optimized by surrogate objectives <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b27">28]</ref> from RGBD data and perform correspondence grouping and hypothesis verification. Recent data-driven approaches propose initial pose from RGB images and refine it with point cloud using ICP <ref type="bibr" target="#b69">[69]</ref> or MCN <ref type="bibr" target="#b31">[32]</ref> algorithms. However, they are time-consuming and are not endto-end optimizable. Instead, works like <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref> add appearance information from CNN on RGB images as complementary information for geometry reasoning on bird-eyeview (BEV) images of point clouds. But they neglect the help of geometry information for RGB representation learning, the BEV ignore the pitch and roll of object pose, and the regular 2D CNN is not good at contiguous geometry reasoning either. Instead, <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b65">65]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>extract features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Keypoints Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D-keypoint-based 6D Pose Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Semantic Segmentation</head><p>Least-Squares Fitting   from RGB images and point clouds using CNN and point cloud network individually and then fused them for pose estimation. Such approaches are more effective and efficient. Nevertheless, since the appearance and geometry features are extracted separately, the two networks are not able to communicate and share information, and thus limit the expression ability of the learned representation. In this work, we add bidirectional fusion modules in the full network flow as communication bridges between the two networks. Assisted by supplementary information from another branch, better representation of appearance and geometry features are obtained for pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Flow Bidirectional Fusion Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Given an RGBD image, the task of object 6D pose estimation aims to predict a transformation matrix that transforms the object from its coordinate system to the camera coordinate system. Such transformation consists of a rotation matrix R ? SO(3) and a translation matrix T ? R 3 . To tackle the problem, pose estimation algorithms should fully explore the texture and the geometric information of both the scene and the target object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We propose a full flow bidirectional fusion network to solve the problem, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a). The proposed framework first extracts the pointwise RGBD feature for per-object 3D keypoints localization. Then the pose parameters are recovered within a least-square fitting manner. More specifically, given an RGBD image as input, we utilize a CNN to extract appearance features from the RGB image, and a point cloud network to extract geometric features from point clouds. During the feature extraction flow of the two networks, point-to-pixel and pixel-to-point fusion modules are added into each layer as communication bridges. In this way, the two branches can utilize the extra appearance (geometric) information from the other to facilitate their own representation learning. The extracted pointwise features are then fed into an instance semantic segmentation and a 3D keypoint detection module to obtain per-object 3D keypoints in the scene. Finally, a least-square fitting algorithm is applied to recover the 6D pose parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Full Flow Bidirectional Fusion Network</head><p>Given an aligned RGBD image, we first lift the depth image to a point cloud with the camera intrinsic matrix. A CNN and a point cloud network (PCN) is then applied for feature extraction from the RGB image and point cloud respectively. When the information flow through the two networks, point-to-pixel and pixel-to-point fusion modules are added for bidirectional communication. In this way, each branch can leverage local and global information from the other to facilitate their representation learning.</p><p>Pixel-to-point fusion from image features to point cloud features. These modules share the appearance information extracted from CNN to the PCN. One naive way is to generate a global feature from the RGB feature map and then concatenate it to each point cloud feature. However, since most of the pixels are background and there are multi objects in the scene, squeezing the RGB feature map globally would lose lots of detailed information and harm the following pose estimation module. Instead, we introduce a novel pixel to point feature fusion module. Since the given RGBD image is well-aligned, we can use the 3D point clouds as a bridge to connect the pixel-wise and the point-wise features. More specifically, we lift the depth of each pixel to its corresponding 3D point with the camera intrinsic matrix and get an XYZ map, aligned with the RGB map. As shown in the left part of <ref type="figure" target="#fig_1">Figure 2</ref>(b), for each point feature with its 3D point coordinate, we find its K r2p nearest point in the XYZ map and gather their corresponding appearance features from the RGB feature map. We then use max polling to integrate these neighboring appearance features following <ref type="bibr" target="#b51">[51]</ref>, and apply shared Multi-Layer Perceptrons (MLPs) to squeeze it to the same channel size as the point cloud feature:</p><formula xml:id="formula_0">F r2p =M LP ( Kr2p max i=1 F ri ),<label>(1)</label></formula><p>where F ri is the i th nearest pixel of RGB feature and F r2p the integrated one. We then concatenate the integrated appearance feature F r2p with the point feature F point and use shared MLP to obtained the fused point feature:</p><formula xml:id="formula_1">F f usedp =M LP (F point ? F r2p ),<label>(2)</label></formula><p>where ? is the concatenate operation. One thing to mention is that in the flow of the appearance feature encoding, the height and width of the RGB feature maps get smaller when the network goes deeper. Therefore, we need to maintain a corresponding XYZ map so that each pixel of feature can find its 3D coordinate. Since the decreased size of the feature map is generated by convolution kernel scanning through the original feature map with stride, the centers of kernels become new coordinates of the feature maps. One simple way is to apply the same size of kernels to calculate the mean of XYZ within it to generate a new XYZ coordinate of a pixel. The corresponding XYZ map is then obtained by scanning through the XYZ map with the mean kernel in the same stride as CNN. However, noise points are produced by the mean operation as the depth changes remarkably on the boundary between the foreground objects and the background. Instead, a better solution is to resize the XYZ map to the same size as the feature map within the nearest interpolation algorithm.</p><p>Point-to-pixel fusion from point cloud features to image features. These modules build bridges to transfer the geometric information obtained from the PCN to the CNN. The procedure is shown on the right side of <ref type="figure" target="#fig_1">Figure 2</ref>(b). Same as the pixel-to-point fusion modules, we fuse the feature densely rather than naively concatenating the global point feature to each pixel. Specifically, for each pixel of feature with its XYZ coordinate, we find its K p2r nearest points from the point cloud and gather the corresponding point features. We squeeze the point features to the same channel size as the RGB feature and then use max pooling to integrate them. The integrated point feature is then concatenated to the corresponding color feature and mapped by a shared MLP to generate the fused one:</p><formula xml:id="formula_2">F p2r = Kp2r max j=1 (M LP (F pj )),<label>(3)</label></formula><formula xml:id="formula_3">F f usedr =M LP (F rgb ? F p2r ),<label>(4)</label></formula><p>where F pj denotes the j th nearest point features, F p2r the integrated point features and ? the concatenate operation.</p><p>Dense RGBD feature embedding With the proposed full flow fusion network, we obtain dense appearance embeddings from the CNN branch and dense geometry features from the PCN branch. We then find the correspondence between them by projecting each point to the image plane with the camera intrinsic matrix. According to the correspondence, we obtain pairs of appearance and geometry features and concatenate them together to form the extracted dense RGBD feature. These features are then fed into an instance semantic segmentation module and a 3D keypoint detection module for object pose estimation in the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Keypoint-based 6D Pose Estimation</head><p>Recently, the PVN3D <ref type="bibr" target="#b16">[17]</ref> work by He et al. opens up new opportunities for using 3D keypoints to estimate object pose. In this work, we follow their 3D keypoint formulation but further improve the 3D keypoint selection algorithm to fully leverage the texture and geometry information of objects. Specifically, we first detect the per-object selected 3D keypoints in the scene and then utilize a least-squares fitting algorithm to recover the pose parameters.</p><p>Per-object 3D keypoint detection So far we have obtained the dense RGBD embeddings. We then follow PVN3D <ref type="bibr" target="#b16">[17]</ref> and obtain the per-object 3D keypoints by adding an instance semantic segmentation module to distinguish different object instances and a keypoint voting module to recover 3D keypoints. The instance semantic segmentation module consists of a semantic segmentation module and a center point voting module, where the former one predicts per-point semantic labels, and the latter one learns the per-point offset to object centers for distinguishment of different instances. For each object instance, the keypoint voting module learns the point-wise offsets to the selected keypoints that vote for 3D keypoint within a MeanShift <ref type="bibr" target="#b9">[10]</ref> clustering manners.</p><p>Keypoint selection Previous works <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b16">17]</ref> select keypoints from the target object surface using the Farthest Point Sampling (FPS) algorithm. Specifically, they maintain a keypoint set initialized by a random point on the object surface and iteratively add other points that are farthest to those within the set until N points are obtained. In this way, the selected keypoints spread on the object surface and stabilize the following pose estimation procedure <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b16">17]</ref>. However, since the algorithm only takes the Euclidean distance into account, the selected points may appear in non-salient regions, such as flat planes without distinctive texture. These points are hard to detect and the accuracy of the estimated pose decrease. To fully leverage the texture and geometry information of objects, we propose a simple but effective 3D keypoint selection algorithm, named SIFT-FPS. Specifically, we use the SIFT <ref type="bibr" target="#b39">[40]</ref> algorithm to detect 2D keypoints that are distinctive in texture images and then lift them to 3D. The FPS algorithm is then applied for the selection of top N keypoints among them. In this way, the selected keypoints not only distribute evenly on the object surface but are also distinctive in texture and easy to detect.</p><p>Least-Squares Fitting Given the selected 3D keypoints in the object coordinates system {p i } N i=1 , and the corresponding 3D keypoints in the camera coordinated system</p><formula xml:id="formula_4">{p * i } N i=1 .</formula><p>The Least-Squares Fitting <ref type="bibr" target="#b0">[1]</ref> algorithm calculate the pose parameters R and T by minimizing the squared loss:</p><formula xml:id="formula_5">L lsf = N i=1 ||p * i ? (R ? p i + T )|| 2 .<label>(5)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Datasets</head><p>We evaluate our method on three benchmark datasets. YCB-Video <ref type="bibr" target="#b4">[5]</ref> contains 92 RGBD videos that capture scenes of 21 selected YCB objects. We followed previous works <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b16">17]</ref> to split the training and testing set. Synthetic images were also taken for training as in <ref type="bibr" target="#b69">[69]</ref> and the hole completion algorithm <ref type="bibr" target="#b29">[30]</ref> is applied for hole filling for depth images as in <ref type="bibr" target="#b16">[17]</ref>. LineMOD <ref type="bibr" target="#b18">[19]</ref> is a dataset with 13 videos of 13 lowtextured objects. The texture-less objects, cluttered scenes, and varying lighting make this dataset challenge. We split the training and testing set following previous works <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b47">47]</ref> and generate synthesis images for training following <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Occlusion LINEMOD <ref type="bibr" target="#b2">[3]</ref> was selected and annotated from the LineMOD datasets. Each scene in this dataset consists of multi annotated objects, which are heavily occluded. The heavily occluded objects make this dataset challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We use the average distance metrics ADD and ADD-S for evaluation. For asymmetric objects, the ADD metric calculate the point-pair average distance between objects vertexes transformed by the predicted and the ground truth pose, defined as follows:</p><formula xml:id="formula_6">ADD = 1 m v?O ||(Rv + T ) ? (R * v + T * )||.<label>(6)</label></formula><p>where v denotes a vertex in object O, R, T the predicted pose and R * , T * the ground truth. For symmetric objects, the ADD-S based on the closest point distance is applied:</p><formula xml:id="formula_7">ADD-S = 1 m v1?O min v2?O ||(Rv 1 + T ) ? (R * v 2 + T * )||.<label>(7)</label></formula><p>In the YCB-Video dataset, we follows previous methods <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b16">17]</ref> and report the area under the accuracy-threshold curve obtained by varying the distance threshold (ADD-S and ADD(S) AUC). In the LineMOD and Occlusion LineMOD datasets, we report the accuracy of distance less than 10% of the objects diameter (ADD-0.1d) as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">47]</ref>.</p><p>PoseCNN <ref type="bibr" target="#b69">[69]</ref> PointFusion <ref type="bibr" target="#b70">[70]</ref> DCF <ref type="bibr" target="#b34">[35]</ref> DF (per-pixel) <ref type="bibr" target="#b66">[66]</ref> PVN3D <ref type="bibr" target="#b16">[17]</ref> Our FFB6D</p><p>Object ADDS ADD(S) ADDS ADD(S) ADDs ADD(S) ADDS ADD(S) ADDS ADD(S) ADDS ADD(S)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training and Implementation</head><p>Network architecture. We apply ImageNet <ref type="bibr" target="#b10">[11]</ref> pretrained ResNet34 <ref type="bibr" target="#b15">[16]</ref> as encoder of RGB images, followed by a PSPNet <ref type="bibr" target="#b72">[72]</ref> as decoder. For point cloud feature extraction, we randomly sample 12288 points from depth images following <ref type="bibr" target="#b16">[17]</ref> and applied RandLA-Net <ref type="bibr" target="#b23">[24]</ref> for representation learning. In each encoding and decoding layers of the two networks, max pooling and shared MLPs are applied to build bidirectional fusion modules. After the process of the full flow bidirectional fusion network, each point has a feature f i ? R C of C dimension. These dense RGBD features are then fed into the instance semantic segmentation and the keypoint offset learning modules consist of shared MLPs.</p><p>Optimization regularization. The semantic segmentation branch is supervised by Focal Loss <ref type="bibr" target="#b36">[37]</ref>. The center point voting and 3D keypoints voting modules are opti-mized by L1 loss as in <ref type="bibr" target="#b16">[17]</ref>. To jointly optimize the three tasks, a multi-task loss with the weighted sum of them is applied following <ref type="bibr" target="#b16">[17]</ref>.</p><p>SIFT-FPS keypoint selection algorithm. We put the target object at the center of a sphere and sample viewpoints of the camera on the sphere equidistantly. RGBD images with camera poses are obtained by render engines. We then detect 2D keypoints from RGB images with SIFT. These 2D keypoints are lifted to 3D and transformed back to the object coordinates system. Finally, an FPS algorithm is applied to select N target keypoints out of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Three Benchmark Datasets.</head><p>We evaluate the proposed models on the YCB-Video, the LineMOD, and the Occlusion LineMOD datasets.</p><p>Evaluation on the YCB-Video dataset. <ref type="table" target="#tab_2">Table 1</ref> shows the quantitative evaluation results of the proposed FFB6D on the YCB-Video dataset. We compare it with other single view methods without iterative refinement. FFB6D advances state-of-the-art results by 1.1% on the ADD-S metric and 1.0% on the ADD(S) metric. Equipped with extra iterative refinement, our approach also achieves the best performance, demonstrated in <ref type="table" target="#tab_3">Table 2</ref>. Note that the proposed FFB6D without any iterative refinement even outperforms state-of-the-arts that require time-consuming postrefinement procedures. Qualitative results are reported in RGB RGB-D PoseCNN DeepIM <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b32">33]</ref> PVNet <ref type="bibr" target="#b47">[47]</ref> CDPN <ref type="bibr" target="#b33">[34]</ref> DPOD <ref type="bibr" target="#b71">[71]</ref> Point-Fusion <ref type="bibr" target="#b70">[70]</ref> Dense-Fusion <ref type="bibr" target="#b66">[66]</ref> G2L-Net <ref type="bibr" target="#b6">[7]</ref> PVN3D <ref type="bibr" target="#b16">[17]</ref> Our FFB6D    the supplementary material. Robustness towards occlusion. We follow <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b16">17]</ref> to report the ADD-S less than 2cm accuracy under the growth of occlusion level on the YCB-Video dataset. As is shown in <ref type="figure" target="#fig_2">Figure 3</ref>, previous methods degrade as the occlusion increase. In contrast, FFB6D didn't suffer from a drop in performance. We think our full flow bidirectional fusion mechanism makes full use of the texture and geometry information in the captured data and enables our approach to locate 3D keypoints even in highly occluded scenes.</p><p>Evaluation on the LineMOD dataset &amp; Occlusion LineMOD dataset. The proposed FFB6D outperforms the state-of-the-art on the LineMOD dataset, presented in <ref type="table" target="#tab_2">Table  10</ref>. We also evaluate FFB6D on the Occlusion LineMOD dataset, shown in <ref type="table" target="#tab_2">Table 11</ref>. In the table, our FFB6D without iterative refinement advances state-of-the-art by 4.7%, further confirming its robustness towards occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In this subsection, we present extensive ablation studies on our design choices and discuss their effect.</p><p>Effect of full flow bidirectional fusion. To validate that building fusion modules between the two modality networks in full flow help, we ablate fusion stages in <ref type="table" target="#tab_7">Table  5</ref>. Compared to the mechanism without fusion, adding fusion modules either on the encoding stage, decoding stage,  or on the final feature maps, can all boost the performance. Among the three stages, fusion on the encoding stages obtained the highest improvement. We think that's because the extracted local texture and geometric information are shared through the fusion bridge on the early encoding stage, and more global features are shared when the network goes deeper. Also, adding fusion modules in full flow of the network, saying that on both the encoding and decoding stages, obtains the highest performance. While adding extra DenseFusion behind full flow fusion obtains no performance gain as the two embeddings have been fully fused. We also ablate the fusion direction in <ref type="table" target="#tab_9">Table 6</ref> to validate the help of bidirectional fusion. Compared with no fusion, both the fusion from RGB to point cloud and the inverse way facilitate better representation learning. Combining the two obtains the best results. On the one hand, from the view of PCN, we think the rich textures information obtained from high-resolution RGB images helps semantic recognition. In addition, the high-resolution RGB features provide rich information for blind regions of depth sensors caused by reflective surfaces. It serves as a completion to point cloud and improve pose accuracy, as shown in <ref type="figure" target="#fig_0">Figure 4(a)</ref>. On the one hand, geometry information extracted from point cloud helps the RGB branch by distinguishing foreground objects from the background that are in similar colors. Moreover, the shape size information extracted from point clouds helps divide objects with a similar appearance but in a different size, as is shown in <ref type="figure" target="#fig_4">Figure 4(b)</ref>.</p><p>Effect of representation learning frameworks. We explore the effect of different representation learning frameworks for the two modalities of data in this part. The result is presented in <ref type="table" target="#tab_10">Table 7</ref>. We find that neither concatenating the XYZ map as extra information to CNN (CNN-R?D) nor adding RGB values as extra inputs to the PCN (PCN-R?D) achieves satisfactory performance. Using two CNNs    <ref type="table">Table 8</ref>: Effect of keypoint selection algorithm. S-F means the proposed SIFT-FPS algorithm.</p><p>(CNN-R+CNN-D) or two PCNs (PCN-R+PCN-D) with full flow bidirectional fusion modules get better but are still far from satisfactory. In contrast, applying CNN on the RGB image and PCN on the point cloud (CNN-R+PCN-D) gets the best performance. We think that the grid-like image data is discrete, on which the regular convolution kernel fits better than continuous PCN. While the geometric information residing in the depth map is defined in a continuous vector space, and thus PCNs can learn better representation. Effect of 3D keypoints selection algorithm. In <ref type="table">Table  8</ref>, we study the effect of different keypoint selection algorithms. Compared with FPS that only considers the mutual distance between keypoints, our SIFT-FPS algorithm taking both object texture and geometry information into account is easier to locate. Therefore, the predicted keypoint error is smaller and the estimated poses are more accurate.</p><p>Effect of the downsample strategy of the assisting XYZ map. The size of RGB feature maps are shrunk by  stridden convolution kernels. To maintain the corresponding XYZ maps, we first scale it down with the same size of mean kernels and got 96.3 ADD-S AUC. However, simply resize the XYZ map with the nearest interpolation got 96.6. We find the average operation produces noise points on the boundary and decrease the performance. Model parameters and time efficiency. In <ref type="table" target="#tab_12">Table 9</ref>, we report the parameters and run-time breakdown of FFB6D. Compared to PVN3D <ref type="bibr" target="#b16">[17]</ref>, which obtained the fused RGBD feature by dense fusion modules <ref type="bibr" target="#b66">[66]</ref> in the final layers, our full flow bidirectional fusion network achieve better performance with fewer parameters and is 2.5 times faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel full flow bidirectional fusion network for representation learning from a single RGBD image, which extract rich appearance and geometry information in the scene for pose estimation. Besides, we introduce a simple but effective SIFT-FPS keypoint selection algorithms that leverage texture and geometry information of objects to simplify keypoint localization for precise pose estimation. Our approach outperforms all previous approaches in several benchmark datasets by remarkable margins. Moreover, we believe the proposed full flow bidirectional fusion network can generalize to more applications built on RGBD images, such as 3D object detection, 3D instance semantic segmentation and salient object detection etc. and expect to see more future research along this line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>A.1. Details of the Network Architecture <ref type="figure">Figure 5</ref> shows the detailed architecture of the proposed FFB6D. We applied ImageNet <ref type="bibr" target="#b10">[11]</ref> pre-trained ResNet34 <ref type="bibr" target="#b15">[16]</ref> and PSPNet <ref type="bibr" target="#b72">[72]</ref> as encoder and decoder of the input RGB image. Meanwhile, a RandLA-Net <ref type="bibr" target="#b23">[24]</ref> is applied for point cloud representation learning. On each encoding and decoding layer of the two networks, point-to-pixel and pixel-to-point fusion modules are added for information communication. Finally, the extracted dense appearance and geometry features are concatenated and fed into the semantic segmentation, center point voting, and 3D keypoints voting modules for pose estimation. Details of each part are as follows:</p><p>Network Input. The input of the convolution neural network (CNN) branch is a full scene image with a size of H ?W ?3, where H is the height of the RGB image, W the width, and 3 the three channels of color information (RGB). For the point cloud learning branch, the input is a randomly subsampled point cloud from the scene depth image, with a size of N ? C in , where N set to 12288 is the number of sampled points, and C in the input coordinate, color and normal information of each point (x-y-z-R-G-B-nx-ny-nz).</p><p>Encoding Layers. We utilize ResNet34 <ref type="bibr" target="#b15">[16]</ref> as the encoder of RGB images, which consists of five convolution layers to reduce the size of feature maps and increase the number of feature channels. The Pyramid Pooling Modules (PPM) from PSPNet <ref type="bibr" target="#b72">[72]</ref> is also applied in the last encoding layer. Meanwhile, in the point cloud network branch, after being processed by a fully connected layer, the point features are fed into four encoding layers of RandLA-Net <ref type="bibr" target="#b23">[24]</ref> for feature encoding, each of which consists of a local feature aggregation module and a random sampling operation designed in the work <ref type="bibr" target="#b23">[24]</ref>.</p><p>Decoding Layers. In the decoding stage, three upsampling modules and a final convolution layer from PSP-Net are used for appearance feature decoding. Meanwhile, in the point cloud network branch, four decoding layers from RandLA-Net are utilized as point cloud features decoders, which consists of the random sampling operations and local feature aggregation modules designed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Bidirectional Fusion Modules. On each encoding and decoding stage, point-to-pixel and pixel-to-point fusion modules (Section 3. Pose Estimation Modules. Given the predicted semantic label and center point offset of each point in the scene, a MeanShift <ref type="bibr" target="#b9">[10]</ref> clustering algorithm is applied to distinguish different object instances with the same semantic. Then, for each instance, each point within it votes for its 3D keypoint with the MeanShift <ref type="bibr" target="#b9">[10]</ref> algorithm. Finally, a least-squares fitting algorithm is applied to recover the object pose parameters according to the detected 3D keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation: Different Representation Learning Frameworks</head><p>In this section, we demonstrate the implementation details of different representation learning frameworks in <ref type="table" target="#tab_6">Table 4</ref>. To implement CNN-R?D, we lift each pixel in the depth image to its corresponding 3D point to get the XYZ map as well as the normal map. We then concatenate them with the RGB map and feed it into a ResNet34-PSPNet encoding-decoding network for feature extraction of each point (pixel). For PCN-R?D, we append RGB values of each point to its 3D coordinate as well as its normal vector and then utilize the RandLA-Net for representation learning. The CNN-R+CNN-D utilizes two ResNet34-PSPNet networks for feature extraction from the RGB image and the XYZ and normal maps respectively. Bidirectional fusion modules (Section 3.2) are added to each encoding and decoding layer. For PCN-R+PCN-D, we leverage one RandLA-Net to extract features from the RGB value of each point and another one for representation learning of the 3D coordinate and the normal vector of each point. In the network flow, bidirectional fusion modules are added to each layer for information communication as well. To implement CNN-R+3DC-D, we replace the RandLA-Net with a 3D convolution neural network. In the encoding stages, the voxel size decreases from 32 <ref type="bibr" target="#b2">3</ref>   <ref type="bibr" target="#b16">[17]</ref> composed by shared MLPs. A clustering algorithm is then applied to distinguish different instances with the same semantic labels and points on the same instance vote for their target keypoints. With detected 3D keypoints, a least-squares fitting algorithm is applied to recover the pose parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB-D PoseCNN</head><p>DeepIM <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b32">33]</ref> PVNet <ref type="bibr" target="#b47">[47]</ref> CDPN <ref type="bibr" target="#b33">[34]</ref> DPOD <ref type="bibr" target="#b71">[71]</ref> Point-Fusion <ref type="bibr" target="#b70">[70]</ref> Dense-Fusion <ref type="bibr" target="#b66">[66]</ref> G2L-Net <ref type="bibr" target="#b6">[7]</ref> PVN3D <ref type="bibr" target="#b16">[17]</ref> Our FFB6D  <ref type="table" target="#tab_2">Table 10</ref>: Quantitative evaluation on the LineMOD dataset. The ADD-0.1d <ref type="bibr" target="#b19">[20]</ref> metric is reported and symmetric objects are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method PoseCNN [69]</head><p>Oberweger <ref type="bibr" target="#b44">[44]</ref> Hu et al. <ref type="bibr" target="#b25">[26]</ref> Pix2Pose <ref type="bibr" target="#b46">[46]</ref> PVNet <ref type="bibr" target="#b47">[47]</ref> DPOD <ref type="bibr" target="#b71">[71]</ref> Hu et al. <ref type="bibr" target="#b24">[25]</ref> HybridPose <ref type="bibr" target="#b56">[56]</ref> PVN3D <ref type="bibr" target="#b16">[17]</ref> Our FFB6D ape 9.  <ref type="table" target="#tab_2">Table 11</ref>: Quantitative evaluation on the Occlusion-LineMOD dataset. The ADD-0.1d <ref type="bibr" target="#b19">[20]</ref> metric is reported and symmetric objects are in bold.</p><p>creases from 32 to 256 (32 ? 64 ? 128 ? 256). In the decoding stage, the voxel size increases and feature dimensions decrease inversely. Finally, the geometry feature of each point is obtained within a trilinear interpolation manner, as in PVCNN <ref type="bibr" target="#b38">[39]</ref>, which is then concatenated with the appearance feature from CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. More Results</head><p>A.3.1 Quantitative result on the LineMOD dataset.</p><p>More results of 6D pose estimation on the LineMOD dataset are shown in <ref type="table" target="#tab_2">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Quantitative result on the Occlusion-LineMOD dataset.</head><p>We report more results on the Occlusion-LineMOD dataset in <ref type="table" target="#tab_2">Table 11</ref>. We follow the state-of-the-art to train our model on the LineMOD dataset and only use this dataset for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Visualization on predicted pose on the YCB-Video Dataset.</head><p>We provide some qualitative results on the YCB-Video dataset in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PVN3D</head><p>Our FFB6D <ref type="figure">Figure 6</ref>: Qualitative results of 6D pose on the YCB-Video dataset. Objects in bounding boxes show the pose that we outperform the state-of-the-art significantly. Object vertexes in the object coordinate system are transformed by the ground truth or predicted pose to the camera coordinate system and then projected to the image by the camera intrinsic matrix. Compared to PVN3D <ref type="bibr" target="#b16">[17]</ref> with the DenseFusion <ref type="bibr" target="#b66">[66]</ref> architecture, our FFB6D is more robust towards occlusion and objects with similar appearance or reflective surfaces, which are quite challenging for either isolated CNN or point cloud network feature extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>The pipeline of FFB6D. A CNN and a point cloud network is utilized for representation learning of RGB image and point cloud respectively. In flow of the two networks, bidirectional fusion modules are added as communicate bridges. The extracted per-point features are then fed into an instance semantic segmentation and a 3D keypoint voting modules to obtain per-object 3D keypoints. Finally, the pose is recovered within a least-squares fitting algorithm. Dense bidirectional fusion modules. (1) The pixel-to-point fusion modules fuse RGB features to point cloud features. For each point, we find its K r2p nearest neighbors in the XYZ map and gather their corresponding appearance features from the RGB feature map. These features are then processed by max pooling and a shared MLP to obtain the most significant appearance features. Finally, a shared MLP fuses the concatenation of the appearance and geometry features to obtain the fused point features. (2) The point-to-pixel fusion modules similarly obtain fused pixel features as the pixel-to-point fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of FFB6D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance of different approaches under increasing levels of occlusion on the YCB-Video dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>of point-to-pixel fusion from point cloud features to image features. The point-to-pixel fusion provides geometry information from PCN on point cloud to help distinguish objects with similar appearance during appearance representation learning. of pixel-to-point fusion from image features to point cloud features. The pixel-to-point fusion provides a vision of reflective surfaces from CNN on RGB to ease point cloud reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Effect of full flow bidirectional fusion, compared to PVN3D<ref type="bibr" target="#b16">[17]</ref> with DenseFusion architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2) are added for bidirectional information communication. For each pixel-to-point fusion module, we set K r2p = 16 and aggregate 16 nearest pixel of appearance features through a max-pooling and a single layer shared MLP, M LP [c r , c p ], where c r denotes the channel size of RGB features and c p the channel size of corresponding point features. The aggregated pixels of appearance features are then concatenated with the corresponding point features and map by a shared MLP, M LP [2 * c p , c p ] to generate each fused point feature. Meanwhile, we set K p2r = 1 and get the fused appearance features similarly in each point-to-pixel fusion module. Prediction Headers. Three headers are added after the extracted dense RGBD features to predict the semantic label, center point offset as wel as the 3D keytpoints offsets of each point. These headers consists of shared MLPs, denoted as M LP [c r +c p , c 1 , c 2 , ..., c k ], where c r and c p represent the channel size of extracted appearance and geometry features respectively, and c i the output channel size of the i-th layer in the MLP. Specifically, the semantic segmentation module consists of M LP [c r + c p , 128, 128, 128, n cls ], the center offset learning module comprises M LP [c r + c p , 128, 128, 128, 3], and the 3D keypoints offset module is composed of M LP [c r + c p , 128, 128, 128, n kps * 3], where n cls denotes number of object classes and n kps means the number of keypoints of each object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">PCNN+ICP DF(iter.)</cell><cell cols="3">MoreFusion PVN3D+ICP FFB6D+ICP</cell></row><row><cell>ADD-S</cell><cell>93.0</cell><cell>93.2</cell><cell>95.7</cell><cell>96.1</cell><cell>97.0</cell></row><row><cell>ADD(S)</cell><cell>85.4</cell><cell>86.1</cell><cell>91.0</cell><cell>92.3</cell><cell>93.1</cell></row></table><note>Quantitative evaluation of 6D Pose without iterative refinement on the YCB-Video Dataset. The ADD-S [69] and ADD(S) [20] AUC are reported. Symmetric objects are in bold. DF (per-pixel) means DenseFusion (per-pixel).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Quantitative evaluation of 6D Pose with itera- tive refinement on the YCB-Video Dataset (ADD-S [69] and ADD(S) AUC [20]). Baselines: PoseCNN+ICP [69], DF(iter.) [66], MoreFusion [65], PVN3D+ICP [17].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Quantitative evaluation of 6D pose on the LineMOD dataset (ADD-0.1d [20] metrics).</figDesc><table><row><cell>Method</cell><cell>PoseCNN</cell><cell>Oberweger</cell><cell>Hu et al.</cell><cell>Pix2Pose</cell><cell>PVNet</cell></row><row><cell></cell><cell>[69]</cell><cell>[44]</cell><cell>[26]</cell><cell>[46]</cell><cell>[47]</cell></row><row><cell>ADD-0.1d</cell><cell>24.9</cell><cell>27.0</cell><cell>27.0</cell><cell>32.0</cell><cell>40.8</cell></row><row><cell>Method</cell><cell>DPOD</cell><cell>Hu et</cell><cell>HybridPose</cell><cell>PVN3D</cell><cell>Our</cell></row><row><cell></cell><cell>[71]</cell><cell>al.[25]</cell><cell>[56]</cell><cell>[17]</cell><cell>FFB6D</cell></row><row><cell>ADD-0.1d</cell><cell>47.3</cell><cell>43.3</cell><cell>47.5</cell><cell>63.2</cell><cell>66.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Quantitative evaluation of 6D pose (ADD-0.1d) on</cell></row><row><cell cols="3">the Occlusion-LineMOD dataset.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fusion Stage</cell><cell></cell><cell cols="2">Pose Result</cell></row><row><cell>FE</cell><cell>FD</cell><cell>DF</cell><cell>ADD-S</cell><cell>ADD(S)</cell></row><row><cell>? ? ?</cell><cell>? ? ?</cell><cell>? ?</cell><cell>91.9 96.2 93.0 94.0 96.6 96.4</cell><cell>87.5 92.2 89.2 90.8 92.7 92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Effect of fusion stages on the YCB-Video dataset.</figDesc><table /><note>FE: fusion during encoding; FD: fusion during decoding; DF: Dense Fusion on the two final feature maps.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Effect of fusion direction on the YCB-Video dataset. P2R means fusion from point cloud embeddings to RGB embeddings, and R2P means fusion from RGB embeddings to point cloud embeddings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="7">Effect of representation learning framework on the</cell></row><row><cell cols="7">two modalities of data. CNN: 2D Convolution Neural Net-</cell></row><row><cell cols="7">work; PCN: point cloud network; 3DC: 3D ConvNet; R:</cell></row><row><cell cols="7">RGB images; D: XYZ maps for CNN, point clouds for PCN</cell></row><row><cell cols="5">and voxelized point clouds for 3D ConvNet.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>FPS4</cell><cell>S-F4</cell><cell>FPS8</cell><cell>S-F8</cell><cell>FPS12</cell><cell>S-F12</cell></row><row><cell>KP err. (cm)</cell><cell>1.3</cell><cell>1.2</cell><cell>1.4</cell><cell>1.2</cell><cell>1.6</cell><cell>1.3</cell></row><row><cell>ADD-S</cell><cell>95.9</cell><cell>96.4</cell><cell>96.1</cell><cell>96.6</cell><cell>96.0</cell><cell>96.5</cell></row><row><cell>ADD(S)</cell><cell>92.0</cell><cell>92.4</cell><cell>92.3</cell><cell>92.7</cell><cell>92.0</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Model parameters and run-time breakdown on the LineMOD dataset. NF: Network Forward; PE: Pose Esti- mation. Our FFB6D with fewer parameters is 2.5x faster.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>to 4 3 (32 3 ? 16 3 ? 8 3 ? 4 3 ) and the feature dimensions in-The detailed architecture of our FFB6D. For the convolution neural network (CNN) branch on the RGB image, we utilize ResNet34<ref type="bibr" target="#b15">[16]</ref> and PSPNet<ref type="bibr" target="#b72">[72]</ref> as encoder and decoder. ConvL: Convolution Layers of ResNet34, PPM: Pyramid Pooling Modules of PSPNet, PUP: PSPNet Up-sampling, Conv2D: 2D convolution layer. For the point cloud network (PCN) branch on the point cloud, we apply RandLA-Net<ref type="bibr" target="#b23">[24]</ref> for feature extraction. FC: Fully Connected layer, LFA: Local Feature Aggregation, RS: Random Sampling, MLP: shared Multi-Layer Perceptron, US: Up-sampling. In the flow of the two networks, point-to-pixel fusion modules, P2RF, and pixel-to-point fusion modules, R2PF consists of max pooling and shared MLPs are added. The extracted features from the two networks are then concatenated and fed into the following semantic segmentation, center point voting and 3D keypoints voting modules</figDesc><table><row><cell>Input RGB Image</cell><cell></cell><cell></cell><cell>Input Point Cloud</cell></row><row><cell>(H, W, 3)</cell><cell></cell><cell></cell><cell>(N, C in )</cell></row><row><cell>ConvL</cell><cell></cell><cell></cell><cell>FC</cell></row><row><cell>(H//4, W//4, 64)</cell><cell></cell><cell></cell><cell>(N, 8)</cell></row><row><cell>ConvL</cell><cell></cell><cell>LFA &amp; RS</cell></row><row><cell></cell><cell>P2RF</cell><cell>R2PF</cell></row><row><cell>(H//4, W//4, 64)</cell><cell></cell><cell></cell><cell>(N//4, 64)</cell></row><row><cell>ConvL</cell><cell></cell><cell>LFA &amp; RS</cell></row><row><cell></cell><cell>P2RF</cell><cell>R2PF</cell></row><row><cell>(H//8, W//8, 128)</cell><cell></cell><cell></cell><cell>(N//16, 128)</cell></row><row><cell>ConvL</cell><cell></cell><cell>LFA &amp; RS</cell></row><row><cell></cell><cell>P2RF</cell><cell>R2PF</cell></row><row><cell>(H//8, W//8, 512)</cell><cell></cell><cell></cell><cell>(N//64, 256)</cell></row><row><cell>ConvL &amp; PPM</cell><cell></cell><cell>LFA &amp; RS</cell></row><row><cell></cell><cell>P2RF</cell><cell>R2PF</cell></row><row><cell>(H//8,W//8,1024)</cell><cell></cell><cell></cell><cell>(N//256, 512)</cell></row><row><cell>PUP</cell><cell></cell><cell>US &amp; MLP</cell></row><row><cell></cell><cell>P2RF</cell><cell>R2PF</cell></row><row><cell>(H//4, W//4, 256)</cell><cell></cell><cell></cell><cell>(N//64, 256)</cell></row><row><cell>PUP</cell><cell></cell><cell>US &amp; MLP</cell></row><row><cell></cell><cell>P2RF</cell><cell>R2PF</cell></row><row><cell>(H//2, W//2, 64)</cell><cell></cell><cell></cell><cell>(N//16, 128)</cell></row><row><cell>PUP</cell><cell></cell><cell>US &amp; MLP</cell></row><row><cell></cell><cell>P2RF</cell><cell>R2PF</cell></row><row><cell>(H, W, 64)</cell><cell></cell><cell></cell><cell>(N//4, 64)</cell></row><row><cell>Conv2D</cell><cell></cell><cell></cell><cell>US &amp; MLP</cell></row><row><cell>(H, W, 64)</cell><cell></cell><cell></cell><cell>(N, 64)</cell></row><row><cell></cell><cell></cell><cell>Gathering &amp; Concatenation</cell></row><row><cell></cell><cell cols="2">(N, 128)</cell></row><row><cell>MLPs</cell><cell></cell><cell>MLPs</cell><cell>MLPs</cell></row><row><cell>Semantic Labels</cell><cell cols="2">Center Point Offsets</cell><cell>3D Keypoint Offsets</cell></row><row><cell cols="2">Voting &amp; Clustering</cell><cell></cell></row><row><cell cols="2">Instance Semantic Segmentation</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Voting &amp; Clustering</cell></row><row><cell></cell><cell cols="2">Per-Object 3D Keypoints</cell></row><row><cell></cell><cell></cell><cell>Least-Squares Fitting</cell></row><row><cell></cell><cell cols="2">6D Pose Parameters</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Least-squares fitting of two 3-d point sets. IEEE Transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Somani Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blostein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="698" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reconstruct locally, localize globally: A model free method for object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3153" to="3163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 international conference on advanced robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11973" to="11982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">G2l-net: Global to local network for real-time 6d pose estimation with embedding vector features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Hyung Jin Chang, Jinming Duan, and Ales Leonardis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The moped framework: Object recognition and pose estimation for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recovering 6d object pose and predicting next-best-view in the crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rigas</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotiris</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3583" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">aware object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meirav</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1275" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative mixture-oftemplates for viewpoint classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient response maps for real-time detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Epos: Estimating 6d pose of objects with symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11703" to="11712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manolis Lourakis, and Xenophon Zabulis. T-less: An rgbd dataset for 6d pose estimation of texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?tep?n</forename><surname>Obdr??lek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="880" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bop: Benchmark for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Glentbuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single-stage 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparing images using the hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Gregory A Klanderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rucklidge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="850" to="863" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified framework for multi-view multi-class object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">independent object class detection using 3d feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joerg</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Schertler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Keypose: Multi-view 3d labeling and keypoint estimation for transparent objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11602" to="11610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointvoxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object recognition from local scaleinvariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">iccv</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality: a hands-on survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global hypothesis generation for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on</title>
		<meeting>the IEEE Conference on</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10710" to="10719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discriminatively trained templates for 3d object detection: A real time scalable approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reyes</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2048" to="2055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="259" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pfrl: Pose-free reinforcement learning for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhun</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11454" to="11463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2686" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Depth-encoded hough voting for joint object detection and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="658" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Latent-class hough forests for 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rigas</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10790</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Morefusion: Multi-object reasoning for 6d pose estimation from volumetric fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3109" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1941" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning deep network for detecting 3d object keypoints and 6d poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinye</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14134" to="14142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A novel depth and color feature fusion framework for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
