<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal ROI Align for Video Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyberspace Security</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Electromagnetic Space Information</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<email>chenkai@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
							<email>wangxinjiang@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
							<email>qchu@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Cyberspace Security</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Electromagnetic Space Information</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
							<email>zhufeng@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<email>dhlin@ie.cuhk.edu.hk</email>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyberspace Security</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Electromagnetic Space Information</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huamin</forename><surname>Feng</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Beijing Electronic Science and Technology Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal ROI Align for Video Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object detection is challenging in the presence of appearance deterioration in certain video frames. Therefore, it is a natural choice to aggregate temporal information from other frames of the same video into the current frame. However, ROI Align, as one of the most core procedures of video detectors, still remains extracting features from a single-frame feature map for proposals, making the extracted ROI features lack temporal information from videos. In this work, considering the features of the same object instance are highly similar among frames in a video, a novel Temporal ROI Align operator is proposed to extract features from other frames feature maps for current frame proposals by utilizing feature similarity. The proposed Temporal ROI Align operator can extract temporal information from the entire video for proposals. We integrate it into single-frame video detectors and other state-of-the-art video detectors, and conduct quantitative experiments to demonstrate that the proposed Temporal ROI Align operator can consistently and significantly boost the performance. Besides, the proposed Temporal ROI Align can also be applied into video instance segmentation. Codes are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recently, deep convolutional neural networks have brought great progress in object detection of still images <ref type="bibr" target="#b23">(Ren et al. 2015;</ref><ref type="bibr" target="#b21">Lin et al. 2017;</ref><ref type="bibr" target="#b1">Cai and Vasconcelos 2018;</ref><ref type="bibr" target="#b26">Tian et al. 2019;</ref><ref type="bibr" target="#b10">Duan et al. 2019)</ref>. Most state-of-the-art single image object detectors usually adopt the region-based detection paradigm <ref type="bibr" target="#b23">(Ren et al. 2015;</ref><ref type="bibr" target="#b1">Cai and Vasconcelos 2018)</ref>. When directly applying these detectors for video object detection (VID), the detection accuracy suffers from deteriorated object appearances in videos, such as motion blur, video defocus and object occlusions. Despite these challenges, video contains temporal information about the same object instance (e.g., its appearance in different poses, and from different viewpoints). Therefore, the key challenge is how to effectively exploit such temporal information for the same object instance in a video.</p><p>As shown in <ref type="figure">Fig. 1 (a)</ref>, region-based detectors usually adopt ROI Align  to extract ROI features. However, ROI Align only utilizes current frame feature map to extract features for current frame proposals, this leads to the extracted ROI features lacking the temporal information of the same object instance in a video. One simple and obvious way to utilize the temporal information is taking other frames feature maps to perform ROI Align for current frame proposals. But the precise location of the current frame proposals in other frame feature maps is unknown, making the simple method infeasible.</p><p>In fact, there are already many previous works <ref type="bibr" target="#b36">(Zhu et al. 2017a;</ref><ref type="bibr" target="#b30">Wu et al. 2019;</ref><ref type="bibr" target="#b31">Xiao and Jae Lee 2018;</ref><ref type="bibr" target="#b22">Liu and Zhu 2018;</ref><ref type="bibr" target="#b2">Chen et al. 2018;</ref><ref type="bibr" target="#b7">Deng et al. 2019a</ref>) attempting to exploit temporal information in videos. Some works try to utilize image level information from nearby frames to help detection. For example, FGFA <ref type="bibr" target="#b36">(Zhu et al. 2017a</ref>), MANet <ref type="bibr" target="#b28">(Wang et al. 2018a</ref>) utilize optical flow <ref type="bibr" target="#b9">(Dosovitskiy et al. 2015)</ref>, and STSN <ref type="bibr" target="#b0">(Bertasius, Torresani, and Shi 2018)</ref> applies deformable convolutions <ref type="bibr" target="#b5">(Dai et al. 2017)</ref> for image feature calibration. PS ROI Pooling <ref type="bibr" target="#b4">(Dai et al. 2016</ref>) is used to pool features for proposals from the calibrated image feature map in these methods. The extracted PS ROI features contain temporal information from nearby frames. However, these methods can only utilize nearby frames within one second (usually at most 30 frames). Performance will degrade with longer time interval, making it hard to leverage information from frames that are far apart in time. Other works attempt to utilize proposal level information from longer video length. SELSA <ref type="bibr" target="#b30">(Wu et al. 2019)</ref> and <ref type="bibr" target="#b25">(Shvets et al. 2019)</ref> aggregate the high-level proposal features (fully connection layer features of proposals) with each other in order to make every proposal features in current frame contain the highlevel proposal features from other frames. However, the ROI features are still extracted from a single image.</p><p>In this paper, a novel operator, named as Temporal ROI Align, is proposed to exploit the temporal information for the same object instance in a video. As shown in <ref type="figure">Fig. 1 (b)</ref>, we define a target frame as a frame where final prediction is done at the moment. The target frame is allowed to have multiple support frames, which are used for strengthening the features of the target frame. <ref type="bibr">Considering</ref>  the same object instance are highly similar among frames in a video, the proposed operator implicitly extracts the most similar ROI features from support frames feature maps for target frame proposals based on feature similarity. The extracted most similar ROI features contain the temporal information of the same object instance in a video. The key challenge now is how to effectively aggregate these ROI features. One simple and obvious way is to average them. However, it is suboptimal due to the fact that an object instance may be blurry in some frames and clear in other frames. It is obvious that the ROI features of clear object instances should play a more important role than features of blurry object instances during aggregation. Therefore, temporal attention mechanism is conducted to aggregate the ROI features and the most similar ROI features. The proposed operator also inherits the advantage of SELSA <ref type="bibr" target="#b30">(Wu et al. 2019)</ref> and <ref type="bibr" target="#b25">(Shvets et al. 2019</ref>) which leverage long-range temporal information of a video. It can also be applied into other video object detectors and other video tasks of computer vision, such as video instance segmentation (VIS).</p><p>The contributions of the paper are summarized as follows: 1. A novel Temporal ROI Align operator is proposed to offer an alternative for the conventional ROI Align operator in videos. It can extract temporal information from an entire video with arbitrary length for a proposal.</p><p>2. We integrate the proposed Temporal ROI Align into single-frame video detectors and other state-of-the-art video detectors, and demonstrate through quantitative experiments that the proposed Temporal ROI Align operator can consistently and significantly boost the performance.</p><p>3. Experiments show that the proposed Temporal ROI Align can also be used in other video tasks of computer vision, such as video instance segmentation (VIS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Object Detection In Still Images</head><p>Most state-of-the-art object detectors <ref type="bibr" target="#b13">(Girshick et al. 2014;</ref><ref type="bibr" target="#b12">Girshick 2015;</ref><ref type="bibr" target="#b23">Ren et al. 2015;</ref><ref type="bibr" target="#b4">Dai et al. 2016)</ref> usually adopt two stage framework composed of a proposal generator and a region classifier. Fast R-CNN <ref type="bibr" target="#b12">(Girshick 2015)</ref> develops a ROI Pooling layer to extract features of proposals as the input of region classifier. Faster R-CNN <ref type="bibr" target="#b23">(Ren et al. 2015)</ref> proposes Region Proposal Network (RPN) to combine proposal generation with region classification into one framework. The methods mentioned above usually adopt ROI Align  or ROI Pooling to extract features for proposals within a single image. As a consequence, when applying these single image detectors to VID, the features extracted from ROI Align lack the temporal information in videos.</p><p>Recently, self-attention mechanisms <ref type="bibr" target="#b27">(Vaswani et al. 2017</ref>) have been proven to be beneficial for single image recognition. Object relation module ) is proposed to model the proposal relationship within a single image.  proposes a method which can extract features for proposals using the features of a whole image rather than only the features covered by proposals. Different from these methods which attempt to interact high-level features among objects (e.g. computer vs mouse in )), we focus on extracting features from a video for the same object instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection In Videos</head><p>Existing methods usually perform image-level or proposallevel feature aggregation to exploit temporal information from a video for the same object instance.</p><p>Image-level features across frames. There has been a line of research that boosts the single-frame detector by utilizing image-level features. D&amp;T <ref type="bibr" target="#b11">(Feichtenhofer et al. 2017</ref>) builds a dense correlation map between two feature maps of nearby video frames, and exploits instance track ids to learn frame-to-frame motion of bounding boxes. DFF <ref type="bibr" target="#b37">(Zhu et al. 2017b</ref>) firstly uses optical flow to propagate and align the features of selected keyframes to nearby non-keyframes in order to reduce redundant calculation and speed up the single-frame detector. FGFA <ref type="bibr" target="#b36">(Zhu et al. 2017a</ref>) utilizes optical flow to aggregate the nearby frames feature maps to target frame feature map in order to improve the single-frame detector. Furthermore, ) designs more advanced image-level feature propagation and keyframe selection mechanisms to achieve a better trade-off between accuracy and speed than FGFA. In order to avoid explicit optical flow computation, STSN <ref type="bibr" target="#b0">(Bertasius, Torresani, and Shi 2018)</ref> proposes using deformable convolution to compute the offsets for feature alignment. <ref type="bibr" target="#b29">(Wang et al. 2018b)</ref> proposes non-local module in order to completely drop the locality of estimated correspondences. However, the performance of these methods degrades quickly with longer time interval. In contrast to these methods, the proposed Temporal ROI Align doesn't perform image-level feature alignment and aggregation, and can take advantage of longer video length (even the entire video) than these methods.</p><p>Proposal-level features across frames. Another line of research attempts to aggregate the proposal-level features across frames. MANet <ref type="bibr" target="#b28">(Wang et al. 2018a</ref>) firstly utilizes optical flow to propagate current frame proposals to nearby frames, then aggregates the features of proposals from multi-  <ref type="figure">Figure 2</ref>: Temporal ROI Align. Firstly, the ROI features X t is extracted from target frame feature map F t for target frame proposal in module (a). Then, Most Similar ROI Align (MS ROI Align) extracts the most similar ROI features X t+i from support frame feature map F t+i for target frame proposal in module (b). Finally, Temporal Attentional Feature Aggregation (TAFA) performs temporal attention to aggre-</p><formula xml:id="formula_0">gate {X t+i } T /2 i=?T /2 in module (c)</formula><p>frames. RDN <ref type="bibr" target="#b8">(Deng et al. 2019b)</ref> introduces the object relation module from still images into videos. It builds two stage relation modules to fully exploit the relationships among proposals. As mentioned above, the performance of these methods degrades quickly with longer time interval. Subsequent works attempt to overcome the shortcomings, i.e. previous works can't leverage long-range temporal information in videos. SELSA <ref type="bibr" target="#b30">(Wu et al. 2019</ref>) firstly generates proposals of multi-frames, and aggregates the high-level proposal features with each other. <ref type="bibr" target="#b25">(Shvets et al. 2019</ref>) takes a further step. It develops a loss function which constraints the the feature aggregation conducted among the proposals belong to the same object instance. Although these methods perform high-level proposals' feature aggregation to utilize the temporal information in videos, they still adopt ROI Align operation to extract the ROI features for proposals, making the extracted ROI features lack temporal information from videos. In contrast to these methods, the proposed Temporal ROI Align can directly extract the temporal information from videos for proposals. Besides, the proposed operator can also be applied into these methods to further boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal ROI Align</head><p>The way to extract the features of proposals is crucial in videos. Existing methods usually adopt ROI Align operation to implement it. However, ROI Align is designed to extract the features of proposals from a single image features, making the extracted ROI features lack the temporal information for video tasks. We propose Temporal ROI Align operator as an alternative to make the features of proposals contain temporal information of videos. Given a set of frames {I t+i } T /2 i=?T /2 from the same video, a set of feature maps {F t+i } T /2 i=?T /2 is extracted by a backbone network g cnn (? ; ? cnn ): </p><formula xml:id="formula_1">F t+i = g cnn (I t+i ; ? cnn )<label>(1)</label></formula><formula xml:id="formula_2">+ ( ) { + ( )} =1 Softmax { ? + ( )} =1 { + ( )} =1 { + ( )} =1</formula><p>Weighted Sum ? + ( ) <ref type="figure">Figure 3</ref>: The technical detailed illustration of MS ROI Align in the proposed Temporal ROI Align where t denotes the index of target frame and t + i (i = 0) denotes the index of support frames. T is the total number of support frames. As shown in <ref type="figure">Fig. 2</ref>, firstly, the ROI features X t is extracted for target frame proposals from target frame feature map F t by the conventional ROI Align operator in module (a). Then, Most Similar ROI Align (MS ROI Align) focuses on extracting the most similar ROI features X t+i from support frame feature map F t+i for target frame proposals in module (b). Specifically, similarity maps are calculated between F t+i and each spatial location of X t . For each similarity map, we find the top K similarity scores as the most similar points, and project these points into F t+i . Based on these points, the most similar features f t+i can be extracted from F t+i . f t+i is weightedly summed by the normalized top K similarity scores to generate the most similar ROI features X t+i . Finally, Temporal Attentional Feature Aggregation (TAFA) utilizes temporal attention to aggregate {X t+i } T /2 i=?T /2 in order to generate the final temporal ROI features X t in module (c). The temporal ROI features X t contain the object features from target frame and support frames in a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most Similar ROI Align</head><p>MS ROI Align aims at extracting the most similar ROI features from support frames feature maps for target frame proposals. Considering the features of an object instance are highly similar among different frames, we propose to implicitly extract the ROI features from support frames feature maps for target frame proposals based on feature similarity.</p><p>As shown in <ref type="figure">Fig. 3</ref>, the input is ROI features X t ? R h?w?C and the support frame feature map F t+i ? R H?W ?C . h, w and C denote the height, width and channel of X t respectively. H and W denote the height and width of F t+i respectively. One spatial location X t (m) is used to describe the technical details, where m denotes the spatial location of X t and m ? {(1, 1), (1, 2), ? ? ? , (h, w)}. Other spatial locations of X t are performed in the same way.</p><p>Firstly both X t (m) and F t+i are L2 normalized along with the channel dimension to generate X t (m) and F t+i . Then, cosine similarity map S t+i (m) ? R H?W is calculated as follows:</p><formula xml:id="formula_3">St+i(m) = S( Xt(m), Ft+i) : R 1?C ?[R (H?W )?C ] T ? R H?W<label>(2)</label></formula><p>where ? denotes matrix multiplication and [?] T denotes matrix transposition. Then, the top K similarity scores {s k t+i (m)} K k=1 ? R K and corresponding spatial locations {p k t+i (m)} K k=1 ? R K?2</p><formula xml:id="formula_4">{ + } =? /2 /2 One Temporal Attention Block Split Concat ? t { + } =? /2 /2 (?) { ( + )} =? /2 /2 ( ) dot product { + ? } =? /2 /2 Softmax { ? + ? } =? /2 /2</formula><p>Weighted Sum ? One Temporal Attention Block</p><p>One Temporal Attention Block <ref type="figure">Figure 4</ref>: Temporal Attentional Feature Aggregation in the proposed Temporal ROI Align are found from the similarity map S t+i (m) ? R H?W . The locations {p k t+i (m)} K k=1 are projected into the support frame feature map F t+i in order to extract the most similar features</p><formula xml:id="formula_5">{f k t+i (m)} K k=1 for X t (m). f t+i (m) is defined as the set of {f k t+i (m)} K k=1 and f t+i (m) ? R K?C . Finally, the similarity scores {s k t+i (m)} K k=1 are used to weight the f t+i (m): s k t+i (m) = exp(s k t+i (m)) K k=1 exp(s k t+i (m)) (3) f t+i (m) = K k=1 s k t+i (m)f k t+i (m) (4) where s k t+i (m) is the normalized weight of s k t+i (m). f t+i (m) ? R C is the weighted most similar features of X t (m).</formula><p>Since there are total h?w spatial locations of ROI features X t , the final most similar ROI features X t+i ? R h?w?C can be extracted by concatenating all f t+i (m). The most similar ROI features X t+i can be regraded as the ROI features extracted from support frame feature map F t+i for the target frame proposals, since X t+i is the most similar features with X t in each spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Attentional Feature Aggregation</head><p>In the MS ROI Align, we have already extracted the ROI features from target frame feature map and support frames feature maps. The remaining key challenge now becomes how to effectively aggregate these ROI features. Since an object instance may be blurry in some frames and clear in other frames, it is natural to learn a group of temporal attention weights to aggregate them. Besides, multi-head attention <ref type="bibr" target="#b27">(Vaswani et al. 2017</ref>) allows model to jointly attend information from different representation subspaces at different channels. Therefore, we constitute multi temporal attention blocks to deal with different patterns in the temporal feature aggregation.</p><p>As shown in <ref type="figure">Fig. 4</ref>, the input is the set of ROI features </p><formula xml:id="formula_6">C N : n C N ]<label>(5)</label></formula><p>where X n t+i ? R h?w? C N and n ? {1, 2, ? ? ? , N }. Each {X n t+i } T /2 i=?T /2 is used to generate one attention map:</p><formula xml:id="formula_7">w n t+i?t (m) = ? n (X n t+i )(m) ? ? n (X n t )(m) (6) w n t+i?t (m) = exp(w n t+i?t (m)) T /2 i=?T /2 exp(w n t+i?t (m))<label>(7)</label></formula><p>where ? n (?) is a tiny embedding network (network weights are not shared across different temporal attention blocks) and ? n (X n t+i ) ? R h?w? C N . m denotes the spatial position of ? n (X n t+i ) and ? n (X n t+i )(m) ? R C N . w n t+i?t ? R h?w and w n t+i?t (m) is the attention weights from t + i frame to t frame of the n-th group attention map on spatial location m. w n t+i?t (m) is the normalized attention weights across frames. Based on the normalized n-th group of temporal attention map {w n t+i?t } T /2 i=?T /2 , the n-th group of ROI features {X n t+i } T /2 i=?T /2 are weightedly summed as follows:</p><formula xml:id="formula_8">X n t (m) = T /2 i=?T /2 w n t+i?t (m)X n t+i (m)<label>(8)</label></formula><p>where X n t ? R h?w? C N . The final temporal ROI features X t can be obtained by concatenating all X n t along the channel dimension and X t ? R h?w?C . The X t is the same size as X t , but it contains the temporal information of the same object instance in a video.</p><p>In some cases like object disappears in some support frames, similar features of MS ROI Align from these supporting frames may be "incorrect". The proposed TAFA can alleviate the issue by the attention weight. The "incorrect" features can be suppressed by the softmax operator if "correct" features (i.e. object features) can be extracted from other support frames. Overall, the MS ROI Align is responsible for extracting the "correct" object features as far as possible. If it fails, the TAFA will minimize the adverse influence of the "incorrect" features by the attention weight. The MS ROI Align with TAFA guarantees that the proposed Temporal ROI Align works on most cases.</p><p>Temporal ROI Align is designed as a general operator to extract better features for objects, and it can also work under some special cases. For example, when an object in target frame is partial occlusion, Temporal ROI Align can still extract the object features from support frames for most spatial locations without occlusion. Therefore, the visible parts are dominant and features from these locations can still get enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments On VID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DataSet And Evaluation Metric</head><p>Experiments are carried out on the ImageNet VID dataset <ref type="bibr" target="#b24">(Russakovsky et al. 2015)</ref> which contains 30 object categories for video object detection. There are total 3862 video snippets in the training set and 555 video snippets in the validation set. The mAP@IoU=0.5 is reported on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Backbone Network. The ResNet-101  (R101) is used as the backbone network g cnn (? ; ? cnn ) for ablation studies. ResNeXt-101-64x4d <ref type="bibr" target="#b32">(Xie et al. 2017</ref>) (X101) is also used for the final results. The stride of the first conv block in the conv5 stage of convolutional layers is modified from 2 to 1 in order to enlarge the resolution of feature maps. As such, the effective stride in the stage is changed from 32 pixels to 16 pixels. All the 3 ? 3 conv layers in the stage are modified by the dilated convolutions to compensate the receptive fields. Region Proposal Network. RPN is placed on the output of conv4 to generate proposals. There are a total of 12 anchors with 4 scales {64 2 , 128 2 , 256 2 , 512 2 } and 3 aspect ratios {1 : 2, 1 : 1, 2 : 1}. 300 proposals are produced on each image. Temporal ROI Align. We replace the conventional ROI Align operator with the proposed Temporal ROI Align operator. The Temporal ROI Align is applied on the ouput of conv5. The spatial size h, w of the ROI features are set to 7. For MS ROI Align, there are a total of 49 similarity maps needed to be calculated for each proposal, and the top 4 (K = 4) similarity scores and corresponding spatial locations are selected for each similarity map. For TAFA, a total of 4 (N = 4) temporal attention blocks are used to aggregate the ROI features and these most similar ROI features, and each ? n (?) is a 3 ? 3 convolution layer. Detection Network. Two 1024-d fully connected layers are applied upon the temporal ROI features followed by classification and bounding box regression. Training and Testing Details. The backbone networks are initialized with ImageNet pre-trained weights. A total of 6 epochs of SGD training is performed with a total batch size of 16 on 16 GPUs. The initial learning rate is 0.02 and is divided by 10 at the 4-th and 6-th epoch. The models are trained with a mixture of ImageNet VID and ImageNet DET datasets <ref type="bibr" target="#b24">(Russakovsky et al. 2015</ref>) (using 30 VID classes and the overlapping 30 of 200 DET classes). For fair comparison, we use the split provided in FGFA <ref type="bibr" target="#b36">(Zhu et al. 2017a)</ref>. At most 15 frames are subsampled from each video, and the DET:VID balance is approximately 1:1. For training, one training frame is sampled along with two random frames from the same video. For inference, T frames (support frames) from the same video are sampled along with the inference frame (target frame). <ref type="figure">Following STSN [1]</ref>, if support frames are beyond the video start/end, we copy the first/last frame of the video. NMS with a threshold of 0.5 is adopted to suppress reduplicate detection boxes. In both training and inference, the images are resized to a shorter side of 600 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Support Frame Sampling Strategies. The sampling strategy for support frame is crucial for video object detection. Here, we conduct experiments to analyse the effect of the number of support frames T and sampling frame strides S. Specifically, by using a sampling frame stride S, one frame in every S frames is used for testing rather than consecutive frames. Firstly, we fix the stride S to 1 and vary the number of T to analyse the effect of the number of consecutive support frames. As shown in <ref type="figure" target="#fig_1">Fig. 5 (a)</ref>, the single-frame detector baseline, which only uses the conventional ROI Align to extract features from target frame feature map, achieves 74.0 mAP without any support frames. As the number of support frames increases, the performance can be improved consistently and finally stabilized at 78.0 mAP after T = 26. It means that using more consecutive support frames usually can bring more performance gains.</p><p>Then, we fix the number of support frames T to 26 and vary the number of S to explore the effect of the fixed sampling stride. As shown in <ref type="figure" target="#fig_1">Fig. 5 (b)</ref>, the performance increases steadily with S raising and finally stabilizes on 79.5 mAP after S = 7. It indicates that sampling support frames with a larger stride can further improve the performance since longer-range temporal information can be leveraged, which also coincides the results in <ref type="bibr" target="#b25">(Shvets et al. 2019)</ref> and <ref type="bibr" target="#b30">(Wu et al. 2019)</ref>.</p><p>Finally, to further utilize the whole video information, we uniformly sample the support frames from the entire video, i.e. the sampling stride S is adaptive to the length of the video according to the number of the support frames T . <ref type="figure" target="#fig_1">Fig.  5 (c)</ref> shows the performance with different values of T . We can see that with 2 support frames that are uniformly sampled from the entire video, the performance already achieves 78.7, which outperforms the 26 consecutive support frames by 0.7 point. With 6 support frames that are uniformly sampled from the entire video, the performance can achieve the same results with 26 support frames that are sampled by the fixed sampling frame stride 8. The performance keeps increasing as T rises and finally stabilizes at 80.5 mAP after T = 14. These results demonstrate that the proposed Temporal ROI Align can utilize long-range temporal information from videos. The uniform sampling strategy with T = 14 is used as the default setting in the following experiments. The Effectiveness of MS ROI Align and TAFA. MS ROI Align in the proposed Temporal ROI Align is designed to find the location corresponding to the proposal of target frame from the support frame feature maps. Inspired by STSN <ref type="bibr" target="#b0">(Bertasius, Torresani, and Shi 2018)</ref>    <ref type="bibr" target="#b35">(Zhu et al. 2019</ref>) (MD ROI Align) to show the effectiveness of MS ROI Align. The MD ROI Align can calculate the offsets between target frame and support frames in order to extract the ROI features from support frames feature maps for target frame proposals. As shown in <ref type="table">Table 1</ref>, "MD ROI Align + TAFA" achieves 76.6 mAP, which only exceeds the singleframe baseline by 2.6 points. The reason behind this limited improvement lies in that deformable operation can't learn the effective offsets when the frame stride is large, which is also shown in the results of STSN <ref type="bibr" target="#b0">(Bertasius, Torresani, and Shi 2018)</ref> where the performance degrades from 78.9 to 77.9 when the frame stride changes from 1 to 4. Compared to "MS ROI Align + TAFA", the performance of "MD ROI Align + TAFA" drops around 4 points, which demonstrates the superiority of the designed MS ROI Align.</p><p>To demonstrate the effectiveness of TAFA for feature aggregation, we also conduct a simple baseline where the ROI features from target frame and support frames are directly averaged. The experiment results are shown in <ref type="table">Table 1</ref>. We can see that "MS ROI Align+ Averaging" only achieves 78.5 mAP, which is lower than "MS ROI Align+ TAFA" by 2 points. This demonstrates that the temporal attention design in TAFA can learn to put more weights on better ROI features. The Effectiveness of Temporal ROI Align. Since Non-Local <ref type="bibr" target="#b29">(Wang et al. 2018b)</ref> can also extract the temporal information from support frames, we also attempt to integrate ROI Align with Non-Local module in order to show the superiority of Temporal ROI Align. As shown in Table 2, the mAP is only 77.9 when applying Non-Local after ROI Align. Non-local inevitably aggregates background features into the ROI features, since the attention weights of Non-Local are computed from all spatial locations in support frames. This leads to the inferior mAP of ROI Align + Non-Local. We also modify the original Non-Local by pick-   <ref type="table">Table 4</ref>: The effect of different number of the temporal attention blocks N in TAFA of Temporal ROI Align ing the Top K attention weights to eliminate the background features as far as possible. However, the result (78.4) is still lower than the proposed Temporal ROI Align (80.5) by 2.1 points. This shows the superior performance of the proposed Temporal ROI Align. Experiments on Hyperparameters. We analyse the effect of two hyperparameters K and N , standing for the number of the most similar locations on each similarity map and the number of the temporal attention blocks respectively. <ref type="table" target="#tab_6">Table 3</ref> shows the effect of different K. With the K increasing from 1 to 4, the performance increases from 79.3 mAP to 80.5 mAP. This shows that the extracted most similar ROI features can benefit from more sampling location K. However, the performance seems to have a downward trend when K is larger than 4, since using large K may find the features of unrelated location, especially for small objects. Therefore, K is chosen to 4 as default setting. <ref type="table">Table 4</ref> shows the effect of using different number of the temporal attention blocks N in TAFA. With the N increasing from 1 to 4, the performance increases from 79.5 mAP to 80.5 mAP. This shows that using more temporal attention blocks can improve the accuracy. The performance saturates when N is bigger than 4. Therefore, N is chosen to 4 as default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison With State-of-the-art Methods</head><p>The proposed Temporal ROI Align offers an alternative for the conventional ROI Align operator. Therefore, it is easy to be implemented into other video detectors. We reproduce the newly proposed SELSA <ref type="bibr" target="#b30">(Wu et al. 2019</ref>) due to its effectiveness, simpleness and also the state-of-the-art performance. As shown in <ref type="table" target="#tab_8">Table 5</ref>, SELSA-ReIm denotes the reimplementation of SELSA, and it basically achieves the same performance with SELSA <ref type="bibr" target="#b30">(Wu et al. 2019)</ref> under ResNet-101 and ResNeXt-101 backbones. When replacing ROI Align by the proposed Temporal ROI Align, the performance further improves by 1.7 points and 1.3 point respectively, which demonstrates the flexibility of the proposed operator. <ref type="table">Table 6</ref> summarizes the performance of the proposed method and other state-of-the-art models on the ImageNet VID validation set. Without bells and whistles, the proposed Temporal ROI Align can achieve 80.5 mAP with ResNet-101 backbone when applied to single-frame detector baseline. When applied to SELSA <ref type="bibr" target="#b30">(Wu et al. 2019)</ref>, the performance further improves to 82.0 mAP. Furthermore, by using   <ref type="table">Table 6</ref>: Performance comparison with other state-of-theart models on the ImageNet VID validation set. denotes using heavy video post-processing methods like Seq-NMS <ref type="bibr">(Han et al. 2016)</ref>. TROI denotes the proposed Temporal ROI Align a stronger ResNeXt-101 backbone, Temporal ROI Align applied to SELSA achieves 84.3 mAP, which is the top one among existing methods that also adopt ResNeXt-101 backbone and do not use any post-processing techniques. Besides, our method uses less support frames (14 frames) than MEGA (35 frames), and the pipeline of our method is more simple than MEGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Experiments On EPIC KITCHENS</head><p>ImageNet VID dataset falls short in the density and diversity of objects. Therefore we evaluate Temporal ROI Align on the EPIC KITCHENS dataset <ref type="bibr" target="#b6">(Damen et al. 2018)</ref>. EPIC contains 290 classes and is far more complex and challenging. 272 video sequences captured in 28 kitchens are used for training. 106 sequences collected in the same 28 kitchens (S1) and 54 sequences collected in other 4 unseen kitchens (S2) are used for evaluation.</p><p>The results is shown in <ref type="table" target="#tab_10">Table 7</ref>. Note that the performance of SELSA-ReIm (38.8 and 36.7) is higher than SELSA <ref type="bibr">(Wu et al. 2019) (38.0 and 34.8)</ref>. When SELSA-ReIm is equipped with the proposed Temporal ROI Align, Method mAP@0.5 (S1) mAP@0.5 (S2) SELSA <ref type="bibr" target="#b30">(Wu et al. 2019)</ref> 38   Since the evaluation on the test set is currently closed, the evaluation is performed on the validation set. MaskTrack R-CNN <ref type="bibr" target="#b33">(Yang, Fan, and Xu 2019)</ref> is based on Mask R-CNN, and builds another branch (track head) to link the same object instances between two frames. Mask-Track R-CNN uses the conventional ROI Align to extract features from single-frame for proposals, and we replace the ROI Align with the proposed Temporal ROI Align. The results are shown in <ref type="table" target="#tab_11">Table 8</ref>. We can see that the Temporal ROI Align consistently improves ResNet-50-FPN and ResNeXt-101-FPN baselines on all metrics involving AP, AP 50 and AP 75 (e.g. 3.2 points in AP of ResNet-50-FPN and 3.1 points in AP of ResNeXt-101-FPN), which further demonstrates the flexibility of proposed Temporal ROI Align.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization and Analysis</head><p>We visualize the most similar positions found by MS ROI Align and the corresponding averaged attention weights across different temporal attention blocks in TAFA. <ref type="figure" target="#fig_2">Fig. 6 (a)</ref> shows the object-related position in the 7 ? 7 ROI features of the proposal. We can see that the red points are localized in the region of the same object instance, and the proposed TAFA can learn reasonable attention weights.  For example, in the third row of <ref type="figure" target="#fig_2">Fig. 6 (a)</ref>, the objects are hard to be classified in the target frames due to motion blur. However, the object is clear in some support frames (e.g. the 3-rd row, 3-rd column in <ref type="figure" target="#fig_2">Fig. 6 (a)</ref>) and the MS ROI Align successfully locates the object-related points (red points). The attention weights of points belonging to clear object (e.g. 0.247 in the 3-rd row, 3-rd column of <ref type="figure" target="#fig_2">Fig. 6 (a)</ref>) are also larger than attention weights of points belonging to blurry object (e.g. 0.067 in the 3-rd row, 1-st column of <ref type="figure" target="#fig_2">Fig. 6 (a)</ref>). After the most similar features from red points in the support frames being integrated with the original ROI features by the reasonable attention weights in TAFA, it is much easier for the classifier to make the correct classification. <ref type="figure" target="#fig_2">Fig. 6 (b)</ref> shows the background-related position in the 7 ? 7 ROI features of the proposal. We can see that most red points are localized in the surrounding region of the same object instance (e.g. the 1-rd row, 3-rd column in <ref type="figure" target="#fig_2">Fig. 6 (b)</ref>), which can be viewed as the context information of the object instance. If the red points are localized in the background region which is far away from object instances (e.g. the 1-rd row, 5-th column in <ref type="figure" target="#fig_2">Fig. 6 (b)</ref>), the corresponding attention weights in TAFA are very small (e.g. 0.032 in the 1-rd row, 5-rd column of <ref type="figure" target="#fig_2">Fig. 6 (b)</ref>), making the irrelevant background features less influence the original ROI features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, a novel Temporal ROI Align is proposed to offer an alternative for the conventional ROI Align in video object detection. Compared with ROI Align which only extracts features of proposals from single-frame, the proposed operator can extract features from the entire video for proposals, making the extracted features contain long-range temporal information and much easier to perform classification for region classifier. The proposed operator is flexible and also validated effectively on different baselines, different datasets and different vision tasks about videos. We hope the proposed operator be applied into other video-related tasks, such as multi object tracking, in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>/2 . There are N temporal attention blocks to aggregate {X t+i } T /2 i=?T /2 . Firstly, {X t+i } T /2 i=?T /2 are split into N groups along the channel dimension: X n t+i = X t+i [:, :, (n ? 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Ablation for different support frame sampling strategies. (a) The effect of different number of support frames T . The frame stride S is fixed to 1. (b) The effect of different sampling frame stride S. The number of support frames T is fixed to 26. (c) The effect of different number of support frames T . The support frames T are uniformly sampled from the entire video</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>An illustration of the most similar positions found by MS ROI Align and the corresponding averaged attention weights across different temporal attention blocks in TAFA (zoom in for a better view). The support frames T is set to 4 during inference for visualization. The first column is the target frames and other columns are support frames. The dash green box is a proposal in target frames. The solid green box is the region projected into the target frame from the object-related position(a) or background-related position (b) in the 7 ? 7 ROI features of the proposal. The red points in support frames are the most similar positions corresponding to the object-related position (a) or background-related position (b) of the 7 ? 7 ROI features in target frame. Only the top 1 similar position rather than top 4 similar positions is visualized for a clear visualization. The floating numbers below each row of images are the averaged attention weights in TAFA corresponding to the object-related position (a) or background-related position (b) in the 7 ? 7 ROI features of the proposal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the features of arXiv:2109.03495v2 [cs.CV] 11 Sep 2021</figDesc><table><row><cell></cell><cell>Target Frame</cell><cell cols="2">Support Frame</cell></row><row><cell></cell><cell></cell><cell cols="2">Feature Similarity</cell></row><row><cell>ROI Features</cell><cell cols="2">ROI Features Temporal Attention</cell><cell>Most Similar ROI Features</cell></row><row><cell></cell><cell cols="2">Temporal ROI Features</cell></row><row><cell>(a) ROI Align</cell><cell cols="3">(b) Temporal ROI Align</cell></row><row><cell cols="4">Figure 1: The Illustration of Temporal ROI Align. (a) The</cell></row><row><cell cols="4">conventional ROI Align. The spatial size of ROI features is</cell></row><row><cell cols="4">set to 2 ? 2 for convenience. (b) Based on feature similarity,</cell></row><row><cell cols="4">Temporal ROI Align implicitly extracts the most similar ROI</cell></row><row><cell cols="4">features from support frames feature maps for target frame</cell></row><row><cell cols="4">proposals. Then, temporal attention mechanism is conducted</cell></row><row><cell cols="4">to aggregate the ROI features and the most similar ROI fea-</cell></row><row><cell cols="4">tures in order to generate final temporal ROI features</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Performance comparison with Non-local and Tem-</cell></row><row><cell>poral ROI Align</cell></row><row><cell>recently proposed Modulated Deformable ROI Align</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The effect of different the most similar K locations in MS ROI Align of Temporal ROI Align</figDesc><table><row><cell>#N</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="7">mAP(%) 79.5 80.1 80.5 80.5 80.4 80.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">: Applying the Temporal ROI Align (TROI) into</cell></row><row><cell cols="3">SELSA video detector. Note that the SELSA-ReIm denotes</cell></row><row><cell>the reimplementation of SELSA</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>mAP(%)</cell></row><row><cell>FGFA (Zhu et al. 2017a)</cell><cell>R101</cell><cell>78.4</cell></row><row><cell>D&amp;T (Feichtenhofer et al. 2017)</cell><cell>R101</cell><cell>80.0</cell></row><row><cell>PLSA (Guo et al. 2019)</cell><cell>R101+DCN</cell><cell>80.0</cell></row><row><cell>SELSA (Wu et al. 2019)</cell><cell>R101</cell><cell>80.25</cell></row><row><cell>Leveraging (Shvets et al. 2019)</cell><cell>R101-FPN</cell><cell>81.0</cell></row><row><cell>RDN (Deng et al. 2019b)</cell><cell>R101</cell><cell>81.8</cell></row><row><cell>MEGA (Chen et al. 2020)</cell><cell>R101</cell><cell>82.9</cell></row><row><cell>Single-frame Detector + TROI</cell><cell>R101</cell><cell>80.5</cell></row><row><cell>SELSA (Wu et al. 2019) + TROI</cell><cell>R101</cell><cell>82.0</cell></row><row><cell>D&amp;T (Feichtenhofer et al. 2017)</cell><cell>X101</cell><cell>81.6</cell></row><row><cell>SELSA (Wu et al. 2019)</cell><cell>X101</cell><cell>83.11</cell></row><row><cell>RDN(Deng et al. 2019b)</cell><cell>X101</cell><cell>83.2</cell></row><row><cell>Leveraging (Shvets et al. 2019)</cell><cell>X101-FPN</cell><cell>84.1</cell></row><row><cell>MEGA (Chen et al. 2020)</cell><cell>X101</cell><cell>84.1</cell></row><row><cell>MEGA (Chen et al. 2020)</cell><cell>X101</cell><cell>85.4</cell></row><row><cell>Single-frame Detector + TROI</cell><cell>X101</cell><cell>82.3</cell></row><row><cell>SELSA (Wu et al. 2019) + TROI</cell><cell>X101</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison on EPIC KITCHENS test set. S1 and S2 indicate Seen and Unseen splits, respectively. SELSA-ReIm denotes the reimplementation of SELSA. TROI denotes Temporal ROI Align</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>MT R-CNN</cell><cell>R50-FPN</cell><cell>30.3</cell><cell>51.1</cell><cell>32.6</cell></row><row><cell>MT-CNN + TROI</cell><cell>R50-FPN</cell><cell>33.5</cell><cell>57.0</cell><cell>36.6</cell></row><row><cell>MT R-CNN</cell><cell cols="2">X101-FPN 34.9</cell><cell>58.8</cell><cell>36.5</cell></row><row><cell cols="3">MT R-CNN + TROI X101-FPN 38.0</cell><cell>63.3</cell><cell>40.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Applying the Temporal ROI Align to MaskTrack</cell></row><row><cell>R-CNN (MT R-CNN) in VIS. R50 denotes ResNet-50. AP</cell></row><row><cell>denotes mask AP which follows the COCO evaluation met-</cell></row><row><cell>ric to use 10 IoU thresholds from 50% to 95% at step 5%</cell></row><row><cell>the mAP@0.5 further improves to 42.2 and 39.6 by 3.4 and</cell></row><row><cell>2.9 points, respectively. This demonstrates that Temporal</cell></row><row><cell>ROI Align can bring more performance gains on more com-</cell></row><row><cell>plex video objection dataset compared with ImageNet VID.</cell></row><row><cell>Extension: Experiments On VIS</cell></row><row><cell>We also investigate Temporal ROI Align on video instance</cell></row><row><cell>segmentation (VIS). VIS aims at simultaneously detecting,</cell></row><row><cell>segmenting and tracking object instances in a video se-</cell></row><row><cell>quence. The dataset of VIS is YouTube-VIS (Yang, Fan,</cell></row><row><cell>and Xu 2019) which contains 40 object categories. It con-</cell></row><row><cell>tains 2238 training videos, 302 validation videos, and 343</cell></row><row><cell>test videos. The training is performed on the training videos.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Natural Science Foundation of China (Grant No. U20B2047, 62002336) and Exploration Fund Project of University of Science and Technology of China (YD3480002001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="331" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimizing video object detection via a scale-time lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7814" to="7823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Memory Enhanced Global-Local Aggregation for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maria Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object Guided External Memory Network for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6678" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation Distillation Networks for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7023" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Progressive Sparse Local Attention for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3909" to="3918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seq-nms for video object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5686" to="5695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveraging Long-Range Temporal Relationships Between Proposals for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9756" to="9764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence Level Semantics Aggregation for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video object detection with an aligned spatial-temporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7210" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
