<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianben</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Institution on Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
							<email>zhangrc@act.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Institution on Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Institution on Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
							<email>ymao@uottawa.ca</email>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning has achieved remarkable success in representation learning via self-supervision in unsupervised settings. However, effectively adapting contrastive learning to supervised learning tasks remains as a challenge in practice. In this work, we introduce a dual contrastive learning (DualCL) framework that simultaneously learns the features of input samples and the parameters of classifiers in the same space. Specifically, DualCL regards the parameters of the classifiers as augmented samples associating to different labels and then exploits the contrastive learning between the input samples and the augmented samples. Empirical studies on five benchmark text classification datasets and their lowresource version demonstrate the improvement in classification accuracy and confirm the capability of learning discriminative representations of DualCL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning is at the heart of modern deep learning. In the context of unsupervised learning, contrastive learning <ref type="bibr">[Hadsell et al., 2006]</ref> has been recently demonstrated as an effective approach to obtain generic representations for downstream tasks <ref type="bibr">[He et al., 2020;</ref><ref type="bibr" target="#b0">Chen et al., 2020]</ref>. Briefly, unsupervised contrastive learning adopts a loss function that forces representations of different "views" of the same example to be similar and representations of different examples to be distinct. Recently the effectiveness of contrastive learning is justified in terms of simultaneously achieving both "alignment" and "uniformity" <ref type="bibr" target="#b9">[Wang and Isola, 2020]</ref>.</p><p>Such a contrastive learning approach has also been adapted to supervised representation learning <ref type="bibr">[Khosla et al., 2020]</ref>, in which a similar contrastive loss is used which insists the representations of examples in the same class to be similar and those for different classes to be distinct. However, despite its demonstrated successes, such an approach appears much less principled, compared with unsupervised contrastive learning. For example, the uniformity of the representations is no longer valid; neither is it required. In fact, we argue that the standard supervised contrastive learning approach is not natural for * Corresponding author:zhangrc@act.buaa.edu.cn <ref type="bibr">Standard Contrastive Learning (before)</ref> Dual Contrastive Learning (before)</p><p>Standard Contrastive Learning (after) Dual Contrastive Learning (after) supervised representation learning. This is at least manifested by the fact that the outcome of this approach does not give us a classifier directly and one is required to develop another classification algorithm to solve the classification task. This paper aims at developing a more natural approach to contrastive learning in the supervised setting. A key insight in our development is that supervised representation learning ought to include learning two kinds of quantities: one is the feature z of the input x in an appropriate space that is sufficiently discriminative for the classification task, and the other is a classifier on that space, or alternatively the parameter ? of the classifier acting on that space; we will refer to this classifier as the "one-example" classifier for example x. In this view, it is natural to associate with each example x two quantities, a vector z ? R d for an appropriate feature space dimension d and a matrix ? ? R d?K defining a linear classifier for x (assuming a K-class classification problem), note that both z and ? depend on x. The representation learning problem in the supervised setting can be regarded as learning to generate the pair (z, ?) for an input example x.</p><p>For the classifier ? to be valid for feature z, we only need to align the softmax transform of ? T z with the label of x using the standard cross-entropy loss. In addition, a contrastive learning approach can be used to force constraints on these (z, ?) representations across examples. Specifically, let ? * denote the column of ? corresponding to the ground-truth label of x, we may design two contrastive losses. The first loss contrasts (z, ? * ) with many (z , ? * )s, where z is the feature of an example having a different label as x; the second contrasts (z, ? * ) with many (z, ? * )s, where ? is the classifier associate with an example from a different class. We refer to this learning framework as dual contrastive learning <ref type="bibr">(DualCL)</ref>.</p><p>Despite that we propose dual contrastive learning based on a conceptual insight, we argue that it is possible to also interpret such a learning scheme as exploiting a unique data augmentation approach. In particular, for each example x, each column of its ? can be regarded as an "label-aware input representation", or an augmented view of x in the feature space with the label information infused. The figures in <ref type="table" target="#tab_0">Table 1</ref> illustrate the benefit of this approach, from the two figures on the left side, it can be seen that standard contrastive learning cannot make use of the label information. On the contrary, from the two figures on the right side, DualCL effectively leverages the label information to classify the input samples in their classes.</p><p>In experiments, we validate the effectiveness of DualCL on five benchmark text classification datasets. By finetuning the pretrained language model (BERT and RoBERTa) using the dual contrastive loss, DualCL achieves the best performance compared to existing supervised baselines with contrastive learning. We also find that DualCL improves the classification accuracy especially on low-resource scenarios. Furthermore, we give some explanations for DualCL through visualizing the learned representations and attention maps.</p><p>Our contributions can be summarized as follows. 1) We propose the dual contrastive learning (DualCL) for naturally adapting the contrastive loss to supervised settings.</p><p>2) We introduce the label-aware data augmentation to obtain multiple views of input samples for the training of DualCL.</p><p>3) We empirically verify the effectiveness of the DualCL framework on five benchmark text classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Consider a text classification task with K classes. We assume that the given dataset {x i , y i } N i=1 contains N training samples, where x i ? R L is the input sentence consisting of L words and y i ? {1, 2, ? ? ? , K} is the label assigned to the input. Throughout this study, we denote the set of indexes of the training samples by I = {1, 2, ? ? ? , N } and the set of indexes of the labels by K = {1, 2, ? ? ? , K}.</p><p>Before introducing our method, we look at the family of self-supervised contrastive learning, whose effectiveness has been widely confirmed in many studies. Given N training samples {x i } N i=1 with a number of augmented samples, where each sample has at least one augmented sample in the dataset. Let j(i) be the index of the augmented one derived from the i-th sample, the standard contrastive loss is defined as:</p><formula xml:id="formula_0">L self = 1 N i?I ? log exp(z i ? z j(i) /? ) a?Ai exp(z i ? z a /? )<label>(1)</label></formula><p>where z i is the normalized representation of x i , A i := I \ {i} is the set of indexes of the contrastive samples, the ? symbol denotes the dot product and ? ? R + is the temperature factor. Here, we define the i-th sample as an anchor, the j(i)-th sample is a positive sample and the remaining N ? 2 samples are negative samples regarding to the i-th sample.</p><p>However, self-supervised contrastive learning is unable to leverage the supervised signals. Previous study <ref type="bibr">[Khosla et al., 2020]</ref> incorporates supervision to contrastive learning in a straightforward way. It simply takes the samples from the same class as positive samples and the samples from different classes as negative samples. The following contrastive loss is defined for supervised tasks:</p><formula xml:id="formula_1">L sup = 1 N i?I 1 |P i | p?Pi ? log exp(z i ? z p /? ) a?Ai exp(z i ? z a /? )<label>(2)</label></formula><p>where P i := {p ? A i : y p = y i } is the set of indexes of positive samples, and |P i | is the cardinality of P i . Although this approach has shown its superiority, we still need to learn a linear classifier using the cross-entropy loss apart from the contrastive term. This is because the contrastive loss can only learn generic representations for the input examples. Thus, we argue that the supervised contrastive learning developed so far appears to be a naive adaptation of unsupervised contrastive learning to the classification problem. One may expect a more elegant approach for contrastive learning in supervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dual Contrastive Learning</head><p>In this paper, we propose a supervised contrastive learning approach that learns "dual" representations. The first one is the input representation of discriminative features for the classification task in an appropriate space, and the second one is a classifier, or equivalently the parameter of the classifier in that space. Let z i ? R d be the feature of an input example x i and ? i ? R d?K be the classifier associating to x i . Our aim is to learn (normalized) representations of z i and ? i to align the softmax transform of ? T i z i with the label of x i using the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Label-Aware Data Augmentation</head><p>In order to obtain different views of the training samples, we utilize the idea of data augmentation to obtain the representations of feature z i and classifier ? i . We regard the k-th column of ? i as a unique view of input example x i associated with label k, denoted by ? k i . We call ? k i as the label-aware input representation since it is an augmented view of x i with the information of label k infused.</p><p>Here, we introduce label-aware data augmentation. Noting that we do not actually introduce additional samples, we get the K + 1 views for each sample just in one single feedforward procedure. Specifically, we use a pretrained encoder to learn 1 feature representation and K label-aware input representations, where K is the number of classes. Pretrained language models (PLMs) show remarkable performance in extracting natural language representations. Thus, we adopt PLMs as the pretrained encoder. Let the pretrained encoder (e.g. BERT, RoBERTa) be f . We feed both the input sentence and all the possible labels to encoder f and regard each label as one token. Specifically, we list all the labels {1, ? ? ? , K} and insert them before the input sentence x i . This process forms a new sequence r i ? R L+K . Then an encoder f is exploited to extract features of each token in this sequence.  We take the feature of the [CLS] token as the representation of each input sentence, and the feature of the token corresponding to each label as the label-aware input representation. We denote the feature representation of input x i by z i and the label-aware input representation of label k by ? k i . In practice, we take the name of labels as the tokens to form sequence r i , such as "positive", "negative", etc. For the labels containing multiple words, we take the mean-pooling of the token features to obtain the label-aware input representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual Contrastive Loss</head><p>With the feature representation z i and the classifier ? i for input example x i , we try to align the softmax transform of ? T i z i with the label of x i . Let ? * i denote the column of ? i , corresponding to the ground-truth label of x i . We expect the dot product ? * T i z i is maximized. Thus, we turn to learn a better representation of ? i and z i with supervised signals.</p><p>Here we define the dual contrastive loss to exploit the relation between different training samples, which tries to maximize</p><formula xml:id="formula_2">? * T i z j if x j has same label with x i while minimizing ? * T i z j if x j carries a different label with x i .</formula><p>Given an anchor z i originating from the input example x i , we take {? * j } j?Pi as positive samples and {? * j } j?Ai\Pi as negative samples and define the following contrastive loss:</p><formula xml:id="formula_3">L z = 1 N i?I 1 |P i | p?Pi ? log exp(? * p ? z i /? ) a?Ai exp(? * a ? z i /? )<label>(3)</label></formula><p>where ? ? R + is the temperature factor, A i := I \ {i} is the set of indexes of the contrastive samples and P i := {p ? A i : y p = y i } is the set of indexes of positive samples, and |P i | is the cardinality of P i . Similarly, given an anchor ? * i , we can also take {z j } j?Pi as positive samples and {z j } j?Ai\Pi as negative samples and define another contrastive loss:</p><formula xml:id="formula_4">L ? = 1 N i?I 1 |P i | p?Pi ? log exp(? * i ? z p /? ) a?Ai exp(? * i ? z a /? )<label>(4)</label></formula><p>The dual contrastive loss is a combination of the above two contrastive loss terms:</p><formula xml:id="formula_5">L Dual = L z + L ? (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Training &amp; Prediction</head><p>To fully exploit the supervised signal, we also expect ? i is a good classifier for z i . Thus we use a modified version of the cross-entropy loss to maximize ? * T i z i for each input example x i :</p><formula xml:id="formula_6">L CE = 1 N i?I ? log exp(? * i ? z i ) k?K exp(? k i ? z i )<label>(6)</label></formula><p>Finally, we minimize the two training objectives to train encoder f . The two objectives simultaneously improve the quality of the representations of the features and the classifiers. The overall loss should be:</p><p>L overall = L CE + ?L Dual (7) where ? is a hyperparameter that controls the influence of the dual contrastive loss term.</p><p>In classification, we use the trained encoder f to generate the feature representation z i and the classifier ? i for an input sentence x i . Here ? i can be seen as a "one-example" classifier for example x i . We regard the argmax result of ? T i z i as the model prediction:? <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the framework of the dual contrastive learning, where e CLS is the feature representation, e POS and e NEG are the classifier representations. In this concrete example, we assume that the target sample with "positive" class serves as the anchor, and there is a positive sample having the same class label and a negative sample having a different class label. The dual contrastive loss is designed to simultaneously attract the feature representations to the classifier representations between positive samples, and repel the feature representation to the classifier between negative samples.  <ref type="table">Table 2</ref>: Accuracy on the test set. The models are trained with 10% of the training data or with full training data. We reproduce the results with the same hyperparameter configurations for all baselines for a fair comparison and report average accuracy across 10 different random seeds.</p><formula xml:id="formula_7">i = arg max k (? k i ? z i )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Duality between Representations</head><p>The contrastive loss adopts the dot product function as a measurement of the similarity between representations. This brings a dual relationship between the feature representation z and the classifier representation ? in DualCL. A similar phenomenon appears in the relationship between the input feature and the parameter in the linear classifiers. Then, we can regard the ? as the parameter of a linear classifier such that the pre-trained encoder f may generate a linear classifier for each input sample. Thus DualCL naturally learns how to generate a linear classifier for each input sample to perform the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Theoretical Justification of DualCL</head><p>A theoretical justification of dual contrastive learning is given in this subsection.</p><formula xml:id="formula_8">Let X = {x i } N i=1 and Y = {y i } N i=1</formula><p>are the inputs and labels of N training samples. The following theorem holds for dual contrastive learning. Theorem. Assume that there is a constant such that p(x i , y i ) ? holds for all i ? I and p(yj |xi)</p><formula xml:id="formula_9">p(yj ) ? ?(x i , y j ): MI(X , Y) ? log N ? L Dual (9)</formula><p>where ? is a symmetric function that can have different definitions. In our case, ?(x i , y j ) = (exp(? * i ? z j )+exp(? * j ? z i ))/2. It can be found that minimizing the dual supervised contrastive loss is equivalent to maximizing the mutual information between the inputs and labels. Please see Appendix for detailed proof.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct our experiments on following five benchmark text classification datasets. <ref type="bibr">SST-2 [Socher et al., 2013</ref>] is a sentiment classification dataset of movie reviews. SUBJ <ref type="bibr" target="#b7">[Pang and Lee, 2004</ref>] is a review dataset with sentence labelled as subjective or objective. TREC <ref type="bibr" target="#b3">[Li and Roth, 2002]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>To adapt the input format to the BERT-family pretrained language models, for each input sentence, we list the names of  all the labels as a token sequence and insert it before the input sentence, and split them with a special [SEP] token. We also insert a special [CLS] token in the front of the input sequence and append a [SEP] token at the end of the input sequence. Both BERT and RoBERTa employ the position embeddings to make use of the order of the tokens in the input sequence, thus the class labels will be associated with the position embeddings if they are listed in a fixed order. In order to mitigate the influence of label orders, we randomly change the order of the labels before forming the input sequence in the training phase. During the test phase, the label order remains unchanged.</p><p>We use the AdamW <ref type="bibr" target="#b5">[Loshchilov and Hutter, 2018]</ref> optimizer to finetune the pretrained BERT-base-uncased and RoBERTa-base model <ref type="bibr" target="#b10">[Wolf et al., 2019]</ref> with a 0.01 weight decay. We train the model for 30 epochs and use a linear learning rate decay from 2 ? 10 ?5 to 10 ?5 . We set the dropout rate to 0.1 for all layers and the batch size to 64 for all datasets. For the hyperparameters, we adopt a grid search strategy to choose the best ? in {0.01, 0.05, 0.1}. The temperature factor ? is chosen as 0.1. Our PyTorch implementation is available at: https://github.com/hiyouga/Dual-Contrastive-Learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>We compare DualCL with three supervised learning baselines: the model trained with cross-entropy loss (CE), with both the cross-entropy loss and the standard supervised contrastive loss (CE+SCL) <ref type="bibr">[Gunel et al., 2021]</ref>, with both the cross-entropy loss and the self-supervised contrastive loss (CE+CL) <ref type="bibr" target="#b0">[Gao et al., 2021]</ref>. The results are shown in <ref type="table">Table 2</ref>.</p><p>From the results, it can be seen that DualCL with both BERT and RoBERTa encoders achieves the best classification performance in almost all settings, except on the TREC dataset where RoBERTa is employed. Compared to CE+CL with full training data, the average improvement of DualCL is 0.46% and 0.39% on BERT and RoBERTa, respectively. Furthermore, we observe that with 10% training data, DualCL outperforms the CE+CL method by a larger margin, which is 0.74% and 0.51% higher on BERT and RoBERTa, respectively. Meanwhile, CE and CE+SCL cannot surpass the performance of DualCL. This is because the CE method neglects the relation between the samples and the CE+SCL method cannot directly learn a classifier for the classification tasks.</p><p>In addition, we find that the dual contrastive loss term helps the model to achieve better performance on all five datasets. It shows that leveraging the relations between samples helps the model to learn better representations in contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>To investigate how dual contrastive learning improves the quality of representations, we draw the tSNE plots of learned representations on the SST-2 test set. We use RoBERTa as the encoder and finetune the encoder with 25 training samples per class. We show the results of CE, DualCL without L Dual and DualCL in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we can find that DualCL learns representations both for the input samples and the classifier associating to each sample. Comparing <ref type="figure" target="#fig_1">Figure 2(b)</ref> with <ref type="figure" target="#fig_1">Figure 2(c)</ref>, we observe that the dual contrastive loss helps the model to learn more discriminative and robust representations for the input features and the classifiers, by exploiting the relation between training samples and imposing additional constraints to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effects in Low-Resource Scenarios</head><p>In DualCL, it uses the label-aware input representation as another view of the input samples. Thus we conjecture that the label-aware data augmentation is supposed to serve for the lowresource scenarios. To validate this, we perform experiments on the SST-2 and SUBJ datasets in low-resource scenarios. We evaluate the model performance using N training samples per class, where N ? <ref type="bibr">{5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100}.</ref> We sketch the results of BERT trained with CE, DualCL without L Dual and DualCL in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z K D W P LJ K W K D Y H E H H Q D S U H G LF W D E O\ K H D U W Z D U P LQ J W D OH LV V X I I X V H G Z LW K F R P S OH [ LW</head><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we can see that DualCL significantly surpasses the CE method on the reduced datasets. Specifically, the improvement is up to 8.5% on the SST2 dataset and 5.4% on the SUBJ dataset with only 5 training samples per class. Even without the dual contrastive loss, the label-aware data augmentation can consistently improve the model's performance on the reduced dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>To validate whether DualCL can capture informative features, we compute the attention score between the feature of [CLS] token and each word in the sentence. We firstly finetune the RoBERTa encoder on the full training set. Then we compute the 2 distance between the features and visualize the attention map in <ref type="figure" target="#fig_3">Figure 4</ref>. It shows that when classifying sentiments, the captured features are different. The above example comes from the SST-2 dataset, we can see that our model pays higher attention to "predictably heart warming" for the sentence expressing a "positive" sentiment. The below example comes from the CR dataset, we can see that our model pays higher attention to "small" for the sentence expressing a "negative" sentiment. On the contrary, the CE method fails to concentrate on these discriminative features. The results suggest that our DualCL can successfully attend to the informative keywords in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text Classification</head><p>Text classification is a typical task of categorizing texts into groups, including sentiment analysis, question answering, etc. Due to the unstructured nature of the text, extracting useful information from texts can be very time-consuming and inefficient. With the rapidly development of deep learning, neural network methods such as RNN <ref type="bibr" target="#b1">[Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b0">Chung et al., 2014]</ref> and CNN <ref type="bibr" target="#b2">[Kim, 2014;</ref><ref type="bibr" target="#b12">Zhang et al., 2015]</ref> have been widely explored for efficiently encoding the text sequences. However, their capabilities are limited by the computational bottlenecks and the problem of long-term dependencies. Recently, large-scale pre-trained language models (PLMs) based on transformers <ref type="bibr">[Vaswani et al., 2017]</ref> has emerged as the art of text modeling. Some of these auto-regressive PLMs include GPT <ref type="bibr" target="#b8">[Radford et al., 2018]</ref> and XLNet <ref type="bibr" target="#b11">[Yang et al., 2019]</ref>, auto-encoding PLMs such as <ref type="bibr">BERT [Devlin et al., 2019]</ref>, <ref type="bibr">RoBERTa [Liu et al., 2019]</ref> and <ref type="bibr">ALBERT [Lan et al., 2019]</ref>. The stunning performance of PLMs mainly comes from the extensive knowledge in the large scale corpus used for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Contrastive Learning</head><p>Despite the optimality of the cross-entropy in supervised learning, a large number of studies have revealed the drawbacks of the cross-entropy loss, e.g., vulnerable to noisy labels <ref type="bibr">[Zhang and Sabuncu, 2018]</ref>, poor margins <ref type="bibr" target="#b0">[Elsayed et al., 2018]</ref> and weak adversarial robustness <ref type="bibr" target="#b7">[Pang et al., 2019]</ref>. Inspired by the InfoNCE loss <ref type="bibr" target="#b6">[Oord et al., 2018]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, from the perspective of text classification tasks, we propose a dual contrastive learning approach, DualCL, for solving the supervised learning tasks. In DualCL, we simultaneously learn two kinds of representations with the pretrained language models. One is the discriminative feature of the input examples and another is a classifier for that example. We introduce the label-aware data augmentation to generate different views of the input samples, containing the feature and the classifier. Then we design a dual contrastive loss to make the classifier to be valid for the input feature. The dual contrastive loss leverages the supervised signal between the training samples to learn better representations. We validate the effectiveness of dual contrastive learning through extensive experiments. DualCL successfully achieves state-of-the-art performance on five benchmark text classification datasets. We also explain the mechanism inside dual contrastive learning by visualizing the learned representations. Finally, we find DualCL is capable of improving the model performance in low-resource datasets. Further exploration on dual contrastive learning in other supervised learning tasks, such as image classification and graph classification will be done in the future.</p><formula xml:id="formula_10">MI(X , Y) ? log N ? L Dual (10)</formula><p>where ? is a symmetric function that can have different definitions. In our case, ?(x i , y j ) = (exp(? * i ? z j )+exp(? * j ? z i ))/2.</p><p>Proof. Let ?(x i , y j ) = (?(x i , y j ) + ?(x j , y i ))/2 and M i = N j=1 p(yj |xi) p(yj ) , where ?(x i , y j ) = exp(? * i ? z j ). We assume that 1 |Pi| p?Pi ?(x i , y p ) = ?(x i , y i ) when |P i | is sufficiently large. We have:</p><formula xml:id="formula_11">MI(X , Y) = 1 N N i=1 N j=1 p(x i , y j ) log p(y j |x i ) p(y j ) = 1 N N i=1 N j=1 p(x i , y j ) log p(y j |x i ) p(y j )M i + log M i = 1 N p(x i , y i ) log ?(x i , y i ) N t=1 ?(x i , y t ) + 1 N j =i p(x i , y j ) log ?(x i , y j ) N t=1 ?(x i , y t ) + log N ? log N + N N i=1 log ?(x i , y i ) N t=1 ?(x i , y t ) = log N + N N i=1 1 |P i | p?Pi log ?(x i , y p ) N t=1 ?(x i , y t ) ? log N + N N i=1 1 |P i | p?Pi log ?(x i , y p ) N t=1 ?(x i , y t ) + N N i=1 1 |P i | p?Pi log</formula><p>?(x p , y i ) N t=1 ?(x t , y i ) = log N ? L Dual This proves that the negative dual contrastive loss is a lower bound of the mutual information MI(X , Y). Thus, when we minimize the dual contrastive loss, the mutual information between inputs and labels is accordingly maximized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework of the proposed dual contrastive learning (DualCL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The tSNE plots of the learned representations on the SST-2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Test accuracy on the SST2 (left) and SUBJ (right) datasets in low-resource scenarios of DualCL with or without L Dual compared with cross-entropy using BERT. The batch size here is set to 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The visualization of attention map for CE and DualCL. The darker blue refers to higher attention scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, contrastive learning [Hadsell et al., 2006] has been widely used in unsupervised learning to learn good generic representations for downstream tasks. For example, [He et al., 2020] leverages a momentum encoder to maintain a look-up dictionary for encoding the input examples. [Chen et al., 2020] produces multiple views of the input example using data augmentations as the positive samples, and compare them to the negative samples in the datasets. [Gao et al., 2021] similarly dropouts each sentence twice to generate positive pairs. In the supervised scenario, [Khosla et al., 2020] clusters the training examples by their labels to maximize the similarity of representations of training examples within the same class while minimizing ones between different classes. [Gunel et al., 2021] extends supervised contrastive learning to the natural language domain with pretrained language models. [Lopez-Martin et al., 2022] studies the network intrusion detection problem using well-designed supervised contrastive loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The concept of our proposed dual contrastive learning. Each color represent one class. The circle and triangle denote the representation of the input sample and the classifier respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>05?0.24 93.05?0.25 93.29?0.22 91.09?0.23 86.58?0.29 90.01?0.25 CE+SCL [Gunel et al., 2021] 86.64?0.17 93.20?0.16 93.70?0.24 91.46?0.23 88.14?0.26 90.63?0.21 CE+CL 87.66?0.28 94.27?0.21 94.20?0.29 91.67?0.27 87.72?0.32 91.10?0.27 DualCL w/o L dual 87.90?0.19 93.50?0.18 94.01?0.31 91.83?0.22 88.13?0.30 91.07?0.24 DualCL 88.40?0.20 94.50?0.21 94.93?0.23 92.36?0.16 89.01?0.28 91.84?0.22 RoBERTa CE 90.91?0.23 94.03?0.19 94.51?0.21 90.65?0.20 92.06?0.27 92.43?0.22 CE+SCL [Gunel et al., 2021] 91.00?0.29 94.37?0.30 94.85?0.24 90.82?0.20 92.32?0.25 92.67?0.26 CE+CL 91.04?0.17 94.47?0.19 95.68?0.26 91.90?0.14 92.55?0.28 93.13?0.21 DualCL w/o L dual 92.48?0.18 94.40?0.17 95.18?0.16 91.50?0.14 92.88?0.20 93.29?0.17 DualCL 92.67?0.21 94.78?0.19 95.36?0.18 92.17?0.20 93.24?0.24 93.64?0.20 full training data BERT CE 91.19?0.23 96.40?0.19 97.21?0.20 95.06?0.14 92.09?0.24 94.39?0.20 CE+SCL [Gunel et al., 2021] 91.71?0.20 96.25?0.19 97.58?0.16 95.26?0.13 93.06?0.20 94.77?0.18 CE+CL 91.95?0.22 96.72?0.15 97.80?0.14 95.21?0.11 93.19?0.19 94.97?0.16 Gunel et al., 2021] 93.65?0.20 96.73?0.23 97.18?0.19 95.35?0.19 93.60?0.17 95.30?0.20 CE+CL 94.33?0.21 97.04?0.17 97.52?0.15 95.32?0.10 93.49?0.25 95.54?0.18 DualCL w/o L dual 94.41?0.23 96.79?0.24 97.10?0.25 95.30?0.12 94.01?0.25 95.52?0.22 DualCL 94.91?0.17 97.34?0.19 97.40?0.17 95.59?0.12 94.39?0.23 95.93?0.18</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell>SST-2</cell><cell>SUBJ</cell><cell>TREC</cell><cell>PC</cell><cell>CR</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell cols="2">10% training data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT</cell><cell cols="7">CE 86.DualCL w/o L dual 91.99?0.15 96.78?0.13 97.70?0.19 95.30?0.15 93.14?0.19 94.97?0.16</cell></row><row><cell></cell><cell>DualCL</cell><cell cols="6">92.40?0.17 97.20?0.17 98.22?0.17 95.56?0.14 93.78?0.17 95.43?0.16</cell></row><row><cell></cell><cell>CE</cell><cell cols="6">94.09?0.24 96.60?0.21 97.10?0.20 95.10?0.19 93.41?0.24 95.26?0.22</cell></row><row><cell></cell><cell>CE+SCL [</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics for the five text classification datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Theorem. Assume that there is a constant such that p(x i , y i ) ? holds for all i ? I and p(yj |xi) p(yj ) ? ?(x i , y j ):</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Raia Hadsell, Sumit Chopra, and Yann LeCun</title>
		<editor>EMNLP, 2021. [Gunel et al., 2021] Beliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Khosla et al., 2020] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan</editor>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim ; Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth ; Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<meeting><address><addrLine>Roberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning over prototype-label embeddings for network intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="200" to="228" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter ; Ilya Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Aaron van den Oord, Yazhe Li, and Oriol Vinyals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee ; Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, and Jun Zhu</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ?ukasz Kaiser, and Illia Polosukhin</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isola</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8792" to="8802" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
