<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
							<email>aviv.shamsian@biu.ac.il</email>
							<affiliation key="aff2">
								<orgName type="department">Bar</orgName>
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
							<email>farleylai@nec-labs.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
							<email>longzh@google.com</email>
							<affiliation key="aff3">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Keypoint-only group activity recognition ? Compositional- ity ? Multiscale representations ? Transformer ? Video understanding</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Group Activity Recognition detects the activity collectively performed by a group of actors, which requires compositional reasoning of actors and objects. We approach the task by modeling the video as tokens that represent the multi-scale semantic concepts in the video. We propose COMPOSER, a Multiscale Transformer based architecture that performs attention-based reasoning over tokens at each scale and learns group activity compositionally. In addition, prior works suffer from scene biases with privacy and ethical concerns. We only use the keypoint modality which reduces scene biases and prevents acquiring detailed visual data that may contain private or biased information of users. We improve the multiscale representations in COMPOSER by clustering the intermediate scale representations, while maintaining consistent cluster assignments between scales. Finally, we use techniques such as auxiliary prediction and data augmentations tailored to the keypoint signals to aid model training. We demonstrate the model's strength and interpretability on two widely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up to +5.4% improvement with just the keypoint modality 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure" target="#fig_5">Fig. 1. (a)</ref> <p>The keypoint-only setup generalizes better for GAR. The Volleyball Olympic split [102] ensures videos having vastly different scene background between training and testing, which can examine GAR model's scene generalization ability. RGB-based methods severely suffer from scene biases and have poor model generalizability. (b) Main idea. We propose COMPOSER that uses keypoint only modality for GAR by modeling a video as tokens that represent the multiscale semantic concepts in the video, which include keypoint, person, person-to-person interaction, person group, object if present, and the clip. Four scales are formed by grouping actor-related tokens according to their semantic hierarchy. Representations of tokens in coarser scales are learned and aggregated from tokens of the finer scales. COMPOSER <ref type="figure" target="#fig_6">(Fig. 3</ref>) facilitates compositional reasoning of group activity in videos.</p><p>The task requires addressing two challenges. First, GAR requires a compositional understanding of the scene <ref type="bibr" target="#b2">[3]</ref>. Because of the crowded scene, it is challenging to learn meaningful representations for GAR over the entire scene <ref type="bibr">[108]</ref>. Since group activity often consists of sub-group(s) of actors and scene objects, the final action label depends on a compositional understanding of these entities <ref type="bibr">[108,</ref><ref type="bibr">116]</ref>. Second, GAR benefits from relational reasoning over scene elements to understand the relative importance of entities and their interactions <ref type="bibr">[41,</ref><ref type="bibr">113]</ref>. For example, in a volleyball game, persons around the ball performing the jumping action are more important than others standing in the scene.</p><p>Existing work has proposed to jointly learn the group activity with individual actions <ref type="bibr">[47,</ref><ref type="bibr">90,</ref><ref type="bibr" target="#b48">84,</ref><ref type="bibr">45,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5]</ref> or person sub-groups <ref type="bibr" target="#b29">[65,</ref><ref type="bibr" target="#b39">75,</ref><ref type="bibr">27]</ref> for a compositional understanding of the group activity. Meanwhile, graph <ref type="bibr">[117,</ref><ref type="bibr">46,</ref><ref type="bibr">107,</ref><ref type="bibr">41]</ref> and transformer <ref type="bibr">[29,</ref><ref type="bibr" target="#b29">65]</ref> based models have been proposed for relational reasoning over scene entities. However, these methods do not sufficiently make use of the multiscale scene elements in the GAR task by modeling over entities at either one semantic scale (e.g., person <ref type="bibr">[29,</ref><ref type="bibr">117,</ref><ref type="bibr">107,</ref><ref type="bibr">41]</ref>) or two scales (person and person group <ref type="bibr" target="#b29">[65,</ref><ref type="bibr" target="#b39">75,</ref><ref type="bibr">27]</ref>, or keypoint and person <ref type="bibr" target="#b46">[82]</ref>). More importantly, explicit multiscale modeling is neglected, lacking consistent compositional representations for the group action tasks. Furthermore, majority of the prior GAR methods rely on the RGB modality (see <ref type="table">Table.</ref> 3), which causes the model more likely to have privacy and ethical issues when deployed in real-world applications <ref type="bibr">[39]</ref>. Last but not least, the RGB input hinders the model's robustness to changes in background, lighting conditions or textures, and often results in poor model generalizability due to scene biases (see <ref type="figure" target="#fig_5">Fig. 1 (a)</ref>) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">94]</ref>.</p><p>In this paper, we present COMPOSER that addresses compositional learning of entities in the video and relational reasoning about these entities. Inspired by how humans are particularly adept at representing objects in different granularities meanwhile reasoning their interactions to turn sensory signals into a high-level knowledge <ref type="bibr">[43,</ref><ref type="bibr" target="#b24">60]</ref>, we approach GAR by modeling a video as tokens that represent the multi-scale semantic concepts in the video ( <ref type="figure" target="#fig_5">Fig. 1 (b)</ref>). Compared to the aforementioned prior works, we consider more fine-grained scene entities that are grouped into four scales. By combining the scales together with Multiscale Transformer <ref type="figure">(Fig. 4)</ref>, COMPOSER provides attention-based reasoning over tokens at each scale, which makes the higher-level understanding of the group activity possible. Moreover, COMPOSER uses only the keypoint modality. Using only the 2D (or 3D) keypoints as input, our method can prevent the sensor camera from acquiring detailed visual data that may contain private or biased information of users 2 . Keypoints also allow the model to focus on the actionspecific cues, and help the model be more invariant to the scene biases. COMPOSER generalizes much better to testing data with different scene backgrounds (see the Volleyball Olympic split results in <ref type="table">Table.</ref> 1).</p><p>COMPOSER learns consistent multiscale representations which boost the performance for GAR ( <ref type="figure" target="#fig_0">Fig. 2</ref>). This is achieved by contrastive clustering assignments of clips. Intuitively, a model can recognize the group activity using representations of entities at just one particular scale. Hence, we consider representations of the clip token learned across scales as representations of different views of the clip. Such perspective allows us to cluster clip representations learned at all scales while enforcing consistency between cluster assignments produced from different scales of the same clip. In order to enforce this consistency, we follow <ref type="bibr" target="#b10">[11]</ref> and use a swapped prediction mechanism where we predict the cluster assignment of a scale from the representation of another scale. However, distinct from related works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>, which use information from multiple augmentations or modalities for self-supervised learning from unlabelled images or videos, we use information from multiple scales for the task of group activity recognition. Contrasting clustering assignments enhance our intermediate representations and the overall performance. Finally, we use techniques such as auxiliary prediction at each scale and data augmentation methods such as Actor Dropout to aid training.</p><p>Our contributions are three-fold: 3) to learn consistent multiscale representations for GAR. This is achieved by clustering clip representations learned at all scales. The clustering objective encourages an "agreement" between scales on the high-level knowledge learned ('Pull Close' representations of the same clip). Contrastive learning is performed on the clusters, which also helps the model to discriminate between clips with different semantic characteristics ('Pull Close' representations of the semanticallysimilar clips and 'Push Apart' those that are semantically-different). In the illustration, we use subscript to denote the scale and use superscript to indicate different clips.</p><p>1. We present COMPOSER for compositional reasoning of group activity in videos. COMPOSER can distill and convey high-level semantic knowledge from the elementary elements of the human-centered videos. We learn contrastive clustering assignment to improve the multiscale representations. By maintaining a consistent cluster assignment across the multiple scales of the same clip, an agreement between scales on the high-level knowledge learned can be promoted to optimize the representations across scales. 2. We use only the keypoint modality that allows COMPOSER to address the privacy and ethical concerns and to be robust to changes in background, with auxiliary prediction and data augmentation methods tailored to learning group activity from the keypoint modality. <ref type="bibr" target="#b2">3</ref>. We demonstrate the model's strength and interpretability on two commonlyused datasets (Volleyball and Collective Activity) and COMPOSER achieves up to +5.4% improvement using just the keypoint modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Much of the recent research on GAR explores how to capture the actor relations <ref type="bibr">[46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">107,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b47">83]</ref>. Several works tackle this problem from a graph-based perspective <ref type="bibr">[46,</ref><ref type="bibr" target="#b35">71,</ref><ref type="bibr">113,</ref><ref type="bibr">112]</ref>. Some utilize attention modeling <ref type="bibr" target="#b48">[84,</ref><ref type="bibr">109,</ref><ref type="bibr" target="#b35">71,</ref><ref type="bibr">116]</ref> including using Transformers <ref type="bibr">[29,</ref><ref type="bibr" target="#b29">65]</ref>. Existing works have primarily used RGB-and/or optical-flow-based features with RoIAlign [36] to represent actors [112, <ref type="bibr" target="#b48">84,</ref><ref type="bibr">107,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Object token(s) <ref type="figure" target="#fig_6">Fig. 3</ref>. COMPOSER. Given tokens that represent the multiscale semantic concepts ( <ref type="figure" target="#fig_5">Fig. 1</ref>) in the human-centered video, COMPOSER jointly learns group activity, individual actions and contrastive clustering assignments of clips. Auxiliary predictions are enforced to aid training (Sec. 3.5).</p><p>A few recent works replace or augment these features with keypoints/poses of the actors [102, <ref type="bibr" target="#b46">82,</ref><ref type="bibr">29,</ref><ref type="bibr">116]</ref>. In this paper, we use only the light-weight coordinatebased keypoint representation. We propose a Multiscale Transformer block to hierarchically reason about entities at different semantic scales and we aid learning group activities by improving the musicale representations. Please see an in-depth discussion on related works in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We present COMPOSER <ref type="figure" target="#fig_6">(Fig. 3</ref>), a novel Multiscale Transformer based architecture for GAR. In Sec. F.2, we describe the multi-scale semantic tokens representing a video with group activities. We introduce COMPOSER and especially its reasoning module Multiscale Transformer in Sec. 3.2. We describe data augmentations in Sec. 3.4 and the exact formulation of auxiliary prediction in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tokenizing a Video as Hierarchical Semantic Entities</head><p>We model a video as semantic tokens that allow our method easily adaptable to understanding any videos with multi-actor multi-object interactions <ref type="bibr" target="#b36">[72]</ref>.</p><p>? Person Keypoint. We define a person keypoint token, k j p ? R d that represents a keypoint joint j (j = 1, . . . , j ? ) of person p (p = 1, . . . , p ? ) in all timestamps, where j ? is the number of joint types and p ? is the number of actors. The initial d-dimensional person keypoint token is learned by encoding the numerical coordinates (in the image space) of a certain keypoint track 3 . The procedure of encoding includes coordinate embedding, time positional embedding, keypoint type embedding, and OKS-based feature embedding [95] to mitigate the issue of noisy estimated keypoints. Details are available in Appendix F.2.</p><p>? Person. A person token is defined as p p ? R d , initially obtained by aggregating the standardized keypoint coordinates of person p over time through concatenation and FFN-based transformation.</p><p>? Person-to-Person Interaction. Modeling the person-to-person interactions is critical for GAR <ref type="bibr">[108]</ref>. Unlike existing works that typically consider an interaction as an edge connecting two person nodes and learn a scalar to depict its importance [113], we model interaction as nodes (tokens) to allow for the modeling of complex higher-order interactions <ref type="bibr" target="#b36">[72]</ref>. The person-to-person interaction token is defined as</p><formula xml:id="formula_0">i i ? R d where i = 1, . . . , p ? ?(p ? ?1) (bi-directed interactions).</formula><p>Initial representation of the interaction between person p and q is learned from concatenation of p p and p q , followed by FFN-based transformation.</p><p>? Person Group. We define the group token g g ? R d where g = 1, . . . , g ? for videos where sub-groups are often separable. g ? denotes the num. of subgroups in the video. Given the person-to-group mapping which can be obtained through various mechanisms (e.g., heuristics <ref type="bibr" target="#b46">[82]</ref>, k-means <ref type="bibr" target="#b29">[65]</ref>, etc <ref type="bibr">[27,</ref><ref type="bibr" target="#b22">58]</ref>.), representation of a group is an aggregate over representations of persons in the group similarly through concatenation and FFN.</p><p>? Clip. The special [CLS] token (? R d ) is a learnable embedding vector and is considered as the clip representation. CLS stands for classification and is often used in Transformers to "summarize" the task-related representative information from all tokens in the input sequence [26].</p><p>? Object. Scene objects can play a crucial role in videos where human(s) interact with object(s). E.g., in a volleyball game where one person is spiking and multiple nearby actors are all jumping with arms up, it can be difficult to tell which person is the key person with information of just the person keypoints due to their similar poses. The ball keypoints can help to distinguish the key person. Object keypoints can be used to represent an object in the scene with similar benefits of person keypoints (e.g., to boost model robustness <ref type="bibr">[49]</ref>). Object keypoint detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">69]</ref> benefits downstream tasks such as human action recognition [42], object detection <ref type="bibr">[49,</ref><ref type="bibr">115]</ref>, tracking <ref type="bibr" target="#b40">[76]</ref>, etc <ref type="bibr" target="#b23">[59]</ref>. Thus, we use object keypoints to represent each object for GAR. We denote object token e e ? R d where e = 1, . . . , e ? and e ? is the maximal number of objects a video might have. Similar to person tokens, the initial object tokens are learned from aggregating the coordinate-represented object keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiscale Transformer</head><p>Multiscale Transformer takes a sequence of multiple-scale tokens as input, and refines representations of these tokens. Specifically, tokens of the four scales are: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale 1:</head><p>[CLS] , e 1 , ? ? ? , e e ? , k 1 1 , ? ? ? , k j ? p ? , Scale 2: {[CLS] , e 1 , ? ? ? , e e ? , p 1 , ? ? ? , p p ? } ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale 3:</head><p>[CLS] , e 1 , ? ? ? , e e ? , i 1 , ? ? ? , i p ? ?(p ? ?1) ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale 4: {[CLS]</head><p>, e 1 , ? ? ? , e e ? , g 1 , ? ? ? , g g ? } .</p><p>(1)</p><p>We utilize a Transformer encoder [103] at each scale to perform relational reasoning of tokens in that scale. We review details of Transformer in Appendix F.1. Hierarchical representations of tokens are maintained in an elaborately designed Multiscale Transformer block ( <ref type="figure">Fig. 4</ref>). In the Multiscale Transformer block, operations in the four scales are the same (but with different parameters) to maintain simplicity. Specifically, given a sequence of tokens of scale s (Eq. 1), Transformer encoder outputs refined representations of these tokens. Then, concatenation and FFN are used to aggregate refined representations of actor-related tokens, in order to form representations of actor-related tokens in the subsequent coarser scale s+1. Such learned representations are summed with their initial representations (input to the Multiscale Transformer) (i.e. Skip Connection). The resulting actor-related tokens, as well as scale s updated [CLS] token and object token(s) form the input sequence of the Transformer encoder in the scale s+1 (see wiring in <ref type="figure">Fig. 4</ref>).</p><p>COMPOSER uses the initial representations of the multi-scale semantic tokens (Sec. F.2) as input, and utilizes multiple blocks of Multiscale Transformer to perform relational reasoning over these tokens. With refined token representations, COMPOSER jointly learns group activity, individual actions and contrastive clustering of clips (the multitask-learning details are in Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive Clustering for Scale Agreement</head><p>We consider the clip tokens learned at different scales as representations of different views of the clip instance. Then, we cluster clip representations learned in all scales while enforcing consistency between cluster assignments produced from different scales of the clip. This can act as regularization of the embedding space during training ( <ref type="figure" target="#fig_0">Fig. 2)</ref>. To enforce consistency, we use a swapped prediction mechanism <ref type="bibr" target="#b10">[11]</ref> where we predict the cluster assignment of a scale from the representation of another scale. COMPOSER jointly learns GAR and the swapped prediction task to capture an agreement of the common semantic information hidden across the scales. Preliminaries. Suppose v n,s ? R d represents the learned representation of clip n in scale s, where s ? {1, 2, 3, 4}. Following prior works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">52]</ref>, we first project the representation to the unit sphere. We then compute a code (i.e., cluster assignment) q n,s ? R K by mapping v n,s to a set of K trainable prototype vectors, {c 1 , . . . , c K }. We denote by C ? R K?d the matrix whose rows are the c 1 , . . . , c K . Swapped Prediction. Suppose s and w denote 2 different scales from the four representation scales. The swapped prediction problem aims to predict the code q n,s from v n,w , and q n,w from v n,s , with the following loss function:</p><formula xml:id="formula_1">L swap (v n,w , v n,s ) = ? (v n,w , q n,s ) + ? (v n,s , q n,w )<label>(2)</label></formula><p>where ? (v n,w , q n,s ) measures the fit between v n,w and q n,s . ? (v n,w , q n,s ) is the cross entropy loss between q n,s and the probability obtained by taking a softmax of the dot products of v n,w and prototypes in C:</p><formula xml:id="formula_2">? (v n,w , q n,s ) = ? K k=1 q (k) n,s log exp 1 ? v n,w c ? k K k ? =1 exp 1 ? v n,w c ? k ?<label>(3)</label></formula><p>where ? is a temperature parameter. The total loss of the swapped prediction problem is taking Eq. (2) computed over all pairs of scales and all N clips,</p><formula xml:id="formula_3">L cluster = 1 N N n=1 ? ? w,s?{1,2,3,4}&amp;w? =s L swap (v n,w , v n,s ) ? ?<label>(4)</label></formula><p>Online Clustering. This step produces the cluster assignments using the learned prototypes C and the learned clip representations only within a batch, V ? R B?d where B denotes the batch size. We perform the clustering in an online fashion for faster training and use the method proposed in <ref type="bibr" target="#b10">[11]</ref>. Specifically, online clustering yields the codes Q ? R B?K . We compute codes Q such that all examples in a batch are equally partitioned by the prototypes (which prevents the trivial solution where every clip has the same code). Q is optimized to maximize the similarity between the learned clip representations and the prototypes,</p><formula xml:id="formula_4">max Q?Q Tr QCV ? + ?H(Q),<label>(5)</label></formula><formula xml:id="formula_5">Q = Q ? R B?K + | 1 B Q = 1 K 1 K , Q1 ? K = 1 B 1 ? B</formula><p>where the trace Tr is the sum of the elements on the main diagonal, H is the entropy function, and ? is a parameter that controls the smoothness of the mapping. 1 K ? R K and 1 B ? R B are a vector of ones to enforce the equipartition constraint. The continuous solution Q * of Eq. (5) is computed with the iterative Sinkhorn-Knopp algorithm <ref type="bibr">[22,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Augmentation for Keypoint Modality</head><p>We use the following data augmentations to aid training and improve generalization ability of the model learned from the keypoint modality. Actor Dropout is performed by removing a random actor in a random frame, inspired by <ref type="bibr" target="#b41">[77]</ref> that masks agents with probabilities to predict agent behaviors for autonomous driving. We remove actors by replacing the representation of the actor with a zero vector.</p><p>Horizontal Flip is often used by existing GAR methods <ref type="bibr">[119,</ref><ref type="bibr">102,</ref><ref type="bibr" target="#b46">82]</ref>, which is performed on the video frame level. This augmentation causes the pose of each person and positions of (left and right) sub-groups flipped horizontally. We add a small random perturbation on each flipped keypoint.</p><p>Horizontal Move means we horizontally move all keypoints in the clip by a certain number of pixel locations, which is randomly determined per video and bounded by a pre-defined number (i.e., 10). Similarly, afterwards a small random perturbation is applied on each keypoint. Vertical Move is done similar to the Horizontal Move, except we move the keypoints in the vertical direction. Novel practices like Actor Dropout, Horizontal/Vertical Move and random perturbations help the model to perform GAR from noisy estimated keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Auxiliary Prediction and Multitask Learning</head><p>We take the learned representation of the clip at each scale of each Multiscale Transformer block, and perform auxiliary group activity predictions ( <ref type="figure" target="#fig_6">Fig. 3</ref>). Specifically, each of the clip representations learned at each scale of each block is sent as input to the group activity classifier to produce one GAR result. In addition, person representation from the last Multiscale Transformer block is the input to a person action classifier. Meanwhile, the loss of the swapped prediction problem is computed given the learned representations of the clip of all 4 scales from the last Multiscale Transformer block. The total loss is:</p><formula xml:id="formula_6">L total = M ?1 m=1 L groupAux + ? (L groupLast + L person + L cluster )<label>(6)</label></formula><p>where L groupAux represents the loss from Auxiliary Prediction incurred by clip representations at different scales and early blocks of the Multiscale Transformer, L groupLast is from the last Multiscale Transformer block, L person is the person action classification loss, and L cluster is the contrastive clustering loss (Eq. 4). m denotes the index of the Multiscale Transformer block, M is the total number of the Multiscale Transformer blocks, and ? is a hyper-parameter that weights the importance of predictions from the last block. For metric evaluation, we use the clip token from the last scale in the last Multiscale Transformer as input to the group activity classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Arts</head><p>Scene Generalization for Keypoint-only Setup To support the keypointonly setup for GAR, we first compare the generalization capability of models using either RGB or the keypoint modality. In <ref type="table" target="#tab_2">Table 1</ref>, I3D and VGG-16 are two commonly-used image backbone by prior RGB-based GAR methods; the rest are all GAR models (all use VGG-16 as the backbone). On VD Olympic split, the best prior RGB-based method is DIN [117] in <ref type="table" target="#tab_2">Table 1</ref>. We substitute DIN with a COMPOSER variant 4 (Sec. 1) that also consumes RGB input instead of keypoint, and the result is 81.1% which is 2% higher than DIN, suggesting the stronger reasoning strength of COMPOSER, but the accuracy is still low due to the RGB signals. POGARS [102] uses the keypoint  modality and has an accuracy of 89.7%, higher than all RGB-based methods. COMPOSER with the keypoint-only modality obtains 95.1% accuracy and significantly outperforms prior methods, yielding +5.4% improvement. These results imply that the keypoint-only setup can reduce scene biases, and generalize better than approaches relying on the RGB modality to testing data with different visual characteristics from training. We also report the results of these methods that we obtained on VD Original split in <ref type="table" target="#tab_2">Table 1</ref>. From this side-by-side comparison, the difference between the Olympic and Original split is vivid. Current GAR methods have quite saturated performances on the Original split of VD and the results are all very high (more evidence later). Therefore, we recommend readers using the more challenging VD Olympic split for future research on GAR. Note that the COMPOSER that outperforms prior methods in <ref type="table" target="#tab_2">Table 1</ref> is only an ablated version of ours in that not using the object token(s). In addition, GroupFormer <ref type="bibr" target="#b29">[65]</ref> is currently the best-performing method ( <ref type="table">Table 4</ref> in Appendix) and its RGB-only variant has the result of 94.1% accuracy on VD Original split. However, GroupFormer uses additional scene features with the Inception-v3 backbone. <ref type="table" target="#tab_3">Table 2</ref>, we compare COMPOSER with more GAR methods that use only the keypoint modality on VD Original split following conventions. COMPOSER achieves a new SOTA 94.6% accuracy with +0.7% improvement. <ref type="table" target="#tab_9">Table 3</ref>. Comparisons with SOTA methods that use a single or multiple modalities on the original split of VD and CAD. "Flow" denotes optical flow input, and "Scene" denotes features of the entire frames. Fewer modalities indicates a stronger capability of the model itself (fewer checks are better ). The top 3 performance scores are highlighted as: First, Second * , Third . COMPOSER outperforms the latest GAR methods that use a single modality (+0.7% improvement on VD and +2.8% improvement on CAD), and performs favorably compared with methods that exploit multiple expensive modalities Among these methods, Zappardino et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Methods Using Keypoint-only Modality In</head><p>[119] use CNNs to learn group activity in Volleyball games, given sequence of person keypoint coordinates, their temporal differences, and keypoint differences from each actor to the pivot-actor that is selected by the model. The model does not model human-object interactions. AT [29] does not consider human-object interactions either, but because AT is also a Transformer-based model like ours, we can easily improve it by feeding our object tokens as additional inputs to AT. Moreoever, GIRN <ref type="bibr" target="#b46">[82]</ref> and POGARS [102] are designed to leverage ball trajectory for learning group activity in videos of Volleyball games. As shown in <ref type="table" target="#tab_3">Table 2</ref>, the object keypoint information can greatly boost the performance by providing additional context. GIRN models interactions between joints within an actor and across actors, as well as joint-object interactions. POGARS uses 1D CNNs to learn spatiotemporal dynamics of actors. AT, GIRN, and POGARS all use dot-product-based attention mechanisms similar to ours, however, they fail to fully model the hier- archical entities in the video (e.g., they all only use attention to learn person-wise importance, and at most consider two scales: keypoint and person), and more importantly, they lack explicit strategy to improve the multiscale representations in order to aid the compositional reasoning of group activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Methods Using Other Modalities</head><p>We compare results of COMPOSER with the best reported results of SOTA methods that use a single or multiple modalities in <ref type="table" target="#tab_9">Table 3</ref> on both VD and CAD. COMPOSER still achieves competitive performance -outperforming methods that use only RGB signals, obtaining +0.7% improvement on VD and +2.8% improvement on CAD if compared with methods that use a single modality (RGB or keypoint), and performing favorably compared with methods that exploit multiple expensive input modalities. GroupFormer <ref type="bibr" target="#b29">[65]</ref> has the highest accuracy on VD and CAD due to learning the representations of the multiscale scene entities (person and person group) with a Clustered Spatial-Temporal Transformer, and leveraging scene context and multiple expensive modalities (FLOPs: GroupFormer 595M v.s. COMPOSER 297M; details are in Appendix C).  <ref type="figure">Fig. 6</ref>. Qualitative results on CAD (video ID '10'). COMPOSER successfully predicts 'Queueing' even when the input keypoints are partially noisy due to occlusion.</p><formula xml:id="formula_7">t =2 t =3 t =1 t =4 t =5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Results</head><p>We visualize the attention weights in <ref type="figure" target="#fig_1">Fig. 5</ref>. We highlight the tokens that the model has mostly attended to at each scale (e.g., wrists of actor 0 at the person keypoint scale). COMPOSER is able to attend to relevant information across different scales, and it can produce interpretable results. In <ref type="figure">Fig. 6</ref>, we visualize the keypoint input to COMPOSER on a CAD instance. COMPOSER implicitly learns the human motion patterns from the keypoint features to handle partial occlusions.</p><p>Please check Appendix for more analyses including ablation studies, confusion matrices, parameter sensitivity analyses w.r.t. the number of scales and the number of prototypes, more qualitative results including failure cases, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose COMPOSER that uses a Multiscale Transformer to learn compositional reasoning at different scales for group activity recognition. We also improve the intermediate representations using contrastive clustering, auxiliary prediction, and data augmentation techniques. We demonstrate the model's strength and interpretability on two widely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up to +5.4% improvement with just the keypoint modality.</p><p>One limitation is that videos with severe occlusions remain challenging for COMPOSER like other existing methods, due to errors from detecting keypoints. Adopting 3D keypoints or stronger backbones that estimate keypoints directly from the video <ref type="bibr" target="#b44">[80,</ref><ref type="bibr" target="#b1">2]</ref> can help to address the issue. Possible future directions include 1) expanding our methods to more complex scenarios, such as crowd understanding that may require modeling additional hierarchical scales; and 2) exploring effective multimodal fusion methods in order to use additional modalities like RGB but without suffering from scene biases, since RGB can be beneficial for activities that involve significant interaction with the background scene.</p><p>Acknowledgments   This appendix is organized as follows:</p><p>A A Results Using Different Num. of Prototypes</p><p>In <ref type="table" target="#tab_2">Table 1</ref>, we evaluate the impact of the number of prototypes K (i.e., the number of clip clusters) that is used for contrastive clustering learning on the GAR accuracy of COMPOSER. We use the Original split of the Volleyball dataset for this evaluation. We observe that varying the number of prototypes does not affect much the performance. The performance first improves as the number of prototypes increases, then decreases as the number of prototypes keeps increasing. The number of prototypes has little influence as long as it is reasonably "enough". The practice is to set the number of prototypes at least one order of magnitude larger than the true number of classes in the dataset <ref type="bibr" target="#b10">[11]</ref>. Hence, for simplicity, we do not spend extensive efforts in fine-tuning COMPOSER w.r.t. this  hyper-parameter; results reported in the main paper are from COMPOSER trained using 1, 000 prototypes for all datasets and splits. We visualize the clip embedding space (after t-SNE 2D projection) learned by COMPOSER using 10 prototypes and 1000 prototypes in <ref type="figure" target="#fig_5">Fig. 1 (a)</ref> and (b), respectively. We take the representation of the CLS token from the last scale and the last Multiscale Transformer block as the representation of the clip to produce the embedding space visualization. In <ref type="figure" target="#fig_5">Fig. 1</ref>, each dot represents a test clip and the color of the dot indicates the group activity label of the clip. A higher number of prototypes can lead to a better grouping of clips with the same group activity class, as well as a better separation of clips in different group activity classes; this accords with the quantitative results shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation Study</head><p>We conduct ablation experiments to verify the effectiveness of proposed techniques; results are in <ref type="table" target="#tab_3">Table 2</ref> (methodology details are in Appendix F.4). Contrastive clustering and scale agreement regularize the multiscale representations. As demonstrated in <ref type="table">Table.</ref> 2, performance drops without the contrastive clustering learning. The swapped prediction setup helps the model to maintain consistency across representations of the multiple scales of the same clip, which regularizes the intermediate representations.</p><p>We also experiment with the 'Label Consistency' method [89] that minimizes the L2 distance between 2 views of an instance in the logit space. Replacing contrastive clustering with Label Consistency for scale agreement, result is better than the previous ablation, but worse than COMPOSER. Better performance of COMPOSER can be attributed to the additional benefits of the clustering loss, which draws clips that are semantically related close together by comparing with the prototypes. Both experiments indicate that encouraging scale agreement can bring benefits for the multi-scale learning models, which is unfortunately neglected by prior works.</p><p>Greater number of scales yields more information and an effective scale agreement. We find that increasing the number of scales leads to a higher accuracy. More scales indicates more information about the entities in the scene. Besides, given more scales, hierarchical representations are able to be better maintained, and techniques such as contrastive clustering and auxiliary predictions are more effective. Data augmentations increase the training data size and inject benign noises, leading to generalization. Results in <ref type="table" target="#tab_3">Table 2</ref> show the gains brought by each data augmentation technique described in Sec. 3.4 of the main paper. Among the four types of data augmentation, Horizontal Flip (which is commonly used by existing works [119,102]) and Actor Dropout are the most critical ones. Even though data augmentation is less effective than other techniques proposed, it increases the training data size and injects noises that help model to generalize. Auxiliary prediction aids learning the intermediate representations.</p><p>For the 'No Auxiliary Prediction' ablation, the loss of group activity is only computed from the clip token from the last scale of the last Multiscale Transformer (note that the person action loss and clustering loss are still in use).</p><p>Performance of this ablation largely drops, indicating auxiliary prediction is a simple but yet effective technique. Multiscale Transformer learns higher-level knowledge of the video by compositionally reasoning over concepts from finest-grained to coarsest-grained. We design an ablation to remove Multiscale Transformer. Specifically, the group activity classifier directly takes features of the object token and person tokens (learned from features of keypoint tokens) as an input, and the person action classifier takes the features of person tokens as an input. The outcome of this ablation is worse performance than the 1-scale ablation that does not even use person features and person action labels -which indicates the importance of relational reasoning.</p><p>Misc. Unlike previous works <ref type="bibr">[27,</ref><ref type="bibr" target="#b22">58]</ref>, person-to-group association mechanism is not our focus. Hence, we use heuristics <ref type="bibr" target="#b46">[82]</ref> for the Volleyball dataset, Kmeans <ref type="bibr" target="#b29">[65]</ref> for the Collective Activity dataset, and set g?=2 without tuning. With K-means, actors are mapped to groups adaptively in each Multiscale Transformer block as person representations are refined. We have experimented with timevarying person grouping with K-means, and the result is 92.8% on the original split of the Volleyball dataset. For this experiment, since we need person and group representations at each timestamp, the number of tokens is increased (multiplied by T ) for the 2 transformer encoders, which potentially leads to issues such as over-smoothing <ref type="bibr">[117]</ref> and raises the challenge for attention. We hypothesize that a carefully-crafted mechanism for learning spatio-temporal relations is required for time-varying person grouping. We have experimented with Multiscale Transformer variations. In Multiscale Transformer, 4 different transformer encoders separately model the contextual information of each scale, which eases learning (because the information granularity and features across scales can vary significantly). Parameter-sharing across the 4 encoders yielded a result of 93.4% on the original split of the Volleyball dataset, and feeding all tokens to one encoder obtained 92.4%. The order of encoders in Multiscale Transformer is in accordance with the hierarchy of person-related tokens, from fine to coarse. This allows COMPOSER to compose the high-level representations from low-level ones and distill the knowledge. One can perform grid search to find the optimal order of the 4 encoders, but that will lead to 24 permutations which require lots of resources and runtime. We have performed an experiment with the reverse order (representations of coarser tokens are broadcast to finer tokens), and the result is 86.8%, which is much worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Efficiency Comparison</head><p>Backbone efficiency comparison to support the keypoint-only setup. Backbones used by prior works vary a lot (see <ref type="table">Table 4</ref> and <ref type="table" target="#tab_11">Table 5</ref>). Because AT [29] is a Transformer-based model like ours and it has reported result of using the keypoint-only modality, we follow AT to use the HRNet [105] as the person keypoint estimation backbone.  <ref type="table" target="#tab_9">(Table 3</ref>). VGG-19 and VGG-16 are more computational expensive than HRNet. Our method is agnostic to the type of the keypoint estimation backbone and robust w.r.t. noisy estimated keypoints. Therefore, real-time applications can use an efficient backbone. Efficiency comparison with SOTA GAR methods. We also compare FLOPs of prior Transformer-based GAR methods with ours. For a fair comparison, the methods all have 2 blocks of their respective GAR reasoning module (e.g., Multiscale Transformer in COMPOSER and the CSTT module in Group-Former) and share hyper-parameters of the Transformers inside (e.g., dimension of the FFN layer inside Transformer). In addition, computation spent on person feature extraction is excluded since different backbones can be used.</p><p>FLOPs are (given a Collective Activity dataset's input): Ours 297M, Group-Former <ref type="bibr" target="#b29">[65]</ref> 595M, and AT [29]] 17M where 'M' stands for million. While AT is the most efficient one, its efficacy (only 92.8% on Collective Activity) and generalization are unsatisfactory. GroupFormer is the most computational expensive one due to 5 Transformers inside its CSTT module and leveraging image scenes -even we only report FLOPs for its most basic version that leverages the least signals possible (RGB + Scene).</p><p>Note that the COMPOSER variant that uses RGB modality (mentioned in the main paper) has 127M FLOPs and is more efficient than COMPOSER that uses keypoints because the former only models 3 scales (person, interaction and group). To further reduce latency for real-life applications (e.g., on-device scenarios), we can have a light-weight COMPOSER variant that only models one scale during inference or uses smaller hidden dimensions and yet retains the most efficacy and generalization by using techniques like knowledge distillation [?]. <ref type="table">Table 4</ref>. Detailed comparisons between our results and the reported SOTA methods' results on the Original split of the Volleyball dataset. We have made an effort to collect and list results of prior works using various backbones and modalities. Note that the backbones used by prior works vary a lot. The top 3 performance scores are highlighted as: First, Second * , Third . COMPOSER outperforms the latest GAR methods that use a single modality (+0.7% improvement), and performs favorably compared against methods that exploit multiple expensive modalities (Appendix C)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMPOSER (ours)</head><p>HRNet -96.2 * *Note: "Flow" denotes optical flow input, and "Scene" denotes additional image context features of the entire frames (fewer checks are better ). Yellow shaded rows highlight that the methods use just the RGB-based input, whereas blue for just keypoint. We are the first to report the result of a keypoint-only method on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Qualitative Results</head><p>We visualize the attention matrices produced by the last Multiscale Transformer block in COMPOSER of CAD test clips in different group activity classes in <ref type="figure" target="#fig_0">Fig. 2, 5</ref>, 6 and 7. In <ref type="figure" target="#fig_1">Fig. 5</ref> of the main paper, we visualize the attention matrices of a VD test clip in group activity class "pass". We provide the same visualization for the other 3 main group activity classes of VD in <ref type="figure" target="#fig_5">Fig. 8, 9 and 10</ref>.</p><p>In the figures, we highlight the tokens that COMPOSER has mostly attended to at each scale (darker color denotes larger attention weights). Each figure contains rich information. Please zoom in on the images to appreciate the details. Here, we summarize the main findings:  <ref type="figure" target="#fig_0">Fig. 2</ref>. Qualitative results of COMPOSER on CAD -showcasing attention matrices of a test set instance in the "talking" class. The person to person group mapping identified by COMPOSER, estimated keypoints from the backbone, and the ground truth person bounding boxes with action labels are also shown on the left side of the <ref type="figure">figure.</ref> (e.g., <ref type="figure">Fig. 8, 9</ref>, etc). In certain cases, the object token is also identified as important (e.g., as shown in <ref type="figure">Fig. 9</ref>, the object token 'ball' is identified as important at scale 1, 3 and 4). COMPOSER is able to attend to relevant entities across different scales and can produce interpretable results. 2. COMPOSER learns to recognize the group activities based upon the unique characteristics of each group activity class. E.g., in <ref type="figure" target="#fig_5">Fig. 10</ref>, the pattern of the attention weights of the winpoint class at scale 3 is quite different from the pattern observed in the other 3 main group activity classes in VD. Recognition of the other group activities in VD (e.g., spike), is often determined by a key player who is performing the key action (e.g., spiking). In contrast, the winpoint group activity is not heavily dependent on one or two key players; instead, it is defined by the overall person-to-person interactions of the team that just scored. As shown in <ref type="figure" target="#fig_5">Fig. 10</ref>, person-to-person interaction tokens formed by players in the right team tend to closely attend to each other. 3. On CAD, COMPOSER has identified interesting person-to-group mappings as shown in <ref type="figure" target="#fig_0">Fig. 2, 5</ref>, etc. On VD, we find that at the group scale, the CLS and object tokens often mostly attend to each other, and the two teams often mostly attend to each other. We posit that this is because, at the previous three scales, the CLS and object tokens mostly attend to actor-related tokens.</p><p>The object token, which contains less correlated information, then becomes the most important signal at scale 4 for the CLS token in order to correctly recognize the group activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Failure Cases and Confusion Matrices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Failure Case Analysis</head><p>Volleyball. Through examining failure cases of COMPOSER, we find numerous mislabeled test clips. We coin these cases as false failure cases (i.e., the annotated label is wrong but the prediction from COMPOSER is correct). Sometimes the annotation of the team is wrong (e.g., a test clip with the left set group activity is annotated as right set as shown in <ref type="figure" target="#fig_5">Fig. 11</ref>), and sometimes the annotated main group activity is wrong (e.g., a test clip with the right set group activity is annotated as right spike as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>). As a result, the models that have achieved high accuracy on the Original split of VD might capture misleading noises and cannot generalize, and therefore the pursuit of beating SOTA might be pointless. For future research, proposing methods that make use of RGB signals but overcome scene biases on the Olympic split is a recommended direction. We visualize two true failure cases of COMPOSER in <ref type="figure" target="#fig_5">Fig. 13</ref> and <ref type="figure" target="#fig_5">Fig. 14.</ref> The wrong prediction of COMPOSER made on case shown in <ref type="figure" target="#fig_5">Fig. 13</ref> could be attributed to the fact that the arms of actor 0 is occluded. In <ref type="figure" target="#fig_5">Fig. 14,</ref> COMPOSER fails to identify which team is performing the group activity spike. However, we notice a major change in the camera location of this test clip, which causes this test clip possibly to be difficult even for humans to make a correct prediction. Collective Activity. We visualize several failure cases of COMPOSER on the test set of CAD in <ref type="figure" target="#fig_1">Fig. 15, 16, 17</ref>, and 18. Failures of COMPOSER can be attributed to severe occlusions <ref type="figure" target="#fig_1">(Fig. 15</ref>), misleading movement of actors ( <ref type="figure" target="#fig_5">Fig. 16 and 17</ref>) and the current use of a relatively short temporal window for each clip <ref type="figure" target="#fig_5">(Fig. 18</ref>), which suggest ways to further improve COMPOSER, e.g., addressing the issue of severe occlusion, using more frames per clip with attention-based relational reasoning in both spatial and temporal domains, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Confusion Matrix Analysis</head><p>We show the confusion matrices of COMPOSER in <ref type="figure" target="#fig_6">Fig. 3</ref>. On the Volleyball dataset (the Original split), for each group activity class COMPOSER achieves an accuracy over 90% with the lowest accuracy for the right set class. Most failures emerge from discriminating the set, pass, and spike activities which can be a result of highly similar actions or positions of the key player in some clips <ref type="bibr">[29,</ref><ref type="bibr">41,</ref><ref type="bibr">112]</ref>. Occasionally, the model struggles to distinguish which team (left or right) performs the activity. We hypothesize that adding more object tokens/keypoints such as the keypoints of the net to COMPOSER may help to address this problem. Nevertheless, different camera positions of some clips in the dataset (e.g., <ref type="figure" target="#fig_5">Fig. 14)</ref> might cause the difficulty for a model to learn which team performs the activity. On the Collective Activity dataset, COMPOSER occasionally mistakes waiting to moving, which may because the current temporal dynamics of clips is too short to catch differences between the two classes [116] and a lot more examples of moving than waiting in the dataset (more than twice more). MSA. Central to the Transformer encoder is the self-attention function. In the self-attention function, the input X ? R n?d is first linearly transformed to three parts, i.e., query Q ? R n?d k , key K ? R n?d k and value V ? R n?dv , where n denotes the number of tokens in the input sequence, and d, d k and d v are the representation dimensions of the input, query (or key), and value, respectively. The Scaled Dot-Product Attention is applied on Q, K, and V :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Method and Implementation Details</head><formula xml:id="formula_8">Attention(Q, K, V ) = softmax QK ? ? d k V<label>(1)</label></formula><p>Then, a linear layer is used to produce the output. We use h paralleled heads of the Scaled Dot-Product Attention to increase the representation power. Specifically, MSA splits the query, key and value for h times and performs the selfattention function in parallel, and then the output values of each head are concatenated and linearly projected to form the final output <ref type="bibr">[35]</ref>.</p><p>MLP. The MLP is for feature transformation and non-linearity:</p><formula xml:id="formula_9">MLP(X) = ? (XW 1 + b 1 ) W 2 + b 2<label>(2)</label></formula><p>where W 1 ? R d?dm and W 2 ? R dm?d are weights of the two fully-connected layers, b 1 ? R dm and b 2 ? R d are the bias terms, and ?(. . . ) is the non-linear activation function such as ReLU <ref type="bibr">[30]</ref> or GELU <ref type="bibr">[38]</ref>.</p><p>Add &amp; Dropout &amp; LN. The output from MSA (or MLP) is added with the input of MSA (or MLP) to enforce the skip connection. Then, a dropout layer is used, followed by the Layer Normalization (LN) that enables stable training and faster convergence. Layer normalization is applied over each sample x ? R d as follows:</p><formula xml:id="formula_10">LN (x) = x ? ? ? ? ? + ?<label>(3)</label></formula><p>where ? ? R and ? ? R are the mean and standard deviation of the features, respectively, ? denotes the element-wise multiplication, ? ? R d and ? ? R d are learnable affine transform parameters for scaling and shifting, respectively. Given the input X, the computations in order in the Transformer encoder are: MSA, Add &amp; Dropout &amp; LN, MLP, and Add &amp; Dropout &amp; LN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Keypoint Initial Representation</head><p>Keypoint is the finest-grained actor-related entity that we consider for GAR. In this subsection, we describe the keypoint initial representation (i.e., representations of the input keypoint tokens of the first Multiscale Transformer layer). As described in the main paper, representations of actor-related tokens in coarser scales are learned and aggregated from that of the finer scales.</p><p>A keypoint in a frame has the information of keypoint type, as well as keypoint coordinate in both of the image space and the time space. We use three GCN <ref type="bibr" target="#b20">[56]</ref> layers to map each person keypoint type into a learned vector in order to encode the intrinsic connections of different keypoint types. We apply feature standardization to the raw 2D keypoint coordinate and the temporal difference of coordinates in consecutive two frames. We also normalize the keypoint coordinate in a person-wise manner to account for rotation, translation, and scale differences <ref type="bibr">[119,</ref><ref type="bibr">98]</ref>. In addition, we use the Learned Fourier Positional Encoding [101] to map each image coordinate into a learned vector, and use the Learned Absolute Positional Encoding to learn a vector for each time coordinate. To mitigate the issue of noisy estimated keypoints, we use the temporal Object Keypoint Similarity (OKS) proposed in [95], and use the mean OKS scores of each person as additional features. The above procedure is summarized in <ref type="figure">Fig. 4</ref>. These features are concatenated to form the initial composite representation of a person keypoint in a frame, and a keypoint token is represented by concatenation and Feed Forward Network (FFN) based transformation of keypoints' representations in all timestamps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-keypoint type -t -x t , y t -standardization(x t , y t ) -standardization(x t -x t-1 , y t -y t-1 ) -person-wise-normalize(x t , y t )</head><p>-OKS((x t , y t ), (x t-1 , y t-1 )) X Y <ref type="figure">Fig. 4</ref>. Representation of a person keypoint in a particular frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Implementation Details of COMPOSER</head><p>For annotation parsing and video preprocessing (e.g., obtaining clips from a long CAD video), we follow prior works <ref type="bibr">[117,</ref><ref type="bibr">107]</ref>. On the Volleyball dataset, annotations of group activity, players' bounding boxes and their actions in the middle frame of each clip are provided. Person tracklet data is provided by <ref type="bibr">[87]</ref>. For the Collective Activity dataset, annotations include actors' bounding boxes and their action labels on the center frame of every ten frames and group activity labels for every ten frames. In order to make a fair comparison with related works <ref type="bibr">[116,</ref><ref type="bibr">117,</ref><ref type="bibr">29</ref>,107], we use T = 10 frames as the input to our model for both training and testing on both datasets.</p><p>We use HRNet [105] 5 to obtain estimated person keypoints following [29,116] (j ? = 17). There are a total of 17 different keypoint types: nose, left eye, right eye, left ear, right ear, left shoulder, right shoulder, left elbow, right elbow, left wrist, right wrist, left hip, right hip, left knee, right knee, left ankle and right ankle. On the Volleyball dataset, the maximal number of actors in a video p ? = 12, and on Collective Activity, p ? = 13. On both datasets, the number of person groups per video g ? = 2. On Volleyball, actors are grouped into the two person groups by heuristics, i.e., according to the horizontal positions of the actors, the left most 6 actors form a sub-group and the rest actors form the other sub-group. We find that using clustering algorithms such as K-means (given the coordinates of the actors as features) can generate similar results as the heuristics on the Volleyball dataset. Hence, we choose to use the heuristics for simplicity for Volleyball, and use K-means 6 to form person groups on the Collective Activity dataset (given the input of the learned person representations from COMPOSER).</p><p>On the Volleyball dataset, the object keypoints are from the ball trajectories annotated by <ref type="bibr" target="#b46">[82]</ref> (e ? = 1) 7 . The initial representation of the object keypoint is similar to that of the person keypoint, i.e., a concatenation of the time positional encoding, Fourier positional encoding, and standardized keypoint coordinate and temporal difference. The initial object token is formed by concatenation and FFN-based transformation of object keypoints' representations in all timestamps. On the other hand, we do not use the object token for the Collective Activity dataset because the Collective Activity dataset does not have any human-object interactions.</p><p>On both datasets, the number of Multiscale Transformer blocks M is set as 2, the number of attention heads of the Transformer encoder at each scale is set to 2, 8, 2 and 2, respectively, and the dropout rate of the Transformer encoder at each scale is set to 0.5, 0.2, 0.2 and 0, respectively. We find that a smaller dropout rate in the coarser scales tends to yield a better performance. The dimension of the MLP layer in all Transformer encoders is set to 1024 (i.e., d m = 1024), and the non-linear activation function is ReLU. The hidden dimension d = 256 (d k , d v and d are equal) on Volleyball and d = 128 on Collective Activity. Because we focus on semantic relational reasoning over temporal relation capturing, we use MLPs with 1 hidden layer of dimensionality d to flatten out the time axis for each entity -in this way, track-based representations are formed for each entity. To aggregate the token representations from a finer scale to the coarser scale, FFNs are used when the number of tokens for aggregation at that scale is fixed (e.g., 2 persons aggregates to an interaction), otherwise summation is used (e.g., the number of persons to form a group varies on the Collective Activity dataset due to K-means). The cross entropy loss is used during training for both group activity and person action classification. We use the Adam optimizer <ref type="bibr" target="#b19">[55]</ref> and train the model for 45 epochs with an initial learning rate 0.0005 and decrease the learning rate to 0.0001 at epoch 40. The weight decay is 0.001, ? is 3, and batch size is 256. Following <ref type="bibr" target="#b10">[11]</ref>, the temperature parameter ? = 0.1, ? = 0.05, and the number of iterations of the Sinkhorn-Knopp algorithm 8 is set to 3. The number of prototypes K is 1000 for all experiments except the ones described in Appendix A. In data augmentations, the range of random perturbation is set to 1 pixel location. We use the PyTorch 9 Python library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Implementation Details of Ablation Studies</head><p>In this subsection, we describe the methodology of the ablations that we present in Appendix B in details. For all of the ablations, we use the same set of hyperparameters as our full model.</p><p>No Clustering. The only difference from this ablation to our full model COMPOSER is the loss function used in training. Instead of using Equation (6) (in the main paper) as the loss function, this ablation uses the following loss function:</p><formula xml:id="formula_11">L total = M ?1 m=1 L groupAux + ? (L groupLast + L person )<label>(4)</label></formula><p>Label Consistency [89] for Scale Agreement. Similar to the previous ablation, the only difference from this ablation to our full model COMPOSER is the loss function used in training, which is formulated as follows:</p><formula xml:id="formula_12">L total = M ?1 m=1 L groupAux + ? (L groupLast + L person + L consistency )<label>(5)</label></formula><p>where L consistency represents the loss term that minimizes the L2 distance between 2 scales of a clip in the logit space. Specifically, for every two pairs of scales, we compute the L2 loss given the two sets of GAR logits of the two scales. The L consistency term is the mean of such L2 losses over all pairs of scales. 1-Scale: Keypoint. For this ablation, there is only one Transformer encoder in the Multiscale Transformer block, and the tokens to the Transformer encoder are:</p><formula xml:id="formula_13">Scale 1: [CLS] , e 1 , ? ? ? , e e ? , k 1 1 , ? ? ? , k j ? p ? .<label>(6)</label></formula><p>Since only the keypoint tokens are refined by the Multiscale Transformer block in this ablation and there is only 1 scale, this ablation uses the following loss function:</p><formula xml:id="formula_14">L total = M ?1 m=1 L groupAux + ?L groupLast<label>(7)</label></formula><p>2-Scale: Keypoint + Person. For this ablation, there are two hierarchical scales in the Multiscale Transformer block, and the tokens to the Transformer encoders are:</p><formula xml:id="formula_15">Scale 1: [CLS]</formula><p>, e 1 , ? ? ? , e e ? , k 1 1 , ? ? ? , k j ? p ? , Scale 2: {[CLS] , e 1 , ? ? ? , e e ? , p 1 , ? ? ? , p p ? } .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(8)</head><p>This ablation uses the same loss function as our full model except that the number of pairs of scales for swapped prediction is only 1, i.e., pair scale1-scale2. 3-Scale: Keypoint + Person + Interaction. For this ablation, there are three hierarchical scales in the Multiscale Transformer block, and the tokens to the Transformer encoders are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale 1:</head><p>[CLS] , e 1 , ? ? ? , e e ? , k 1 1 , ? ? ? , k j ? p ? , Scale 2: {[CLS] , e 1 , ? ? ? , e e ? , p 1 , ? ? ? , p p ? } ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale 3:</head><p>[CLS] , e 1 , ? ? ? , e e ? , i 1 , ? ? ? , i p ? ?(p ? ?1) .</p><p>This ablation uses the same loss function as our full model except that the number of pairs of scales for swapped prediction is 3, i.e., pair scale1-scale2, pair scale1-scale3, and pair scale2-scale3.</p><p>No Auxiliary Prediction. The only difference from this ablation to our full model COMPOSER is the loss function used in training. This ablation uses the following loss function:</p><formula xml:id="formula_17">L total = L groupLast + L person + L cluster<label>(10)</label></formula><p>No Multiscale Transformer. In this ablation, the group activity classifier simply takes features of the initial object token and person tokens as inputs, and the person tokens are aggregated from the initial representations of keypoint tokens through concatenation and FFN. In addition, features of the person tokens are the inputs to the person action classifier. No Transformers are used in this ablation. Due to the lack of relational reasoning performed at multiple scales, this ablation uses the following loss function:</p><formula xml:id="formula_18">L total = L groupLast + L person<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Miscellaneous</head><p>The  <ref type="bibr" target="#b11">12</ref> to obtain the person regional features and then use these person features to predict the individual actions and the group activity. All of these RGB-based methods (including our COMPOSER RGB-based variant) use the RGB-only modality and share the same VGG-16 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Extended Discussion on Related Work</head><p>Group Activity Recognition. Early work on GAR relies on handcrafted features [20, <ref type="bibr" target="#b25">61,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">21,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b38">74]</ref>; yet notable progress has been made by Deep Learning (DL) based approaches <ref type="bibr">[47,</ref><ref type="bibr">25]</ref>. We review DL-based methods and refer readers to the comprehensive review of GAR presented in [108].</p><p>Early DL-based methods use Convolutional Neural Networks (CNNs) to extract the low-level visual features and then apply Recurrent Neural Networks such as LSTM [40] for temporal modeling <ref type="bibr">[106,</ref><ref type="bibr">91,</ref><ref type="bibr" target="#b30">66,</ref><ref type="bibr" target="#b17">53]</ref>. Since learning inter-person interactions is essential for GAR [108], much of the recent research explores how to capture the contextual information about the actor and their relations <ref type="bibr">[46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">107,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b47">83]</ref>. Several works tackle this problem from a graph-based perspective <ref type="bibr">[46,</ref><ref type="bibr" target="#b35">71,</ref><ref type="bibr">113,</ref><ref type="bibr">112]</ref> such as applying Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b20">[56]</ref> for deep relationship modeling <ref type="bibr">[107]</ref>. More recent works utilize attention modeling <ref type="bibr" target="#b48">[84,</ref><ref type="bibr">109,</ref><ref type="bibr" target="#b35">71,</ref><ref type="bibr">116]</ref> including using Transformers <ref type="bibr">[29,</ref><ref type="bibr" target="#b29">65]</ref> to perform relational reasoning, with a focus on determining the most critical persons <ref type="bibr">[107,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b47">83,</ref><ref type="bibr">116]</ref>, groups <ref type="bibr">[27,</ref><ref type="bibr" target="#b29">65]</ref>, or interactions <ref type="bibr">[113]</ref>. Existing works in the field of GAR have primarily used RGB-and/or optical-flow-based features with RoIAlign [36] to represent actors <ref type="bibr">[112,</ref><ref type="bibr" target="#b48">84,</ref><ref type="bibr">107,</ref><ref type="bibr" target="#b7">8]</ref>. A few recent works replace or augment these features with keypoints/poses of the actors <ref type="bibr" target="#b34">[70,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">102,</ref><ref type="bibr" target="#b29">65]</ref>. Some only use the numerical coordinate-based keypoint representation <ref type="bibr">[119,</ref><ref type="bibr">102,</ref><ref type="bibr" target="#b46">82,</ref><ref type="bibr" target="#b47">83]</ref> while others use a high-dimensional vector from a deep pose backbone <ref type="bibr">[29,</ref><ref type="bibr">116]</ref> which is not as efficient. In this paper, we use Transformers [103] for higherorder relationship modeling and use only the light-weight coordinate-based keypoint representation. Our work differs from prior methods in that we propose a Multiscale Transformer block to hierarchically reason about entities at different semantic scales and we aid learning group activities by improving the multiscale representations.</p><p>Action Recognition and Keypoint-based Prediction. Action Recognition is one of the primary tasks in video understanding. There has been rapid progress in recent years, starting from recognition of the low-level atomic actions performed by an individual (e.g., hand-waving, dancing, jumping), to paired-actions being acted by two persons <ref type="bibr" target="#b45">[81,</ref><ref type="bibr">92,</ref><ref type="bibr">118,</ref><ref type="bibr" target="#b21">57]</ref> (e.g., shaking hands, hugging, punching), towards group activities that encompass many actors at once <ref type="bibr">[20,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b50">86,</ref><ref type="bibr" target="#b46">82]</ref> (e.g., attack and defense in a sports game, pedestrians queuing). Our paper focuses on the most spatially complex scenarios, where multiple interacting individuals form the group activity. In addition, keypoint-based action recognition has drawn much attention <ref type="bibr" target="#b28">[64,</ref><ref type="bibr">114,</ref><ref type="bibr">122,</ref><ref type="bibr" target="#b42">78,</ref><ref type="bibr">121]</ref>. Keypoint-based representation can be regarded as a high-level representation for dynamic behaviors, and is preferred due to benefits such as being compact and robust to variations of viewpoints, appearances, and surrounding distractions <ref type="bibr">[23,</ref><ref type="bibr" target="#b31">67]</ref>. We study keypoint-based group activity recognition. We propose to use techniques including auxiliary prediction and data augmentations that can aid learning group activity from the keypoint modality.</p><p>Compositionality and Multiscale Learning. Compositionality is an active field of research in computer vision (CV) <ref type="bibr">[120,</ref><ref type="bibr" target="#b18">54,</ref><ref type="bibr">111,</ref><ref type="bibr">99]</ref>, natural language processing <ref type="bibr">[96,</ref><ref type="bibr">104,</ref><ref type="bibr">48,</ref><ref type="bibr">24]</ref> and machine reasoning <ref type="bibr">[44,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b9">10]</ref>. In terms of understanding videos centered on human actions, compositionality can be studied from different lenses, e.g., through formulating an activity as compositions of atomic actions temporally <ref type="bibr" target="#b49">[85,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">72]</ref>  We tackle compositional video understanding by formulating a visual-semantic hierarchy, where each semantic hierarchy is regarded as representation of the video at a particular scale. Such an idea of multiscale learning has been a longstanding topic in CV as well <ref type="bibr" target="#b28">[64,</ref><ref type="bibr" target="#b27">63,</ref><ref type="bibr">31,</ref><ref type="bibr">33]</ref>. Recently, researchers have started to introduce the concept of multiscale learning to Transformers [28, <ref type="bibr" target="#b32">68,</ref><ref type="bibr">35]</ref> by operating self-attention over various scales of resolutions and/or channels, in order to obtain a multiscale pyramid of features often observed in CNNs. Distinct from prior works, we design COMPOSER that models semantic scene entities at different hierarchical scales to learn group activities effectively. COMPOSER is the first Transformer-based method with explicit multiscale modeling for GAR that improves the musicale representations with a contrastive clustering based objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Discussion of COMPOSER</head><p>Motivation Intuitively, each scale provides enough information for GAR; only the information granularity varies, which causes recognition confidence to vary scale by scale. Therefore, we consider scales as different but correlated views of the clip, and utilize multiscale contrastive clustering learning (MCCL) to allow one scale to complement another. This allows learning better compositional structures and higher-order representations.</p><p>Pull close: Equation 2 in the main paper (swapped prediction) trains the model to produce multiscale representations of 1 clip such that the cluster assignment of the clip representation at one scale can be predicted from the clip representation at another scale, allowing representations of the same clip to be pulled close.</p><p>Pull away: If 2 clips are semantically different, as Equation 6 in the main paper includes the supervised GAR loss and the unsupervised MCCL loss, the 2 clips will be put to different clusters and pushed further as training goes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key Insight</head><p>The key novelty of COMPOSER is that the model learns consistent multiscale representations. The idea of considering scales as views and encouraging scale agreement is applicable to numerous Computer Vision tasks, including general-purpose visual model pre-training, because the entities and scales we have considered are common in human-centered videos. By design, COMPOSER is capable of modeling multi-actor multi-object interactions in images or videos. Moreover, COMPOSER offers numerous useful practices, including auxiliary prediction to aid training stacks of Transformers, and techniques that can aid the model to learn the high-level knowledge from the low-level coordinate-based keypoint signals (e.g. data augmentations with random perturbation and OKS-based keypoint features to mitigate the issue of noisy estimated keypoints).</p><p>Limitation As shown in the failure cases of COMPOSER, videos with severe occlusions remain challenging. Severe occlusion can be a limitation for all GAR methods as their modalities are derived from the RGB input. COMPOSER might handle occlusion better than RGB-based methods. In partial occlusion scenarios (i.e., only a ratio of keypoints are occluded for the person -examples are cases shown in <ref type="figure">Fig. 6</ref> of the main paper, <ref type="figure">Fig. 7 and Fig. 9</ref> in the Appendix), because COMPOSER learns human motion dynamics, better representation of occluded persons can be inferred from the keypoints. In addition, COMPOSER is agnostic to the keypoint backbone, and many SOTA keypoint extractors are robust to occlusion (e.g, BlazePose [?,2]).</p><p>Societal Impact Human group activity recognition has widespread societal implications in a variety of domains including security, surveillance, kinesiology, sports analysis, and rehabilitation. Privacy and ethical concerns might be raised when deployed in real-world settings if not done in a careful manner. In response to these concerns, COMPOSER utilizes only keypoint input and does not use any personally identifiable information for inferring group activity. Even for the backbone which COMPOSER is agnostic to, there are existing works [39] that perform privacy-preserving pose estimation. Hence, our method can prevent the sensor camera from acquiring detailed visual data that may contain private or biased information of users.  <ref type="figure">Fig. 9</ref>. Qualitative results of COMPOSER on VD -showcasing attention matrices of a test set instance in the "left spike" class (key actor is actor 0).  <ref type="figure" target="#fig_0">Fig. 12</ref>. Mislabeled test set clip example of VD. The annotated label is wrong but the prediction from COMPOSER is correct. Actor 8 is performing the key action setting and interacting with the ball. Actor 7 is performing an action very similar to spiking but actor 7 is missing interaction with the ball.   <ref type="figure" target="#fig_5">Fig. 18</ref>. A failure case on CAD. In the clip, persons were waiting before starting to cross the street. The dynamics are not easily perceptible due to the short temporal window of the clip.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Embedding space learned by COMPOSER. COMPOSER exploits a contrastive clustering objective (Sec. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results of COMPOSER on VD -showcasing attention matrices of an instance in the "right pass" class (key actor is actor 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The research was supported in part by NSF awards: IIS-1703883, IIS-1955404, IIS-1955365, RETTL-2119265, and EAGER-2122119. This material is based upon work supported by the U.S. Department of Homeland Security under Grant Award Number 22STESE00001 01 01. Disclaimer: The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Department of Homeland Security. International Conference on Neural Information Processing Systems. pp. 853-865 (2019) 18. Choi, W., Savarese, S.: A unified framework for multi-target tracking and collective activity recognition. In: European Conference on Computer Vision. pp. 215-230. Springer (2012) 19. Choi, W., Savarese, S.: Understanding collective activitiesof people from videos. IEEE transactions on pattern analysis and machine intelligence 36(6), 1242-1257 (2013) 20. Choi, W., Shahid, K., Savarese, S.: What are they doing?: Collective activity classification using spatio-temporal relationship among people. In: 2009 IEEE 12th international conference on computer vision workshops, ICCV Workshops. pp. 1282-1289. IEEE (2009) 21. Choi, W., Shahid, K., Savarese, S.: Learning context for collective activity recognition. In: CVPR 2011. pp. 3273-3280. IEEE (2011) 22. Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems 26, 2292-2300 (2013) 23. Dang, L.M., Min, K., Wang, H., Piran, M.J., Lee, C.H., Moon, H.: Sensor-based and vision-based human activity recognition: A comprehensive survey. Pattern Recognition 108, 107561 (2020) 24. Dankers, V., Bruni, E., Hupkes, D.: The paradox of the compositionality of natural language: a neural machine translation case study. arXiv preprint arXiv:2108.05885 (2021) 25. Deng, Z., Vahdat, A., Hu, H., Mori, G.: Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4772-4781 (2016) 26. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018) 27. Ehsanpour, M., Abedin, A., Saleh, F., Shi, J., Reid, I., Rezatofighi, H.: Joint learning of social groups, individuals action and sub-group activities in videos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics 2, 207-218 (2014) 97. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15(1), 1929-1958 (2014) 98. Sun, J.J., Zhao, J., Chen, L.C., Schroff, F., Adam, H.,Liu,  T.: View-invariant probabilistic embedding for human pose. In: European Conference on Computer Vision. pp. 53-70. Springer (2020) 99. Sun, P., Wu, B., Li, X., Li, W., Duan, L., Gan, C.: Counterfactual debiasing inference for compositional action recognition. In: Proceedings of the 29th ACM International Conference on Multimedia. pp. 3220-3228 (2021) 100. Sur?s, D., Liu, R., Vondrick, C.: Learning the predictability of the future. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12607-12617 (2021) 101. Tancik, M., Srinivasan, P.P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J.T., Ng, R.: Fourier features let networks learn high frequency functions in low dimensional domains. arXiv preprint arXiv:2006.10739 (2020) 102. Thilakarathne, H., Nibali, A., He, Z., Morgan, S.: Pose is all you need: The pose only group activity recognition system (pogars). arXiv preprint arXiv:2108.04186 (2021)103. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems. pp. 5998-6008 (2017) 104. Vendrov, I., Kiros, R., Fidler, S., Urtasun, R.: Order-embeddings of images and language. arXiv preprint arXiv:1511.06361 (2015) 105. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y.,Liu, D., Mu, Y., Tan, M., Wang, X., et al.: Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence (2020) 106. Wang, M., Ni, B., Yang, X.: Recurrent modeling of interaction context for collective activity recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3048-3056 (2017) 107. Wu, J., Wang, L., Wang, L., Guo, J., Wu, G.: Learning actor relation graphs for group activity recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9964-9974 (2019) 108. Wu, L.F., Wang, Q., Jian, M., Qiao, Y., Zhao, B.X.: A comprehensive review of group activity recognition in videos. International Journal of Automation and Computing pp. 1-17 (2021) 109. Xu, D., Fu, H., Wu, L., Jian, M., Wang, D., Liu, X.: Group activity recognition by using effective multiple modality relation representation with temporal-spatial attention. IEEE Access 8, 65689-65698 (2020) 110. Yan, R., Tang, J., Shu, X., Li, Z., Tian, Q.: Participation-contributed temporal dynamic model for group activity recognition. In: Proceedings of the 26th ACM international conference on Multimedia. pp. 1292-1300 (2018) 111. Yan, R., Xie, L., Shu, X., Tang, J.: Interactive fusion of multi-level features for compositional activity recognition. arXiv preprint arXiv:2012.05689 (2020) 112. Yan, R., Xie, L., Tang, J., Shu, X., Tian, Q.: Higcin: hierarchical graph-based cross inference network for group activity recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020) 113. Yan, R., Xie, L., Tang, J., Shu, X., Tian, Q.: Social adaptive module for weaklysupervised group activity recognition. In: European Conference on Computer Vision. pp. 208-224. Springer (2020) 114. Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for skeleton-based action recognition. In: Thirty-second AAAI conference on artificial intelligence (2018) 115. Yang, Z., Liu, S., Hu, H., Wang, L., Lin, S.: Reppoints: Point set representation for object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9657-9666 (2019) 116. Yuan, H., Ni, D.: Learning visual context for group activity recognition. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 3261-3269 (2021) 117. Yuan, H., Ni, D., Wang, M.: Spatio-temporal dynamic inference network for group activity recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7476-7485 (2021) 118. Yun, K., Honorio, J., Chattopadhyay, D., Berg, T.L., Samaras, D.: Two-person interaction detection using body-pose features and multiple instance learning. In: 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. pp. 28-35. IEEE (2012) 119. Zappardino, F., Uricchio, T., Seidenari, L., Del Bimbo, A.: Learning group activities from skeletons without individual action labels. In: 2020 25th International Conference on Pattern Recognition (ICPR). pp. 10412-10417. IEEE (2021) 120. Zhan, Y., Yu, J., Yu, T., Tao, D.: Multi-task compositional network for visual relationship detection. International Journal of Computer Vision 128(8), 2146-2165 (2020) 121. Zhao, L., Wang, Y., Zhao, J., Yuan, L., Sun, J.J., Schroff, F., Adam, H., Peng, X., Metaxas, D., Liu, T.: Learning view-disentangled human pose representation by contrastive cross-view mutual information maximization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12793-12802 (2021) 122. Zhu, W., Lan, C., Xing, J., Zeng, W., Li, Y., Shen, L., Xie, X.: Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks. In: Proceedings of the AAAI conference on artificial intelligence. vol. 30 (2016)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>t-SNE visualizations on the Volleyball dataset show that the clip embedding space learned by COMPOSER using different number of prototypes: (a) 10 prototypes, and (b) 1000 prototypes. Best viewed in color. More number of prototypes can lead to a better separation of the clips in distinct group activity classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Confusion matrices of COMPOSER on (a) the Volleyball dataset (the Original split) and (b) the Collective Activity dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F. 1 Transformer</head><label>1</label><figDesc>We briefly describe the Transformer encoder [103] used in Multiscale Transformer in this subsection. The basic components of the Transformer encoder include 1) Multi-head Self-Attention (MSA), 2) Multi-Layer Perceptron (MLP), and 3) Skip Connection [37], Dropout [97] and Layer Normalization [7] (Add &amp; Dropout &amp; LN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5 (Fig. 10 .Fig. 11 .</head><label>51011</label><figDesc>actor 0, actor 4-actor 1, actor 4-actor 2, actor 4-actor 3, actor 4-actor Qualitative results of COMPOSER on VD -showcasing attention matrices of a test set instance in the "right winpoint" class. Mislabeled test set clip example of VD. The annotated label is wrong but the prediction from COMPOSER is correct (key actor is actor 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>8 Fig. 13 .Fig. 14 .</head><label>81314</label><figDesc>A failure case on VD. Actor 0 is the one performing the key action. At scale 2 and 3, COMPOSER successfully identifies tokens associated with the key person as the most important tokens. However, at scale 1, COMPOSER fails to focus on keypoints of actor 0 and eventually makes a wrong group activity prediction. A failure case on VD. Camera position of this clip is different from other clips in VD, and COMPOSER fails to distinguish which team performs the activity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3 Fig. 15 .Fig. 16 .Fig. 17 .</head><label>3151617</label><figDesc>A failure case on CAD. Because of the severe occlusion, the person queue is interrupted and COMPOSER predicts waiting instead of queueing. A failure case on CAD. The movement of actor 1's leg might cause the wrong prediction of COMPOSER. A failure case on CAD. The movement of actor 0's leg might cause the wrong prediction of COMPOSER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 4. Multiscale Transformer performs relational reasoning with four Transformer Encoders to operate self-attention on tokens of each scale, while stringing tokens of the four scales together with FFNs and Skip Connections to learn hierarchical representations that make a high-level understanding of group activity possible.</figDesc><table><row><cell>Inputs</cell><cell></cell><cell></cell><cell>Scale 4</cell><cell>Outputs</cell></row><row><cell>Group</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tokens</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>+</cell><cell></cell></row><row><cell>Interaction</cell><cell></cell><cell></cell><cell>Scale 3</cell></row><row><cell>tokens</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Person</cell><cell></cell></row><row><cell>Person</cell><cell>Scale 2</cell><cell>To Group (FFN)</cell><cell>+</cell></row><row><cell>tokens</cell><cell></cell><cell>Person</cell><cell></cell></row><row><cell>Scale 1</cell><cell>+</cell><cell>To Interaction</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(FFN)</cell><cell></cell></row><row><cell>Keypoint</cell><cell>Keypoint</cell><cell></cell><cell></cell></row><row><cell>tokens</cell><cell>To Person</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(FFN)</cell><cell></cell><cell></cell></row><row><cell>Object</cell><cell></cell><cell></cell><cell></cell></row><row><cell>token(s)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLS (Clip token)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Test accuracy on VD under different train/test splits.</figDesc><table><row><cell></cell><cell></cell><cell>Yel-</cell></row><row><cell cols="3">low shaded rows highlight the methods</cell></row><row><cell cols="3">use RGB input, and blue for keypoint</cell></row><row><cell>Model</cell><cell cols="2">VD Acc. (%) ? Olympic Original</cell></row><row><cell>I3D [12]</cell><cell>73.9</cell><cell>84.6</cell></row><row><cell>VGG-16 [93]</cell><cell>76.4</cell><cell>91.6</cell></row><row><cell>PCTDM [110]</cell><cell>75.2</cell><cell>91.7</cell></row><row><cell>SACRF [83]</cell><cell>71.1</cell><cell>91.8</cell></row><row><cell>AT [29]</cell><cell>76.9</cell><cell>93.0</cell></row><row><cell>ARG [107]</cell><cell>77.8</cell><cell>93.3</cell></row><row><cell cols="2">TCE-STBiP [116] 78.5</cell><cell>93.5</cell></row><row><cell>DIN [117]</cell><cell>79.1</cell><cell>93.6</cell></row><row><cell>POGARS [102]</cell><cell>89.7</cell><cell>93.2</cell></row><row><cell>COMPOSER (ours)</cell><cell>95.1</cell><cell>93.7</cell></row><row><cell>Improvement</cell><cell cols="2">+5.4% +0.1%</cell></row><row><cell cols="3">*Note: Keypoint-based methods do NOT use</cell></row><row><cell cols="3">ball keypoint in this table in order to have</cell></row><row><cell cols="3">a rigorous comparison because RGB-based</cell></row><row><cell cols="2">methods are unaware of such info.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparisons</figDesc><table><row><cell></cell><cell>with state-of-the-</cell></row><row><cell cols="2">art (SOTA) methods that leverage only</cell></row><row><cell cols="2">keypoint information on the VD Orig-</cell></row><row><cell cols="2">inal split. COMPOSER outperforms existing</cell></row><row><cell cols="2">methods and achieves a new highest record</cell></row><row><cell>(+0.7% improvement)</cell><cell></cell></row><row><cell>Model</cell><cell>Keypoint Acc. Actor Object</cell></row><row><cell>Zappardino et al. [119]</cell><cell>91.0</cell></row><row><cell>GIRN [82]</cell><cell>88.4</cell></row><row><cell></cell><cell>92.2</cell></row><row><cell>AT [29]</cell><cell>92.3</cell></row><row><cell></cell><cell>92.8</cell></row><row><cell>POGARS [102]</cell><cell>93.2</cell></row><row><cell></cell><cell>93.9</cell></row><row><cell>COMPOSER (ours)</cell><cell>93.7</cell></row><row><cell></cell><cell>94.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Proceedings, Part IX16. pp. 177-195. Springer (2020)   28. Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.:Multiscale vision transformers. arXiv preprint arXiv:2104.11227 (2021) 29. Gavrilyuk, K., Sanford, R., Javan, M., Snoek, C.G.: Actor-transformers for group activity recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 839-848 (2020) 30. Glorot, X., Bordes, A.,Bengio, Y.: Deep sparse rectifier neural networks. In: Proceedings of the fourteenth international conference on artificial intelligence and statistics. pp. 315-323. JMLR Workshop and Conference Proceedings (2011) 31. Gong, Z., Zhong, P., Yu, Y., Hu, W., Li, S.: A cnn with multiscale convolution and diversified metric for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing 57(6), 3599-3618 (2019) 32. Grunde-McLaughlin, M., Krishna, R., Agrawala, M.: Agqa: A benchmark for compositional spatio-temporal reasoning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11287-11297 (2021) 33. Haber, E., Ruthotto, L., Holtham, E., Jun, S.H.: Learning across scalesmultiscale methods for convolution neural networks. In: Thirty-Second AAAI Conference on Artificial Intelligence (2018) 34. Hajimirsadeghi, H., Yan, W., Vahdat, A., Mori, G.: Visual recognition by counting instances: A multi-instance cardinality potential kernel. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2596-2605 (2015) 35. Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer. arXiv preprint arXiv:2103.00112 (2021) 36. He, K., Gkioxari, G., Doll?r, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961-2969 (2017) 37. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016) 38. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016) 39. Hinojosa, C., Niebles, J.C., Arguello, H.: Learning privacy-preserving optics for human pose estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2573-2582 (2021) 40. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735-1780 (1997) 41. Hu, G., Cui, B., He, Y., Yu, S.: Progressive relation learning for group activity recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 980-989 (2020) 42. Huang, Y., Kadav, A., Lai, F., Patel, D., Graf, H.P.: Learning higher-order object interactions for keypoint-based video understanding (2021) 43. Hudson, D., Manning, C.D.: Learning by abstraction: The neural state machine. Advances in Neural Information Processing Systems 32, 5903-5916 (2019) 44. Hudson, D.A., Manning, C.D.: Compositional attention networks for machine reasoning. In: International Conference on Learning Representations (2018) 45. Ibrahim, M.S., Mori, G.: Hierarchical relational networks for group activity recognition and retrieval. In: Proceedings of the European conference on computer vision (ECCV). pp. 721-736 (2018) 46. Ibrahim, M.S., Mori, G.: Hierarchical relational networks for group activity recognition and retrieval. In: Proceedings of the European conference on computer vision (ECCV). pp. 721-736 (2018) 47. Ibrahim, M.S., Muralidharan, S., Deng, Z., Vahdat, A., Mori, G.: A hierarchical deep temporal model for group activity recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1971-1980 (2016) 48. Irsoy, O., Cardie, C.: Deep recursive neural networks for compositionality in language. In: Advances in neural information processing systems. pp. 2096-2104 (2014) 49. Jaiswal, A., Singh, S., Wu, Y., Natarajan, P., Natarajan, P.: Keypoints-aware object detection. In: NeurIPS 2020 Workshop on Pre-registration in Machine Learning. pp. 62-72. PMLR (2021) 50. Ji, J., Krishna, R., Fei-Fei, L., Niebles, J.C.: Action genome: Actions as compositions of spatio-temporal scene graphs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10236-10247 (2020) 51. Jia, B., Chen, Y., Huang, S., Zhu, Y., Zhu, S.c.: Lemma: A multi-view dataset for learning multi-agent multi-task activities. In: European Conference on Computer Vision. pp. 767-786. Springer (2020) 52. Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., Krishnan, D.: Supervised contrastive learning. arXiv preprint arXiv:2004.11362 (2020) 87. Sendo, K., Ukita, N.: Heatmapping of people involved in group activities. In: 2019 16th International Conference on Machine Vision Applications (MVA). pp. 1-6. IEEE (2019) 88. Shao, D., Zhao, Y., Dai, B., Lin, D.: Finegym: A hierarchical video dataset for fine-grained action understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2616-2625 (2020) 89. Shi, C., Holtz, C., Mishne, G.: Online adversarial purification based on selfsupervised learning. In: International Conference on Learning Representations (2020) 90. Shu, T., Todorovic, S., Zhu, S.C.: Cern: confidence-energy recurrent network for group activity recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5523-5531 (2017) 91. Shu, T., Todorovic, S., Zhu, S.C.: Cern: confidence-energy recurrent network for group activity recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5523-5531 (2017) 92. Shu, X., Tang, J., Qi, G., Liu, W., Yang, J.: Hierarchical long short-term concurrent memory for human interaction recognition. IEEE transactions on pattern analysis and machine intelligence (2019) 93. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 94. Singh, K.K., Mahajan, D., Grauman, K., Lee, Y.J., Feiszli, M., Ghadiyaram, D.: Don't judge an object by its context: Learning to overcome contextual bias. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11070-11078 (2020) 95. Snower, M., Kadav, A., Lai, F., Graf, H.P.: 15 keypoints is all you need. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6738-6748 (2020) 96. Socher, R., Karpathy, A.</figDesc><table><row><cell>. In:</cell></row><row><cell>Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August</cell></row><row><cell>23-28, 2020,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 .</head><label>1</label><figDesc>Impact of the number of prototypes. GAR accuracy of COMPOSER on the Original split of the Volleyball dataset using different number of prototypes.</figDesc><table><row><cell>Number of Prototypes</cell><cell>10</cell><cell>50</cell><cell>100 1, 000 5, 000 10, 000</cell></row><row><cell>GAR Accuracy (%)</cell><cell cols="3">94.02 94.54 94.69 94.62 94.54 94.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of COMPOSER on Volleyball original split. The ablation study verifies the effectiveness of each proposed technique.</figDesc><table><row><cell>Ablation</cell><cell>Test Acc. (%) ?</cell></row><row><cell>No Clustering</cell><cell>93.4</cell></row><row><cell>Label Consistency for Scale Agreement</cell><cell>93.9</cell></row><row><cell>1-Scale: Keypoint</cell><cell>91.2</cell></row><row><cell>2-Scale: Keypoint + Person</cell><cell>93.2</cell></row><row><cell>3-Scale: Keypoint + Person + Interaction</cell><cell>93.9</cell></row><row><cell>No Actor Dropout</cell><cell>94.0</cell></row><row><cell>No Horizontal Flip</cell><cell>94.0</cell></row><row><cell>No Horizontal Move</cell><cell>94.2</cell></row><row><cell>No Vertical Move</cell><cell>94.2</cell></row><row><cell>No Auxiliary Prediction</cell><cell>93.3</cell></row><row><cell>No Multiscale Transformer</cell><cell>88.1</cell></row><row><cell>Transformer Encoder Reverse Order</cell><cell>86.8</cell></row><row><cell>Transformer Encoder Parameter Sharing</cell><cell>93.4</cell></row><row><cell>All Tokens to One Transformer Encoder</cell><cell>92.4</cell></row><row><cell>Time-Varying Person Grouping</cell><cell>92.8</cell></row><row><cell>COMPOSER (our full model)</cell><cell>94.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 .</head><label>3</label><figDesc>Efficiency Comparison (FLOPs). Please see details in Appendix C. We also report the results of COMPOSER using POGARS's person keypoint estimation backbone -Hourglass [?] since POGARS [102] is the keypoint-only method that has the closest result to ours on the Volleyball dataset. Comparing COMPOSER with POGARS (both use Hourglass as the backbone): on VD Olympic split COMPOSER 92.9% v.s. POGARS 89.7%; and on VD Original split COMPOSER 94.3% v.s. POGARS 93.9%. The superiority of COMPOSER is not affected because COMPOSER is able to address noisy estimated keypoints and disregard inaccurate keypoints by modeling attention over the keypoints (unlike POGARS or AT that just model attention at the person scale). To support the keypoint-only setup, we compute the FLOPs for HRNet (our keypoint backbone) and RGB backbones used by prior works when obtaining features of all persons in a clip on the Volleyball dataset. FLOPs are: HRNet 0.9T, VGG-19 [93] 3.6T, VGG-16 [93] 2.8T, Inception-v3 [?] 0.7T, ResNet-18 [37] 0.3T, and AlexNet [?] 0.1T</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Backbone</cell><cell></cell><cell></cell><cell></cell><cell>GAR Model</cell></row><row><cell cols="8">HRNet VGG-19 VGG-16 Inception-v3 ResNet-18 AlexNet COMPOSER GroupFormer [65]</cell></row><row><cell>0.9 T</cell><cell>3.6 T</cell><cell>2.8 T</cell><cell>0.7 T</cell><cell>0.3 T</cell><cell>0.1 T</cell><cell>297 M</cell><cell>595 M</cell></row><row><cell cols="7">*Note: 'T' stands for trillion and 'M' for million. Ours are marked in bold.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc>Detailed comparisons between our results and the reported SOTA methods' results on the Collective Activity dataset. The top 3 performance scores are highlighted as: First, Second * , Third . COMPOSER outperforms the latest GAR methods that use a single modality (+2.8% improvement), and performs favorably compared against methods that exploit multiple expensive modalities (ours is the second best)</figDesc><table><row><cell>Method</cell><cell cols="2">Modality Keypoint RGB Flow Scene Keypoint RGB/Flow/Scene Backbone</cell><cell>Acc. ? (%)</cell></row><row><cell>HDTM [47]</cell><cell>-</cell><cell>AlexNet</cell><cell>81.5</cell></row><row><cell>CERN [90]</cell><cell>-</cell><cell>VGG-16</cell><cell>87.2</cell></row><row><cell>stagNet [84]</cell><cell>-</cell><cell>VGG-16</cell><cell>89.1</cell></row><row><cell>ARG [107]</cell><cell>--</cell><cell>VGG-16 Inception-v3</cell><cell>90.1 91.0</cell></row><row><cell>HiGCIN [112]</cell><cell>--</cell><cell>AlexNet ResNet-18</cell><cell>92.5 93.4</cell></row><row><cell>CRM [5]</cell><cell>-</cell><cell>I3D</cell><cell>85.8</cell></row><row><cell>Ehsanpour et al. [27]</cell><cell>-</cell><cell>I3D</cell><cell>89.4</cell></row><row><cell></cell><cell>HRNet</cell><cell>I3D</cell><cell>91.0</cell></row><row><cell>AT [29]</cell><cell>HRNet</cell><cell>I3D</cell><cell>91.2</cell></row><row><cell></cell><cell>-</cell><cell>I3D</cell><cell>92.8</cell></row><row><cell>SACRF [83]</cell><cell>-AlphaPose</cell><cell>I3D I3D</cell><cell>94.6 95.2</cell></row><row><cell></cell><cell>-</cell><cell>Inception-v3</cell><cell>93.6</cell></row><row><cell>GroupFormer [65]</cell><cell>-</cell><cell>I3D</cell><cell>94.7</cell></row><row><cell></cell><cell>AlphaPose</cell><cell>I3D</cell><cell>96.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>1 .</head><label>1</label><figDesc>Actor-related tokens associated with the key person(s) are often identified as the most important tokens for the CLS and object token by COMPOSER</figDesc><table><row><cell></cell><cell></cell><cell cols="2">video 11 clip 591</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Ground Truth: Talking Prediction: Talking</cell><cell>actor 0's keypoints</cell><cell cols="2">CLS</cell><cell cols="2">CLS</cell><cell>actor 1's knees, ankles</cell><cell>CLS actor 0</cell><cell>CLS</cell><cell>actor 2</cell></row><row><cell>4 NA</cell><cell>3 NA</cell><cell>1 Talking</cell><cell>0 Talking 2 Talking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>actor 1 actor 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>actor 4's keypoints</cell><cell></cell><cell></cell><cell></cell><cell>actor 3 actor 4</cell></row><row><cell></cell><cell></cell><cell cols="2">Group 1: 0,1,2</cell><cell>Paddings</cell><cell></cell><cell></cell><cell></cell><cell>Paddings</cell></row><row><cell></cell><cell></cell><cell cols="2">Group 2: 3,4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Scale 1: Keypoint</cell><cell>(b) Scale 2: Per son</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CLS</cell><cell cols="2">CLS</cell><cell>actor 2-actor 3 actor 2-actor 1, actor 2-actor 0,</cell><cell>actor 3-actor 1 actor 3-actor 0, actor 4-actor 0, actor 4-actor 1</cell><cell>CLS</cell><cell>Gr oup 1 Gr oup 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLS</cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell><cell cols="3">inter actions actor 4's</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gr oup 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gr oup 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) Scale 3: Inter action</cell><cell>(d) Scale 4: Gr oup</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Original split of the Volleyball dataset allows the GAR method to leverage scene biases in order to achieve a high accuracy, on the other hand, the Olympic split can better test the model generalization ability. None of the exsting RGB-baesd GAR methods have performed experiments using the Olympic split. To obtain the results of prior RGB-based GAR methods on the Olympic split of the Volleyball dataset (Table 1 in the main paper), our implementations of PCTDM [110], SACRF [83], AT [29], ARG [107], TCE-STBiP [116] and DIN [117] are based on the public available codebase 1011 and we have verified the implementations through obtaining the results of these methods on the Original split of the Volleyball dataset and then comparing with the reported results from authors of each method. For VGG-16 [93], we use RoIAlign [36]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>or semantically [88,100], or decomposing actions by action-based aspects (verbs) and object components (noun) [50,51,73,62,72].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Fig. 5. Qualitative results of COMPOSER on CAD -showcasing attention matrices of a test set instance in the "moving" class. Qualitative results of COMPOSER on CAD -showcasing attention matrices of a test set instance in the "quequeing" class.Fig. 7. Qualitative results of COMPOSER on CAD -showcasing attention matrices of a test set instance in the "waiting" class. It is noteworthy that the keypoints of actor 4 and 5 are noisy due to occlusion; yet COMPOSER makes a correct prediction. Qualitative results of COMPOSER on VD -showcasing attention matrices of a test set instance in the "right set" class (key actor is actor 0).</figDesc><table><row><cell>actor 1's nose, eyes actor 0-actor 2 (a) Scale 1: Keypoint (c) Scale 3: Inter action CLS actor 2's knees, ankles CLS actor 2's inter actions actor 0's keypoints CLS actor 2's keypoints Paddings CLS actor 1-actor 2 actor 1's knees, ankles actor 0-actor 2 actor 0-actor 3 (a) Scale 1: Keypoint (c) Scale 3: Inter action CLS actor 3's knees, ankles CLS actor 2's inter actions actor 0's keypoints CLS actor 3's keypoints Paddings CLS actor 2's knees, ankles actor 1-actor 2 actor 1-actor 3 actor 2-actor 3 actor 3-actor 2 Gr oup 2 CLS Gr oup 1 CLS actor 1 actor 2 actor 0 Paddings CLS Gr oup 2 Gr oup 1 CLS actor 1 actor 2 actor 3 actor 0 Paddings CLS actor 2 (b) Scale 2: Per son (d) Scale 4: Gr oup CLS Gr oup 1 Gr oup 2 CLS actor 2 (b) Scale 2: Per son (d) Scale 4: Gr oup CLS Gr oup 1 Gr oup 2 Fig. 6. CLS video 16 clip 331 Ground Truth: Moving Prediction: Moving ... ... Group 1: 0,1 Group 2: 2 1 Moving 0 Moving 2 Moving video 25 clip 181 Ground Truth: Queueing Prediction: Queueing ... ... 1 Queueing Group 1: 1,2,3 Group 2: 0 2 Queueing 3 Queueing 0 Queueing actor 0 actor 1-actor 0 (a) Scale 1: Keypoint (b) Scale 2: Per son (c) Scale 3: Inter action (d) Scale 4: Gr oup CLS Gr oup 1 Gr oup 2 CLS Gr oup 2 Gr oup 1 CLS actor 1 actor 2 CLS actor 1's knees, ankles CLS actor 5's inter actions actor 0's keypoints video 28 clip 161 Ground Truth: Waiting Prediction: Waiting CLS actor 5's keypoints Paddings CLS ... ... actor 0 Paddings actor 3-actor 0 0 Waiting 1 Waiting 3 NA 5 NA 4 NA 2 Waiting actor 3 actor 4 actor 5 actor 4 actor 3 actor 5 actor 2-actor 0 Group 1: 0,1,2,4 Group 2: 3,5 (a) Scale 1: Keypoint (b) Scale 2: Per son (c) Scale 3: Inter action (d) Scale 4: Gr oup ball CLS Left Gr oup Right Gr oup CLS ball Right Gr oup Left Gr oup CLS ball actor 0 actor 1 actor 2 actor 3 actor 4 actor 5 actor 6 actor 7 actor 8 actor 9 actor 10 actor 11 CLS ball video 20 clip 51145 right set ... 1 2 4 10 7 9 3 ... ... 6 8 11 0 5 CLS ball actor 11's inter actions left knee, r ight knee, left ankle, r ight ankle left wr ist, r ight wr ist actor 0 ball CLS ball CLS actor 0 actor 4 actor 11 actor 0-actor 1, actor 0-actor 2 actor 0-actor 4, actor 0-actor 5 (b) 0 5 8 6 11 7 10 9 1 2 4 3 ball ball CLS actor 0 actor 1 actor 11 (a) Scale 1: Keypoint (b) Scale 2: Per son (c) Scale 3: Inter action (d) Scale 4: Gr oup ball CLS Left Gr oup Right Gr oup actor 2 CLS ball Right Gr oup Left Gr oup CLS ball actor 0 actor 1 actor 2 actor 3 actor 4 actor 5 actor 6 actor 7 actor 8 actor 9 actor 10 actor 11 ankles of actor 2 CLSball CLS ball actor 0's inter actions actor 4's inter actions actor 0's keypoints video 11 clip 80715 left spike ... 1 2 4 10 7 9 3 ... ... CLS ball actor 3's keypoints actor 11's keypoints actor 11's keypoints 6 8 11 0 5 CLS ball actor 11's inter actions 3 4 2 1 9 10 7 11 6 8 5 0 actor 0's inter actions actor 1's inter actions actor 2's inter actions ankles of actor 1 Fig. 8. wr ists of actor 0 actor 2-actor 0 actor 2-actor 1 ball</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Even for the keypoint extraction backbone which our method is agnostic to, there are existing works [39] that perform privacy-preserving keypoint estimation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use track-based representations [102,119,29,65] to represent each token.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This COMPOSER variant consumes RGB-based ROI-aligned person features as input, and thus only models 3 scales: person, interaction, and the group scale.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/subhadarship/kmeans_pytorch 7 For real-world data, one can resort to ball trajectory extraction<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">79]</ref> for sports videos or object keypoint detection tools [42,<ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">69</ref>]. 8 https://github.com/facebookresearch/swav 9 https://pytorch.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/JacobYuan7/DIN-Group-Activity-Recognition-Benchmark 11 https://github.com/wjchaoGit/Group-Activity-Recognition 12 https://github.com/longcw/RoIAlign.pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ball recognition and tracking in live volleyball game</title>
		<ptr target="https://github.com/tprlab/vball" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mediapipe pose: Ml solution for high-fidelity body pose tracking from rgb video frames</title>
		<ptr target="https://google.github.io/mediapipe/solutions/pose.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Groupsense: recognizing and understanding group physical activities using multi-device embedded sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Abkenar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Loke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaslavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rahayu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems (TECS)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional relational machine for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Atigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nickabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7892" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional relational machine for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Atigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nickabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7892" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Social scene understanding: End-to-end multi-person action localization and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4315" to="4324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-automatic 3d object keypoint annotation and detection for the masses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Blomqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07665</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From machine learning to machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyfourth Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12671</idno>
		<title level="m">Multimodal clustering networks for self-supervised learning from unlabeled videos</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Group activity recognition via computing human pose motion history and collective map from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="705" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rit-18: A novel dataset for compositional group activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="362" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Group activity recognition by gaussian processes estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3228" to="3231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why can&apos;t i dance in a mall? learning to mitigate scene bias in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd</title>
		<meeting>the 33rd</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative context learning with gated recurrent unit for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="149" to="161" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02109</idno>
		<title level="m">Safcar: Structured attention fusion for compositional action recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning human interaction by interactive phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="300" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrastive learning for sports video: Unsupervised player classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koshkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pidaparthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4528" to="4536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object keypoints for perception and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Discriminative latent models for recognizing contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Robinovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1549" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08914</idno>
		<title level="m">C 3 : Compositional counterfactual constrastive learning for video-grounded dialogues</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual saliency detection based on multiscale deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic multiscale graph neural networks for 3d skeleton based human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Groupformer: Group activity recognition with clustered spatial-temporal transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13668" to="13677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sbgar: Semantics based group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choo Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2876" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ms2l: Multi-task self-supervised learning for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2490" to="2498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Few-shot keypoint detection with uncertainty learning for unseen species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06183</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatio-temporal attention mechanisms based model for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="162" to="174" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gaim: Graph attention interaction model for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="524" to="539" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Moma: Multi-object multi-actor activity parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Somethingelse: Compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1049" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal poselets for collective activity detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="500" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Group activity recognition using joint learning of individual action recognition and people grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sendo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 17th International Conference on Machine Vision and Applications (MVA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Consensus-based matching and tracking of keypoints for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="862" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venugopal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08417</idno>
		<title level="m">Scene transformer: A unified architecture for predicting multiple agent trajectories</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Geomnet: A neural network based on riemannian geometries of spd matrix space and cholesky space for 3d skeleton-based interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13379" to="13389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interaction relational network for mutual action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Skeleton-based relational reasoning for group activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition p</title>
		<imprint>
			<biblScope unit="page">108360</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Empowering relational network by self-attention augmented conditional random fields for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R A</forename><surname>Pramono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="71" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">stagnet: An attentive semantic rnn for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Home action genome: Cooperative compositional action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kozuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishizaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11184" to="11193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3043" to="3053" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
