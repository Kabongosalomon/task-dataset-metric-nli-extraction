<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Self-Supervised Visual Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
							<email>akolesnikov@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<email>xzhai@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<email>lbeyer@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Z?rich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Switzerland</surname></persName>
						</author>
						<title level="a" type="main">Revisiting Self-Supervised Visual Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automated computer vision systems have recently made drastic progress. Many models for tackling challenging tasks such as object recognition, semantic segmentation or object detection can now compete with humans on complex visual benchmarks <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b13">14]</ref>. However, the success of such systems hinges on a large amount of labeled data, which is not always available and often prohibitively expensive to acquire. Moreover, these systems are tailored to specific scenarios, e.g. a model trained on the ImageNet (ILSVRC-2012) dataset <ref type="bibr" target="#b41">[41]</ref> can only recognize 1000 semantic categories or a model that was trained to perceive road traffic at daylight may not work in darkness <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>As a result, a large research effort is currently focused on systems that can adapt to new conditions without leverag-* equal contribution Rotation <ref type="bibr" target="#b9">[10]</ref> Exemplar <ref type="bibr" target="#b7">[8]</ref> Rel. Patch Loc. <ref type="bibr" target="#b5">[6]</ref> Jigsaw <ref type="bibr" target="#b29">[29]</ref>  RevNet50 ResNet50 v2 ResNet50 v1 <ref type="figure">Figure 1</ref>. Quality of visual representations learned by various self-supervised learning techniques significantly depends on the convolutional neural network architecture that was used for solving the self-supervised learning task. In our paper we provide a large scale in-depth study in support of this observation and discuss its implications for evaluation of self-supervised models.</p><p>ing a large amount of expensive supervision. This effort includes recent advances on transfer learning, domain adaptation, semi-supervised, weakly-supervised and unsupervised learning. In this paper, we concentrate on self-supervised visual representation learning, which is a promising subclass of unsupervised learning. Self-supervised learning techniques produce state-of-the-art unsupervised representations on standard computer vision benchmarks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>The self-supervised learning framework requires only unlabeled data in order to formulate a pretext learning task such as predicting context <ref type="bibr" target="#b6">[7]</ref> or image rotation <ref type="bibr" target="#b10">[11]</ref>, for which a target objective can be computed without supervision. These pretext tasks must be designed in such a way that high-level image understanding is useful for solving them. As a result, the intermediate layers of convolutional neural networks (CNNs) trained for solving these pretext tasks encode high-level semantic visual representations that are useful for solving downstream tasks of interest, such as image recognition.</p><p>Most of the prior work, which aims at improving performance of self-supervised techniques, does so by proposing novel pretext tasks and showing that they result in improved representations. Instead, we propose to have a closer look at CNN architectures. We revisit a prominent subset of the previously proposed pretext tasks and perform a large-scale empirical study using various architectures as base models. As a result of this study, we uncover numerous crucial insights. The most important are summarized as follows:</p><p>? Standard architecture design recipes do not necessarily translate from the fully-supervised to the selfsupervised setting. Architecture choices which negligibly affect performance in the fully labeled setting, may significantly affect performance in the selfsupervised setting.</p><p>? In contrast to previous observations with the AlexNet architecture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b34">34]</ref>, the quality of learned representations in CNN architectures with skip-connections does not degrade towards the end of the model.</p><p>? Increasing the number of filters in a CNN model and, consequently, the size of the representation significantly and consistently increases the quality of the learned visual representations.</p><p>? The evaluation procedure, where a linear model is trained on a fixed visual representation using stochastic gradient descent, is sensitive to the learning rate schedule and may take many epochs to converge.</p><p>In Section 4 we present experimental results supporting the above observations and offer additional in-depth insights into the self-supervised learning setting. We make the code for reproducing our core experimental results publicly available <ref type="bibr" target="#b0">1</ref> .</p><p>In our study we obtain new state-of-the-art results for visual representations learned without labeled data. Interestingly, the context prediction <ref type="bibr" target="#b6">[7]</ref> technique that sparked the interest in self-supervised visual representation learning and that serves as the baseline for follow-up research, outperforms all currently published results (among papers on self-supervised learning) if the appropriate CNN architecture is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervision is a learning framework in which a supervised signal for a pretext task is created automatically, in an effort to learn representations that are useful for solving real-world downstream tasks. Being a generic framework, self-supervision enjoys a wide number of applications, ranging from robotics to image understanding. 1 https://github.com/google/revisiting-self-supervised In robotics, both the result of interacting with the world, and the fact that multiple perception modalities simultaneously get sensory inputs are strong signals which can be exploited to create self-supervised tasks <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Similarly, when learning representation from videos, one can either make use of the synchronized cross-modality stream of audio, video, and potentially subtitles <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b47">47]</ref>, or of the consistency in the temporal dimension <ref type="bibr" target="#b44">[44]</ref>.</p><p>In this paper we focus on self-supervised techniques that learn from image databases. These techniques have demonstrated impressive results for learning high-level image representations. Inspired by unsupervised methods from the natural language processing domain which rely on predicting words from their context <ref type="bibr" target="#b31">[31]</ref>, Doersch et al. <ref type="bibr" target="#b6">[7]</ref> proposed a practically successful pretext task of predicting the relative location of image patches. This work spawned a line of work in patch-based self-supervised visual representation learning methods. These include a model from <ref type="bibr" target="#b34">[34]</ref> that predicts the permutation of a "jigsaw puzzle" created from the full image and recent follow-ups <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b36">36]</ref>.</p><p>In contrast to patch-based methods, some methods generate cleverly designed image-level classification tasks. For instance, in <ref type="bibr" target="#b10">[11]</ref> Gidaris et al. propose to randomly rotate an image by one of four possible angles and let the model predict that rotation. Another way to create class labels is to use clustering of the images <ref type="bibr" target="#b2">[3]</ref>. Yet another class of pretext tasks contains tasks with dense spatial outputs. Some prominent examples are image inpainting <ref type="bibr" target="#b40">[40]</ref>, image colorization <ref type="bibr" target="#b50">[50]</ref>, its improved variant split-brain <ref type="bibr" target="#b51">[51]</ref> and motion segmentation prediction <ref type="bibr" target="#b39">[39]</ref>. Other methods instead enforce structural constraints on the representation space. Noroozi et al. propose an equivariance relation to match the sum of multiple tiled representations to a single scaled representation <ref type="bibr" target="#b35">[35]</ref>. Authors of <ref type="bibr" target="#b37">[37]</ref> propose to predict future patches in representation space via autoregressive predictive coding.</p><p>Our work is complimentary to the previously discussed methods, which introduce new pretext tasks, since we show how existing self-supervision methods can significantly benefit from our insights.</p><p>Finally, many works have tried to combine multiple pretext tasks in one way or another. For instance, Kim et al. extend the "jigsaw puzzle" task by combining it with colorization and inpainting in <ref type="bibr" target="#b25">[25]</ref>. Combining the jigsaw puzzle task with clustering-based pseudo labels as in <ref type="bibr" target="#b2">[3]</ref> leads to the method called Jigsaw++ <ref type="bibr" target="#b36">[36]</ref>. Doersch and Zisserman <ref type="bibr" target="#b7">[8]</ref> implement four different self-supervision methods and make one single neural network learn all of them in a multi-task setting.</p><p>The latter work is similar to ours since it contains a comparison of different self-supervision methods using a unified neural network architecture, but with the goal of combining all these tasks into a single self-supervision task. The au-thors use a modified ResNet101 architecture <ref type="bibr" target="#b16">[16]</ref> without further investigation and explore the combination of multiple tasks, whereas our focus lies on investigating the influence of architecture design on the representation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-supervised study setup</head><p>In this section we describe the setup of our study and motivate our key choices. We begin by introducing six CNN models in Section 3.1 and proceed by describing the four self-supervised learning approaches used in our study in Section 3.2. Subsequently, we define our evaluation metrics and datasets in Sections 3.3 and 3.4. Further implementation details can be found in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architectures of CNN models</head><p>A large part of the self-supervised techniques for visual representation approaches uses AlexNet <ref type="bibr" target="#b27">[27]</ref> architecture. In our study, we investigate whether the landscape of self-supervision techniques changes when using modern network architectures. Thus, we employ variants of ResNet and a batch-normalized VGG architecture, all of which achieve high performance in the fully-supervised training setup. VGG is structurally close to AlexNet as it does not have skip-connections and uses fully-connected layers.</p><p>In our preliminary experiments, we observed an intriguing property of ResNet models: the quality of the representations they learn does not degrade towards the end of the network (see Section 4.5). We hypothesize that this is a result of skip-connections making residual units invertible under certain circumstances <ref type="bibr" target="#b1">[2]</ref>, hence facilitating the preservation of information across the depth even when it is irrelevant for the pretext task. Based on this hypothesis, we include RevNets <ref type="bibr" target="#b11">[12]</ref> into our study, which come with stronger invertibility guarantees while being structurally similar to ResNets.</p><p>ResNet was introduced by He et al. <ref type="bibr" target="#b16">[16]</ref>, and we use the width-parametrization proposed in <ref type="bibr" target="#b49">[49]</ref>: the first 7 ? 7 convolutional layer outputs 16 ? k channels, where k is the widening factor, defaulting to 4. This is followed by a series of residual units of the form y := x + F(x), where F is a residual function consisting of multiple convolutions, ReLU non-linearities <ref type="bibr" target="#b33">[33]</ref> and batch normalization layers <ref type="bibr" target="#b20">[20]</ref>. The variant we use, ResNet50, consists of four blocks with 3, 4, 6, and 3 such units respectively, and we refer to the output of each block as block1, block2, etc. The network ends with a global spatial average pooling producing a vector of size 512 ? k, which we call pre-logits as it is followed only by the final, task-specific logits layer. More details on this architecture are provided in <ref type="bibr" target="#b16">[16]</ref>.</p><p>In our experiments we explore k ? {4, 8, 12, 16}, resulting in pre-logits of size 2048, 4096, 6144 and 8192 respectively. For some self-supervised techniques we skip config-urations that do not fit into memory.</p><p>Moreover, we analyze the sensitivity of the selfsupervised setting to underlying architectural details by using two variants of ordering operations known as ResNet v1 <ref type="bibr" target="#b16">[16]</ref> and ResNet v2 <ref type="bibr" target="#b17">[17]</ref> as well as a variant without ReLU preceding the global average pooling, which we mark by a "(-)". Notably, these variants perform similarly on the pretext task.</p><p>RevNet slightly modifies the design of the residual unit such that it becomes analytically invertible <ref type="bibr" target="#b11">[12]</ref>. We note that the residual unit used in <ref type="bibr" target="#b11">[12]</ref> is equivalent to double application of the residual unit from <ref type="bibr" target="#b21">[21]</ref> or <ref type="bibr" target="#b5">[6]</ref>. Thus, for conceptual simplicity, we employ the latter type of unit, which can be defined as follows. The input x is split channel-wise into two equal parts x 1 and x 2 . The output y is then the concatenation of y 2 := x 2 and y 1 := x 1 + F(x 2 ).</p><p>It easy to see that this residual unit is invertible, because its inverse can be computed in closed form as x 2 = y 2 and</p><formula xml:id="formula_0">x 1 = y 1 ? F(x 2 ).</formula><p>Apart from this slightly different residual unit, RevNet is structurally identical to ResNet and thus we use the same overall architecture and nomenclature for both. In our experiments we use RevNet50 network, that has the same depth and number of channels as the original Resnet50 model. In the fully labelled setting, RevNet performs only marginally worse than its architecturally equivalent ResNet.</p><p>VGG as proposed in <ref type="bibr" target="#b45">[45]</ref> consists of a series of 3 ? 3 convolutions followed by ReLU non-linearities, arranged into blocks separated by max-pooling operations. The VGG19 variant we use has 5 such blocks of 2, 2, 4, 4, and 4 convolutions respectively. We follow the common practice of adding batch normalization between the convolutions and non-linearities.</p><p>In an effort to unify the nomenclature with ResNets, we introduce the widening factor k such that k = 8 corresponds to the architecture in <ref type="bibr" target="#b45">[45]</ref>, i.e. the initial convolution produces 8 ? k channels and the fully-connected layers have 512 ? k channels. Furthermore, we call the inputs to the second, third, fourth, and fifth max-pooling operations block1 to block4, respectively, and the input to the last fullyconnected layer pre-logits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-supervised techniques</head><p>In this section we describe the self-supervised techniques that are used in our study.</p><p>Rotation <ref type="bibr" target="#b10">[11]</ref>: Gidaris et al. propose to produce 4 copies of a single image by rotating it by {0?, 90?, 180?, 270?} and let a single network predict the rotation which was applied-a 4-class classification task. Intuitively, a good model should learn to recognize canonical orientations of objects in natural images.</p><p>Exemplar <ref type="bibr" target="#b8">[9]</ref>: In this technique, every individual image corresponds to its own class, and multiple examples of it are generated by heavy random data augmentation such as translation, scaling, rotation, and contrast and color shifts. We use data augmentation mechanism from <ref type="bibr" target="#b46">[46]</ref>. <ref type="bibr" target="#b7">[8]</ref> proposes to use the triplet loss <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b18">18]</ref> in order to scale this pretext task to a large number of images (hence, classes) present in the ImageNet dataset. The triplet loss avoids explicit class labels and, instead, encourages examples of the same image to have representations that are close in the Euclidean space while also being far from the representations of different images. Example representations are given by a 1000-dimensional logits layer.</p><p>Jigsaw <ref type="bibr" target="#b34">[34]</ref>: the task is to recover relative spatial position of 9 randomly sampled image patches after a random permutation of these patches was performed. All of these patches are sent through the same network, then their representations from the pre-logits layer are concatenated and passed through a two hidden layer fully-connected multi-layer perceptron (MLP), which needs to predict a permutation that was used. In practice, the fixed set of 100 permutations from <ref type="bibr" target="#b34">[34]</ref> is used.</p><p>In order to avoid shortcuts relying on low-level image statistics such as chromatic aberration <ref type="bibr" target="#b34">[34]</ref> or edge alignment, patches are sampled with a random gap between them. Each patch is then independently converted to grayscale with probability 2 ?3 and normalized to zero mean and unit standard deviation. More details on the preprocessing are provided in Supplementary Material. After training, we extract representations by averaging the representations of nine uniformly sampled, colorful, and normalized patches of an image.</p><p>Relative Patch Location <ref type="bibr" target="#b6">[7]</ref>: The pretext task consists of predicting the relative location of two given patches of an image. The model is similar to the Jigsaw one, but in this case the 8 possible relative spatial relations between two patches need to be predicted, e.g. "below" or "on the right and above". We use the same patch prepossessing as in the Jigsaw model and also extract final image representations by averaging representations of 9 cropped patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation of Learned Visual Representations</head><p>We follow common practice and evaluate the learned visual representations by using them for training a linear logistic regression model to solve multiclass image classification tasks requiring high-level scene understanding. These tasks are called downstream tasks. We extract the representation from the (frozen) network at the pre-logits level, but investigate other possibilities in Section 4.5.</p><p>In order to enable fast evaluation, we use an efficient convex optimization technique for training the logistic regression model unless specified otherwise. Specifically, we precompute the visual representation for all training images and train the logistic regression using L-BFGS <ref type="bibr" target="#b30">[30]</ref>.</p><p>For consistency and fair evaluation, when comparing to the prior literature in <ref type="table" target="#tab_0">Table 1</ref>, we opt for using stochastic gradient descent (SGD) with momentum and use data augmentation during training.</p><p>We further investigate this common evaluation scheme in Section 4.3, where we use a more expressive model, which is an MLP with a single hidden layer with 1000 channels and the ReLU non-linearity after it. More details are given in Supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Datasets</head><p>In our experiments, we consider two widely used image classification datasets: ImageNet and Places205.</p><p>ImageNet contains roughly 1.3 million natural images that represent 1000 various semantic classes. There are 50 000 images in the official validation and test sets, but since the official test set is held private, results in the literature are reported on the validation set. In order to avoid overfitting to the official validation split, we report numbers on our own validation split (50 000 random images from the training split) for all our studies except in <ref type="table" target="#tab_2">Table 2</ref>, where for a fair comparison with the literature we evaluate on the official validation set.</p><p>The Places205 dataset consists of roughly 2.5 million images depicting 205 different scene types such as airfield, kitchen, coast, etc. This dataset is qualitatively different from ImageNet and, thus, a good candidate for evaluating how well the learned representations generalize to new unseen data of different nature. We follow the same procedure as for ImageNet regarding validation splits for the same reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section we present and interpret results of our large-scale study. All self-supervised models are trained on ImageNet (without labels) and consequently evaluated on our own hold-out validation splits of ImageNet and Places205. Only in <ref type="table" target="#tab_2">Table 2</ref>, when we compare to the results from the prior literature, we use the official ImageNet and Places205 validation splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on ImageNet and Places205</head><p>In <ref type="table" target="#tab_0">Table 1</ref> we highlight our main evaluation results: we measure the representation quality produced by six different CNN architectures with various widening factors (Section 3.1), trained using four self-supervised learning techniques (Section 3.2). We use the pre-logits of the trained self-supervised networks as representation. We follow the standard evaluation protocol (Section 3.3) which measures representation quality as the accuracy of a linear regression model trained and evaluated on the ImageNet dataset. Now we discuss key insights that can be learned from the table and motivate our further in-depth analysis. First, we observe that similar models often result in visual representations that have significantly different performance. Importantly, neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures. For instance, the RevNet50 v2 model excels under Rotation self-supervision, but is not the best model in other scenarios. Similarly, relative patch location seems to be the best method when basing the comparison on the ResNet50 v1 architecture, but not otherwise. Notably, VGG19-BN consistently demonstrates the worst performance, even though it achieves performance similar to ResNet50 models on standard vision benchmarks <ref type="bibr" target="#b45">[45]</ref>. Note that VGG19-BN performs better when using representations from layers earlier than the prelogit layer are used, though still falls short. We investigate this in Section 4.5. We depict the performance of the models with the largest widening factor in <ref type="figure" target="#fig_1">Figure 2</ref> (left), which displays these ranking inconsistencies.</p><p>Our second observation is that increasing the number of channels in CNN models improves performance of selfsupervised models. While this finding is in line with the fully-supervised setting <ref type="bibr" target="#b49">[49]</ref>, we note that the benefit is more pronounced in the context of self-supervised represen-    tation learning, a fact not yet acknowledged in the literature.</p><p>We further evaluate how visual representations trained in a self-supervised manner on ImageNet generalize to other datasets. Specifically, we evaluate all our models on the Places205 dataset using the same evaluation protocol. The performance of models with the largest widening factor are reported in <ref type="figure" target="#fig_1">Figure 2</ref> (right) and the full result table is provided in Supplementary Material. We observe the following pattern: ranking of models evaluated on Places205 is consistent with that of models evaluated on ImageNet, indicating that our findings generalize to new datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to prior work</head><p>In order to put our findings in context, we select the best model for each self-supervision from <ref type="table" target="#tab_0">Table 1</ref> and compare them to the numbers reported in the literature. For this experiment only, we precisely follow standard protocol by training the linear model with stochastic gradient descent (SGD) on the full ImageNet training split and evaluating it on the public validation set of both ImageNet and Places205. We note that in this case the learning rate schedule of the evaluation plays an important role, which we elaborate in Section 4.7. <ref type="table" target="#tab_2">Table 2</ref> summarizes our results. Surprisingly, as a result of selecting the right architecture for each self-supervision and increasing the widening factor, our models significantly outperform previously reported results. Notably, context prediction <ref type="bibr" target="#b6">[7]</ref> Jigsaw <ref type="figure">Figure 4</ref>. A look at how predictive pretext performance is to eventual downstream performance. Colors correspond to the architectures in <ref type="figure" target="#fig_3">Figure 3</ref> and circle size to the widening factor k. Within an architecture, pretext performance is somewhat predictive, but it is not so across architectures. For instance, according to pretext accuracy, the widest VGG model is the best one for Rotation, but it performs poorly on the downstream task.</p><p>Importantly, our design choices result in almost halving the gap between previously published self-supervised result and fully-supervised results on two standard benchmarks. Overall, these results reinforce our main insight that in selfsupervised learning architecture choice matters as much as choice of a pretext task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">A linear model is adequate for evaluation.</head><p>Using a linear model for evaluating the quality of a representation requires that the information relevant to the evaluation task is linearly separable in representation space. This is not necessarily a prerequisite for a "useful" representation. Furthermore, using a more powerful model in the evaluation procedure might make the architecture choice for a self-supervised task less important. Hence, we consider an alternative evaluation scenario where we use a multi-layer perceptron (MLP) for solving the evaluation task, details of which are provided in Supplementary Material. <ref type="figure" target="#fig_3">Figure 3</ref> clearly shows that the MLP provides only marginal improvement over the linear evaluation and the relative performance of various settings is mostly unchanged. We thus conclude that the linear model is adequate for evaluation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Better performance on the pretext task does not always translate to better representations.</head><p>In many potential applications of self-supervised methods, we do not have access to downstream labels for evaluation. In that case, how can a practitioner decide which model to use? Is performance on the pretext task a good proxy?</p><p>In <ref type="figure">Figure 4</ref> we plot the performance on the pretext task against the evaluation on ImageNet. It turns out that performance on the pretext task is a good proxy only once the model architecture is fixed, but it can unfortunately not be used to reliably select the model architecture. Other labelfree mechanisms for model-selection need to be devised, which we believe is an important and underexplored area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Skip-connections prevent degradation of representation quality towards the end of CNNs.</head><p>We are interested in how representation quality depends on the layer choice and how skip-connections affect this dependency. Thus, we evaluate representations from five intermediate layers in three models: Resnet v2, RevNet and VGG19-BN. The results are summarized in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>Similar to prior observations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b34">34]</ref> for AlexNet <ref type="bibr" target="#b28">[28]</ref>, the quality of representations in VGG19-BN deteriorates towards the end of the network. We believe that this happens because the models specialize to the pretext task in the later layers and, consequently, discard more general semantic features present in the middle layers.</p><p>In contrast, we observe that this is not the case for models with skip-connections: representation quality in ResNet consistently increases up to the final pre-logits layer. We hypothesize that this is a result of ResNet's residual units being invertible under some conditions <ref type="bibr" target="#b1">[2]</ref>. Invertible units preserve all information learned in intermediate layers, and, thus, prevent deterioration of representation quality.</p><p>We further test this hypothesis by using the RevNet model that has stronger invertibility guarantees. Indeed, it boosts performance by more than 5 % on the Rotation task, albeit it does not result in improvements across other tasks. We leave identifying further scenarios where Revnet models result in significant boost of performance for the future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Model width and representation size strongly</head><p>influence the representation quality. <ref type="table" target="#tab_0">Table 1</ref> shows that using a wider network architecture consistently leads to better representation quality. It should be noted that increasing the network's width has the side- . Disentangling the performance contribution of network widening factor versus representation size. Both matter independently, and larger is always better. Scores are accuracies of logistic regression on ImageNet. Black squares mark models which are also present in <ref type="table" target="#tab_0">Table 1.</ref> effect of also increasing the dimensionality of the final representation (Section 3.1). Hence, it is unclear whether the increase in performance is due to increased network capacity or to the use of higher-dimensional representations, or to the interplay of both.</p><p>In order to answer this question, we take the best rotation model (RevNet50) and disentangle the network width from the representation size by adding an additional linear layer to control the size of the pre-logits layer. We then vary the widening factor and the representation size independently of each other, training each model from scratch on ImageNet with the Rotation pretext task. The results, evaluated on the ImageNet classification task, are shown in  <ref type="figure">Figure 7</ref>. Performance of the best models evaluated using all data as well as a subset of the data. The trend is clear: increased widening factor increases performance across the board. <ref type="figure" target="#fig_5">Figure 6</ref>. In essence, it is possible to increase performance by increasing either model capacity, or representation size, but increasing both jointly helps most. Notably, one can significantly boost performance of a very thin model from 31 % to 43 % by increasing representation size.</p><p>Low-data regime. In principle, the effectiveness of increasing model capacity and representation size might only work on relatively large datasets for downstream evaluation, and might hurt representation usefulness in the low-data regime. In <ref type="figure">Figure 7</ref>, we depict how the number of channels affects the evaluation using both full and heavily subsampled (10 % and 5 %) ImageNet and Places205 datasets.</p><p>We observe that increasing the widening factor consistently boosts performance in both the full-and low-data regimes. We present more low-data evaluation experiments in Supplementary Material. This suggests that selfsupervised learning techniques are likely to benefit from using CNNs with increased number of channels across wide range of scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">SGD for training linear model takes long time to converge</head><p>In this section we investigate the importance of the SGD optimization schedule for training logistic regression in downstream tasks. We illustrate our findings for linear evaluation of the Rotation task, others behave the same and are provided in Supplementary Material.</p><p>We train the linear evaluation models with a mini-batch size of 2048 and an initial learning rate of 0.1, which we decay twice by a factor of 10. Our initial experiments suggest that when the first decay is made has a large influence on the final accuracy. Thus, we vary the moment of first decay, applying it after 30, 120 or 480 epochs. After this first decay, we train for an extra 40 extra epochs, with a second decay after the first 20. The first learning rate decay starts after 30, 120 and 480 epochs. We observe that accuracy on the downstream task improves even after very large number of epochs.</p><p>progresses depending on when the learning rate is first decayed. Surprisingly, we observe that very long training (? 500 epochs) results in higher accuracy. Thus, we conclude that SGD optimization hyperparameters play an important role and need to be reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have investigated self-supervised visual representation learning from the previously unexplored angles. Doing so, we uncovered multiple important insights, namely that (1) lessons from architecture design in the fullysupervised setting do not necessarily translate to the selfsupervised setting; (2) contrary to previously popular architectures like AlexNet, in residual architectures, the final prelogits layer consistently results in the best performance; (3) the widening factor of CNNs has a drastic effect on performance of self-supervised techniques and (4) SGD training of linear logistic regression may require very long time to converge. In our study we demonstrated that performance of existing self-supervision techniques can be consistently boosted and that this leads to halving the gap between selfsupervision and fully labeled supervision.</p><p>Most importantly, though, we reveal that neither is the ranking of architectures consistent across different methods, nor is the ranking of methods consistent across architectures. This implies that pretext tasks for self-supervised learning should not be considered in isolation, but in conjunction with underlying architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-supervised model details</head><p>For training all self-supervised models we use stochastic gradient descent (SGD) with momentum. The initial learning rate is set to 0.1 and the momentum coefficient is set to 0.9. We train for 35 epochs in total and decay the learning rate by a factor of 10 after 15 and 25 epochs. As we use large mini-batch sizes B during training, we leverage two recommendations from <ref type="bibr" target="#b12">[13]</ref>: (1) a learning rate scaling, where the learning rate is multiplied by B 256 and (2) a linear learning rate warm-up during the initial 5 epochs.</p><p>In the following we give additional details that are specific to the choice of self-supervised learning technique.</p><p>Rotation: During training we use the data augmentation mechanism from <ref type="bibr" target="#b46">[46]</ref>. We use mini-batches of B = 1024 images, where each image is repeated 4 times: once for every rotation. The model is trained on 128 TPU <ref type="bibr" target="#b24">[24]</ref> cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exemplar:</head><p>In order to generate image examples, we use the data augmentation mechanism from <ref type="bibr" target="#b46">[46]</ref>. During training, we use mini-batches of size B = 512, and for each image in a mini-batch we randomly generate 8 examples. We use an implementation 2 of the triplet loss <ref type="bibr" target="#b43">[43]</ref> from the tensorflow package <ref type="bibr" target="#b0">[1]</ref>. The margin parameter of the triplet loss is set to 0.5. We use 32 TPU cores for training.</p><p>Jigsaw: Similar to <ref type="bibr" target="#b34">[34]</ref>, we preprocess the input images by: (1) resizing the input image to 292 ? 292 and randomly cropping it to 255 ? 255; (2) converting the image to grayscale with probability 2 ?3 by averaging the color channels; (3) splitting the image into a 3 ? 3 regular grid of cells (size 85 ? 85 each) and randomly cropping 64 ? 64-sized patches inside every cell; (4) standardize every patch individually such that its pixel intensities have zero mean and unit variance. We use SGD with batch size B = 1024. For each image individually, we randomly select 16 out of the 100 pre-defined permutations and apply all of them. The model is trained on 32 TPU cores.</p><p>Relative Patch Location: We use the same patch prepossessing, representation extraction and training setup as in the Jigsaw model. The only difference is the loss function as discussed in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Downstream training details</head><p>Training linear models with SGD: For training linear models with SGD, we use a standard data augmentation technique in the Rotation and Exemplar cases: (1) resize the image, preserving its aspect ratio such that its smallest side is 256. (2) apply a random crop of size 224 ? 224. For the patch-based methods, we extract representations by Training linear models with L-BFGS: We use a publicly available implementation of the L-BFGS algorithm <ref type="bibr" target="#b30">[30]</ref> from the scipy <ref type="bibr" target="#b23">[23]</ref> package with the default parameters and set the maximum number of updates to 800. For training all our models we apply l 2 penalty ?||W || 2 2 , where W ? R M ?C is the matrix of model parameters, M is the size of the representation, and C is the number of classes. We set ? = 100.0 M C . Training MLP models with SGD: In the MLP evaluation scenario, we use a single hidden layer with 1000 channels. At training time, we apply dropout <ref type="bibr" target="#b19">[19]</ref> to the hidden layer with a drop rate of 50%. The l 2 regularization scheme is the same as in the L-BFGS setting. We optimize the MLP model using stochastic gradient descent with momentum (the momentum coefficient is 0.9) for 180 epochs. The batch size is 512, initial learning rate is 0.01 and we decay it twice by a factor of 10: after 60 and 120 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training linear models with SGD</head><p>In <ref type="figure" target="#fig_7">Figure 9</ref> we demonstrate how accuracy on the validation data progresses during the course of SGD optimization. We observe that in all cases achieving top accuracy requires training for a very large number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results on Places205 and ImageNet</head><p>For completeness, we present full result tables for various settings considered in the main paper. These include numbers for ImageNet evaluated on 10 % of the data <ref type="table" target="#tab_5">(Table 3</ref>) as well as all results when evaluating on the Places205 dataset <ref type="table" target="#tab_6">(Table 4</ref>) and a random subset of 5 % of the Places205 dataset <ref type="table">(Table 5)</ref>.</p><p>Finally, <ref type="table">Table 6</ref> is an extended version of <ref type="table" target="#tab_2">Table 2</ref> in the main paper, additionally providing the top-5 accuracies of our various best models on the public ImageNet validation set.    <ref type="table">Table 6</ref>. Comparison of the published self-supervised models to our best models. The scores correspond to accuracy of linear logistic regression that is trained on top of representations provided by self-supervised models. Official validation splits of ImageNet and Places205 are used for computing accuracies. The "Family" column shows which basic model architecture was used in the referenced literature: AlexNet, VGG-style, or Residual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Family</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet Places205</head><p>Prev </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Different network architectures perform significantly differently across self-supervision tasks. This observation generalizes across datasets: ImageNet evaluation is shown on the left and Places205 is shown on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Comparing linear evaluation ( ) of the representations to non-linear ( ) evaluation, i.e. training a multi-layer perceptron instead of a linear model. Linear evaluation is not limiting: conclusions drawn from it carry over to the non-linear evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Evaluating the representation from various depths within the network. The vertical axis corresponds to downstream ImageNet performance in percent. For residual architectures, the pre-logits are always best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure 6. Disentangling the performance contribution of network widening factor versus representation size. Both matter independently, and larger is always better. Scores are accuracies of logistic regression on ImageNet. Black squares mark models which are also present in Table 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 Figure 8 .</head><label>88</label><figDesc>depicts how accuracy on our validation split Downstream task accuracy curve of the linear evaluation model trained with SGD on representations from the Rotation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Downstream task accuracy curve of the linear evaluation model trained with SGD on representations learned by the four self-supervision pretext tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of representations from self-supervised techniques based on various CNN architectures. The scores are accuracies (in %) of a linear logistic regression model trained on top of these representations using ImageNet training split. Our validation split is used for computing accuracies. The architectures marked by a "(-)" are slight variations described in Section 3.1. Sub-columns such as 4? correspond to widening factors. Top-performing architectures in a column are bold; the best pretext task for each model is underlined.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Rotation</cell><cell></cell><cell></cell><cell>Exemplar</cell><cell></cell><cell cols="2">RelPatchLoc</cell><cell cols="2">Jigsaw</cell></row><row><cell>Model</cell><cell>4?</cell><cell>8?</cell><cell>12?</cell><cell>16?</cell><cell>4?</cell><cell>8?</cell><cell>12?</cell><cell>4?</cell><cell>8?</cell><cell>4?</cell><cell>8?</cell></row><row><cell>RevNet50</cell><cell>47.3</cell><cell>50.4</cell><cell>53.1</cell><cell>53.7</cell><cell>42.4</cell><cell>45.6</cell><cell>46.4</cell><cell>40.6</cell><cell>45.0</cell><cell>40.1</cell><cell>43.7</cell></row><row><cell>ResNet50 v2</cell><cell>43.8</cell><cell>47.5</cell><cell>47.2</cell><cell>47.6</cell><cell>43.0</cell><cell>45.7</cell><cell>46.6</cell><cell>42.2</cell><cell>46.7</cell><cell>38.4</cell><cell>41.3</cell></row><row><cell>ResNet50 v1</cell><cell>41.7</cell><cell>43.4</cell><cell>43.3</cell><cell>43.2</cell><cell>42.8</cell><cell>46.9</cell><cell>47.7</cell><cell>46.8</cell><cell>50.5</cell><cell>42.2</cell><cell>45.4</cell></row><row><cell>RevNet50 (-)</cell><cell>45.2</cell><cell>51.0</cell><cell>52.8</cell><cell>53.7</cell><cell>38.0</cell><cell>42.6</cell><cell>44.3</cell><cell>33.8</cell><cell>43.5</cell><cell>36.1</cell><cell>41.5</cell></row><row><cell>ResNet50 v2 (-)</cell><cell>38.6</cell><cell>44.5</cell><cell>47.3</cell><cell>48.2</cell><cell>33.7</cell><cell>36.7</cell><cell>38.2</cell><cell>38.6</cell><cell>43.4</cell><cell>32.5</cell><cell>34.4</cell></row><row><cell>VGG19-BN</cell><cell>16.8</cell><cell>14.6</cell><cell>16.6</cell><cell>22.7</cell><cell>26.4</cell><cell>28.3</cell><cell>29.0</cell><cell>28.5</cell><cell>29.4</cell><cell>19.8</cell><cell>21.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the published self-supervised models to our best models. The scores correspond to accuracy of linear logistic regression that is trained on top of representations provided</figDesc><table /><note>by self-supervised models. Official validation splits of ImageNet and Places205 are used for computing accuracies. The "Family" column shows which basic model architecture was used in the ref- erenced literature: AlexNet, VGG-style, or Residual.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, one of the earliest published methods, achieves 51.4 % top-1 accuracy on ImageNet. Our strongest model, using Rotation, attains unprecedentedly high accuracy of 55.4 %. Similar observations hold when evaluating on Places205.</figDesc><table><row><cell>10 20 30 40 50 60 Downstream ImageNet Accuracy [%]</cell><cell>91 92 93 94 95 Rotation</cell><cell>Rel. Patch Loc. 55 60 65 70 Pretext Task Accuracy [%]</cell><cell>93</cell><cell>95</cell><cell>97</cell><cell>99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Evaluation on ImageNet with 10 % of the data.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Rotation</cell><cell></cell><cell></cell><cell>Exemplar</cell><cell></cell><cell cols="2">RelPatchLoc</cell><cell cols="2">Jigsaw</cell></row><row><cell>Model</cell><cell>4?</cell><cell>8?</cell><cell>12?</cell><cell>16?</cell><cell>4?</cell><cell>8?</cell><cell>12?</cell><cell>4?</cell><cell>8?</cell><cell>4?</cell><cell>8?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Evaluation on Places205.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Rotation</cell><cell></cell><cell></cell><cell>Exemplar</cell><cell></cell><cell cols="2">RelPatchLoc</cell><cell cols="2">Jigsaw</cell></row><row><cell>Model</cell><cell>4?</cell><cell>8?</cell><cell>12?</cell><cell>16?</cell><cell>4?</cell><cell>8?</cell><cell>12?</cell><cell>4?</cell><cell>8?</cell><cell>4?</cell><cell>8?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.tensorflow.org/api_docs/python/tf/ contrib/losses/metric_learning/triplet_semihard_ loss averaging the representations of all nine, colorful, standardized patches of an image. At final evaluation-time, fixed patches are obtained by scaling the image to 255 ? 255, cropping the central 192 ? 192 patch and taking the 3 ? 3 grid of 64 ? 64-sized patches from it.We use a batch-size of 2048 for evaluation of representations from Rotation and Exemplar models and of 1024 for Jigsaw and Relative Patch Location models. As we use large mini-batch sizes, we perform learning-rate scaling, as suggested in<ref type="bibr" target="#b12">[13]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<title level="m">Tensorflow: a system for large-scale machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00995</idno>
		<title level="m">Invertible residual networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptive faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02575</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Deep invertible networks</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grasp2Vec: Learning object representations from self-supervised grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SciPy: Open source scientific tools for Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<title level="m">Learning image representations by completing damaged jigsaw puzzles. Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00230</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10191</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On the limited memory bfgs method for large scale optimization. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cross and learn: Cross-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03879</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
