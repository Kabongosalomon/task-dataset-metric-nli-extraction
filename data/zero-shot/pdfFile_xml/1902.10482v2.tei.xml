<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Induction Networks for Few-Shot Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiying</forename><surname>Geng</surname></persName>
							<email>ruiying.gry@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhua</forename><surname>Li</surname></persName>
							<email>binhua.lbh@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">ECE</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Jian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>jian.sun@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Induction Networks for Few-Shot Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the samplewise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification. * * Corresponding authors: Y.Li and P.Jian.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has achieved a great success in many fields such as computer vision, speech recognition and natural language processing <ref type="bibr" target="#b9">(Kuang et al., 2018)</ref>. However, supervised deep learning is notoriously greedy for large labeled datasets, which limits the generalizability of deep models to new classes due to annotation cost. Humans on the other hand are readily capable of rapidly learning new classes of concepts with few examples or stimuli. This notable gap provides a fertile ground for further research.</p><p>Few-shot learning is devoted to resolving the data deficiency problem by recognizing novel classes from very few labeled examples. The limitation of only one or very few examples challenges the standard fine-tuning method in deep learning. Early studies <ref type="bibr" target="#b19">(Salamon and Bello, 2017)</ref> applied data augmentation and regularization techniques to alleviate the overfitting problem caused by data sparseness, only to a limited extent. Instead, researchers have explored meta-learning <ref type="bibr" target="#b4">(Finn et al., 2017)</ref> to leverage the distribution over similar tasks, inspired by human learning. Contemporary approaches to few-shot learning often decompose the training procedure into an auxiliary metalearning phase, which includes many meta-tasks, following the principle that the testing and training conditions must match. They extract some transferable knowledge by switching the metatask from mini-batch to mini-batch. As such, fewshot models can classify new classes with just a small labeled support set.</p><p>However, existing approaches for few-shot learning still confront many important problems, including the imposed strong priors <ref type="bibr" target="#b3">(Fei-Fei et al., 2006)</ref>, complex gradient transfer between tasks <ref type="bibr" target="#b14">(Munkhdalai and Yu, 2017)</ref>, and fine-tuning the target problem <ref type="bibr" target="#b16">(Qi et al., 2018)</ref>. The approaches proposed by <ref type="bibr" target="#b20">Snell et al. (2017)</ref> and <ref type="bibr" target="#b22">Sung et al. (2018)</ref>, which combine non-parametric methods and metric learning, provide potential solutions to some of those problems. The non-parametric methods allow novel examples to be rapidly assimilated, without suffering from catastrophic overfitting. Such non-parametric models only need to learn the representation of the samples and the metric measure. However, instances in the same class are interlinked and have their uniform fraction and their specific fractions. In previous studies, the class-level representations are calculated by simply summing or averaging represen-tations of samples in the support set. In doing so, essential information may be lost in the noise brought by various forms of samples in the same class. Note that few-shot learning algorithms do not fine-tune on the support set. When increasing size of the support set, the improvement brought by a bigger data size will also be diminished by more sample level noises.</p><p>Instead, we explore a better approach by performing induction at the class-wise level: ignoring irrelevant details and encapsulating general semantic information from samples with various linguistic forms in the same class. As a result, there is a need for a perspective architecture that can reconstruct hierarchical representations of support sets and dynamically induce sample representations to class representations.</p><p>Recently, capsule network <ref type="bibr" target="#b18">(Sabour et al., 2017)</ref> has been proposed, which possesses the exciting potential to address the aforementioned issue. A capsule network uses "capsules" that perform dynamic routing to encode the intrinsic spatial relationship between parts and whole that constitutes viewpoint invariant knowledge. Following a similar spirit, we can regard samples as parts and class as a whole. We propose the Induction Networks, which aims to model the ability of learning generalized class-level representation from samples in a small support set, based on the dynamic routing process. First, an Encoder Module generates representations for a query and support samples. Next, an Induction Module executes a dynamic routing procedure, in which the matrix transformation can be seen as a map from the sample space to the class space, and then the generation of the class representation is all depending on the routing-by-agreement procedure other than any parameters, which renders a robust induction ability to the proposed model to deal with unseen classes. By regarding the samples' representations as input capsules and the classes' as output capsules, we expect to recognize the semantics of classes that is invariant to sample-level noise. Finally, the interaction between queries and classes is modelled-their representations are compared by a Relation Module to determine if the query matches the class or not. Defining an episodebased meta-training strategy, the holistic model is meta-trained end-to-end with the generalizability and scalability to recognize unseen classes.</p><p>The specific contributions of our work are listed as follows:</p><p>? We propose the Induction Networks for fewshot text classification. To deal with samplewise diversity in the few-shot learning task, our model is the first, to the best of our knowledge, that explicitly models the ability to induce class-level representations from small support sets.</p><p>? The proposed Induction Module combines the dynamic routing algorithm with typical meta-learning frameworks. The matrix transformation and routing procedure enable our model to generalize well to recognize unseen classes.</p><p>? Our method outperforms the current state-ofthe-art models on two few-shot text classification datasets, including a well-studied sentiment classification benchmark and a realworld dialogue intent classification dataset.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-Shot Learning</head><p>The seminal work on few-shot learning dates back to the early 2000s <ref type="bibr" target="#b2">(Fe-Fei et al., 2003;</ref><ref type="bibr" target="#b3">Fei-Fei et al., 2006)</ref>. The authors combined generative models with complex iterative inference strategies. More recently, many approaches have used a meta-learning <ref type="bibr" target="#b4">(Finn et al., 2017;</ref><ref type="bibr" target="#b13">Mishra et al., 2018)</ref> strategy in the sense that they extract some transferable knowledge from a set of auxiliary tasks, which then helps them to learn the target few-shot problem well without suffering from overfitting. In general, these approaches can be divided into two categories.</p><p>Optimization-based Methods This type of approach aims to learn to optimize the model parameters given the gradients computed from the fewshot examples. <ref type="bibr" target="#b14">Munkhdalai and Yu (2017)</ref> proposed the Meta Network, which learnt the metalevel knowledge across tasks and shifted its inductive biases via fast parameterization for rapid generalization. <ref type="bibr" target="#b13">Mishra et al. (2018)</ref> introduced a generic meta-learning architecture called SNAIL which used a novel combination of temporal convolutions and soft attention.</p><p>Distance Metric Learning These approaches are different from the above approaches that entail some complexity when learning the target few-shot problem. The core idea in metric-based fewshot learning is similar to nearest neighbours and kernel density estimation. The predicted probability over a set of known labels is a weighted sum of labels of support set samples. <ref type="bibr" target="#b23">Vinyals et al. (2016)</ref> produced a weighted K-nearest neighbour classifier measured by the cosine distance, which was called Matching Networks. <ref type="bibr" target="#b20">Snell et al. (2017)</ref> proposed the Prototypical Networks which learnt a metric space where classification could be performed by computing squared Euclidean distances to prototype representations of each class. Different from fixed metric measures, the Relation Network learnt a deep distance metric to compare the query with given examples <ref type="bibr" target="#b22">(Sung et al., 2018)</ref>.</p><p>Recently, some studies have been presented focusing specifically on few-shot text classification problems. <ref type="bibr" target="#b27">Xu et al. (2018)</ref> studied lifelong domain word embeddings via meta-learning.  argued that the optimal meta-model may vary across tasks, and they employed the multimetric model by clustering the meta-tasks into several defined clusters. <ref type="bibr" target="#b17">Rios and Kavuluru (2018)</ref> developed a few-shot text classification model for multi-label text classification where there was a known structure over the label space. <ref type="bibr" target="#b26">Xu et al. (2019)</ref> proposed a open-world learning model to deal with the unseen classes in the product classification problem. We solve the few-shot learning problem from a different perspective and propose a dynamic routing induction method to encapsulate the abstract class representation from samples, achieving state-of-the-art performances on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Capsule Network</head><p>The Capsule Network was first proposed by <ref type="bibr" target="#b18">Sabour et al. (2017)</ref>, which allowed the network to learn robustly the invariants in part-whole relationships. Lately, Capsule Network has been explored in the natural language processing field.  successfully applied Capsule Network to fully supervised text classification problem with large labeled datasets. Unlike their work, we study few-shot text classification. <ref type="bibr" target="#b25">Xia et al. (2018)</ref> reused the supervised model similar to that of  for intent classification, in which a capsule-based architecture is extended to compute similarity between the target intents and source intents. Unlike their work, we propose Induction Networks for few-shot learning, in which we propose to use capsules and dynamic routing to learn generalized class-level representation from samples based. The dynamic routing method makes our model generalize better in the few-shot text classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Few-Shot Classification</head><p>Few-shot classification <ref type="bibr" target="#b23">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b20">Snell et al., 2017)</ref> is a task in which a classifier must be adapted to accommodate new classes not seen in training, given only a few examples of each of these new classes. We have a large labeled training set with a set of classes C train . However, after training, our ultimate goal is to produce classifiers on the testing set with a disjoint set of new classes C test , for which only a small labeled support set will be available. If the support set contains K labeled examples for each of the C unique classes, the target few-shot problem is called a C-way Kshot problem. Usually, the K is too small to train a supervised classification model. Therefore, we aim to perform meta-learning on the training set, and extract transferable knowledge that will allow us to deliver better few-shot learning on the support set and thus classify the test set more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Procedure</head><p>The training procedure has to be chosen carefully to match inference at test time. An effective way to exploit the training set is to decompose the training procedure into an auxiliary meta-learning phase and mimic the few-shot learning setting via episode-based training, as proposed in <ref type="bibr" target="#b23">Vinyals et al. (2016)</ref>. We construct an meta-episode to compute gradients and update our model in each training iteration. The meta-episode is formed by randomly selecting a subset of classes from the training set first, and then choosing a subset of examples within each selected class to act as the support set S and a subset of the remaining examples to serve as the query set Q. The meta-training procedure explicitly learns to learn from the given support set S to minimise a loss over the query set Q. We call this strategy as episode-based meta training, and the details are shown in Algorithm 1. It is worth noting that there are exponentially many possible meta tasks to train the model on, making it hard to overfit. For example, if a dataset contains 159 training classes, this leads to Algorithm 1 Episode-Based Meta Training 1: for each episode iteration do 2:</p><p>Randomly select C classes from the class space of the training set; 3:</p><p>Randomly select K labeled samples from each of the C classes as support set S = {(x s , y s )} m s=1 (m = K ? C), and select a fraction of the reminder of those C classes' samples as query set Q = {(x q , y q )} n q=1 ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Feed the support set S to the model and update the parameters by minimizing the loss in the query set Q; 5: end for 159 5 = 794, 747, 031 possible 5?way tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Models</head><p>Our Induction Networks, depicted in <ref type="figure" target="#fig_2">Figure 1</ref> (the case of 3-way 2-shot model), consists of three modules: Encoder Module, Induction Module and Relation Module. In the rest of this section, we will show how these modules work in each metaepisode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder Module</head><p>This module is a bi-direction recurrent neural network with self-attention as shown in <ref type="bibr" target="#b11">Lin et al. (2017)</ref>. Given an input text x = (w 1 , w 2 , ..., w T ), represented by a sequence of word embeddings. We use a bidirectional LSTM to process the text:</p><formula xml:id="formula_0">? ? h t = ????? LST M (w t , h t?1 ) (1) ? ? h t = ????? LST M (w t , h t+1 )<label>(2)</label></formula><p>And we concatenate ? ? h t with ? ? h t to obtain a hidden state h t . Let the hidden state size for each unidirectional LSTM be u. For simplicity, we note all the T h t s as H = (h 1 , h 2 , ..., h T ). Our aim is to encode a variable length of text into a fixed size embedding. We achieve that by choosing a linear combination of the T LST M hidden vectors in H. Computing the linear combination requires the self-attention mechanism, which takes the whole LSTM hidden states H as input, and outputs a vector of weights a:</p><formula xml:id="formula_1">a = sof tmax(W a2 tanh(W a1 H T ))<label>(3)</label></formula><p>here W a1 ? R da?2u and W a2 ? R da are weight matrices and d a is a hyperparameter. The final rep-resentation e of the text is the weighted sum of H:</p><formula xml:id="formula_2">e = T t=1</formula><p>a t ? h t (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Induction Module</head><p>This section introduces the proposed dynamic routing induction algorithm. We regard these vectors e obtained from the support set S by Eq 4 as sample vectors e s , and the vectors e from the query set Q as query vectors e q . The most important step is to extract the representation for each class in the support set. The main purpose of the induction module is to design a non-linear mapping from sample vector e s ij to class vector c i :</p><formula xml:id="formula_3">e s ij ? R 2u i=1,...C,j=1...K ? c i ? R 2u C i=1 .</formula><p>We apply the dynamic routing algorithm <ref type="bibr" target="#b18">(Sabour et al., 2017)</ref> in this module, in the situation where the number of the output capsule is one. In order to accept any-way any-shot inputs in our model, a weight-sharable transformation across all sample vectors in the support set is employed. All of the sample vectors in the support set share the same transformation weights W s ? R 2u?2u and bias b s , so that the model is flexible enough to handle the support set at any scale. Each sample prediction vector? s ij is computed by:</p><formula xml:id="formula_4">e s ij = squash(W s e s ij + b s )<label>(5)</label></formula><p>where squash is a non-linear squashing function through the entire vector, which leaves the direction of the vector unchanged but decreases its magnitude. Given input vector x, squash is defined as:</p><formula xml:id="formula_5">squash(x) = x 2 1 + x 2 x x<label>(6)</label></formula><p>Eq 5 encodes important invariant semantic relationships between lower level sample features and higher level class features <ref type="bibr" target="#b8">(Hinton et al., 2011)</ref>.</p><p>To ensure the class vector encapsulates the sample feature vectors of this class automatically, dynamic routing is applied iteratively. In each iteration, the process dynamically amends the connection strength and makes sure that the coupling coefficients d i sum to 1 between class i and all support samples in this class by a "routing softmax":    where b i is the logits of coupling coefficients, and initialized by 0 in the first iteration. Given each sample prediction vector? s ij , each class candidate vector? i is a weighted sum of all sample prediction vectors? s ij in class i:</p><formula xml:id="formula_6">d i = sof tmax (b i )<label>(7)</label></formula><formula xml:id="formula_7">C ? K ? d</formula><formula xml:id="formula_8">K 0 T E W l E W Z 6 I b B p L F P G U t x V X M u r l g Q R L G r B O e b e t 4 5 5 w J y b N 0 X 1 3 m 7 C g J T l J + z K N A E d W v V L c P F U + Y d H d d C w b 9 S s 2 r e 2 a 5 P 4 F v Q Q 1 2 N b P K M w 4 x Q I Y I B R I w p F C E Y w S Q 9 P T g w 0 N O 3 B G u i B O E u I k z X G O K t A V l M c o I i D 2 j 7 w n t e p Z N a a 8 9 p V F H d E p M r y C l i x X S Z J Q n C O v T X B M v j L N m f / O + M p 7 6 b p f 0 D 6 1 X Q q z C K b F / 6 Y a Z / 9 X p W h S O s W l q 4 F R T b h h d X W R d C t M V f X P 3 U 1 W K H H L i N B 5 Q X B C O j H L Y Z 9 d o p K l d 9 z Y w 8 V e T q V</formula><formula xml:id="formula_9">K 0 T E W l E W Z 6 I b B p L F P G U t x V X M u r l g Q R L G r B O e b e t 4 5 5 w J y b N 0 X 1 3 m 7 C g J T l J + z K N A E d W v V L c P F U + Y d H d d C w b 9 S s 2 r e 2 a 5 P 4 F v Q Q 1 2 N b P K M w 4 x Q I Y I B R I w p F C E Y w S Q 9 P T g w 0 N O 3 B G u i B O E u I k z X G O K t A V l M c o I i D 2 j 7 w n t e p Z N a a 8 9 p V F H d E p M r y C l i x X S Z J Q n C O v T X B M v j L N m f / O + M p 7 6 b p f 0 D 6 1 X Q q z C K b F / 6 Y a Z / 9 X p W h S O s W l q 4 F R T b h h d X W R d C t M V f X P 3 U 1 W K H H L i N B 5 Q X B C O j H L Y Z 9 d o p K l d 9 z Y w 8 V e T q V</formula><formula xml:id="formula_10">c i = j d ij ?? s ij<label>(8)</label></formula><p>then a non-linear "squashing" function is applied to ensure that the length of the vector output of the routing process will not exceed 1:</p><formula xml:id="formula_11">c i = squash(? i )<label>(9)</label></formula><p>The last step in every iteration is to adjust the logits of coupling coefficients b ij by a "routing by agreement" method. If the produced class candidate vector has a large scalar output with one sample prediction vector, there is a top-down feedback which increases the coupling coefficient for that sample and decreases it for other samples. This type of adjustment is very effective and robust for the few-shot learning scenario because it does not need to restore any parameters. Each b ij is updated by:</p><formula xml:id="formula_12">b ij = b ij +? s ij ? c i<label>(10)</label></formula><p>Formally, we call our induction method as dynamic routing induction and summarize it in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relation Module</head><p>After the class vector c i is generated by the Induction Module and each query text in the query set is encoded to a query vector e q by the Encoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Dynamic Routing Induction</head><p>Require: sample vector e s ij in support set S and initialize the logits of coupling coefficients b ij = 0 Ensure: class vector c i 1: for all samples j = 1, ..., K in class i:</p><formula xml:id="formula_13">2:? s ij = squash(W s e s ij + b s ) 3: for iter iterations do 4: d i = sof tmax (b i ) 5:? i = j d ij ?? s ij 6: c i = squash(? i ) 7:</formula><p>for all samples j = 1, ..., K in class i:</p><formula xml:id="formula_14">8: b ij = b ij +? s</formula><p>ij ? c i 9: end for 10: Return c i Module, the next essential procedure is to measure the correlation between each pair of query and class. The output of the Relation Module is called the relation score, representing the correlation between c i and e q , which is a scalar between 0 and 1. Specifically, we use the neural tensor layer <ref type="bibr" target="#b21">(Socher et al., 2013)</ref> in this module, which has shown great advantages in modeling the relationship between two vectors <ref type="bibr" target="#b24">(Wan et al., 2016;</ref><ref type="bibr" target="#b6">Geng et al., 2017)</ref>. We choose it as an interaction function in this paper. The tensor layer outputs a relation vector as follows:</p><formula xml:id="formula_15">v(c i , e q ) = f c i T M [1:h] e q<label>(11)</label></formula><p>where M k ? R 2u?2u , k ? [1, ..., h] is one slice of the tensor parameters and f is a non-linear activation function called RELU <ref type="bibr" target="#b7">(Glorot et al., 2011)</ref>.</p><p>The final relation score r iq between the i-th class and the q-th query is calculated by a fully connected layer activated by a sigmoid function.</p><formula xml:id="formula_16">r iq = sigmoid(W r v(c i , e q ) + b r )<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Objective Function</head><p>We use the mean square error (MSE) loss to train our model, regressing the relation score r iq to the ground truth y q : matched pairs have similarity 1 and the mismatched pair have similarity 0. Given the support set S with C classes and query set Q = {(x q , y q )} n q=1 in an episode, the loss function is defined as:</p><formula xml:id="formula_17">L (S, Q) = C i=1 n q=1 (r iq ? 1(y q == i)) 2 (13)</formula><p>conceptually we are predicting relation scores, which can be considered as a regression problem and the ground truth is within the space { 0, 1} .</p><p>All parameters of the three modules are trained jointly by backpropagation. The Adagrad <ref type="bibr" target="#b1">(Duchi et al., 2011)</ref> is used on all parameters in each training episode. Our model does not need any finetuning on the classes it has never seen due to its generalization nature. The induction and comparison ability are accumulated in the model along with the training episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model by conducting experiments on two few-shot text classification datasets. All the experiments are implemented with Tensorflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Amazon Review Sentiment Classification (ARSC) Following , we use the multiple tasks with the multi-domain sentiment classification <ref type="bibr" target="#b0">(Blitzer et al., 2007)</ref> dataset. The dataset comprises English reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets then form 23 ? 3 = 69 tasks in total. Following , we select 12(4 ? 3) tasks from 4 domains (Books, DVD, Electronics and Kitchen) as the test set, and there are only five examples as support set for each label in the test set. We create 5-shot learning models on this dataset.  Matching Networks <ref type="bibr" target="#b23">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type="bibr" target="#b20">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type="bibr" target="#b5">(Garcia and Bruna, 2017)</ref> 82.61 Relation Network <ref type="bibr" target="#b22">(Sung et al., 2018)</ref> 83.07 SNAIL <ref type="bibr" target="#b13">(Mishra et al., 2018)</ref> 82.57 ROBUSTTC-FSL  83.12 Induction Networks (ours) 85.63 Open Domain Intent Classification for Dialog System (ODIC) We create this dataset by fetching the log data on a real-world conversational platform. The enterprises submit various dialogue tasks with a great number of intents, but many intents have only a few labeled samples, which is a typical few-shot classification application. Following the definition of the few-shot learning task, we divide the ODIC into a training set and a testing set and ensure that the labels of the two sets have no intersection. The details of the set partition are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Setup</head><p>Baselines In this section, the baseline models in our experiments are introduced as follows.</p><p>? Matching Networks: a few-shot learning model using a metric-based attention method <ref type="bibr" target="#b23">(Vinyals et al., 2016)</ref>.</p><p>? Prototypical Networks: a deep metric-based method using sample average as class prototypes <ref type="bibr" target="#b20">(Snell et al., 2017)</ref>.</p><p>? Graph Network: a graph-based few-shot learning model that implements a task-driven message passing algorithm on the samplewise level <ref type="bibr" target="#b5">(Garcia and Bruna, 2017)</ref>.</p><p>? Relation Network: a few-shot learning model which uses a neural network as the distance metric and sums up sample vectors in the support set as class vectors <ref type="bibr" target="#b22">(Sung et al., 2018)</ref>.</p><p>? SNAIL: a class of simple and generic metalearner architectures that use a novel combi-Model 5-way Acc. 10-way Acc. 5-shot 10-shot 5-shot 10-shot Matching Networks <ref type="bibr" target="#b23">(Vinyals et al., 2016)</ref> 82.54?0.12 84.63?0.08 73.64?0.15 76.72?0.07 Prototypical Networks <ref type="bibr" target="#b20">(Snell et al., 2017)</ref> 81.82?0.08 85.83?0.06 73.31?0.14 75.97?0.11 Graph Network <ref type="bibr" target="#b5">(Garcia and Bruna, 2017)</ref> 84.15?0.16 87.24?0.09 75.58?0.12 78.27?0.10 Relation Network <ref type="bibr" target="#b22">(Sung et al., 2018)</ref> 84.41?0.14 86.93?0.15 75.28?0.13 78.61?0.06 SNAIL <ref type="bibr" target="#b13">(Mishra et al., 2018)</ref> 84   <ref type="bibr" target="#b13">(Mishra et al., 2018)</ref>.</p><p>? ROBUSTTC-FSL: This approach combines several metric-based methods by clustering the tasks .</p><p>The baseline results on ARSC are reported in  and we implemented the baseline models on ODIC with the same text encoder module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use 300-dimension Glove embeddings <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref> for ARSC dataset and 300-dimension Chinese word embeddings trained by  for ODIC. We set the hidden state size of LSTM u = 128 and the attention dimension d a = 64. The iteration number iter used in dynamic routing algorithm is 3. The relation module is a neural tensor layer with h = 100 followed by a fully connected layer activated by sigmoid. We build 2-way 5-shot models on ARSC following , and build episode-based meta training with C = [5, 10] and K = [5, 10] for comparison on ODIC. In addition to K sample texts as support set, the query set has 20 query texts for each of the C sampled classes in every training episode. This means, for example, that there are 20 ? 5 + 5 ? 5 = 125 texts in one training episode for the 5-way 5-shot experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Methods</head><p>We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning <ref type="bibr" target="#b20">(Snell et al., 2017;</ref><ref type="bibr" target="#b22">Sung et al., 2018)</ref>. To evaluate the proposed model with the baselines objectively, we compute mean few-shot classification accuracies on ODIC over 600 randomly selected episodes from the testing set. We sample 10 test texts per class in each episode for evaluation in both 5-shot and 10-shot scenarios. Note that for ARSC, the support set for testing is fixed by . Consequently, we just need to run the test episode once for each of the target tasks. The mean accuracy of the 12 target task is compared to the baseline models following .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment Results</head><p>Overall Performance Experiment results on ARSC are presented in <ref type="table" target="#tab_3">Table 2</ref>. The proposed Induction Networks achieves a 85.63% accuracy, outperforming the existing state-of-the-art model, ROBUSTTC-FSL, by a notable 3% improvement. We due the improvement to the fact that ROBUSTTC-FSL builds a general metric method by integrating several metrics at the sample level, which faces the difficulty of getting rid of the noise among different expressions in the same class. In addition to that, the task-clustering-based method used by ROBUSTTC-FSL must be found on the relevance matrix, which is inefficient when applied to real-world scenarios where the tasks change rapidly. Our Induction Networks, however, is trained in the meta-learning framework with more flexible generalization and its induction ability can hence be accumulated through different tasks.</p><p>We also evaluate our method with a real-world intent classification dataset ODIC. The experiment results are listed in <ref type="table" target="#tab_5">Table 3</ref>. We can see that our proposed Induction Networks achieves best classification performances on all of the four experiments. In the distance metric learning models (Matching Networks, Prototypical Networks, Graph Network and Relation Network), all the learning occurs in representing features and measuring distances at the sample-wise level. Our work builds an induction module focusing on the class-wise level of representation, which we claim to be more robust to variation of samples in the support set. Our model also outperforms the latest optimization-based method-SNAIL. The difference between Induction Networks and SNAIL shown in <ref type="table" target="#tab_5">Table 3</ref> is statistically significant under the paired at the 99% significance level. In  addition, the performance difference between our model and other baselines in the 10-shot scenario is more significant than in the 5-shot scenario. This is because in the 10-shot scenario, for the baseline models the improvement brought by a bigger data size is also diminished by more sample level noises.</p><p>Ablation Study To analyze the effect of varying different components of the Induction Module and Relation Module, we further report the ablation experiments on the ARSC dataset as shown in <ref type="table" target="#tab_7">Table 4</ref>. We can see that the best performance is achieved when we used 3 iterations, corresponding to the best result reported in <ref type="table" target="#tab_3">Table 2</ref> (more rounds of iterations did not further improve the performance), and the table shows the effectiveness of the routing component. We also changed the Induction Module with sum and self-attention and changed Relation Module with cosine distance. Changes in the performances validate the benefit of both the Relation Module and Induction Module. The Attention+Relation models the induction ability by self-attention mechanism, but the ability is limited by the learnt attention parameters. Conversely, the proposed dynamic routing induction method captures class-level information by automatically adjusting the coupling coefficients according to inputted support sets, which is more suitable for the few-shot learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Further Analysis</head><p>We further analyze the effect of transformation and visualize query text vectors to show the advantage of the Induction Networks.</p><p>Effect of Transformation <ref type="figure">Figure 2</ref> shows the t-SNE (Maaten and Hinton, 2008) visualization of support sample vectors before and after matrix transformation under the 5-way 10-shot scenario. We randomly select a support set with 50 texts (10 texts per class) from the ODIC testing set, and ob- after transformation. We can see that the vectors after matrix transformation are more separable, demonstrating the effectiveness of matrix transformation to encode semantic relationships between lower-level sample features and higher-level class features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Text Vector Visualization</head><p>We also find out that our induction module does not only work well in generating effective class-level features, but also helps the encoders to learn better text vectors, as it can give different weights to instances and features during backpropagation. <ref type="figure">Figure 3</ref> shows the t-SNE (Maaten and Hinton, 2008) visualization of text vectors from the same randomly selected five classes, learnt by the Relation Network and our Induction Networks. It is clear that the text vectors learnt by Induction Networks are better separated semantically than those of Relation Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose the Induction Networks, a novel neural model for few-shot text classification. We propose to induce the class-level representations from support sets to deal with samplewise diversity in few-shot learning tasks. The Induction Module combines the dynamic routing algorithm with a meta-learning framework, and the routing mechanism makes our model more general to recognize unseen classes. The experiment results show that the proposed model outperforms the existing state-of-the-art few-shot text classification models. We found that both the matrix transformation and routing procedure contribute consistently to the few-shot learning tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s E V D M 1 R o s q Z V g 3 4 K U x l 5 l p 6 f Q 4 I = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r / q u d e k m W A R X J R F B l 0 U 3 g p s K 9 i F V S p K O O p g X M x N R R N y J W 3 / A r f 6 R + A f 6 F 9 4 Z p + A D 0 Q l J z p x 7 z 5 m 5 9 4 Z 5 z K X y v J e S M z I 6 N j 5 R n p y a n p m d m 6 8 s V N s y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>m 9 j 2 x u g T d 9 S x q w / 3 2 c P 0 F 7 r e 5 7 d X 9 v v d b Y s q M u Y w n L W K V 5 b q C B H T T R I u 8 L P O A R T 8 6 B c + P c O n c f q U 7 J a h b x Z T n 3 7 5 P p l i 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s E V D M 1 R o s q Z V g 3 4 K U x l 5 l p 6 f Q 4 I = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r / q u d e k m W A R X J R F B l 0 U 3 g p s K 9 i F V S p K O O p g X M x N R R N y J W 3 / A r f 6 R + A f 6 F 9 4 Z p + A D 0 Q l J z p x 7 z 5 m 5 9 4 Z 5 z K X y v J e S M z I 6 N j 5 R n p y a n p m d m 6 8 s V N s y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>m 9 j 2 x u g T d 9 S x q w / 3 2 c P 0 F 7 r e 5 7 d X 9 v v d b Y s q M u Y w n L W K V 5 b q C B H T T R I u 8 L P O A R T 8 6 B c + P c O n c f q U 7 J a h b x Z T n 3 7 5 P p l i 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s E V D M 1 R o s q Z V g 3 4 K U x l 5 l p 6 f Q 4 I = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r / q u d e k m W A R X J R F B l 0 U 3 g p s K 9 i F V S p K O O p g X M x N R R N y J W 3 / A r f 6 R + A f 6 F 9 4 Z p + A D 0 Q l J z p x 7 z 5 m 5 9 4 Z 5 z K X y v J e S M z I 6 N j 5 R n p y a n p m d m 6 8 s V N s y K 0 T E W l E W Z 6 I b B p L F P G U t x V X M u r l g Q R L G r B O e b e t 4 5 5 w J y b N 0 X 1 3 m 7 C g J T l J + z K N A E d W v V L c P F U + Y d H d d C w b 9 S s 2 r e 2 a 5 P 4 F v Q Q 1 2 N b P K M w 4 x Q I Y I B R I w p F C E Y w S Q 9 P T g w 0 N O 3 B G u i B O E u I k z X G O K t A V l M c o I i D 2 j 7 w n t e p Z N a a 8 9 p V F H d E p M r y C l i x X S Z J Q n C O v T X B M v j L N m f / O + M p 7 6 b p f 0D 6 1 X Q q z C K b F / 6 Y a Z / 9 X p W h S O s W l q 4 F R T b h h d X W R d C t M V f X P 3 U 1 W K H H L i N B 5 Q X B C O j H L Y Z 9 d o p K l d 9 z Y w 8 V e T q Vm 9 j 2 x u g T d 9 S x q w / 3 2 c P 0 F 7 r e 5 7 d X 9 v v d b Y s q M u Y w n L W K V 5 b q C B H T T R I u 8 L P O A R T 8 6 B c + P c O n c f q U 7 J a h b x Z T n 3 7 5 P p l i 4 = &lt; / l a t e x i t &gt; C ? d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o P E 6 s S k a s J d B L S Z j b G A x T R L v Z T U = " &gt; A A A C z X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F n s x p 0 V 7 A N r k W Q 6 r Y N 5 M Z k I p e r W H 3 C r v y X + g f 6 F d 8 Y U 1 C I 6 I c m Z c + 8 5 M / d e P w l E q h z n t W D N z M 7 N L x Q X S 0 v L K 6 t r 5 f W N V h p n k v E m i 4 N Y d n w v 5 Y G I e F M J F f B O I r k X + g F v + 9 d 1 H W / f c J m K O D p T o 4 T 3 Q m 8 Y i Y F g n i L q v G 5 f K B H y 1 O 5 f l i t O 1 T H L n g Z u D i r I V y M u v + A C f c R g y B C C I 4 I i H M B D S k 8 X L h w k x P U w J k 4 S E i b O c Y c S a T P K 4 p T h E X t N 3 y H t u j k b 0 V 5 7 p k b N 6 J S A X k l K G z u k i S l P E t a n 2 S a e G W f N / u Y 9 N p 7 6 b i P 6 + 7 l X S K z C F b F / 6 S a Z / 9 X p W h Q G O D Q 1 C K o p M Y y u j u U u m e m K v r n 9 p S p F D g l x G v c p L g k z o 5 z 0 2 T a a 1 N S u e + u Z + J v J 1 K z e s z w 3 w 7 u + J Q 3 Y / T n O a d D a q 7 p O 1 T 3 d r 9 S O 8 l E X s Y V t 7 N I 8 D 1 D D M R p o k n e E R z z h 2 T q x M u v W u v 9 M t Q q 5 Z h P f l v X w A c E J k s Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o P E 6 s S k a s J d B L S Z j b G A x T R L v Z T U = " &gt; A A A C z X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F n s x p 0 V 7 A N r k W Q 6 r Y N 5 M Z k I p e r W H 3 C r v y X + g f 6 F d 8 Y U 1 C I 6 I c m Z c + 8 5 M / d e P w l E q h z n t W D N z M 7 N L x Q X S 0 v L K 6 t r 5 f W N V h p n k v E m i 4 N Y d n w v 5 Y G I e F M J F f B O I r k X+ g F v + 9 d 1 H W / f c J m K O D p T o 4 T 3 Q m 8 Y i Y F g n i L q v G 5 f K B H y 1 O 5 f l i t O 1 T H L n g Z u D i r I V y M u v + A C f c R g y B C C I 4 I i H M B D S k 8 X L h w k x P U w J k 4 S E i b O c Y c S a T P K 4 p T h E X t N 3 y H t u j k b 0 V 5 7 p k b N 6 J S A X k l K G z u k i S l P E t a n 2 S a e G W f N / u Y 9 N p 7 6 b i P 6 + 7 l X S K z C F b F / 6 S a Z / 9 X p W h Q G O D Q 1 C K o p M Y y u j u U u m e m K v r n 9 p S p F D g l x G v c p L g k z o 5 z 0 2 T a a 1 N S u e + u Z + J v J 1 K z e s z w 3 w 7 u + J Q 3 Y / T n O a d D a q 7 p O 1 T 3 d r 9 S O 8 l E X s Y V t 7 N I 8 D 1 D D M R p o k n e E R z z h 2 T q x M u v W u v 9 M t Q q 5 Z h P f l v X w A c E J k s Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o P E 6 s S k a s J d B L S Z j b G A x T R L v Z T U = " &gt; A A A C z X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F n s x p 0 V 7 A N r k W Q 6 r Y N 5 M Z k I p e r W H 3 C r v y X + g f 6 F d 8 Y U 1 C I 6 I c m Z c + 8 5 M / d e P w l E q h z n t W D N z M 7 N L x Q X S 0 v L K 6 t r 5 f W N V h p n k v E m i 4 N Y d n w v 5 Y G I e F M J F f B O I r k X + g F v + 9 d 1 H W / f c J m K O D p T o 4 T 3 Q m 8 Y i Y F g n i L q v G 5 f K B H y 1 O 5 f l i t O 1 T H L n g Z u D i r I V y M u v + A C f c R g y B C C I 4 I i H M B D S k 8 X L h w k x P U w J k 4 S E i b O c Y c S a T P K 4 p T h E X t N 3 y H t u j k b 0 V 5 7 p k b N 6 J S A X k l K G z u k i S l P E t a n 2 S a e G W f N / u Y 9 N p 7 6 b i P 6 + 7 l X S K z C F b F / 6 S a Z / 9 X p W h Q G O D Q 1 C K o p M Y y u j u U u m e m K v r n 9 p S p F D g l x G v c p L g k z o 5 z 0 2 T a a 1 N S u e + u Z + J v J 1 K z e s z w 3 w 7 u + J Q 3 Y / T n O a d D a q 7 p O 1 T 3 d r 9 S O 8 l E X s Y V t 7 N I 8 D 1 D D M R p o k n e E R z z h 2 T q x M u v W u v 9 M t Q q 5 Z h P f l v X w A c E J k s Q = &lt; / l a t e x i t &gt; C ? K ? d&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s E V D M 1 R o s q Z V g 3 4 K U x l 5 l p 6 f Q 4 I = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r / q u d e k m W A R X J R F B l 0 U 3 g p s K 9 i F V S p K O O p g X M x N R R N y J W 3 / A r f 6 R + A f 6 F 9 4 Z p + A D 0 Q l J z p x 7 z 5 m 5 9 4 Z 5 z K X y v J e S M z I 6 N j 5 R n p y a n p m d m 6 8 s V N s y K 0 T E W l E W Z 6 I b B p L F P G U t x V X M u r l g Q R L G r B O e b e t 4 5 5 w J y b N 0 X 1 3 m 7 C g J T l J + z K N A E d W v V L c P F U + Y d H d d C w b 9 S s 2 r e 2 a 5 P 4 F v Q Q 1 2 N b P K M w 4 x Q I Y I B R I w p F C E Y w S Q 9 P T g w 0 N O 3 B G u i B O E u I k z X G O K t A V l M c o I i D 2 j 7 w n t e p Z N a a 8 9 p V F H d E p M r y C l i x X S Z J Q n C O v T X B M v j L N m f / O + M p 7 6 b p f 0 D 6 1 X Q q z C K b F / 6 Y a Z / 9 X p W h S O s W l q 4 F R T b h h d X W R d C t M V f X P 3 U 1 W K H H L i N B 5 Q X B C O j H L Y Z 9 d o p K l d 9 z Y w 8 V e T q V m 9 j 2 x u g T d 9 S x q w / 3 2 c P 0 F 7 r e 5 7 d X 9 v v d b Y s q M u Y w n L W K V 5 b q C B H T T R I u 8 L P O A R T 8 6 B c + P c O n c f q U 7 J a h b x Z T n 3 7 5 P p l i 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s E V D M 1 R o s q Z V g 3 4 K U x l 5 l p 6 f Q 4 I = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r / q u d e k m W A R X J R F B l 0 U 3 g p s K 9 i F V S p K O O p g X M x N R R N y J W 3 / A r f 6 R + A f 6 F 9 4 Z p + A D 0 Q l J z p x 7 z 5 m 5 9 4 Z 5 z K X y v J e S M z I 6 N j 5 R n p y a n p m d m 6 8 s V N s y K 0 T E W l E W Z 6 I b B p L F P G U t x V X M u r l g Q R L G r B O e b e t 4 5 5 w J y b N 0 X 1 3 m 7 C g J T l J + z K N A E d W v V L c P F U + Y d H d d C w b 9 S s 2 r e 2 a 5 P 4 F v Q Q 1 2 N b P K M w 4 x Q I Y I B R I w p F C E Y w S Q 9 P T g w 0 N O 3 B G u i B O E u I k z X G O K t A V l M c o I i D 2 j 7 w n t e p Z N a a 8 9 p V F H d E p M r y C l i x X S Z J Q n C O v T X B M v j L N m f / O + M p 7 6 b p f 0 D 6 1 X Q q z C K b F / 6 Y a Z / 9 X p W h S O s W l q 4 F R T b h h d X W R d C t M V f X P 3 U 1 W K H H L i N B 5 Q X B C O j H L Y Z 9 d o p K l d 9 z Y w 8 V e T q V m 9 j 2 x u g T d 9 S x q w / 3 2 c P 0 F 7 r e 5 7 d X 9 v v d b Y s q M u Y w n L W K V 5 b q C B H T T R I u 8 L P O A R T 8 6 B c + P c O n c f q U 7 J a h b x Z T n 3 7 5 P p l i 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s E V D M 1 R o s q Z V g 3 4 K U x l 5 l p 6 f Q 4 I = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r / q u d e k m W A R X J R F B l 0 U 3 g p s K 9 i F V S p K O O p g X M x N R R N y J W 3 / A r f 6 R + A f 6 F 9 4 Z p + A D 0 Q l J z p x 7 z 5 m 5 9 4 Z 5 z K X y v J e S M z I 6 N j 5 R n p y a n p m d m 6 8 s V N s y K 0 T E W l E W Z 6 I b B p L F P G U t x V X M u r l g Q R L G r B O e b e t 4 5 5 w J y b N 0 X 1 3 m 7 C g J T l J + z K N A E d W v V L c P F U + Y d H d d C w b 9 S s 2 r e 2 a 5 P 4 F v Q Q 1 2 N b P K M w 4 x Q I Y I B R I w p F C E Y w S Q 9 P T g w 0 N O 3 B G u i B O E u I k z X G O K t A V l M c o I i D 2 j 7 w n t e p Z N a a 8 9 p V F H d E p M r y C l i x X S Z J Q n C O v T X B M v j L N m f / O + M p 7 6 b p f 0 D 6 1 X Q q z C K b F / 6 Y a Z / 9 X p W h S O s W l q 4 F R T b h h d X W R d C t M V f X P 3 U 1 W K H H L i N B 5 Q X B C O j H L Y Z 9 d o p K l d 9 z Y w 8 V e T q V m 9 j 2 x u g T d 9 S x q w / 3 2 c P 0 F 7 r e 5 7 d X 9 v v d b Y s q M u Y w n L W K V 5 b q C B H T T R I u 8 L P O A R T 8 6 B c + P c O n c f q U 7 J a h b x Z T n 3 7 5 P p l i 4 = &lt; / l a t e x i t &gt; d ? d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W T M n V Y N f M U r q j 7 7 E c 5 N T i U 1 v D N g = " &gt; A A A C z X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R 7 L L g x p 0 V 7 A N b k S S d 1 q F 5 M Z k I p e r W H 3 C r v y X + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 G a m 1 9 Y X C o u l 1 Z W 1 9 Y 3 y p t b r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x 8 d q 3 j 7 h o m M J / G 5 H K f s M v K G M R / w w J N E X f T t n u Q R y + z + V b n i V B 2 9 7 F n g G l C B W Y 2 k / I I e + k g Q I E c E h h i S c A g P G T 1 d u H C Q E n e J C X G C E N d x h j u U S J t T F q M M j 9 g R f Y e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b e y R J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 R H u q u 4 3 p 7 x u v i F i J a 2 L / 0 k 0 z / 6 t T t U g M U N M 1 c K o p 1 Y y q L j A u u e 6 K u r n 9 p S p J D i l x C v c p L g g H W j n t s 6 0 1 m a 5 d 9 d b T 8 T e d q V i 1 D 0 x u j n d 1 S x q w + 3 O c s 6 B 1 U H W d q n t 2 W K n X z K i L 2 M E u 9 m m e R 6 j j B A 0 0 y T v G I 5 7 w b J 1 a u X V r 3 X + m W g W j 2 c a 3 Z T 1 8 A A 2 f k t s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W T M n V Y N f M U r q j 7 7 E c 5 N T i U 1 v D N g = " &gt; A A A C z X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R 7 L L g x p 0 V 7 A N b k S S d 1 q F 5 M Z k I p e r W H 3 C r v y X + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 G a m 1 9 Y X C o u l 1 Z W 1 9 Y 3 y p t b r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x 8 d q 3 j 7 h o m M J / G 5 H K f s M v K G M R / w w J N E X f T t n u Q R y + z + V b n i V B 2 9 7 F n g G l C B W Y 2 k / I I e + k g Q I E c E h h i S c A g P G T 1 d u H C Q E n e J C X G C E N d x h j u U S J t T F q M M j 9 g R f Y e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b e y R J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 R H u q u 4 3 p 7 x u v i F i J a 2 L / 0 k 0 z / 6 t T t U g M U N M 1 c K o p 1 Y y q L j A u u e 6 K u r n 9 p S p J D i l x C v c p L g g H W j n t s 6 0 1 m a 5 d 9 d b T 8 T e d q V i 1 D 0 x u j n d 1 S x q w + 3 O c s 6 B 1 U H W d q n t 2 W K n X z K i L 2 M E u 9 m m e R 6 j j B A 0 0 y T v G I 5 7 w b J 1 a u X V r 3 X + m W g W j 2 c a 3 Z T 1 8 A A 2 f k t s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W T M n V Y N f M U r q j 7 7 E c 5 N T i U 1 v D N g = " &gt; A A A C z X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R 7 L L g x p 0 V 7 A N b k S S d 1 q F 5 M Z k I p e r W H 3 C r v y X + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 G a m 1 9 Y X C o u l 1 Z W 1 9 Y 3 y p t b r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x 8 d q 3 j 7 h o m M J / G 5 H K f s M v K G M R / w w J N E X f T t n u Q R y + z + V b n i V B 2 9 7 F n g G l C B W Y 2 k / I I e + k g Q I E c E h h i S c A g P G T 1 d u H C Q E n e J C X G C E N d x h j u U S J t T F q M M j 9 g R f Y e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b e y R J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 R H u q u 4 3 p 7 x u v i F i J a 2 L / 0 k 0 z / 6 t T t U g M U N M 1 c K o p 1 Y y q L j A u u e 6 K u r n 9 p S p J D i l x C v c p L g g H W j n t s 6 0 1 m a 5 d 9 d b T 8 T e d q V i 1 D 0 x u j n d 1 S x q w + 3 O c s 6 B 1 U H W d q n t 2 W K n X z K i L 2 M E u 9 m m e R 6 j j B A 0 0 y T v G I 5 7 w b J 1 a u X V r 3 X + m W g W j 2 c a 3 Z T 1 8 A A 2 f k t s = &lt; / l a t e x i t &gt; Induction Networks architecture for a C-way K-shot (C = 3, K = 2) problem with one query example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Effect of Transformation under the 5-way 10shot scenario. (a) The support sample vectors before matrix transformation. (b) The support sample vectors after matrix transformation. Query text vector visualization learnt by (a) Relation Network and (b) Induction Networks. tain the sample vectors e s ij i=1,...5,j=1...10 after the encoder module and the sample prediction vector ? s ij i=1,...5,j=1...10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Details of ODIC</figDesc><table><row><cell>Model</cell><cell>Mean Acc</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of mean accuracy (%) on ARSC</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Comparison of mean accuracy (%) on ODIC</cell></row><row><cell>nation of temporal convolutions and soft at-</cell></row><row><cell>tention</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of Induction Networks on ARSC dataset</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the organizers of EMNLP-IJCNLP2019 and the reviewers for their helpful suggestions. This research work is sup- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association of computational linguistics</title>
		<meeting>the 45th annual meeting of the association of computational linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fe-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Oneshot learning of object categories. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno>abs/1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implicit discourse relation identification based on tree structure neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiying</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Asian Language Processing (IALP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="334" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention focusing for neural machine translation by bridging source and target embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1767" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analogical reasoning on chinese morphological and semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renfen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple neural attentive metalearner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR. org</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fewshot and zero-shot multi-label learning for structured label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3132" to="3142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and data augmentation for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2835" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot user intent detection via capsule neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3090" to="3099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Openworld learning and application to product classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3413" to="3419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lifelong domain word embedding via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09991</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Investigating capsule networks with dynamic routing for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soufei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3110" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diverse few-shot text classification with multiple metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1206" to="1215" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
