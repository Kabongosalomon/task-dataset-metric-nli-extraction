<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Debesh</forename><surname>Jha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, Umapada Pal, Senior Member, IEEE</roleName><forename type="first">Sukalpa</forename><surname>Chanda</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">H?vard</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Dag</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, P?l Halvorsen Member, IEEE</roleName><forename type="first">Sharib</forename><surname>Ali</surname></persName>
						</author>
						<title level="a" type="main">MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Medical image segmentation</term>
					<term>MSRF-Net</term>
					<term>multi- scale fusion</term>
					<term>generalization</term>
					<term>colonoscopy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Methods based on convolutional neural networks have improved the performance of biomedical image segmentation. However, most of these methods cannot efficiently segment objects of variable sizes and train on small and biased datasets, which are common for biomedical use cases. While methods exist that incorporate multi-scale fusion approaches to address the challenges arising with variable sizes, they usually use complex models that are more suitable for general semantic segmentation problems. In this paper, we propose a novel architecture called Multi-Scale Residual Fusion Network (MSRF-Net), which is specially designed for medical image segmentation. The proposed MSRF-Net is able to exchange multi-scale features of varying receptive fields using a Dual-Scale Dense Fusion (DSDF) block.</p><p>Our DSDF block can exchange information rigorously across two different resolution scales, and our MSRF sub-network uses multiple DSDF blocks in sequence to perform multi-scale fusion. This allows the preservation of resolution, improved information flow and propagation of both high-and low-level features to obtain accurate segmentation maps. The proposed MSRF-Net allows to capture object variabilities and provides improved results on different biomedical datasets. Extensive experiments on MSRF-Net demonstrate that the proposed method outperforms the cutting-edge medical image segmentation methods on four publicly available datasets. We achieve the Dice Coefficient (DSC) of 0.9217, 0.9420, and 0.9224, 0.8824 on Kvasir-SEG, CVC-ClinicDB, 2018 Data Science Bowl dataset, and ISIC-2018 skin lesion segmentation challenge dataset respectively. We further conducted generalizability tests and achieved DSC of 0.7921 and 0.7575 on CVC-ClinicDB and Kvasir-SEG, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>lesion assessment, such as polyps in the colon, to inspect if they are cancerous and remove them if necessary. Thus, the segmentation results can help to detect missed lesions, prevent diseases, and improve therapy planning and treatment. The significant challenge in medical imaging is the requirement of a large number of high-quality labeled and annotated datasets. This is a key factor in the development of robust algorithm for automated medical image segmentation task.</p><p>The manual pixel-wise annotation of medical image data is very time-consuming, requires collaborations with experienced medical experts, and is costly. During the annotation of the regions in medical images (for example, polyps in still frames), the guidelines and protocols are set based on which expert performs the annotations. However, there might exist discrepancies among the experts, e.g., while considering a particular area in the lesion as cancerous or non-cancerous. Additionally, the lack of standard annotation protocols for various imaging modalities and low image quality can influence annotation quality. Other factors such as the annotator's attentiveness, type of display device, image-annotation software and data misinterpretation due to lightning conditions can also affect the quality of annotations <ref type="bibr" target="#b3">[4]</ref>. An alternative solution to manual image segmentation is an automated computer aided segmentation based diagnosis-assisting system that can provide a faster, more accurate, and more reliable solution to transform clinical procedures and improve patient care. Computer aided diagnosis will reduce the expert's burden and also reduce the overall treatment cost. Due to the diverse nature of medicalimaging data, computer aided diagnosis based segmentation models must be robust to variations in imaging modalities <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the past years, Convolutional Neural Networks (CNNs) based approaches have overcome the limitations of traditional segmentation methods <ref type="bibr" target="#b5">[6]</ref> in various medical imaging modalities such as X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), endoscopy, wireless capsule endoscopy, dermatoscopy, and in high-throughput imaging like histopathology and electron microscopy. Modern semantic and instance segmentation architectures are usually encoderdecoder based networks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The success of deep encoderdecoder based CNNs is largely due to their skip connections, which allows the propagation of deep, semantically meaningful, and dense feature maps from the encoder network to the decoder sub-networks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, encoder-decoder based image segmentation architectures have limitations in optimal depth and design of the skip connections <ref type="bibr" target="#b10">[11]</ref>. The optimal depth of the architectures can vary from one biomed-ical application to another. The number of samples in the dataset used in training also contributes to the limitation on the complexity of the network. The design of skip connections are sometimes unnecessarily restrictive, demanding the fusion of the same-scale encoder and decoder feature maps. Moreover, traditional CNN methods do not make use of the hierarchical features.</p><p>In this paper, we propose a novel medical image segmentation architecture, called MSRF-Net, which aims to overcome the above discussed limitations. Our proposed MSRF-Net maintains high-resolution representation throughout the process, which is conducive to potentially achieving high spatial accuracy. The MSRF-Net utilizes a novel Dual-Scale Dense Fusion (DSDF) block that performs dual scale feature exchange and a sub-network that exchanges multi-scale features using the DSDF block. The DSDF block takes two different scale inputs and employs a residual dense block that exchanges information across different scales after each convolutional layer in their corresponding dense blocks. The densely connected nature of blocks allows relevant high-and low-level features to be preserved for the final segmentation map prediction. The multi-scale information exchange in our network preserves both high-and low-resolution feature representations, thereby producing finer, richer and spatially accurate segmentation maps. The repeated multi-scale fusion helps in enhancing the high-resolution feature representations with the information propagated by low-resolution representations. Further, layers of residual networks allow redundant DSDF blocks to die out, and only the most relevant extracted features contribute to the predicted segmentation maps.</p><p>Additionally, we propose adding a complimentary gated shape stream that can leverage the combination of high-and low-level features to compute shape boundaries accurately. We have evaluated the MSRF-Net segmentation model using four publicly available biomedical datasets. The results demonstrate that the proposed MSRF-Net outperforms the State Of The Art (SOTA) segmentation methods on most standard computer vision evaluation metrics.</p><p>The main contributions of this work are as following: 1) Our proposed MSRF-Net architecture is based on a DSDF block that comprises of residual dense connections and exchanging information across multiple scales. This allows both high-resolution and low-resolution features to propagate, thereby extracting semantically meaningful features that improve segmentation performance on various biomedical datasets. 2) MSRF-Net computes the multi-scale features and fuses them effectively using a DSDF block. The residual nature of DSDF block improves gradient flow which improves the training efficiency, i.e., reducing the need for large datasets for training.</p><p>3) The effectiveness of MSRF-Net is demonstrated on four public datasets: Kvasir-SEG <ref type="bibr" target="#b11">[12]</ref>, CVC-ClinicDB <ref type="bibr" target="#b12">[13]</ref>, 2018 Data Science Bowl (DSB) Challenge <ref type="bibr" target="#b1">[2]</ref>, and ISIC 2018 Challenge <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. We conduct a generalizability study of the proposed network for which we trained our model on Kvasir-SEG and tested on the CVC-ClinicDB and vice versa. The experimental results and their comparison with established computer vision methods confirmed that our approach is more generalizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Medical image segmentation</head><p>Long et al. <ref type="bibr" target="#b15">[16]</ref> proposed a Fully Convolutional Network (FCN) that included only convolutional layers for semantic segmentation. Subsequently, Ronneberger et al. <ref type="bibr" target="#b16">[17]</ref> modified the FCN with an encoder-decoder U-Net architecture for segmentation of HeLa cells and neuronal structures of electron microscopic stacks. In the U-Net <ref type="bibr" target="#b16">[17]</ref>, low-and high-level feature maps are combined through skip connections. The high-level feature maps are processed by deeper layers of the encoder network and propagated through the decoder whereas, the low-level features are propagated from the initial layers of the network. This may cause a semantic gap between the highand low-level features. Ibtehaz et al. <ref type="bibr" target="#b17">[18]</ref> proposed to add convolutional units along the path of skip connections to reduce the semantic gap. Oktay et al. <ref type="bibr" target="#b18">[19]</ref> proposed an attention U-Net that used an attention block to alter the feature maps propagated through the skip-connections. Here, the previous decoder block output was used to form a gating mechanism to prune unnecessary spatial features passing from the skipconnections and to keep only the relevant features. In addition, various other extensions of the U-Net have been proposed <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. To incorporate global context information for the task of scene parsing, PSPNet <ref type="bibr" target="#b22">[23]</ref> generated hierarchical feature maps through a Pyramid Pooling Module (PPM). Similarly, Chen et al. <ref type="bibr" target="#b23">[24]</ref> used the Atrous Spatial Pyramid Pooling (ASPP) to aggregate the global features. Later, the same group proposed the DeepLabV3+ <ref type="bibr" target="#b24">[25]</ref> architecture that used skip connections between the encoder and decoder. Both of these networks have been widely used by the biomedical imaging community <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>.</p><p>Hu et al. <ref type="bibr" target="#b28">[29]</ref> proposed SE-Net, which pioneered channelwise attention. The Squeeze and Excitation (S&amp;E) block was able to model interdependencies between the channels and derive a global information map that helps in emphasizing relevant features and suppressing irrelevant features. FED-Net <ref type="bibr" target="#b29">[30]</ref> incorporated these S&amp;E blocks in their modified U-Net architecture. Kaul et al. <ref type="bibr" target="#b30">[31]</ref> incorporated both types of attention, i.e., spatial and channel-wise attention, in their proposed FocusNet. Jha et al. <ref type="bibr" target="#b19">[20]</ref> modified ResUNet <ref type="bibr" target="#b31">[32]</ref> adding ASPP, S&amp;E block <ref type="bibr" target="#b28">[29]</ref> and attention mechanisms to boost the performance of the network further. Taikkawa et al. <ref type="bibr" target="#b32">[33]</ref> proposed Gated-SCNN, which pioneered the idea of gated shape stream to generate finer segmentation maps leveraging the shape and boundaries of the target object. The shape stream was recently also employed by Sun at al. <ref type="bibr" target="#b21">[22]</ref> to capture the shape and boundaries of the target segmentation map for medical segmentation problems. Fan et al. <ref type="bibr" target="#b33">[34]</ref> devised a parallel partial decoder (PraNet) that aggregated high-level features to generate a guidance map that estimates a rough location of the region of interest. The guidance map used in PraNet was then used with a reverse attention module to extract finer boundaries from the lowlevel features. Kim et al. <ref type="bibr" target="#b34">[35]</ref> modified the U-Net architecture and added additional encoder and decoder modules. Saliency maps computed by a prediction module in the UACA-Net is used to compute foreground, background and uncertain area maps for each representation. The relationship between each representation is computed and used by the next prediction module. A detailed summary of advances of deep-learning based methodologies in medical image segmentation can be found in <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Residual dense blocks</head><p>Dense connections are a unique approach of improving information flow and keeping a collection of diversified features. The architectures based on dense connections are characterized by each layer receiving inputs from all previous layers. Various medical image segmentation methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b41">[42]</ref> leverage the diversified features captured by such dense connections to improve segmentation performance. Guan et al. <ref type="bibr" target="#b38">[39]</ref> modified the U-Net architecture by substituting standard encoderdecoder units with densely connected convolutional units. Zhou et al. <ref type="bibr" target="#b10">[11]</ref> conceived an architecture where the encoder and decoder are connected through dense and nested skip pathways for efficient feature fusion between the feature maps of encoder and decoder. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> proposed Residual Dense Blocks (RDB) to extract local features via densely connected convolutional layers. Additionally, their architecture allowed them to connect the previous RDB block to all the current RDB blocks and a final global fusion through 1 ? 1 convolutions for maintaining global hierarchical feature extraction. In ResUNet <ref type="bibr" target="#b40">[41]</ref> and Residual Dense U-Net (RD-U-Net) <ref type="bibr" target="#b41">[42]</ref>, the RDBs are included in a standard U-Net based architecture to make use of hierarchical features. Dolz el al. <ref type="bibr" target="#b42">[43]</ref> proposed HyperDense-Net, which introduced a twostream CNN designed to process each modality in a separate stream for multi-modal image segmentation. The dense connections were used across layers of the same path and also between layers of a different path, therefore, increasing the capacity of the network to learn more complex combination between different modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-scale fusion</head><p>Maintaining a high-resolution representation of the image is important for segmentation architectures to precisely capture the spatial information and give accurate segmentation maps <ref type="bibr" target="#b43">[44]</ref>. Rather than recovering such representations from low-level representations, multi-scale fusion can help exchange high-and low-resolution features throughout the segmentation process. Wang et al. <ref type="bibr" target="#b43">[44]</ref> demonstrated that such exchange of features improves the flow of high-resolution features and can potentially lead to a more spatially accurate segmentation map. They achieved this by processing all the resolution streams in parallel, keeping the resolution representation for each resolution, and performing the feature fusion across all resolution scales.</p><p>The previous works by Ronneberger et al. <ref type="bibr" target="#b16">[17]</ref> and Badrinarayanan et al. <ref type="bibr" target="#b44">[45]</ref> used skip-connections to concatenate high-resolution feature representations at each level with the upscaled features in the decoder to preserve both high-and low-resolution feature representations. Zhao et al. <ref type="bibr" target="#b22">[23]</ref> used pyramid pooling to perform multi-resolution fusion while Chen et al. <ref type="bibr" target="#b23">[24]</ref> used ASPP and multiple Atrous convolutions with different sampling rates. Similarly, Yang et al. <ref type="bibr" target="#b45">[46]</ref> used densely connected atrous convolutional layers in their DenseASPP network to gather multi-scale features with a large range of receptive fields. Lin et al. <ref type="bibr" target="#b46">[47]</ref> proposed ZigZa-gNet, which fused multi-resolution features by exchanging information in a zig-zag fashion between the encoder-decoder architecture. Wang et al. <ref type="bibr" target="#b47">[48]</ref> proposed Deeply-Fused Nets that applies fusion of intermediate resolutions allowing varying receptive fields with different sizes. Additionally, the authors used the same-sized receptive field derived from two other base networks to capture different characteristics in the extracted features. Deep fusion was further studied in <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Our approach</head><p>To address the challenges of the existing approaches, we introduce a DSDF block that takes two different scale features as input. While propagating information flow in the same resolution, the DSDF block also performs a cross resolution fusion. This establishes a dual-scale fusion of features that inherit both high-and low-resolution feature representations. An encoder network is used to feed the feature representations to the MSRF sub-network that consists of multiple DSDF blocks, thereby performing multi-scale feature exchange. Later, decoder layers with skip-connections from our sub-network and a triple attention mechanism are used to process our fused feature maps together with the shape stream. It is to be noted that the fusion strategy is interchangeable, i.e., low-to-high resolution and vice-versa. <ref type="figure" target="#fig_1">Figure 2</ref>(a) represents the MSRF-Net that consists of an encoder block, the MSRF sub-network, a shape stream block, and a decoder block. The encoder block consists of squeeze and excitation modules, and the MSRF sub-network is used to process low-level feature maps extracted at each resolution scale of the encoder. The MSRF sub-network incorporates several DSDF blocks. A gated shape stream is applied after the MSRF sub-network, and decoders consisting of triple attention blocks are used in the proposed architecture. A triple attention block has the advantage of using spatial and channel-wise attention along with spatially gated attention, where irrelevant features from MSRF sub-network are pruned. Below, we briefly describe each component of our MSRF-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE MSRF-NET ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoder</head><p>The encoder blocks (E1-E4) in <ref type="figure" target="#fig_1">Figure 2</ref>(a) are comprised of two consecutive convolutions followed by a squeeze and excitation module. The S&amp;E block in the network increases the network's representative power by computing the interdependencies between channels. During the squeezing step, global average pooling is used to aggregate feature maps across the channel's spatial dimensions. In the excitation step, a collection of per-channel weights are produced to capture channelwise dependencies <ref type="bibr" target="#b28">[29]</ref>. At each encoder stage, max pooling with the stride of two is used for downscaling the resolution, and drop out is utilized for the model regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The DSDF block and MSRF sub-network</head><p>Maintaining the resolution throughout the feature encoding process can help the target images become more semantically richer and spatially accurate. The DSDF block helps to exchange information between scales, preserve low-level features, and improves information flow while maintaining resolution. The block has two parallel streams for two different resolution scales <ref type="figure" target="#fig_0">(Figure 1(a)</ref>). If we let a 3 ? 3 convolution followed by a LeakyRelu activation be represented by the operation CLR(?), then each stream has a densely connected residual block with five CLR operations in series. The output feature map M d,h of the d-th CLR operation is computed from the high-resolution input X h as follows:</p><formula xml:id="formula_0">M d,h = CLR(M d?1,h ? M d?1,l ? M d?2,h ? ? ? ? ? M 0,h ) (1)</formula><p>Here, ? is the concatenation operation, and h represents CLR operation is on the higher resolution stream of the DSDF block. M d?1,l is processed by a transposed convolutional layer with a 3 ? 3 kernel size and stride of 2 before being concatenated. Similarly, for lower resolution stream the output of the d-th CLR operation is denoted by M d,l and represented as:</p><formula xml:id="formula_1">M d,l = CLR(M d?1,l ? M d?1,h ? M d?2,l ? ? ? ? ? M 0,l ) (2)</formula><p>Here, M d?1,h is processed by a convolutional layer with kernel size of 3 ? 3 and stride of 2 before being concatenated. In Equation 1 and Equation 2, d ranges from 1 ? d ? 5.</p><p>Initially, X h (or M 0,h ) and X l (or M 0,l ) are the higher and lower resolution stream input, respectively. The output of each CLR has k output channels denoting the growth factor, which regulates the amount of new features the layer can extract and propagate further in the network. Since the growth factor varies for each scale, we only use two scales at once in the DSDF to reduce the model's computational complexity for making the training feasible. Further, local residual learning is used to improve information flow, and residual scaling is used to prevent instability <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Scaling factor 0 ? w ? 1 can be used for residual scaling. The final output of the DSDF block can be written as (see <ref type="figure" target="#fig_0">Figure 1</ref>(a)):</p><formula xml:id="formula_2">X r = w ? M 5,r + X r ,<label>(3)</label></formula><p>where r ? [h, l] is the resolution with h indicating highresolution representation and l for low resolution representation. Next, we present an MSRF sub-network that comprises of several DSDF blocks to achieve a global multi-scale context using the dual-scale fusion mechanism. As shown in <ref type="bibr" target="#b39">[40]</ref>, our approach has a contiguous memory mechanism that allows retaining multi-scale feature representations since the inputs of each DSDF is passed to each subsequent DSDF blocks in the same resolution stream.</p><p>Algorithm 1 MSRF sub-network <ref type="bibr">1:</ref> Information exchange across all scales in MSRF Subnetwork 2: N is no. of DSDF layers (N = 6 in <ref type="figure" target="#fig_0">Figure 1</ref></p><formula xml:id="formula_3">(b)) 3: H ? X? ,1 , X? ,3 , X? +1,1 , X? +1,3 , ... (High-res. input) 4: L ? Xl ,2 , Xl ,4 , Xl +1,2 , Xl +1,4 , ... (Low-res. input) 5: p ? {1, 3} and q ? {2, 4} are scale pairs 6: X? +1,p , Xl +1,q = DSDF(X? ,p , Xl ,q ) 7: Update:X? ,p = X? +1,p ,Xl ,q = Xl +1,q 8: for 2 ?L? N ? 3 do 9: X? +1,p , Xl +1,q = DSDF(X? ,p , Xl ,q ) 10: Update:X? ,p ,Xl ,q = X? +1,p ,Xl +1,q 11: Xl +1,2 ,X? +1,3 = DSDF(Xl ,2 , X? ,3 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update:Xl ,2 ,X? ,3 = Xl +1,2 ,X? +1,3 13: end for 14: X? +1,p , Xl +1,q = DSDF(X? ,p , Xl ,q ) 15: Update:X? ,p = w.X? +1,p +X 0,p ,Xl ,q = w.Xl +1,q +X 0,q</p><p>In Algorithm 1, we define inputs in the MSRF sub-network as the process of demarcating all the resolution scale pairs and feeding them in their respective DSDF blocks. For this, we start with the first layer with each layer consisting of four resolution scales with H and L representing a highresolution and low-resolution set of features, respectively, and each respective block is denoted by? andl. The DSDF(?)  function performs feature fusion across scales in the DSDF block, where X? ,p , Xl ,q is jointly computed from the p and q scale pairs. Moreover,X represents the feature exchange in the center DSDF. Already after the fourth layer of the MSRF sub-network, we effectively exchange features across all scales and attain global multi-scale fusion (refer to the red rectangular block in <ref type="figure" target="#fig_0">Figure 1(b)</ref>). We can observe that X 0,r , ?r ? {1, 2, 3, 4} is able to transmit its features to all the parallel resolution representations through multiple DSDF blocks. Using this method, we exchange features globally in a more effective way, even when the number of resolution scales is greater than 4. Similar to the DSDF block, the output of the last layer of the sub-network is again scaled by w and added to the original input of the MSRF sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Shape stream</head><p>We have incorporated the gated shape stream <ref type="bibr" target="#b32">[33]</ref> in MSRF-Net for the shape prediction (see the shape stream block in <ref type="figure" target="#fig_1">Figure 2</ref>(a)). The DSDF blocks can extract relevant high-level feature representations that include important information about shape and boundaries and can be used in the shape stream. Similar to <ref type="bibr" target="#b21">[22]</ref>, we define S l as the shape stream feature maps where l is the number of layers and X is the output of the MSRF-sub-network. Bilinear interpolation is used so that X can match spatial dimensions of S l , attention map ? l at the gated convolution is computed as:</p><formula xml:id="formula_4">? l = ? (Conv 1?1 (S l ? X))<label>(4)</label></formula><p>where ?(?) is the sigmoid activation function. Finally, S l+1 is computed as S l+1 = RB(S l ??), where RB represents residual block with two CLR operations followed by a skip-connection. The output of the shape stream is concatenated with the image gradients of the input image and merged with the original segmentation stream before the last CLR operation. This is done to increase the spatial accuracy of the segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Decoder</head><p>The decoder block (D2-D4) has skip-connections from the MSRF sub-network and the previous decoder output (say D ? ) except for D2, where the previous layer connection is the MSRF sub-network output of the E4 <ref type="figure" target="#fig_1">(Figure 2(a)</ref>). In the decoder block <ref type="figure" target="#fig_1">(Figure 2(b)</ref>), we use two attention mechanisms. The first attention mechanism applies channel and spatial attention, whereas the second attention uses a gating mechanism. We have used a S&amp;E block for the calculation of channel-wise scale coefficients denoted by X ?se . Spatial attention is also calculated at the same top stream where the input channels C are reduced to 1 using 1 ? 1 convolution. The sigmoid activation function ?(?) is used to scale the values between 0 and 1 to produce an activation map, which is stacked C times to give X ?s . The output of the spatial and channel attention can be represented as:</p><formula xml:id="formula_5">D sc = (X ?s + 1) ? X ?se<label>(5)</label></formula><p>where ? denotes the Hadamard product, and X ?s is increased by a magnitude of 1 to amplify relevant features determined by the activation map. We also use the attention gated mechanism <ref type="bibr" target="#b18">[19]</ref>. Let the features coming from MSRF-Net be X, and the output from the previous decoder block be D ? , then the attention coefficients can be calculated as:</p><formula xml:id="formula_6">D AG = ? ? ?(?(X) + ?(D ? ))<label>(6)</label></formula><p>where ?(?) is the convolution operation with stride 2, kernel size 1, and G channel outputs; ?(?) is a convolution operation with stride 1 and kernel size 1 ? 1 applied to D ? giving the same G channels; and ?(?) is convolution function with 1 ? 1 kernel size applied to a combined features from ?(?) and ?(?) making output channel equal to 1. Finally, ?(?) is applied to obtain the activation map on which transpose convolution operation ?(?) is applied. D AG captures the contextual information and identifies the target regions and structures of the image.D AG = D AG ?X allows the irrelevant features to be pruned and relevant target structure and regions to be propagated further.D AG is updated as:</p><formula xml:id="formula_7">D AG =D AG ? ?(D ? )<label>(7)</label></formula><p>Now, the final output of the triple attention decoder block (i.e., the combination of channel, spatial and gated spatial attention) is D ? = D sc ?D AG , which is then followed by two CLR operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss computation</head><p>We have used binary cross-entropy loss L BCE as defined in <ref type="figure">Equation 8</ref> where y is the ground truth value and? is the predicted value. We have also used dice loss L DCS , which is defined in <ref type="bibr">Equation 9</ref>.</p><formula xml:id="formula_8">L BCE = (y ? 1) log(1 ??) ? y log?<label>(8)</label></formula><formula xml:id="formula_9">L DCS = 1 ? 2y? + 1 y +? + 1<label>(9)</label></formula><p>The sum of the two loss functions, L comb = ? 1 L BCE + ? 2 L DCS , is used for gradient minimization between the predicted maps and the labels, while only L BCE has been used for shape stream. Here, we set the values of ? 1 and ? 2 to 1. For the latter loss, predicted edge maps and ground truth maps are used during computation. Deep supervision is also used to improve the flow of the gradients and regularization <ref type="bibr" target="#b52">[53]</ref>. Thus, our final loss function can be represented as:</p><formula xml:id="formula_10">L MSRF = ?L comb + ? 1 L DS 0 comb + ? 2 L DS 1 comb + ?L SS BCE<label>(10)</label></formula><p>where L DS 0 comb and L DS 1 comb representing the two deep supervision outputs losses (see <ref type="figure" target="#fig_1">Figure 2</ref>(a)) and L SS is the loss computed for the shape stream. Here, we set the values of ? = 1, ? 1 = 1, ? 2 = 1 and ? = 1 for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP A. Dataset</head><p>To evaluate the effectiveness of the MSRF-Net, we have used four publicly available biomedical imaging datasets; Kvasir-SEG <ref type="bibr" target="#b11">[12]</ref>, CVC-ClinicDB <ref type="bibr" target="#b12">[13]</ref>, 2018 Data Science Bowl <ref type="bibr" target="#b1">[2]</ref>, and ISIC-2018 Challenge <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The details about the datasets, number of training and testing samples used, and their availability is presented in <ref type="table" target="#tab_1">Table I</ref>. All of these datasets consist of the images and their corresponding ground truth masks. An example of each dataset can be found in <ref type="figure" target="#fig_2">Figure 3</ref>. The chosen datasets are commonly used in biomedical image segmentation. The main reason for choosing diverse imaging modalities datasets is to evaluate the performance and robustness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>Standard computer vision metrics for medical image segmentation such as Dice Coefficient (DSC), mean Intersection over Union (mIoU), recall, precision, and Frames Per Second (FPS) have been used for the evaluation of our experimental results. The standard deviations for DSC, mIoU, r and p are also provided. Additionally, we conduct a paired t-test between the DSC achieved by our proposed MSRF-Net and the DSC attained by other SOTA methods. The p-values of the paired t-tests are also reported. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>We have implemented the proposed architecture using the Keras framework <ref type="bibr" target="#b53">[54]</ref> with TensorFlow <ref type="bibr" target="#b54">[55]</ref> as backend. All experiments are conducted on an NVIDIA DGX-2 machine that uses NVIDIA V100 Tensor Core GPUs. The Adam optimizer was used with a learning rate of 0.0001, and a dropout regularization with p = 0.2 was used. The scaling factor for our DSDF and MSRF sub-network was set to 0.4 (w = 0.4). The growth factor k is set to 16, 32, and 64 for resolution scale pairs in the DSDF. For Kvasir-SEG and 2018 DSB, the images are resized to 256?256. ISIC-2018 images are resized to 384 ? 512, and images from CVC-ClinicDB are resized to 384 ? 288 resolution. We have used the batch size of 16 for Kvasir-SEG and 2018 DSB, eight for CVC-ClinicDB, and four for the ISIC-2018 Challenge dataset. We have empirically set the number of epochs for all datasets to 200 epochs. We have used 80% of the dataset for training, 10% for validation, and the remaining 10% for testing. Data augmentation techniques such as random cropping, random rotation, horizontal flipping, vertical flipping, and grid distortion were applied. It is to be noted that we have used open-source code provided by the respective authors for all the baseline comparisons. The proposed model is available at https://github.com/NoviceMAnprog/MSRF-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SOTA method comparisons</head><p>In this section, we present the comparison of our MSRF-Net with other SOTA methods.</p><p>1) Comparison on Kvasir-SEG Early detection of polyps, before they potentially change into colorectal cancer, can improve the survival rate <ref type="bibr" target="#b57">[58]</ref>. Therefore, we have selected two popular colonoscopy datasets in our experiment. The first colonoscopy dataset is Kvasir-SEG. We report the quantitative evaluation of MSRF-Net in <ref type="table" target="#tab_1">Table II</ref> and qualitative results in <ref type="figure" target="#fig_2">Figure 3</ref>. From the quantitative results, we can observe that our method outperforms all the other SOTA methods on all metrics. It achieves 1.39% improvement on DSC as compared to PraNet <ref type="bibr" target="#b33">[34]</ref>, 3.39% improvement on mIoU as compared Deeplabv3+ with Xception backbone <ref type="bibr" target="#b24">[25]</ref>. Our method also achieves an improvement of 1.70% on precision and 1.04% on recall as compared to Deeplabv3+ with Xception backbone and U-Net <ref type="bibr" target="#b16">[17]</ref>, respectively. The network's ability to segment polyps can be observed from the ground truth comparison with the predicted mask. <ref type="figure" target="#fig_2">(Figure 3)</ref>.</p><p>2) Comparison on CVC-ClinicDB CVC-ClinicDB is the second colonoscopy dataset used in our experiment. The quantitative results from <ref type="table" target="#tab_1">Table III show  TABLE II  RESULT COMPARISON ON THE KVASIR-SEG DATASET.</ref>   that our approach surpasses other SOTA methods and achieves a DSC of 0.9420 ? 0.0804, which is 1.76% improvement in DSC over the best-performing HRNetV2-W48 <ref type="bibr" target="#b43">[44]</ref>. We report a mIoU of 0.9043 ? 0.1009 and a recall of 0.9567 ? 0.0620, which is 1.44% improvement in mIoU and 2.82% improvement in recall over SOTA combination of ResUNet++ and conditional random field <ref type="bibr" target="#b56">[57]</ref> and UACANet-S <ref type="bibr" target="#b34">[35]</ref>, respectively. Additionally, MSRF-Net achieves a precision of 0.9427, which is competitive with the best performing DoubleUNet <ref type="bibr" target="#b9">[10]</ref>. Our method produces prediction masks with nearly the same boundaries and shape of the polyp as compared to the ground truth masks <ref type="figure" target="#fig_2">(Figure 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WE HAVE NOT COMPUTED PAIRED T-TEST (P VALUES) FOR THE SAME NETWORK (MSRF-NET).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Comparison on 2018 Data Science Bowl</head><p>Finding nuclei in a cell from a large variety of microscopy images is a challenging problem. We experiment with the 2018 Data Science Bowl challenge dataset. <ref type="table" target="#tab_1">Table IV</ref> shows the comparison of the result of the proposed MSRF-Net with some of the presented approaches. MSRF-Net obtains a DSC of 0.9224 ? 0.0538, mIoU of 0.8534 ? 0.0870, recall of 0.9402 ? 0.0734 and precision of 0.9022 ? 0.0601 which outperforms the best performing ColonSegNet <ref type="bibr" target="#b4">[5]</ref> in most metrics (see <ref type="table" target="#tab_1">Table IV</ref>). From the qualitative results in <ref type="figure" target="#fig_2">Figure 3</ref>, we observe that the predicted masks are visually similar to the ground truth masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparison on ISIC-2018 Skin Lesion Segmentation challenge</head><p>An automatic diagnosis tool for skin lesions can help in accurate melanoma detection, which is also a commonly occurring cancer and can save life up to 99% <ref type="bibr" target="#b58">[59]</ref> of cases. The quantitative results for the ISIC-2018 challenge are shown in <ref type="table" target="#tab_4">Table V</ref>. Our method achieved a DSC of 0.8824 ? 0.1602, mIoU of 0.8373 ? 0.1818, recall of 0.8893 ? 0.1889, and precision of 0.9348 ? 0.1488. We can observe an improvement of 0.43% and 1.37% over Deeplabv3+ with the Mobilenet backbone <ref type="bibr" target="#b23">[24]</ref> in DSC and mIoU, respectively. We also observe a 0.63% improvement in recall over Deeplabv3+(MobileNet) <ref type="bibr" target="#b23">[24]</ref>. Our results are comparable with DoubleU-Net <ref type="bibr" target="#b9">[10]</ref> which reports the highest DSC of 0.8938. A higher recall shows that our method is more medically relevant, which is considered as the major strength of our architecture <ref type="bibr" target="#b59">[60]</ref>. From <ref type="figure" target="#fig_2">Figure 3</ref>, we can observe that our method can segment skin lesions of varying sizes accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization study</head><p>To ensure generalizability, we have trained our model and other SOTA methods on one dataset and then experimented on a new unseen dataset which comes from a different institution, consisting of different cohort populations and acquired using different imaging protocols. To this end, we have used the Kvasir-SEG collected in Vestre Viken Health Trust in Nor-way for training and tested our trained model on the CVC-ClinicDB, which was captured in Hospital Clinic in Barcelona, Spain. Similarly, we conducted this study on an opposite setup as well, i.e., training on CVC-ClinicDB and testing on Kvasir-SEG.</p><p>1) Generalizability results on CVC-ClinicDB <ref type="table" target="#tab_1">Table VI</ref> shows the generalizability results of the MSRF-Net model trained on Kvasir-SEG and tested on CVC-ClinicDB. Despite using two datasets acquired using two different imaging protocols, MSRF-Net obtained an acceptable DSC of 0.7921 ? 0.2564, mIoU of 0.6498 ? 0.2729, recall of 0.9001 ? 0.2980, and precision of 0.7000 ? 0.1572. We observe that our MSRF-Net performs better than other SOTA methods in terms of DSC. HRNetV2-W48 <ref type="bibr" target="#b43">[44]</ref> obtained a competitive DSC of 0.7901.</p><p>2) Generalizability results on Kvasir-SEG Similarly, we present the results of the models trained on CVC-ClinicDB and tested on Kvasir-SEG in <ref type="table" target="#tab_1">Table VII</ref>. We report that our model achieves a DSC of 0.7575 ? 0.2643, mIoU of 0.6337 ? 0.2815, recall of 0.7197 ? 0.2775 and precision of 0.8414 ? 0.2731, which outperforms other SOTA methods in DSC and mIoU. The second best performing method is PraNet <ref type="bibr" target="#b33">[34]</ref> with DSC of 0.7293 ? 0.3004, and mIoU of 0.6262 ? 0.3128. Our method outperforms PraNet [34] by 2.82% in DSC and 0.75% in mIoU but PraNet <ref type="bibr" target="#b33">[34]</ref> records the highest recall of 0.8007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study</head><p>We have conducted an extensive ablation study on the Kvasir-SEG. For this, we ablated the impact of the MSRF sub-network, scaling mechanism used in the network, the effect of the number of DSDF blocks used, the impact of the MSRF sub-network on shape prediction in the shape stream in Section V-C, the effect observed when shape stream and deep supervision is removed from the architecture of MSRF-Net. <ref type="table" target="#tab_1">Table VIII</ref> shows the quantitative results of our ablation study. Initially, we removed the MSRF sub-network which resulted in the least DSC of 0.8771. The addition of a subset of the original MSRF sub-network (The red dotted region in <ref type="figure" target="#fig_0">Figure 1(b)</ref>) raises the DSC to 0.8986. Further, we removed the DSDF with second and third scale inputs (also see <ref type="figure" target="#fig_0">Figure 1</ref>(b), where middle DSDF blocks represent them, i.e., layer three and layer five are removed) from the original MSRF subnetwork to achieve a DSC of 0.9013. To further investigate the contribution of our MSRF sub-network, we remove the shape stream to achieve a DSC of 0.9194 which is comparable to highest 0.9217 DSC reported by the original MSRF-Net configuration. We disable the triple attention mechanism in the decoder block to get a DSC 0.9067 ? 0.1834. Our ablation on further removing deep supervision resulted in a lower DSC of 0.8988. We also report the effect of using a combination of dice loss and binary cross entropy loss in L comb used in Equation 10 to supervise MSRF-Net during training. First, we set L comb = L BCE and secure a DSC of 0.9059 which was followed by setting L comb = L DCS which scored a DSC of 0.8861. A similar trend was observed for other metrics.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>Multi-scale fusion methodologies have been studied previously, however, there are some disadvantages. For example, U-Net <ref type="bibr" target="#b16">[17]</ref> uses skip-connections for feature fusion, but the resulting combination of features suffer from semantic gap since it combines low level features of the encoder and high level features of the decoder. Similarly, U-Net++ <ref type="bibr" target="#b10">[11]</ref> performs low to high feature fusion to overcome this problem, but high to low feature fusion remains lacking. Pyramid features are fused in Deeplabv3+ <ref type="bibr" target="#b24">[25]</ref> while without maintaining the high resolution representations. Similar to our approach, HR-Net builds upon the multi-scale feature fusion process by adding repeated feature fusion while keeping high resolution representation, however, their fusion modules consist of a larger number of trainable parameters and informative low level features are also lost during the segmentation process <ref type="bibr" target="#b60">[61]</ref>. The disadvantages stated above can result in Deeplabv3+ <ref type="bibr" target="#b24">[25]</ref> and HRNetV2 <ref type="bibr" target="#b61">[62]</ref> to perform considerably worse on the 2018 Data Science Bowl challenge where finer segmentation maps were required for a high DSC score. The results on Kavisir-SEG, CVC-ClinicDB, and ISIC-2018 also show similar performance gaps between proposed and other multi-scale fusion methods (see <ref type="table" target="#tab_1">Table I</ref> -IV).</p><p>The proposed MSRF-Net uses DSDF blocks (arranged as described in Algorithm 1) to attain global multi-scale fusion while increasing the frequency of multi-scale fusion operations and reporting a lower computational complexity as compared to HRNetV2 <ref type="bibr" target="#b61">[62]</ref>. The DSDF blocks itself allow effective feature fusion between high-and low resolution scales by continuous feature exchange across different scales. Additionally, its residual structure permits the relevant high-and lowlevel features to be deftly propagated enabling the proposed MSRF-Net to effectively capture the variability in size, shape and structure of the region of interest. We can observe that the residual densely connected nature of the DSDF blocks and its subsequent arrangement allows our proposed MSRF-Net to achieve highest DSC of 0.9217 and mIoU of 0.8914, on the Kvasir-SEG (see <ref type="table" target="#tab_1">Table II</ref>). Similarly, we report the highest values for DSC, mIoU and recall of 0.9420, 0.9043 and 0.9567, respectively, on CVC-ClinicDB (see <ref type="table" target="#tab_1">Table III</ref>). The ability of our MSRF-Net to recognize smaller and finer cell structures in 2018 Data Science Bowl is evident in <ref type="table" target="#tab_1">Table IV</ref>, where we report the best DSC of 0.9224. Additionally, we report best mIoU and recall on the ISIC-2018 skin lesion dataset. Our result is competitive to DoubleUNet in terms of DSC. We present training loss of MSRF-Net (Kvasir-SEG) with respect to the number of epochs elapsed in <ref type="figure">Figure 6</ref>(a). We can see that the model starts converging from epoch number 75 steadily.</p><p>In practical clinical environments, the performance of deep learning based segmentation methods decreases due to differences in the imaging protocols and patient variability. The models which are able to generalize across multi-center dataset are more desirable in a clinical setting <ref type="bibr" target="#b62">[63]</ref>. MSRF-Net achieves the highest DSC of 0.7921 when trained on Kvasir-SEG and tested on CVC-ClinicDB (see <ref type="table" target="#tab_1">Table VI</ref>). Similarly, MSRF-Net achieves highest DSC of 0.7575 and mIoU of and 0.6337, when trained on CVC-ClinicDB and tested on Kvasir-SEG (see <ref type="table" target="#tab_1">Table VII</ref>). HRNetV2-W48 <ref type="bibr" target="#b61">[62]</ref> was competitive to our method. The above results suggest that our proposed MSRFNet is more generalizable. This can be evidently due to our multi-scale fusion that exploits the feature at different scales, preserving some class representative features.</p><p>We performed an ablation study (see <ref type="table" target="#tab_1">Table VIII</ref>) to demonstrate that the combination of relevant high-and low-level multi-scale features obtained by the MSRF sub-network is instrumental in recognizing the shape or boundaries of the target object that can boost the segmentation performance. To verify the contribution of the MSRF sub-network, we disable the entire MSRF sub-network from the full network while keeping each component of the network intact and train the model. <ref type="table" target="#tab_1">Table VIII</ref> shows that when the MSRF sub-network is removed from the proposed MSRF-Net, the DSC drops by 4.46%. This performance degradation illustrates that each MSRF subnetwork contributes to the network. The combination of highand low-level resolution feature representations of varying receptive fields extracted from the MSRF sub-network contribute significantly towards improving the model's performance. We also ablated if multi-scale fusion was suitable for the entire network. Sub-Network without DSDF refers to the removal of DSDF with 2nd and 3rd scale inputs (also see <ref type="figure" target="#fig_0">Figure 1</ref>(b), where middle DSDF blocks represent them, i.e., layer three and layer five are removed). <ref type="table" target="#tab_1">Table VIII</ref> shows the result when global multi-scale fusion is absent from the network. As a result, we observe a 2.04% performance drop in DSC. Therefore, it is noticeable that the multi-scale fusion used in the MSRF sub-network improves performance. To study the impact of the number of DSDF blocks on the segmentation performance, we reduced the number of DSDF layers from six (ours) to three, i.e., only red rectangular block in <ref type="figure" target="#fig_0">Figure 1(b)</ref> is used. Even though this enables us to exchange global multiscale feature representations, our results in <ref type="table" target="#tab_1">Table VIII</ref> show that reducing the number of DSDF blocks decreases the DSC by 2.31% .</p><p>The sub-network without scaling in <ref type="table" target="#tab_1">Table VIII</ref> demonstrates the influence of scaling factor w in the network (see Equation 3). For this experiment, we did not scale the output of DSDF by a constant while adding to the block's input. Drop of 0.80% in DSC was observed when the features were not scaled. Furthermore, our empirical experiments (see <ref type="figure">Figure 6</ref>(b) using different scaling values of w introduced in Equation 3) demonstrate our optimal choice of w to be 0.4.</p><p>We design a variant model where, the MSRF sub-network is placed after the shape-stream in the MSRF-Net. Here, we keep the number of parameters same for both the models (i.e., MSRF-Net and variant model) to analyze the impact of MSRF sub-network on the shape stream. The qualitative results (see <ref type="figure" target="#fig_4">Figure 5</ref>) show that the MSRF-Net can define more precise and more spatially accurate boundaries than the variant model. The variant model fails to recognize the boundaries of the target structure as it is deprived of the multi-scale features extracted by the MSRF sub-network. This validates our choice of putting the MSRF sub-network before the shape stream block. Only a minor drop of 0.23% in DSC is seen when no shape stream is applied and it still outperforms most SOTA methods.</p><p>We also investigated the impact of our triple attention block by disabling the mechanism prior to training the MSRF-Net. We disable deep supervision in another experiment, while training MSRF-Net. Both of these experiments showed performance drop compared to our proposed MSRF-Net (1.50% drop in former and 2.29% drop in latter on DSC metric). We also evaluate the impact of the combination of L BCE and L DCS used in L comb (see Section III-E). For this, we trained the MSRF-Net with L comb = L DCS and then with L comb = L BCE . When L comb = L DCS + L BCE , we obtained an increase of 3.56% in DSC, 4.68% in mIoU, 0.59% in recall and 4.90% in precision as compared to the L comb = L DCS setting. Similar trend was observed when L comb was equal to L BCE (see <ref type="table" target="#tab_1">Table VIII)</ref>.</p><p>MSRF-Net clearly shows the strength of fusing low-and high-resolution features through DSDF blocks and MSRF sub-network. Alongside, complementary inclusion of scaling factor, deep supervision in the encoder block and triple at-tention in the decoder block showed further improvements. In <ref type="figure" target="#fig_3">Figure 4</ref>, we show the qualitative results for the suboptimal cases. The qualitative results show poor performance for oblique samples in polyp datasets. Similarly, the model also failed for extremely low contrast images with 2018 DSB and scattered similar patches in ISIC 2018.</p><p>VII. CONCLUSION In this paper, we proposed the MSRF-Net architecture for medical image segmentation that takes advantage of multiscale resolution features passed through a sequence of DSDF blocks. Such densely connected residual blocks with dualscale feature exchange enable efficient feature extraction with varying receptive fields. Additionally, we have also shown that the features from DSDF blocks are better suited to capture a target object's entire shape boundaries, even for objects with variable sizes. Our experiments revealed that MSRF-Net outperformed several SOTA methods on four independent biomedical datasets. Our investigation using cross-datasets testing to evaluate the generalizability of the MSRF-Net confirmed that our model can produce competitive results in such scenarios. We also identified some challenges of the proposed method, such as that the model fails when extremely low contrast images are part of the data. For future work, we plan to investigate the identified challenges further and adjust the design of the network to address the challenging cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Components of our MSRF-Net, a) Proposed Dual-Scale Dense Fusion (DSDF) block and b) Multi-Scale Residual Fusion (MSRF). Dotted rectangle block in (b) represents multi-scale feature exchange in MSRF-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed MSRF-Net architectures. a) Overall block diagram of our network and b) overview of our decoder network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The Figure shows qualitative results of the MSRF-Net on four biomedical imaging datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results of the MSRF-Net for sub-optimal cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results showing polyp contours when shape stream is used before MSRF sub-network and after the MSRF sub-network in the MSRF-Net Fig. 6. a) Plot of training loss versus number of epochs and b) Variation in DSC with respect to hyper parameter w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>MEDICAL DATASETS USED IN OUR EXPERIMENTS.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Images Input size</cell><cell cols="3">Train Valid Test</cell></row><row><cell>Kvasir-SEG [12]</cell><cell>1000</cell><cell>Variable</cell><cell>800</cell><cell>100</cell><cell>100</cell></row><row><cell>CVC-ClinicDB [13]</cell><cell>612</cell><cell>384 ? 288</cell><cell>490</cell><cell>61</cell><cell>61</cell></row><row><cell>2018 Data Science</cell><cell>670</cell><cell>256 ? 256</cell><cell>536</cell><cell>67</cell><cell>67</cell></row><row><cell>Bowl [2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ISIC-2018</cell><cell>2596</cell><cell>384 ? 512</cell><cell cols="2">2078 259</cell><cell>259</cell></row><row><cell>Challenge [14], [15]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>ON THE 2018 DATA SCIENCE BOWL</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>P-values</cell><cell>Parameters</cell><cell>FPS</cell></row><row><cell>U-Net [17]</cell><cell>0.9080 ? 0.0638</cell><cell>0.8314 ? 0.1019</cell><cell>0.9029 ? 0.0981</cell><cell>0.9130 ? 0.0719</cell><cell>1.308e-03</cell><cell>7.11M</cell><cell>25.02</cell></row><row><cell>U-Net++ [11]</cell><cell>0.7705 ? 0.3010</cell><cell>0.5265 ? 0.3078</cell><cell>0.7159 ? 0.3171</cell><cell>0.6657 ? 0.2745</cell><cell>4.832e-04</cell><cell>9.04M</cell><cell>21.86</cell></row><row><cell>ResUNet++ [20]</cell><cell>0.9098 ? 0.0797</cell><cell>0.8370 ? 0.1154</cell><cell>0.9169 ? 0.0947</cell><cell>0.9057 ? 0.0853</cell><cell>2.420e-01</cell><cell>4.07M</cell><cell>19.81</cell></row><row><cell>Deeplabv3+ (Xception) [25]</cell><cell>0.8857 ? 0.1674</cell><cell>0.8367 ? 0.1702</cell><cell>0.9141 ? 0.1751</cell><cell>0.9081 ? 0.1689</cell><cell>4.641e-01</cell><cell>41.25M</cell><cell>16.20</cell></row><row><cell>Deeplabv3+ (Mobilenet) [25]</cell><cell>0.8239 ? 0.1613</cell><cell>0.7402 ? 0.1618</cell><cell>0.8896 ? 0.1720</cell><cell>0.8151 ? 0.1657</cell><cell>1.044e-06</cell><cell>2.14M</cell><cell>23.70</cell></row><row><cell>DoubleUNet [10]</cell><cell>0.9109 ? 0.0876</cell><cell>0.8429 ? 0.1109</cell><cell>0.9278 ? 0.0962</cell><cell>0.9020 ? 0.0997</cell><cell>8.929e-02</cell><cell>29.29M</cell><cell>7.47</cell></row><row><cell>HRNetV2-W18-Smallv2 [44]</cell><cell>0.8495 ? 0.3267</cell><cell>0.7585 ? 0.1521</cell><cell>0.8640 ? 0.1659</cell><cell>0.8398 ? 0.1602</cell><cell>4.978e-04</cell><cell>26.20M</cell><cell>58.03</cell></row><row><cell>HRNetV2-W48 [44]</cell><cell>0.8488 ? 0.1470</cell><cell>0.7588 ? 0.1499</cell><cell>0.8359 ? 0.1618</cell><cell>0.8913 ? 0.0550</cell><cell>8.460e-05</cell><cell>65.84M</cell><cell>29.41</cell></row><row><cell>ColonSegNet [5]</cell><cell>0.9197 ? 0.0605</cell><cell>0.8466 ? 0.0953</cell><cell>0.9153 ? 0.0917</cell><cell>0.9312 ? 0.0532</cell><cell>6.433e-01</cell><cell>5.01M</cell><cell>16.56</cell></row><row><cell>DDANet [56]</cell><cell>0.9182 ? 0.0684</cell><cell>0.8452 ? 0.1037</cell><cell>0.9139 ? 0.0964</cell><cell>0.9289 ? 0.0575</cell><cell>5.922e-01</cell><cell>6.83M</cell><cell>19.02</cell></row><row><cell>ResUNet++ + CRF [57]</cell><cell>0.7806 ? 0.2223</cell><cell>0.7322 ? 0.2386</cell><cell>0.7534 ? 0.2558</cell><cell>0.6308 ? 0.1752</cell><cell>6.971e-06</cell><cell>4.02M</cell><cell>72.78</cell></row><row><cell>PraNet [34]</cell><cell>0.8751 ? 0.0871</cell><cell>0.7868 ? 0.1169</cell><cell>0.9182 ? 0.0736</cell><cell>0.8438 ? 0.1138</cell><cell>1.553e-07</cell><cell>32.54M</cell><cell>11.88</cell></row><row><cell>UACANet-S [35]</cell><cell>0.8687 ? 0.0913</cell><cell>0.7774 ? 0.1208</cell><cell>0.9092 ? 0.0960</cell><cell>0.8385 ? 0.1115</cell><cell>2.940e-08</cell><cell>26.90M</cell><cell>28.08</cell></row><row><cell>UACANet-L [35]</cell><cell>0.8688 ? 0.0999</cell><cell>0.7791 ? 0.1283</cell><cell>0.9061 ? 0.1057</cell><cell>0.8414 ? 0.1141</cell><cell>8.975e-07</cell><cell>69.15M</cell><cell>32.26</cell></row><row><cell>MSRF-Net (Ours)</cell><cell>0.9224 ? 0.0538</cell><cell>0.8534 ? 0.0870</cell><cell>0.9402 ? 0.0734</cell><cell>0.9022 ? 0.0601</cell><cell>-</cell><cell>18.38M</cell><cell>6.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V RESULTS</head><label>V</label><figDesc>ON THE ISIC-2018 SKIN LESION SEGMENTATION CHALLENGE</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>P-values</cell><cell>Parameters</cell><cell>FPS</cell></row><row><cell>U-Net [17]</cell><cell>0.8554 ? 0.1848</cell><cell>0.7847 ? 0.2094</cell><cell>0.8204 ? 0.2186</cell><cell>0.9474 ? 0.1296</cell><cell>1.315e-02</cell><cell>7.11M</cell><cell>79.43</cell></row><row><cell>U-Net++ [11]</cell><cell>0.8094 ? 0.2261</cell><cell>0.7288 ? 0.2452</cell><cell>0.7866 ? 0.2369</cell><cell>0.9084 ? 0.2222</cell><cell>6.336e-08</cell><cell>9.04M</cell><cell>60.44</cell></row><row><cell>ResUNet++ [20]</cell><cell>0.8557 ? 0.2014</cell><cell>0.8135 ? 0.2210</cell><cell>0.8801 ? 0.2320</cell><cell>0.8676 ? 0.1562</cell><cell>1.813e-02</cell><cell>4.07M</cell><cell>40.93</cell></row><row><cell>Deeplabv3+ (Xception) [25]</cell><cell>0.8772 ? 0.1465</cell><cell>0.8128 ? 0.1806</cell><cell>0.8681 ? 0.1792</cell><cell>0.9272 ? 0.13602</cell><cell>4.314e-02</cell><cell>41.25M</cell><cell>43.53</cell></row><row><cell>Deeplabv3+ (Mobilenet) [25]</cell><cell>0.8781 ? 0.1371</cell><cell>0.8236 ? 0.1711</cell><cell>0.8830 ? 0.1725</cell><cell>0.9244 ? 0.1317</cell><cell>9.503e-02</cell><cell>2.14M</cell><cell>61.10</cell></row><row><cell>HRNetV2-W18-Smallv2 [44]</cell><cell>0.8561 ? 0.3696</cell><cell>0.7821 ? 0.2091</cell><cell>0.8556 ? 0.2029</cell><cell>0.8974 ? 0.1862</cell><cell>8.780e-03</cell><cell>26.20M</cell><cell>57.47</cell></row><row><cell>HRNetV2-W48 [44]</cell><cell>0.8667 ? 0.2453</cell><cell>0.8109 ? 0.2630</cell><cell>0.8584 ? 0.2936</cell><cell>0.9155 ? 0.2755</cell><cell>4.270e-02</cell><cell>65.84M</cell><cell>28.54</cell></row><row><cell>DoubleU-Net [10]</cell><cell>0.8938 ? 0.1362</cell><cell>0.8212 ? 0.1659</cell><cell>0.8780 ? 0.1573</cell><cell>0.9459 ? 0.1353</cell><cell>1.000e+00</cell><cell>29.29M</cell><cell>7.46</cell></row><row><cell>ResUNet++ + CRF [57]</cell><cell>0.8688 ? 0.1719</cell><cell>0.8209 ? 0.1971</cell><cell>0.8826 ? 0.2063</cell><cell>0.8736 ? 0.1540</cell><cell>1.813e-02</cell><cell>4.02M</cell><cell>79.11</cell></row><row><cell>MSRF-Net (Ours)</cell><cell>0.8824 ? 0.1602</cell><cell>0.8373 ? 0.1818</cell><cell>0.8893 ? 0.1889</cell><cell>0.9348 ? 0.1488</cell><cell>-</cell><cell>18.38M</cell><cell>16.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI GENERALIZABILITY</head><label>VI</label><figDesc>RESULTS OF THE MODELS TRAINED ON KVASIR-SEG AND TESTED ON CVC-CLINICDB</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII ABLATION</head><label>VIII</label><figDesc>STUDY OF MSRF-NET ON THE KVASIR-SEG</figDesc><table><row><cell>Experiment description</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>Parameters</cell><cell>FPS</cell></row><row><cell>MSRF-Net (ours)</cell><cell>0.9217 ? 0.1685</cell><cell>0.8914 ? 0.1938</cell><cell>0.9198 ? 0.1919</cell><cell>0.9666 ? 0.1379</cell><cell>18.38M</cell><cell>16.24</cell></row><row><cell>Without sub-network</cell><cell>0.8771 ? 0.2062</cell><cell>0.8103 ? 0.2400</cell><cell>0.8911 ? 0.1742</cell><cell>0.8993 ? 0.2172</cell><cell>2.66M</cell><cell>32.64</cell></row><row><cell>Sub-network without scaling</cell><cell>0.9137 ? 0.1772</cell><cell>0.8898 ? 0.2013</cell><cell>0.9625 ? 0.1932</cell><cell>0.9218 ? 0.1484</cell><cell>18.38M</cell><cell>8.92</cell></row><row><cell>Sub-network without DSDF (across 2,3 scale)</cell><cell>0.9013 ? 0.2111</cell><cell>0.8782 ? 0.2329</cell><cell>0.9460 ? 0.2098</cell><cell>0.9246 ? 0.2068</cell><cell>17.24M</cell><cell>17.28</cell></row><row><cell>Subset of the sub-network</cell><cell>0.8986 ? 0.1997</cell><cell>0.8570 ? 0.1262</cell><cell>0.9228 ? 0.2879</cell><cell>0.9232 ? 0.2666</cell><cell>8.67M</cell><cell>13.10</cell></row><row><cell>Without deep supervision</cell><cell>0.8988 ? 0.2060</cell><cell>0.8449 ? 0.2313</cell><cell>0.9053 ? 0.1795</cell><cell>0.9267 ? 0.2152</cell><cell>18.38M</cell><cell>16.58</cell></row><row><cell>Without decoder block</cell><cell>0.9067 ? 0.1834</cell><cell>0.8691 ? 0.2080</cell><cell>0.9143 ? 0.1875</cell><cell>0.9461 ? 0.1807</cell><cell>17.41M</cell><cell>16.90</cell></row><row><cell>Without shape stream</cell><cell>0.9194 ? 0.1779</cell><cell>0.8907 ? 0.1984</cell><cell>0.9700 ? 0.1976</cell><cell>0.9159 ? 0.1348</cell><cell>18.37M</cell><cell>17.19</cell></row><row><cell>MSRF-Net with Lcomb = LDCS</cell><cell>0.8861 ? 0.2192</cell><cell>0.8446 ? 0.2389</cell><cell>0.9139 ? 0.2078</cell><cell>0.9176 ? 0.2003</cell><cell>18.38M</cell><cell>16.18</cell></row><row><cell>MSRF-Net with Lcomb = LBCE</cell><cell>0.9059 ? 0.1859</cell><cell>0.8677 ? 0.2116</cell><cell>0.9446 ? 0.1938</cell><cell>0.9143 ? 0.17000</cell><cell>18.38M</cell><cell>16.11</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>DSC mIoU Recall Precision Parameters FPS U-Net <ref type="bibr" target="#b16">[17]</ref> 0.7172 ? 0.2911 0.6133 ? 0.2870 0.7255 ? 0.3246 0.7986 ? 0.2775 7.11M 24.15 U-Net++ <ref type="bibr" target="#b10">[11]</ref> 0.4265 ? 0.3922 0.3345 ? 0.3518 0.3939 ? 0.4480 0.6894 ? 0.4111 9.04M 21.18 ResUNet++ <ref type="bibr" target="#b19">[20]</ref> 0 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dermoscopy image analysis: overview and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="474" to="478" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 data science bowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1247" to="1253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for detection and segmentation of artefact and disease instances in gastrointestinal endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Imag. Anal</title>
		<imprint>
			<biblScope unit="page">102002</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Annotation of endoscopic videos on mobile devices: a bottom-up approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Multimedia Systems Conference</title>
		<meeting>the 4th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-Time Polyp Detection, Localisation and Segmentation in Colonoscopy Using Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Acc</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Imag. Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparative validation of multi-instance instrument segmentation in endoscopy: results of the robust-mis 2019 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ro?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Imag. Anal</title>
		<imprint>
			<biblScope unit="page">101920</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
	<note>in Dee. learn. da. label. medi. applicat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat. Sympo. Comp.-Bas. Med. Syst</title>
		<meeting>of Internat. Sympo. Comp.-Bas. Med. Syst</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">UNet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: A Segmented Polyp Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat. Conf. Multimed. Model</title>
		<meeting>of Internat. Conf. Multimed. Model</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer. Medi. Imag. Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat</title>
		<meeting>of Internat</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scienti. Da</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt. Recogn</title>
		<meeting>of Comput. Vis. and Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat</title>
		<meeting>of Internat</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neur. Networ</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention U-Net: learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ResUNet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat. Sympos. Multime</title>
		<meeting>of Internat. Sympos. Multime</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UNet++: A nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
	<note>in Deep learn. med. ima. anal. multimo. learn. clini. deci. sup.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saunet: shape attentive unet for interpretable medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Darbehani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat</title>
		<meeting>of Internat</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt. Recogn</title>
		<meeting>of Comput. Vis. and Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Patt. Analy. and Mach. Intelli</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. conf. comput. vis</title>
		<meeting>of the Europ. conf. comput. vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Evaluation of Deep Segmentation Models for the Extraction of Retinal Lesions from Multimodal Retinal Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Usman</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cell-Net: embryonic cell counting and centroid localization via residual incremental atrous pyramid and progressive upsampling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Havelock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Acc</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="81" to="945" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Polyp segmentation with fully convolutional deep neural networks -extended evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matuszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jour. of Imag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt</title>
		<meeting>of Comput. Vis. and Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature fusion encoder decoder network for automatic liver lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of internat. sympos. biomed. imag</title>
		<meeting>of internat. sympos. biomed. imag</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="430" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Focusnet: An attention-based fully convolutional network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat</title>
		<meeting>of Internat</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="455" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. and Remo. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gated-SCNN: Gated shape CNNs for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt. Recogn</title>
		<meeting>of Comput. Vis. and Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5229" to="5238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat</title>
		<meeting>of Internat</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Uacanet: Uncertainty augmented context attention for polyp semgnetaion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02368</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: achievements and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jour. of digi. imag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural networks in medical image understanding: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sarvamangala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on u-shaped networks in medical image segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">409</biblScope>
			<biblScope unit="page" from="244" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully dense unet for 2-d sparse photoacoustic tomography artifact removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sikdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Chitnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt. Recogn</title>
		<meeting>of Comput. Vis. and Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Road Detection via Deep Residual Dense U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">O</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pro. of Internat. Joi. Conf. on Neu</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual dense U-Net for resolution enhancement in accelerated MRI acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Medi. Imag. 2019: Ima. Proce</title>
		<meeting>of Medi. Imag. 2019: Ima. e</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10949</biblScope>
			<biblScope unit="page">109490</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hyperdense-net: a hyper-densely connected cnn for multi-modal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1116" to="1126" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Other</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Patt. Analy. Mach. Intelli</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Patt. Analy. and Mach. Intelli</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt</title>
		<meeting>of Comput. Vis. and Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zigzagnet: Fusing top-down and bottom-up context for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt. Recogn</title>
		<meeting>of Comput. Vis. and Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7490" to="7499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deeply-fused nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07716</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat. Conf. Compu. Vis</title>
		<meeting>of Internat. Conf. Compu. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4373" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Igcv3: Interleaved low-rank group convolutions for efficient deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00178</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt. Recogn. Worksh</title>
		<meeting>of Comput. Vis. and Patt. Recogn. Worksh</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Conf. Artifi. Intelli</title>
		<meeting>of AAAI Conf. Artifi. Intelli</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artifi. intelli. stat</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
	<note>in 12th {USENIX} sympo. operat. syst. desi. implement. ({OSDI} 16</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICPR 2020 Worksh. and Chall</title>
		<meeting>of the ICPR 2020 Worksh. and Chall</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Screening and surveillance for the early detection of colorectal cancer and adenomatous polyps, 2008: a joint guideline from the american cancer society, the us multi-society task force on colorectal cancer, and the american college of radiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1570" to="1595" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Cancer facts &amp; figures 2018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Society</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The missing pieces of artificial intelligence in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gilvary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elkhader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elemento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tren. pharmacolo. sci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="555" to="564" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hrcnet: high-resolution context extraction network for semantic segmentation of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. patt. analy. mach</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">PolypGen: a multi-center polyp detection and segmentation dataset for generalisability assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04463</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
