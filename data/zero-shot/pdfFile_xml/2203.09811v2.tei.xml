<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingning</forename><surname>Dong</surname></persName>
							<email>dongxingning1998@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gan</surname></persName>
							<email>gantian@sdu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cheng</surname></persName>
							<email>chengyuan.c@antgroup.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene Graph Generation, which generally follows a regular encoder-decoder pipeline, aims to first encode the visual contents within the given image and then parse them into a compact summary graph. Existing SGG approaches generally not only neglect the insufficient modality fusion between vision and language, but also fail to provide informative predicates due to the biased relationship predictions, leading SGG far from practical. Towards this end, we first present a novel Stacked Hybrid-Attention network, which facilitates the intra-modal refinement as well as the intermodal interaction, to serve as the encoder. We then devise an innovative Group Collaborative Learning strategy to optimize the decoder. Particularly, based on the observation that the recognition capability of one classifier is limited towards an extremely unbalanced dataset, we first deploy a group of classifiers that are expert in distinguishing different subsets of classes, and then cooperatively optimize them from two aspects to promote the unbiased SGG. Experiments conducted on VG and GQA datasets demonstrate that, we not only establish a new state-of-the-art in the unbiased metric, but also nearly double the performance compared with two baselines. Our code is available at https://github.com/dongxingning/SHA-GCL-for-SGG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene Graph Generation (SGG) <ref type="bibr" target="#b40">[41]</ref> targets at organizing all the objects and their pairwise relationships into a compact summary graph. As an intermediate visual understanding task, SGG could benefit various vision-andlanguage tasks, including cross-modal retrieval <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>, image captioning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51]</ref>, and visual question answering <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48]</ref>. However, SGG is still far from satisfactory for practical applications due to the insufficient modality fusion and the biased relationship predictions. ? Corresponding authors.  <ref type="figure">Figure 1</ref>. Two intentions to promote the unbiased SGG. <ref type="bibr" target="#b0">(1)</ref> For the insufficient modality fusion, we aim to enhance both the intra-modal refinement and the inter-modal interaction (see the top-right corner of the figure). And <ref type="bibr" target="#b1">(2)</ref> we split the extremely unbalanced dataset into a set of relatively balanced groups, based on which we configure the classification space for all the newlyadded classifiers (see the rest part of the figure).</p><p>Though it is manifestly proved that incorporating semantic cues (language priors of object class names) into visual contents (object proposals) could significantly improve the generation capability <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>, most of the recent approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> simply fuse these visual and semantic features by summing up directly or concatenation, which limits the model to further infer their interaction information. To address this under-explored insufficient modality fusion between visual contents and semantic cues, we aim to strengthen the encoder via jointly exploring the intra-modal refinement and the inter-modal interaction, as illustrated in <ref type="figure">Figure 1</ref>. To implement this intention, we first design the Self-Attention (SA) unit and the Cross-Attention (CA) unit to capture the intra-modal and intermodal information, respectively. We then organize these two units into a Hybrid-Attention (HA) layer, and stack several HA layers to build the encoder. The proposed Stacked Hybrid-Attention (SHA) network could adequately explore the multi-modal interaction, thus improving the relationship prediction performance.</p><p>The other prominent issue faced by existing SGG methods is the biased relationship predictions due to the longtailed data distribution. Since only a few head predicates (e.g., on, has) possess massive and various instances, they would dominate the training procedure and lead the output scene graphs with few informative tail predicates (e.g., riding, watching), which could hardly support a wide range of downstream tasks. Though various debiasing approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> have been proposed, they are vulnerable to overfitting the tail classes and sacrificing much on the head ones, leading to the other extreme. In a sense, we conjecture that this dilemma may root in the fact that a naive SGG model, regardless of the conventional or debiasing one, could only differentiate a limited range of predicates whose amount of training instances are relatively equal.</p><p>Intuitively, since a single classifier struggles in achieving a reasonable prediction trade-off, we can divide the biased predicate classes into several balanced subsets, then introduce more classifiers to conquer each of them, and ultimately leverage these classifiers to cooperatively address this challenge. To fulfill this "divide-conquer-cooperate" intuition, we propose the Group Collaborative Learning (GCL) strategy, where we 1) first divide: As a single classifier is adequate to differentiate the classes within a balanced dataset, we first divide all the predicates into a set of relatively balanced groups according to their amount of training instances, as illustrated in <ref type="figure">Figure 1</ref>. 2) Then conquer: We then borrow the idea from the class-incremental learning <ref type="bibr" target="#b13">[14]</ref> to force all the classifiers to follow a continuously growing classification space, i.e., each classifier would extend the previous classification space by incorporating a newly-added group of predicates. Besides, we devise the Median Re-Sampling strategy to provide each classifier with a relatively balanced training set. Based on this groupincremental configuration, these nested classifiers could fairly treat the predicates within their classification space, thus they would be more likely to learn the discriminating representations, especially towards the newly-added group. 3) Ultimately cooperate: We further leverage these classifiers to cooperatively enhance the unbiased relationship predictions from two aspects. First, we propose the Parallel Classifier Optimization (PCO) to jointly optimize all the classifiers. This can be seen as a "weak constraint", since we expect that gathering all the gradients could promote the recognition capability of each classifier. Second, we devise the Collaborative Knowledge Distillation (CKD) to ensure that the discriminating capability learned previously could be well translated to the subsequent classifiers. This can be seen as a "strong constraint", since we force each classifier to mimic the prediction behavior from its predecessors. By employing these two constraints, we effectively mitigate the overwhelming punishments to the tail classes as well as compensate for the under-fitting on the head ones.</p><p>The contributions of our work are three-folds:</p><p>? We present a novel Stacked Hybrid Attention network to strengthen the encoder in SGG, which addresses the under-explored insufficient modality fusion problem. ? We design the Group Collaborative Learning strategy to optimize the decoder in SGG. Particularly, we deploy a group of classifiers and cooperatively optimize them from two aspects, thus effectively addressing the intractable biased relationship prediction problem. ? Experiments conducted on VG and GQA dataset indicate that, we not only establish a new state-of-theart in the unbiased metric, but also nearly double the performance compared with two typical baselines when employing our model-agnostic GCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene Graph Generation. SGG provides an efficient way for scene understanding by decoding the visual relationships into a summary graph. Early approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> were mainly dedicated to incorporating more features from various modalities, but they neglected the rich visual context, leading to sub-optimal performance. In order to tackle such deficiency, later approaches employed more powerful feature refinement modules to encode the rich contextual information, such as message passing strategy <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40]</ref>, sequential LSTMs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47]</ref>, graph neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46]</ref>, and self-attention networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>. Though the performance is improved in the regular metrics, the relations they predicted are often trivial and less informative due to the biased training data, which could hardly support the downstream vision-and-language tasks. Therefore, various approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> have been proposed to tackle the biased relationship predictions, including employing debiasing strategies like re-sampling <ref type="bibr" target="#b16">[17]</ref> or re-weighting <ref type="bibr" target="#b41">[42]</ref>, disentangling unbiased representations from the biased <ref type="bibr" target="#b29">[30]</ref>, and utilizing the tree structure to filter the irrelevant predicates <ref type="bibr" target="#b42">[43]</ref>. However, these approaches are vulnerable to over-fitting on the tail classes with much sacrifice on the head ones. Based on the observation that a single classifier could hardly differentiate all the classes within a biased dataset, and inspired by the "divide-conquer-cooperate" intuition, we propose the Group Collaborative Learning strategy to guide the training of the decoder. In this way, we not only significantly improve the prediction performance towards the tail classes, but also effectively preserve the discriminating capability learned by the head ones, thus achieving a reasonable prediction trade-off. Cross-attention Models. Research towards improving multi-modal fusion <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> and building cross-attention models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b49">50]</ref>  FC FC FC ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? FC FC  <ref type="figure">Figure 2</ref>. The framework of the common pipeline in SGG, which includes five key components. Notably, we improve three key components marked in red in the figure. Specifically, we propose the Stacked Hybrid-Attention network to enhance the object encoder and the relation encoder, and we also devise the Group Collaborative Learning strategy to guide the training of the relation decoder.</p><p>fully model the interaction between question words and image regions in VQA, and Lu et al. <ref type="bibr" target="#b21">[22]</ref> proposed ViL-BERT to extend BERT architecture for jointly pre-training images and texts. Nevertheless, few of the approaches in SGG dedicate to addressing the insufficient modality fusion between object proposals and their corresponding class names. Therefore, we propose the Stacked Hybrid-Attention (SHA) network to facilitate both the intra-modal refinement and the inter-modal interaction. Knowledge Distillation. Knowledge distillation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref> aims to distill the knowledge from a larger deep network into a small one, which is widely employed in various tasks, including model compression <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>, label smoothing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>, and data augmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Note that the conventional knowledge distillation approaches generally follow a teacher-student pipeline. These two networks are optimized in different time steps as the teacher network is usually available beforehand. Different from this model-tomodel paradigm, after adding several classifiers, we allow the previous classifiers to generate the outputs as soft labels to constrain the training of the subsequent, thus establishing a layer-to-layer "knowledge transfer".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>SGG aims to generate a summary graph G that highly generalizes the contents of a given image I. Towards this end, we first detect all the objects within the image I, denoted as O = {o i } N i=1 . Then for each object pair (o i , o j ), we predict its predicate p i?j . Ultimately, we organize all these predictions in the form of triplets to construct the scene graph, which can be formulated as</p><formula xml:id="formula_0">G = {(o i , p i?j , o j )|o i , o j ? O, p i?j ? P},</formula><p>where P stands for the set of all the possible predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Framework</head><p>As illustrated in <ref type="figure">Figure 2</ref>, our framework is based on the common pipeline followed by typical SGG approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>, which is a regular encoder-decoder structure.</p><p>Proposal Network is actually a pre-trained object detector. Given an image I, it generates a set of object</p><formula xml:id="formula_1">predictions O = {o i } N i=1 .</formula><p>For each object o i , it provides a visual feature v i , a spatial feature s i of the bounding box coordinates, and an initial object label prediction l i .</p><p>Object Encoder aims to obtain the refined object feature x i for further predictions, which is calculated as:</p><formula xml:id="formula_2">x i = Enc obj ([v i , F C(s i )], Emb(l i )),<label>(1)</label></formula><p>where Enc obj (?) represents the object encoder, which can be any feature refinement modules (e.g., BiLSTMs <ref type="bibr" target="#b46">[47]</ref> and GNNs <ref type="bibr" target="#b2">[3]</ref>), [, ?, ] denotes the concatenation operation, F C(?) represents a fully-connected layer, and Emb(?) refers to a pre-trained language model to acquire the semantic feature of o i based on its initial object label prediction l i . Object Decoder aims to obtain the final object label prediction l ? i based on the refined object feature x i , which is calculated as:</p><formula xml:id="formula_3">l ? i = argmax(Softmax(Dec obj (x i ))),<label>(2)</label></formula><p>where Dec obj (?) represents the object decoder, which is a single fully-connected layer.</p><p>Relation Encoder works on obtaining the final object feature x ? i for predicate predictions, which is calculated as:</p><formula xml:id="formula_4">x ? i = Enc rel ([v i , x i ], Emb(l ? i )),<label>(3)</label></formula><p>where Enc rel (?) represents the relation encoder, which shares the same architecture with the object encoder. Relation Decoder is responsible for predicting the predicate label p i?j based on the final object features of subject  <ref type="figure">Figure 3</ref>. One single Stacked Hybrid-Attention (SHA) layer is composed of two types of attention units, i.e., Self-Attention (SA) unit to facilitate the intra-modal refinement and Cross-Attention (CA) unit to promote the inter-modal interaction. o i and object o j , which is calculated as:</p><formula xml:id="formula_5">p i?j = argmax(Softmax(Dec rel (x ? i , x ? j , u ij )),<label>(4)</label></formula><p>where Dec rel (?) represents the relation decoder. We also follow <ref type="bibr" target="#b46">[47]</ref> to employ the union feature u ij of the object pair (o i , o j ) to enhance the predicate predictions.</p><p>It is worth noting that we improve three key components marked in red in <ref type="figure">Figure 2</ref> to promote the unbiased SGG. Specifically, for the object encoder and the relation encoder, we propose the Stacked Hybrid-Attention (SHA) network to alleviate the insufficient modality fusion problem. Regarding the relation decoder, we devise the Group Collaborative Learning (GCL) strategy to address the intractable biased relationship prediction problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Encoder: Stacked Hybrid-Attention</head><p>Beyond understanding the visual contents (object proposals) of a given image, the semantic cues (refer to the class names in SGG) are also indispensable for robust relationship predictions. Unfortunately, most of the approaches in SGG simply fuse these two modal features by summing up directly or concatenation, which may be insufficient to mine the underlying inter-modal interaction, thus resulting in sub-optimal performance. To address this deficiency, we propose the Stacked Hybrid Attention (SHA) network, which is composed of several SHA layers. Each SHA layer contains two parallel Hybrid-Attention (HA) cells, and each HA cell is a composition of two types of attention units, i.e., the Self-Attention (SA) unit to facilitate the intra-modal refinement, and the Cross-Attention (CA) unit to model the inter-modal interaction. As shown in <ref type="figure">Figure 3</ref>, both the SA unit and CA unit are built upon a multi-head attention module and a feed-forward module based on the attention mechanism <ref type="bibr" target="#b32">[33]</ref>. The difference between SA and CA is whether the input features belong to the same modality.</p><p>Ultimately, we build our SHA network by cascading L SHA layers in sequential order. For the l-th SHA layer, the feature propagation process can be formulated as:</p><formula xml:id="formula_6">X (l) = SA(X (l?1) ) + CA(X (l?1) , Y (l?1) ), Y (l) = SA(Y (l?1) ) + CA(Y (l?1) , X (l?1) ),<label>(5)</label></formula><p>where SA(?) and CA(?) denote the self-attention and crossattention computation, respectively. For the first SHA layer, we set its input feature X (0) = X and Y (0) = Y, where X and Y denote the original visual feature and semantic feature, respectively. After obtaining the final visual feature X (L) and semantic feature Y (L) generated by the last SHA layer, we sum them up to get the refined output, which contains rich multi-modal interaction information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decoder: Group Collaborative Learning</head><p>As aforementioned, when facing an extremely unbalanced dataset, a naive SGG model could hardly achieve a satisfactory prediction performance on all the predicate classes. Towards this end, we aim to deploy several classifiers which are expert in distinguishing different subsets of predicates, and organize these classifiers to cooperatively address the biased relationship predictions. Based on this "divide-conquer-cooperate" intention, we propose the Group Collaborative Learning (GCL) strategy. As shown in <ref type="figure">Figure 4</ref>, GCL contains five key steps as follows:</p><p>Predicate Class Grouping aims to split the unbalanced dataset into several relatively balanced groups, and then configure the classification space for all the classifiers. Based on the observation that the recognition capability would suffer from the biased data distribution, we aim to provide each classifier with a relatively balanced training set, thus it could adequately learn the discriminating representations towards a subset of predicates. Therefore, We first sort the predicate classes by their amount of training instances in descending order, obtaining a sorted set P all = {p i } M i=1 . We then divide P all into K mutually exclusive groups {P k } K k=1 according to the pre-defined threshold ?. The workflow is summarized in Algorithm 1, where Count(p i ) denotes the amount of training instances towards the predicate p i . Line 3 in Algorithm 1 ensures that, for each group P k , the maximal amount of training instances will be no more than ? times of the minimal amount, thus the predicates in P k share a relatively equal amount.</p><p>Algorithm 1: Predicate Class Grouping.</p><formula xml:id="formula_7">Input: A sorted predicate set P all = {p i } M i=1 , ? Output: K mutually exclusive groups {P k } K k=1 1 Set cur = 1, k = 1, and P 1 = {}; 2 for i ? 1 to M do 3 if Count(p cur ) &gt; ? * Count(p i ) then 4 cur = i; 5 k = k + 1; 6 Set P k = {}; 7 end 8 P k = P k ? {p i } 9 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collaborative Knowledge Distillation Class Probability Prediction Balanced Sample Preparation Predicate Class Grouping Parallel Classifier Optimization</head><p>Logit-1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Ground-truth Labels CE Loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE Loss</head><p>Logit-2 Logit-5 ? ? ? KL Loss ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? KL Loss ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? KL Loss ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Logit-1 Logit-  <ref type="figure">Figure 4</ref>. Illustration of the proposed Group Collaborative Learning (GCL) strategy, which includes five key steps. It is worth noting that we design two optimization mechanisms, namely Parallel Classifier Optimization (PCO) and Collaborative Knowledge Distillation (CKD), to jointly guide the training of the relation decoder.</p><p>We then borrow the idea from the class-incremental learning <ref type="bibr" target="#b13">[14]</ref>, and deploy a set of classifiers {C k } K k=1 which follow a continuously growing classification space. Except for the first classifier C 1 , other classifiers should recognize the predicate classes from both previous and current groups, i.e., the classification space in C k is P ? k = P 1 ?P 2 ?? ? ??P k . Note that we only choose the last classifier C K to obtain the final predicate predictions in the evaluation stage.</p><p>Balanced Sample Preparation aims to achieve several balanced training sets provided for further joint optimization by re-sampling the instances. For each classifier C k that incorporates a newly-added group P k to extend the previous classification space P ? k?1 as P ? k = P k ? P ? k?1 , we expect it could adequately learn the discriminating representations towards the predicates, particularly within the newly-added group P k . Therefore, for the predicates in the group P k , we should retain all of its training instances to facilitate the convergence. And for the predicates in the previous classification space P ? k?1 , since they have more samples in the original dataset, we should under-sample their training instances to avoid biased predictions.</p><p>To implement this intention, we propose the Median Re-Sampling strategy to perform the re-sampling operation. For each classification space P ? k , we first calculate the median amount M ed(P ? k ) over all the classes within P ? k . For example, if P ? k is sorted in descending order and contains 9 predicate classes, the median amount M ed(P ? k ) is equal to Count(p 5 ). Then for each predicate class p k i in P ? k , we calculate the sampling rate ? k i as follows:</p><formula xml:id="formula_8">? k i = ? ? ? M ed(P ? k ) Count(p i ) , if M ed(P ? k ) &lt; Count(p i ), 1.0 , if M ed(P ? k ) ? Count(p i ).<label>(6)</label></formula><p>By employing the above strategy, each classifier would be expert in distinguishing the predicates, particularly in the newly-added group. For example, since we would undersample the instances in Group 3 for training the 4 th and 5 th classifiers, the 3 rd classifier is more likely to achieve a better performance in distinguishing the predicates in Group 3, as we retain all the samples of this group to let the 3 rd classifier adequately learn the discriminating representations.</p><p>Class Probability Prediction aims to parse the sampled instances into the class probability logits for further loss computation and model optimization. For an object pair (o i , o j ) chosen by the Median Re-Sampling strategy, after obtaining the subject feature x ? i , the object feature x ? j , and their union feature u ij , the class probability prediction w k ij generated by the classifier C k is calculated as follows:</p><formula xml:id="formula_9">w k ij = Softmax(F C([x ? i , x ? j ]) ? u ij ),<label>(7)</label></formula><p>where ? denotes the element-wise product. Parallel Classifier Optimization aims to regularize the final classifier C K by jointly optimizing all the classifiers. In the training stage, the parameters of all the K predicate classifiers would be optimized simultaneously, where the objective function can be defined as:</p><formula xml:id="formula_10">L P CO = K k=1 1 |D k | (oi,oj )?D k L CE (y ij , w k ij ),<label>(8)</label></formula><p>where D k denotes the set of the object pairs chosen by the Median Re-Sampling strategy, | ? | denotes the length of the given set, y ij denotes the ground-truth predicate label of the object pair (o i , o j ), and L CE (?) is a regular Cross-Entropy cost function. The Parallel Classifier Optimization can be seen as a "weak constraint" for Group Collaborative Learning, since we expect that gathering gradients from all the classifiers would facilitate the convergence of the final classifier C K .</p><p>Collaborative Knowledge Distillation aims to establish a knowledge transfer mechanism to promote the unbiased prediction capability of the final classifier C K . As aforementioned, each classifier specializes in distinguishing the predicates, particularly within the newly-added group. In order to preserve and translate this well-learned knowledge to compensate for the under-fitting on the head classes, we propose the Collaborative Knowledge Distillation (CKD), whose objective function is defined as:</p><formula xml:id="formula_11">L CKD = 1 |Q| (m,n)?Q 1 |D n | (oi,oj )?Dn L KL (w m ij , w n ij ),<label>(9)</label></formula><p>where Q denotes the set of pairwise knowledge matching from the classifier C m to the classifier C n (m &lt; n). We provide two alternatives, namely Adjacent and Top-Down strategy, to configure the set Q (these two strategies are illustrated in <ref type="figure">Figure 6</ref> and Parameter Analysis). Note that the output w n ij generated by the classifier C n incorporates new predicate classes which are not included in the previous classification space P ? m , we utilize w n ij to indicate the sliced output by cutting off the incrementally-added classes which are not included in P ? m , thus ensuring that w n ij shares the same dimension as w m ij . L KL (?) is a regular Kullback-Leibler Divergence loss, which is defined as:</p><formula xml:id="formula_12">L KL (w m , w n ) = ? L l=1 w l m log w l n .<label>(10)</label></formula><p>By taking the previous predicate probability output w m ij from the classifier C m as the soft label, CKD forces the current classifier C n to mimic the prediction behaviour that C m is expert in, thus can be treated as a "strong constraint". Ultimately, the objective function of our proposed Group Collaborative Learning (GCL) is the combination of PCO and CKD, which is defined as:</p><formula xml:id="formula_13">L GCL = L P CO + ?L CKD ,<label>(11)</label></formula><p>where ? is the pre-defined hyper-parameters to weigh the total loss L GCL . By employing these two types of constraint, we effectively mitigate the overwhelming punishments to the tail classes and compensate for the underfitting on the head ones, which benefits in establishing a reasonable trade-off during the predicate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset. We present the experimental results on two datasets: Visual Genome (VG) <ref type="bibr" target="#b15">[16]</ref> and GQA <ref type="bibr" target="#b14">[15]</ref>. VG is the most widely-used benchmark for SGG, which is composed of more than 108K images and 2.3M relation instances. Following the prior approaches <ref type="bibr">[3, 4, 17, 20, 29-31, 40, 42, 43, 47, 49]</ref>, we adopt the most widely-used VG150 split, which contains the most frequent 150 object classes and 50 predicate classes. GQA is another visionand-language benchmark with more than 3.8M relation annotations. In order to achieve a representative split like VG150, we manually clean up a substantial fraction of annotations that have poor-quality or ambiguous meanings, and then select Top-200 object classes as well as Top-100 predicate classes by their frequency, thus establishing the GQA200 split. For both VG150 and GQA200, we use 70% of the images for training and the remaining 30% for testing. We also follow <ref type="bibr" target="#b46">[47]</ref> to sample a 5K validation set from the training set for parameter tuning.</p><p>Tasks. To comprehensively evaluate the performance, we follow three conventional tasks: 1) Predicate Classification (PredCls) predicts the relationships of all the pairwise objects by employing the given ground-truth bounding boxes and classes; 2) Scene Graph Classification (SGCls) predicts the objects classes and their pairwise relationships by employing the given ground-truth object bounding boxes; and 3) Scene Graph Detection (SGDet) detects all the objects in an image, and predicts their bounding boxes, classes and pairwise relationships. Evaluation Metrics. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, we use mean Recall@K (mR@K) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>, which computes the average Recall@K (R@K) for each predicate class, to evaluate the unbiased SGG. As R@K is easily dominated by the head classes due to the extremely unbiased dataset, mR@K could give a fair performance appraisal for both head and tail classes, which is widely used as an unbiased evaluation metric. Implementation Details. We adopt a pre-trained Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b38">[39]</ref> provided by <ref type="bibr" target="#b29">[30]</ref> as the object detector. We employ Glove <ref type="bibr" target="#b23">[24]</ref> to obtain the semantic embedding. The object encoder and the relation encoder contain four and two SHA layers, respectively. We set the division threshold ? = 4, and employ the Top-Down strategy (each classifier is forced to learn the prediction behavior from all its predecessors, see <ref type="figure">Figure 6</ref> for more details) to construct the pairwise knowledge matching set Q. The hyper-parameter ? which balances the optimization objective is set to be 1.0. We optimize the proposed network by the Adam optimizer with a momentum of 0.9. For all three tasks, the total training stage lasts for 60,000 steps with a batch size of 8. The initial learning rate is 0.001, and we adopt the same warm-up and decayed strategy as <ref type="bibr" target="#b29">[30]</ref>. One RTX2080 Ti is used to conduct all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compared Methods</head><p>We want to declare that our proposed method is not only powerful in generating unbiased scene graphs, but also applicable for a variety of SGG approaches. For the former, we compare it with state-of-the-art approaches, including re-produced IMP+ <ref type="bibr" target="#b39">[40]</ref>, KERN <ref type="bibr" target="#b2">[3]</ref>, GPS-Net <ref type="bibr" target="#b19">[20]</ref>, PCPL <ref type="bibr" target="#b41">[42]</ref>, re-produced VTransE+ <ref type="bibr" target="#b48">[49]</ref> and BGNN <ref type="bibr" target="#b16">[17]</ref>. For the latter, we adopt two typical baselines, namely Motifs <ref type="bibr" target="#b46">[47]</ref> and VCTree <ref type="bibr" target="#b30">[31]</ref>, to give a fair comparison with other model-agnostic approaches, such as Reweighting <ref type="bibr" target="#b3">[4]</ref>, TDE <ref type="bibr" target="#b29">[30]</ref>, CogTree <ref type="bibr" target="#b42">[43]</ref>, DLFE <ref type="bibr" target="#b3">[4]</ref> and EBM <ref type="bibr" target="#b28">[29]</ref>. <ref type="table" target="#tab_3">Table 1</ref> and <ref type="table">Table 2</ref> present the performance of different approaches conducted on VG150 and GQA200, respectively. We have several observations as follows: 1) Our proposed SHA+GCL significantly outperforms all the baselines on all three tasks. To the best of our knowledge, our work is the first to breakthrough the 40% precision in both mR@50 and mR@100 on PredCls, and we also achieve the Model PredCls SGCls SGDet mR@20 mR@50 mR@100 mR@20 mR@50 mR@100 mR@20 mR@50 mR@100 IMP+ ? -9.8 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>As aforementioned, we propose the Stacked Hybrid Attention (SHA) network to improve the object encoder and the relation encoder, and propose the Group Collaborative Learning (GCL) strategy, which employs the Parallel Classifier Optimization (PCO) as the "weak constraint" and Collaborative Knowledge Distillation (CKD) as the "strong constraint", to guide the training of the decoder. In order to prove the effectiveness of the above components, we test various ablation models on VG150 as follows:</p><p>? w/o-GCL: To evaluate the effectiveness of GCL, we let the relation decoder be a one-layer classifier, where a regular Cross-Entropy loss is performed. ? w/o PCO&amp;CKD: To evaluate the effectiveness of PCO in GCL, we remove the PCO loss and CKD loss, and only employ the Median Re-Sampling strategy and a regular Cross-Entropy loss in the optimization step. ? w/o CKD: To evaluate the effectiveness of CKD in GCL, we remove the CKD loss but retain all the classifiers to compute the PCO loss. ? w/o CA or w/o SA: To evaluate the effectiveness of SHA, we remove either the Cross-Attention (CA) unit or the Self-Attention (SA) unit in every SHA layer. <ref type="table">Table 3</ref> presents the results of all the ablation models. We have several observations as follows: 1) Compared with w/o-GCL, SHA+GCL nearly doubles the per-  <ref type="table">Table 3</ref>. Ablation study of the proposed method on VG150.  formance. Moreover, in <ref type="figure" target="#fig_3">Figure 5a</ref>, we compare w/o-GCL and SHA+GCL with respect to R@100 of all the predicate classes. As can be observed, SHA+GCL obviously improves the performance on most of the predicate classes, only with an acceptable decay on the head classes in Group 1 and Group 2, showing a powerful capability in generating unbiased scene graphs. 2) Compared with w/o-PCO&amp;CKD, w/o-CKD evidently improves the prediction performance, demonstrating that the "weak constraint", namely gathering gradients from all the classifiers, would facilitate the convergence of the final classifier. 3) Compared with w/o-CKD, we witness an obvious performance gain in SHA+GCL. Moreover, we compare w/o-CKD and SHA+GCL on the detailed precision towards every predicate class on VG150. As shown in <ref type="figure" target="#fig_3">Figure 5b</ref>, CKD effectively prevents the model from sacrificing much on the head classes, as well as achieves a comparable performance towards the tail predictions. It demonstrates that the "strong constraint", namely a knowledge transfer paradigm, could effectively compensate for the under-fitting on the head classes by preserving the discriminating capability learned previously, and thus benefits in achieving a reasonable trade-off. 4) From the last three rows in <ref type="table">Table 3</ref>, we witness an obvious performance decay when removing either the CA unit or the SA unit. It verifies that combining both attentions would effectively alleviate the insufficient modality fusion, thus leading to more accurate predictions.  <ref type="table">Table 4</ref>. Parameter analysis towards the threshold ? and the pairwise knowledge matching strategies of GCL on VG150. Top-Down Strategy</p><formula xml:id="formula_14">1 2 3 4</formula><p>Pairwise Knowledge Matching Strategy <ref type="figure">Figure 6</ref>. Illustration of three configurations of the balanced group divisions according to the threshold ? (top), and two alternatives of the pairwise knowledge matching strategy (down).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Parameter Analysis</head><p>As aforementioned, the threshold ? and the organization strategy would influence the performance of GCL. As <ref type="figure">Figure 6</ref> illustrates, for the former, we set ? =3, 4, and 5, and obtain 6, 5, and 4 group divisions, respectively. For the latter, we provide two alternatives, namely Adjacent and Top-Down strategy, whose difference is whether each classifier could learn the knowledge from its nearest predecessor (Adjacent) or from all the predecessors (Top-Down). <ref type="table">Table 4</ref> presents the performance comparisons, where ? = 4 and the Top-Down strategy is the best combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we declare two concerns that restrict the practical applications of SGG, namely insufficient modality fusion and biased relationship predictions. To address such deficiency, we propose the Stacked Hybrid-Attention network and the Group Collaborative Learning strategy. In this way, we establish a new state-of-the-art in the unbiased metric and provide a model-agnostic debiasing method. In the future, we plan to explore more robust group dividing methods and devise more knowledge distillation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this supplementary material, we present more analyses, experiments, and visualization results, as well as discuss the limitations and future work of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Parameter Statistics</head><p>We compare the total number of parameters between three baseline methods (i.e., Motifs, VCTree, and SHA) and their enhanced versions that are equipped with our modelagnostic GCL in <ref type="table">Table 5</ref>. As can be observed, compared with the original methods which possess a massive number of total trainable parameters (about 200M), GCL only additionally introduces a limited number of parameters (about 2M), which could hardly influence the overall training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detailed Performance</head><p>We present the complete results of our experiments employing the regular Recall@K <ref type="bibr" target="#b20">[21]</ref>, the unbiased Mean Recall@K <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>, and their mean <ref type="bibr" target="#b19">[20]</ref> on all three tasks (i.e., PredCls, SGCls, and SGDet) on VG150 <ref type="bibr" target="#b15">[16]</ref> and GQA200 <ref type="bibr" target="#b14">[15]</ref> dataset in <ref type="table">Table 6</ref>, where K ? {50, 100}. Note that all the methods are implemented with a pretrained Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b38">[39]</ref> provided by <ref type="bibr" target="#b29">[30]</ref> as the object detector, thus we could give a fair comparison to prove the superiority of our method.</p><p>From <ref type="table">Table 6</ref>, we observe that 1) our proposed SHA+GCL achieves the best performance on all three tasks towards the unbiased metric mR@K in both two datasets. In VG150, we breakthrough the 40% precision in both mR@50 and mR@100 on PredCls, and 20% precision in mR@100 on both SGCls and SGDet, thus establishing a new state-of-the-art in the unbiased metric. 2) Our improvement towards the relation decoder, namely GCL strategy, is model-agnostic and could largely enhance the unbiased SGG. In both VG150 or GQA200, the method equipped with GCL nearly doubles the performance compared with the original one, showing the outstanding capability in generating unbiased scene graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visualization Results</head><p>To get an intuitive perception of the superior performance in generating unbiased scene graphs of our proposed GCL, we visualize several PredCls examples generated from the biased SHA and the unbiased SHA+GCL. As shown in <ref type="figure">Figure 8</ref>, the model employing the proposed GCL strategy prefers to providing more informative and specific relationship predictions (e.g., lying on and riding) rather than common and trivial ones (e.g., on and has), e.g., "person1-riding-elephant" in the top-right example and "train-pulling-car" in the bottom-left example. Moreover,  <ref type="figure">Figure 7</ref>. The group-incremental configuration (left) may not be the only alternative to fulfill the "conquer" step in GCL. For example, the group-split configuration (right) is another promising strategy. Therefore, we aim to explore more robust group dividing methods and classifier configuration strategies in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group-Incremental Configuration</head><p>the model equipped with our model-agnostic GCL could also capture potential reasonable relationships, such as "person1-watching-person2" in the top-right example and "sidewalk-beside-train" in the bottom-left example. In a nutshell, the proposed GCL could enhance the unbiased relationship predictions, thus achieving more informative scene graphs to support various down-stream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>In this section, we would like to discuss the limitations of our method, based on which we provide several potential directions to further improve our SHA+GCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">More Configurations Could be Further Explored</head><p>As aforementioned, we follow the intuition of "divideconquer-cooperate" to address the biased relationship predictions. In the second step, namely "conquer", we borrow the idea from class-incremental learning <ref type="bibr" target="#b13">[14]</ref> and employ the group-incremental configuration. Actually, we employ this configuration mainly due to its simplicity and efficiency, as we could directly leverage the final classifier that covers all the candidate classes to obtain the predictions in the evaluation stage. However, we should argue that it is not the only alternative to fulfill the "conquer" step. Therefore, in the future, we aim to explore more robust group dividing methods as well as classifier configuration strategies to promote the unbiased SGG, e.g., the groupsplit configuration in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">"Strong Constraint" Could be Further Enhanced</head><p>As aforementioned, in the "cooperate" step, we use the collaborative knowledge distillation to establish an effective knowledge transfer mechanism, where a regular Kullback-Leibler Divergence loss is employed. However, since various novel methods have been proposed in the knowledge distillation area, we could further enhance our GCL by devising more efficient strategies, thus strengthening the "Strong Constraint" and promoting the unbiased SGG.  <ref type="table">Table 5</ref>. Comparison of different methods on the number of parameters. "Fixed" counts the number of parameters that belong to the pre-trained object detector, and "Trainable" counts the number of parameters that can be updated during the training procedure.  <ref type="figure">Figure 8</ref>. Qualitative comparisons between SHA and SHA+GCL with regard to R@20 on PredCls setting. Green edges represent the ground truth relationships that are correctly predicted, red edges represent the ground truth relationships that are failed to be detected, and purple edges represent the reasonable relationships which are predicted by the model but are not annotated in the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>R@100 of all the predicate classes of w/o-GCL and SHA+GCL on VG150. R@100 of all the predicate classes of w/o-CKD and SHA+GCL on VG150.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>R@100 of 50 predicate classes on PredCls on VG150.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>mR@50/100 R@50/100 mR@50/100 R@50/100 mR@50/100 R-M mR-M IMP ?<ref type="bibr" target="#b28">[29]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of different methods on PredCls, SGCls, and SGDet tasks of VG150 with respect to mR@20/50/100 (%). The superscript ? denotes that the method employs Faster R-CNN with VGG-16 as the object detector, while the subscript d denotes that the method is model-agnostic and targets to address the biased relationship predictions in SGG.</figDesc><table><row><cell>5</cell><cell>-</cell><cell>5.8</cell><cell>6.0</cell><cell>-</cell><cell>3.8</cell><cell>4.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work is supported by the National Natural Science Foundation of China, No.: 62176137, No.:U1936203, and No.: 62006140; the Shandong Provincial Natural Science and Foundation, No.: ZR2020QF106; Beijing Academy of Artificial Intelligence(BAAI); Ant Group.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, and their mean (%). R-M and mR-M denote the mean on all three tasks over R@50/100 and mR@50/100, respectively. The optimal results from the same baseline (i.e., VTransE, Motifs and VCTree) in VG150 are underlined. The global optimal results over all the methods in VG150 and GQA200 are in bold. The superscript ? denotes that the method is reproduced. Note that all the methods are implemented on the same object detector, i.e., a pre-trained Faster R-CNN with ResNeXt-101-FPN.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Few shot network compression via cross distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoli</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3203" to="3210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Say as you wish: Fine-grained control of image caption generation with abstract scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9962" to="9971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Recovering the unbiased scene graphs from the biased ones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Jiun</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02112</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3076" to="3086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image manipulation using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azade</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5213" to="5222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to augment for data-scarce domain bert knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Explaining sequencelevel knowledge distillation as data-augmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03334</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unpaired image captioning via scene graph alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10323" to="10332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual relations augmented cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scene graph reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Koner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01072</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bipartite graph network with adaptive message passing for unbiased scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual relationship detection with deep structural ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kongming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural language guided visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="444" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gps-net: Graph property sensing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinquan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5191" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Classification by attention: Scene graph classification with prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><forename type="middle">Moayed</forename><surname>Baharlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10084</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kwang-Ting Cheng, and Marios Savvides. Is label smoothing truly incompatible with knowledge distillation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00676</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial-temporal graphs for cross-modal text2video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Energy-based learning for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behjat</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayan</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">and Anton van Den Hengel. Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Private model compression via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1190" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural multimodal cooperative learning toward micro-video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unbiased scene graph generation via rich and fair semantic extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00176</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Co-attention for conditioned image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15920" to="15929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of scene graph: Generation and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pcpl: Predicate-correlation perception learning for unbiased scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaotian</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cogtree: Cognition tree loss for unbiased scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07526</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bridging knowledge graphs to generate scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="606" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An empirical study on leveraging scene graphs for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12133</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5532" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep co-attention network for multi-view subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1528" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comprehensive image captioning via scene graph decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="211" to="229" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
