<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Tsetlin Machine -A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Granmo</surname></persName>
						</author>
						<title level="a" type="main">The Tsetlin Machine -A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bandit Problem</term>
					<term>Game Theory</term>
					<term>Interpretable Pattern Recognition</term>
					<term>Propo- sitional Logic</term>
					<term>Tsetlin Automata Games</term>
					<term>Online Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks. Unknown to many, there exists an arguably even simpler and more versatile learning mechanism, namely, the Tsetlin Automaton. Merely by means of a single integer as memory, it learns the optimal action in stochastic environments through increment and decrement operations. In this paper, we introduce the Tsetlin Machine, which solves complex pattern recognition problems with easy-to-interpret propositional formulas, composed by a collective of Tsetlin Automata. To eliminate the longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our theoretical analysis establishes that the Nash equilibria of the game align with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones. We argue that the Tsetlin Machine finds the propositional formula that provides optimal accuracy, with probability arbitrarily close to unity. In five benchmarks, the Tsetlin Machine provides competitive accuracy compared with SVMs, Decision Trees, Random Forests, Naive Bayes Classifier, Logistic Regression, and Neural Networks. The Tsetlin Machine further has an inherent computational advantage since both inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on bit manipulation. The combination of accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains. Being the first of its kind, we believe the Tsetlin Machine will kick-start new paths of research, with a potentially significant impact on the AI field and the applications of AI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks <ref type="bibr" target="#b0">[1]</ref>. Highly successful, deep neural networks often require huge amounts of training data and extensive computational resources. Unknown to many, there exists an arguably even more fundamental and versatile learning mechanism than the artificial neuron, namely, the Tsetlin Automaton, developed by M.L. Tsetlin in the Soviet Union in the early 1960s <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this paper, we address a long-standing challenge in the field of Finite State Learning Automata <ref type="bibr" target="#b2">[3]</ref>, referred to as the vanishing signal-to-noise ratio problem. This problem has hindered successful use of Tsetlin Automata in large-scale and complex pattern recognition, constraining such solutions to small-scale pattern recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Tsetlin Automaton</head><p>Tsetlin Automata have been used to model biological systems, and have attracted considerable interest because they can learn the optimal action when operating in unknown stochastic environments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Furthermore, they combine rapid and accurate convergence with low computational complexity.</p><p>In all brevity, the Tsetlin Automaton is one of the pioneering solutions to the well-known multi-armed bandit problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. It performs actions sequentially in an environment, and each action triggers either a reward or a penalty. Assuming two bantit arms, action ? r , r ? {1, 2}, is rewarded with probability p r , otherwise it is penalized (i.e., with probability 1 ? p r ). The reward probabilities are unknown to the automaton and may even change over time. Under such challenging conditions, the goal is to identify the action with the highest reward probability using as few attempts as possible. The mechanism driving the Tsetlin Automaton is surprisingly simple. Informally, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, a Tsetlin Automaton is simply a fixed finite-state automaton <ref type="bibr" target="#b5">[6]</ref> with an unusual interpretation:</p><p>? The current state of the automaton decides which action to perform. The automaton in the figure has 2N states. Action 1 is performed in the states with index 1 to N , while Action 2 is performed in states with index N + 1 to 2N .</p><p>? The state transitions of the automaton govern learning. One set of state transitions is activated on reward (solid lines), and one set of state transitions is activated on penalty (dotted lines). As seen, rewards and penalties trigger specific transitions from one state to another, designed to reinforce successful actions (those eliciting rewards).</p><p>Formally, a Two-Action Tsetlin Automaton can be defined as a quintuple <ref type="bibr" target="#b2">[3]</ref>:</p><p>{?, ?, ?, F (?, ?), G(?)}. ? = {? 1 , ? 2 , . . . , ? 2N } is the set of internal states. ? = {? 1 , ? 2 } is the set of automaton actions. ? = {? Penalty , ? Reward } is the set of inputs that can be given to the automaton. An output function, G(? u ), determines the next action performed by the automaton, given the current automaton state ? u :</p><formula xml:id="formula_0">G(? u ) = ? 1 , if 1 ? u ? N ? 2 , if N + 1 ? u ? 2N.</formula><p>Finally, a transition function, F (? u , ? v ), determines the new automaton state from: (1) the current automaton state, ? u , and (2) the response, ? v , of the environment to the action performed by the automaton:</p><formula xml:id="formula_1">F (? u , ? v ) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? u+1 , if 1 ? u ? N and v = Penalty ? u?1 , if N + 1 ? u ? 2N and v = Penalty ? u?1 , if 1 &lt; u ? N and v = Reward ? u+1 , if N + 1 ? u &lt; 2N and v = Reward ? u , otherwise.</formula><p>Implementation-wise, a Tsetlin Automaton simply maintains an integer (the state index), and learning is performed through increment and decrement operations, according to the transitions specified by F (? u , ? v ) (and depicted in <ref type="figure" target="#fig_0">Figure 1</ref>). The Tsetlin Automaton is thus extremely simple computationally, with a very small memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">State-of-the-art in the Field of Finite State Learning Automata</head><p>The simple Tsetlin Automaton approach has formed the core for more advanced finite state learning automata designs that solve a wide range of problems. This includes resource allocation <ref type="bibr" target="#b6">[7]</ref>, decentralized control <ref type="bibr" target="#b7">[8]</ref>, knapsack problems <ref type="bibr" target="#b8">[9]</ref>, searching on the line <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, meta-learning <ref type="bibr" target="#b11">[12]</ref>, the satisfiability problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, graph colouring <ref type="bibr" target="#b13">[14]</ref>, preference learning <ref type="bibr" target="#b14">[15]</ref>, frequent itemset mining <ref type="bibr" target="#b15">[16]</ref>, adaptive sampling <ref type="bibr" target="#b16">[17]</ref>, spatio-temporal event detection <ref type="bibr" target="#b17">[18]</ref>, equi-partitioning <ref type="bibr" target="#b18">[19]</ref>, streaming sampling for social activity networks <ref type="bibr" target="#b19">[20]</ref>, routing bandwidthguaranteed paths <ref type="bibr" target="#b20">[21]</ref>, faulty dichotomous search <ref type="bibr" target="#b21">[22]</ref>, learning in deceptive environments <ref type="bibr" target="#b22">[23]</ref>, as well as routing in telecommunication networks <ref type="bibr" target="#b23">[24]</ref>. The unique strength of all of these finite state learning automata solutions is that they provide state-of-the-art performance when problem properties are unknown and stochastic, while the problem must be solved as quickly as possible through trial and error.</p><p>Note that there exists another family of learning automata, referred to as variable structure learning automata (the interested reader is referred to <ref type="bibr" target="#b24">[25]</ref>). Although still simple, these are significantly more complex than the Tsetlin Automaton because they need to maintain an action probability vector for sampling actions. Their success in pattern recognition has been limited to small scale problems, again being restricted by constrained pattern representation capability (linearly separable classes and simple decision trees) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b2">3]</ref>. of machine learning. Decision tree learning <ref type="bibr" target="#b26">[27]</ref>, for instance, is one of the most common approaches to interpretable pattern recognition. In all brevity, learning of decision trees is based on greedy growing of decision rules, organized as a tree, by extending the leafs of the tree, one input variable at a time. Recently, however, it has turned out that taking a global perspective on the production of decision rules has advantages over greedy local strategies. Heuristic approaches, such as alternating minimization, Block Coordinated Monte Carlo, and associative rule mining with randomized search, are used to learn rule sets, jointly optimizing rule sparsity and classification accuracy <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. These techniques typically require offline batch based learning and are mainly addressing smaller scale pattern recognition problems.</p><p>In the above perspective, we here propose a completely new approach to global optimization of decision rule sets, founded on the Tsetlin Automaton and the bandit problem. We demonstrate further that decision rules can be learned on-line, under particularly noisy conditions. To establish a common reference point towards related work in interpretable pattern recognition, we include results on decision trees in our experiments on MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Paper Contributions</head><p>In this paper, we attack the limited pattern expression capability and vanishing signal-to-noise ratio of learning automata based pattern recognition, introducing the Tsetlin Machine. The contributions of the paper can be summarized as follows:</p><p>? We introduce the Tsetlin Machine, which solves complex pattern recognition problems with propositional formulas, composed by a collective of Tsetlin Automata.</p><p>? We eliminate the longstanding vanishing signal-to-noise ratio problem with a unique decentralized learning scheme based on game theory <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b1">2]</ref>. The game we have designed allows thousands of Tsetlin Automata to successfully cooperate.</p><p>? The game orchestrated by the Tsetlin Machine is based on resource allocation principles <ref type="bibr" target="#b30">[31]</ref>, in inter-play with frequent itemset mining <ref type="bibr" target="#b15">[16]</ref>. By allocating sparse pattern representation resources, according to the frequency of the patterns, the Tsetlin Machine is able to capture intricate unlabelled sub-patterns, for instance addressing the so-called Noisy XOR-problem.</p><p>? Our theoretical analysis establishes that the Nash equilibria of the game are aligned with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones.</p><p>? We further argue that the Tsetlin Machine finds a propositional formula that provides optimal pattern recognition accuracy, with probability arbitrarily close to unity. 0 0 * 1 * 0 0 0 * 0 * 1 * 0 0 0 0 * * 1 * * * 0 0 * * * * 0 0 * 0 0 0 * * 0 0 0 0 * 0 * * * 0 0 0 0 * 1 * * * 0 0 0 0 * 1 * * * <ref type="table">Table 1</ref>: A bit pattern produced by the Tsetlin Machine for handwritten digits '1'. The '*' symbol can either take the value '0' or '1'. The remaining bit values require strict matching. The pattern is relatively easy to interpret for humans compared to, e.g., a neural network. It is also efficient to evaluate for computers. Despite this simplicity, the Tsetlin Machine produces bit patterns that deliver state-of-the-art pattern recognition accuracy for several datasets, demonstrated in Section 5.</p><p>? The propositional formulas are represented as bit patterns. These bit patterns are relatively easy to interpret, compared to e.g. a neural network (see <ref type="table">Table 1</ref> for an example bit pattern). This facilitates human quality assurance and scrutiny, which for instance can be important in safety-critical domains such as medicine.</p><p>? The Tsetlin Machine is particularly suited for digital computers, being directly based on bit manipulation with AND-, OR-, and NOT operators. Both input, hidden patterns, and output are represented directly as bits, while recognition and learning rely on manipulating those bits.</p><p>? In our empirical evaluation on five datasets, the Tsetlin Machine provides competitive performance in comparison with Multilayer Perceptron Networks, Support Vector Machines, Decision Trees, Random Forests, the Naive Bayes Classifier, and Logistic Regression.</p><p>? In our experiments, it further turns out that the Tsetlin Machine requires less data than neural networks, outperforming even the Naive Bayes Classifier in data sparse environments.</p><p>? Overfitting is inherently combated by leveraging frequent itemset mining <ref type="bibr" target="#b15">[16]</ref>. Even while accuracy on the training data approaches 99.9%, mean accuracy on the test data continues to increase as well. This is quite different from the behaviour of back-propagation in neural networks, where accuracy on test data starts to drop at some point, without proper regularization mechanisms.</p><p>? We demonstrate how the Tsetlin Machine can be used as a building block to create more advanced architectures.</p><p>We believe that the combination of accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains. Being the first of its kind, we further believe it will kick-start completely new paths of research, with a potentially significant impact on the field of AI and the applications of AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Paper Organization</head><p>The paper is organized as follows. In Section 2, we define the exact nature of the pattern recognition problem we are going to solve, also introducing the crucial concept of sub-patterns. Then, in Section 3, we cover the Tsetlin Machine in detail. We first present the propositional logic based pattern representation framework, before we introduce the Tsetlin Automata teams that write conjunctive clauses in propositional logic. These Tsetlin Automata teams are in turn organized to recognize complex patterns. We conclude the section by presenting the Tsetlin Machine game that we use to coordinate thousands of Tsetlin Automata, eliminating the vanishing signal-to-noise ratio problem.</p><p>In Section 4, we analyze pertinent properties of the Tsetlin Machine formally, and establish that the Nash equilibria of the game are aligned with the propositional formulas that solve the pattern recognition problem at hand. This allows the Tsetlin Machine as a whole to robustly and accurately uncover complex patterns with propositional logic.</p><p>In our empirical evaluation in Section 5, we evaluate the performance of the Tsetlin Machine on five datasets: Flower categorization, digit recognition, board game planning, the Noisy XOR Problem with Non-informative Features, as well as the MNIST dataset.</p><p>The Tsetlin Machine has been designed to act as a building block in more advanced architectures, and in Section 6 we demonstrate how four distinct architectures can be built by interconnecting multiple Tsetlin Machines.</p><p>As the first step in a new research direction, the Tsetlin Machine also opens up a range of new research questions. In Section 7, we summarize our main findings and provide pointers to some of the open research problems ahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Pattern Recognition Problem</head><p>We here define the pattern recognition problem to be solved by the Tsetlin Machine, starting with the input to the system. The input to the Tsetlin Machine is an observation vector of o propositional variables, X = [x 1 , x 2 , . . . , x o ], with x k ? {0, 1}, 1 ? k ? o. From this input vector, the Tsetlin Machine is to produce an output vector Y = [y 1 , y 2 , . . . , y n ] of n propositional variables, y i ? {0, 1}, 1 ? i ? n. <ref type="bibr" target="#b0">1</ref> Given an input X = [x 1 , x 2 , . . . , x o ], we assume that an independent underlying stochastic process of arbitrary complexity randomly produces either y i = 0 or y i = 1, for each output, y i , 1 ? i ? n. To deal with the stochastic nature of these processes, we take a probabilistic approach. In all brevity, all uncertainty is completely captured by the probability P (y i = 1|X), that is, the probability that y i takes the value 1 given the input X. Being binary, the probability that y i takes the value 0 follows: P (y i = 0|X) = 1.0 ? P (y i = 1|X). Under these conditions, the optimal decision is to assign y i the value, v ? {0, 1}, with the largest posterior probability <ref type="bibr" target="#b31">[32]</ref>: y i = argmax v?{0,1} P (y i = v|X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(y i = 0 | X ) &gt; P(y i = 1 | X )</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subpattern X i</head><p>Subpattern X i Subpattern X i Subpattern X i P(y i = 0 | X ) &lt;= P(y i = 1 | X )  <ref type="figure">Figure 2</ref>: A partitioning of the input space according to the posterior probability of the output variable y i , highlighting distinct sub-patterns in the input space, X ? X . Sub-patterns most likely belonging to output y i = 1 can be found on the right side, while sub-patterns most likely belonging to y i = 0 on the left. Now consider the complete set of possible inputs, X = {x 1 , . . . , x o ? [0, 1] o }. Each input X occurs with probability P (X), and thus the joint input-output distribution becomes P (X, y i ) = P (y i |X)P (X).</p><p>As illustrated in <ref type="figure">Figure 2</ref>, the input space X can be partitioned in two parts, X i = {X|P (y i = 0|X) ? P (y i = 1|X)} and X i = {X|P (y i = 0|X) &gt; P (y i = 1|X)}. For observations in X i it is optimal to assign y i the value 1, while for partition X i it is optimal to assign y i the value 0. We now come to the crucial concept of unlabelled sub-patterns. As illustrated in the figure, X i sub-divides further into q sub-parts, forming distinct sub-patterns, X i u , u = 1, . . . , q (the same goes for X i ). Together, these sub-patterns span the whole input space, apart from a minimal level of outlier patterns that occur with probability, ?, close to zero. That is,</p><formula xml:id="formula_2">P (X i 1 ? . . . ? X i q ? X i 1 ? . . . ? X i q ) = 1.0 ? ?,</formula><p>for a minimal ? (i.e., the outliers remaining after a sub-division of the input space into q parts).</p><p>Note that we do not have direct access to the above sub-patterns during pattern learning and recognition. Rather, we only observe samples (X,? i ) from the joint input-output distribution P (X, y i ). Which sub-patternX is sampled from is unavailable to us. However, what we know, by definition, is that each sub-pattern X u occurs with probability P (X i u ) &gt; 1 s . The challenging task we are going to solve is to learn all the sub-patterns merely by observing a limited sample from the joint input-output probability distribution P (X, y i ), and by doing so, provide optimal pattern recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Tsetlin Machine</head><p>We now present the core concepts of the Tsetlin Machine in detail. We first present the propositional logic based pattern representation framework, before we introduce the Tsetlin Automata teams that write conjunctive clauses in propositional logic. These Tsetlin Automata teams are then organized to recognize complex sub-patterns. We conclude the section by presenting the Tsetlin Machine game that we use to coordinate thousands of Tsetlin Automata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expressing Patterns with Propositional Formulas</head><p>The accuracy of a machine learning technique is bounded by its pattern representation capability. The Naive Bayes Classifier, for instance, assumes that input variables are independent given the output category <ref type="bibr" target="#b32">[33]</ref>. When critical patterns cannot be fully represented by the machine learning technique, accuracy suffers. Unfortunately, compared to the representation capability of the underlying language of digital computers, namely, Boolean algebra 2 , most machine learning techniques appear rather limited, with neural networks being one of the exceptions. Indeed, let f (X) refer to an arbitrary propositional formula. With o input variables, X = [x 1 , x 2 , . . . , x o ], there are no less than 2 2 o unique formula f (X). Perhaps only a single one of them will provide optimal pattern recognition accuracy for the task at hand.</p><p>The Satisfiability Problem (SAT). The representation power of propositional logic is perhaps best seen in light of the Satisfiabiliy (SAT) problem, which also can be solved using a team of Tsetlin Automata <ref type="bibr" target="#b33">[34]</ref>. The SAT problem is known to be NP-complete <ref type="bibr" target="#b34">[35]</ref> and plays a central role in a number of applications in the fields of VLSI Computer-Aided design, Computing Theory, Theorem Proving, and Artificial Intelligence. A SAT problem is defined in so-called conjunctive normal form. To facilitate Tsetlin Automata based learning, we will instead represent patterns using disjunctive normal form.</p><p>Patterns in Disjunctive Normal Form. Briefly stated, we represent the relation between an input, X = [x 1 , x 2 , . . . , x o ], and the output, y i , using a propositional formula ? i in disjunctive normal form:</p><formula xml:id="formula_3">? i = m j=1 C i j .<label>(1)</label></formula><p>The formula consists of a disjunction of m conjunctive clauses, C i j . Each conjunctive clause in turn, represents a specific sub-pattern governing the output y i .</p><p>Sub-Patterns and Conjunctive Clauses. Each clause C i j in the propositional formula ? i has the form:</p><formula xml:id="formula_4">C i j = 1 ? ? ? ? k?I i j x k ? ? ? ? ? ? ? k?? i j ?x k ? ? ? .<label>(2)</label></formula><p>That is, the clause is a conjunction of literals, where a literal is a propositional variable, x k , or its negation ?x k . Here, I i j and? i j are non-overlapping subsets of the input variable indexes,</p><formula xml:id="formula_5">I i j ,? i j ? {1, .....o}, I i j ?? i j = ?.</formula><p>The subsets decide which of the input variables take part in the clause, and whether they are negated or not. The input variables from I i j are included as is, while the input variables from? i j are negated. For example, the propositional formula (P ? ?Q) ? (?P ? Q) consists of two conjunctive clauses, and four literals, P , Q, ?P , and ?Q. The formula evaluates to 1 if ? P = 1 and Q = 0, or if ? P = 0 and Q = 1.</p><p>All other truth value assignments evaluate to 0, and thus the formula captures the renowned XOR-relation.</p><p>Definition 1 (The Problem of Pattern Learning with Propositional Logic). A set S of independent samples, (X,? i ), from the joint input-output probability distribution P (X, y i ) is provided. In the Problem of Pattern Learning with Propositional Logic, one must determine the propositional formula ? i (X) that evaluates to 0 iff P (y i = 0|X) &gt; P (y i = 1|X) and to 1 iff P (y i = 0|X) ? P (y i = 1|X), merely based on the samples in S.</p><p>The above problem decomposes into identifying the conjunctive clauses C i j whose disjunction evaluates to 1 iff X ? X i (see Section 2 for the definition of X i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Tsetlin Automata Team for Composing Clauses</head><p>At the core of the Tsetlin Machine we find the conjunctive clauses, C i j , j = 1, . . . , m, from Eqn. 2. For each clause C i j , we form a team of 2o Tsetlin Automata, two Tsetlin Automata per input variable x k . <ref type="figure" target="#fig_2">Figure 3</ref> captures the role of each of these Tsetlin Automata, and how they interact to form the clause C i j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TA1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exclude x1</head><p>Include x1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TA2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exclude ?x1 Include ?x1</head><p>TA2o-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exclude xo Include xo</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TA2o</head><p>Exclude ?xo Include ?xo As seen, o input variables, X = [x 1 , . . . , x o ], are fed to the clause. The critical task of the Tsetlin Automata team is to decide which input variables to include in I i j and which input variables to include in? i j . If a literal is excluded by its associated Tsetlin Automaton, it does not take part in the conjunction. That is, Tsetlin Automaton TA 2k?1 is responsible for deciding whether to "Include" or "Exclude" input variable x k , while another Tsetlin Automaton, TA 2k , decides whether to "Include" or "Exclude" ?x k . The input variable x k can thus take part in the clause C i j as is, take part in negated form, ?x k , or not take part at all. As illustrated to the right in the figure, the clause is formed after each Tsetlin Automaton has made its decision (to include or exclude its associated literal). After these decisions have been made, resulting in a selection of literals, {x u 1 , ?x u 2 , . . . , x uz }, the output of the clause can be calculated: </p><formula xml:id="formula_6">? ? ? x u 1 ? x u 2 x u z ? ? ? 0 1 0 ? 1 0 1 Input 0/1 Output</formula><formula xml:id="formula_7">C i j (X) = x u 1 ? ?x u 2 ? . . . ? x uz .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Basic Tsetlin Machine Architecture</head><p>We are now ready to build the complete Tsetlin Machine. We do this by assigning m clauses, C i j , j = 1, 2, . . . , m, to each output y i , i = 1, 2, . . . , n. The number of clauses m per output y i is a meta-parameter that is decided by the number of sub-patterns associated with each y i . If the latter is unknown, an appropriate m can be found using a grid search, corresponding to selecting the number of hidden nodes in a neural network layer.</p><p>With the clause structure in place, we assign one Tsetlin Automata team,</p><formula xml:id="formula_8">G i j = {TA i,j k |1 ? k ? 2o}, to each clause C i j .</formula><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the architecture consists of m ? n conjunctive clauses, each formed by an independent Tsetlin Automata team. Each Tsetlin Automata team, G i j , thus governs the selection of which literals to include in its respective clause, C i j . The collective of teams accordingly addresses the whole pattern recognition problem. As indicated, the clauses corresponds to the hidden layer of a neural network, although instead of having neurons with nonlinear activation functions, we have formulas in propositional logic that evaluates to 0 or 1. That is, a single clause corresponds to a single neuron, however, can be represented more compactly in bit form.</p><p>The basic Tsetlin Machine architecture can, accordingly, express any formula in propositional logic, constrained by the number of clauses. Therefore, this basic architecture is interesting by itself. However, real-world problems do not necessarily fit the pattern recognition problem laid out in Section 2 exactly. This raises the need for additional robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Extended Tsetlin Machine Architecture</head><p>In order to render the architecture more robust towards noise and intricate real-world data, we now replace the OR operator with a summation operator and a threshold function. It turns out that this additional robustness also supports more compact representation of patterns. <ref type="figure" target="#fig_4">Figure 5</ref> depicts the resulting extended architecture. Again, the architecture consists of a number of conjunctive clauses, each associated with a dedicated Tsetlin Automata team. However, instead of simply taking part in an OR relation, each clause,</p><formula xml:id="formula_9">C i j ? C i = {C i j |j = 1, . . . , m}, i ? {1, .</formula><p>. . , n}, is now given a fixed polarity. For simplicity, we assign positive polarity to clauses with an odd index j, while clauses with an even index are assigned negative polarity. In the figure, polarity is indicated with a '+' or '-' sign, attached to the output of each clause.</p><p>Clauses with positive polarity contribute to a final output of y i = 1, while clauses with a negative polarity contribute towards a final output of y i = 0. The contributions can be seen as votes, with each clause either casting a vote, C i j (X) = 1, or declining to vote, C i j (X) = 0. A positive vote means that the corresponding clause has recognized a sub-pattern associated with output y i = 1, while a negative vote means that the corresponding clause has recognized a sub-pattern associated with the opposite output, y i = 0.</p><p>After the clauses have produced their output, a summation operator, , associated with the output y i , sums the votes it receives from the clauses, C i j , j = 1, . . . , m. Clauses with positive polarity contribute positively while those with negative polarity contribute negatively. Overall, the purpose is to reach a balanced output decision, weighting positive evidence against negative evidence:</p><formula xml:id="formula_10">f i (X) = ? ? j?{1,3,...,m?1} C i j (X) ? ? ? ? ? j?{2,4,...,m} C i j (X) ? ? (3)</formula><p>The final output, y i , is decided by a threshold function</p><formula xml:id="formula_11">f t (x) = 0 for x &lt; 0 1 for x ? 0 .</formula><p>that outputs 1 if the outcome of the summation is larger than or equal to zero. Otherwise, it outputs 0. The final output can thus be calculated directly from input X simply by summing the signed output of the m conjunctive clauses, followed by activating the threshold function:</p><formula xml:id="formula_12">y i = f t (f i (X)).<label>(4)</label></formula><p>The crucial remaining issue, then, is how to learn the conjunctive clauses from data, to obtain optimal pattern recognition accuracy. We attack this problem next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Tsetlin Machine Game for Learning Conjunctive Clauses</head><p>We here introduce a novel game theoretic learning mechanism that guides the Tsetlin Automata stochastically towards solving the pattern recognition problem from Definition 1. The game is designed to deal with the problem of vanishing signal-to-noise ratio for large Tsetlin Automata teams that contain thousands of Tsetlin Automata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Tsetlin Automata Games</head><p>Recall that a Tsetlin Automaton can be formally represented as a quintuple {?, ?, ?, F (?, ?), G(?)}. A game of Tsetlin Automata involves W Tsetlin Automata and is played over several rounds <ref type="bibr" target="#b2">[3]</ref>. In each round of the game, every Tsetlin Automaton independently decide upon an action from ?. Thus, with two actions available to each automaton, there are 2 W unique action configurations.</p><p>After the Tsetlin Automata have decided upon an action, the round ends with the Tsetlin Automata being penalized/rewarded. That is, they are individually rewarded/penalized based on the configuration of actions selected. To fully specify the game, we thus need to specify one reward probability for each Tsetlin Automaton, for each unique configuration of actions.</p><p>We refer to the above reward probabilities as the payoff matrix of the game. As an example, with two action outcomes, ? = {? Penalty , ? Reward }, we need W 2 W reward probabilities to fully specify the payoff matrix for a game of W Tsetlin Automata players.</p><p>The complexity of the Tsetlin Machine game is immense, because the decisions of every single Tsetlin Automaton jointly decide the behaviour of the Tsetlin Machine as a whole. Indeed, under the right conditions, a single Tsetlin Automaton has the power to completely disrupt a clause by introducing a contradiction. The payoffs of the game must therefore be designed carefully, so that the Tsetlin Automata always are guided towards the optimal propositional formula f (X) that solves the pattern recognition problem at hand. To complicate further, an explicit enumeration of the payoffs is impractical due to the potentially tremendous size of the game payoff matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Design of the Payoff Matrix</head><p>We specify the payoffs associated with each cell of the game implicitly, so that they can be calculated lazily, on demand. In brief, we design the payoff matrix for the game based on the notion of:</p><p>? True positive output. We define true positive output as correctly providing output y i = 1.</p><p>? False negative output. We define false negative output as incorrectly providing the output y i = 0 when the output should have been y i = 1.</p><p>? False positive output. We define false positive output as incorrectly providing the output y i = 1 when the output should have been y i = 0.</p><p>? True negative output. We define true negative output as correctly providing the output y i = 0.</p><p>By progressively reducing false negative and false positive output, and reinforcing true positive and true negative output, we intend to guide the Tsetlin Automata towards optimal pattern recognition accuracy. This guiding is based on what we will refer to as Type I and Type II Feedback. In the following, we will introduce these two types of feedback, considering clauses with positive polarity (for clauses with negative polarity, the two types of feedback swap roles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Type I Feedback -Combating False Negative Output</head><p>Type I Feedback is decided by two factors, connecting the players of the game together:</p><p>? The choices of the Tsetlin Automata team G i j as a whole, summarized by the output of the clause C i j (X) (the truth value of the clause).</p><p>? The truth value of the literal x k /?x k assigned to the Tsetlin Automaton TA i,j 2k?1 /TA i,j 2k . <ref type="table">Table 2</ref> contains the probabilities that we use to generate Type I Feedback. For instance, assume that:</p><p>1. Clause C i j (X) evaluates to 1, 2. Literal x k is 1, and 3. Automaton TA i,j 2k?1 has selected the action Include Literal.</p><p>By examining the corresponding cell in <ref type="table">Table 2</ref>, we observe that the probability of receiving a reward, P (Reward), is s?1 s , the probability of inaction, P (Inaction), is 1 s , while the probability of receiving a penalty, P (Penalty), is zero.</p><p>Note that the Inaction feedback is a novel extension to the Tsetlin Automaton, which traditionally receives either a Reward or a Penalty. When receiving the Inaction feedback, the Tsetlin Automaton is simply unaffected.  <ref type="table">Table 2</ref>: Type I Feedback -Feedback from the perspective of a single Tsetlin Automaton deciding to either Include or Exclude a given literal x k /?x k in the clause C i j . Type I Feedback is triggered to increase the number of clauses that correctly evaluates to 1 for a given input X. <ref type="table">Table 2</ref>). The feedback probabilities in <ref type="table">Table 2</ref> have been selected based on mathematical derivations (see <ref type="bibr">Section 4)</ref>. For certain real-life data sets, however, it turns out that boosting rewarding of Include Literal actions can be beneficial. That is, pattern recognition accuracy can be enhanced by boosting rewarding of these actions when they produce true positive outcomes. Penalizing of Exclude Literal actions is then adjusted accordingly. In all brevity, we boost rewarding in this manner by replacing s?1 s with 1.0 and 1 s with 0.0 in Column 1 of <ref type="table">Table 2</ref>. Brief analysis of the Type I Feedback. Notice how the reward probabilities are designed to "tighten" clauses up to a certain point. That is, the probability of receiving rewards when selecting Include Literal is larger than the probability of receiving rewards when selecting Exclude Literal. The ratio between the two probabilities is controlled by the parameter s. In this manner, s effectively decides how "fine grained" patterns the clauses are going to capture. The larger the value of s, the more the Tsetlin Automata team is stimulated to include literals in the clause. The only countering force is the input examples, X ? X i , that do not match the clause. Obviously, the probability of encountering such examples grows as s is increased (the clause is "tightened"). When these forces are in balance, we have a Nash equilibrium as discussed further in the next section.</p><formula xml:id="formula_13">Truth Value of Target Clause C i j 1 0 Truth Value of Target Literal x k /?x k 1 0 1 0 Include Literal (k ? I i j /k ?? i j ) P (Reward) s?1 s NA 0 0 P (Inaction) 1 s NA s?1 s s?1 s P (Penalty) 0 NA 1 s 1 s Exclude Literal (k / ? I i j /k / ?? i j ) P (Reward) 0 1 s 1 s 1 s P (Inaction)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boosting of True Positive Feedback (Column 1 in</head><p>The above mechanism is a critical part of the Tsetlin Machine, allowing learning of any sub-pattern X i j , no matter how infrequent, as decided by s. This novel mechanism is studied both theoretically and empirically in the two following sections. As a rule of thumb, a large s leads to more "fine grained" clauses, that is, clauses with more literals, while a small s produces "coarser" clauses, with fewer literals included. <ref type="table">Table 3</ref> covers Type II Feedback, that is, feedback that combats false positive output. This type of feedback is triggered when the output is y i = 1 when it should have been y i = 0. Then we want to achieve the opposite of what we seek with Feedback Type I. In all brevity, we now seek to modify clauses that evaluate to 1, so that they instead evaluate to 0. To achieve this, for each offending clause, we identify the Tsetlin Automata that have selected the Exclude Literal action and whose corresponding literal evaluates to 0. By merely switching from Exclude Literal to Include Literal for a single one of these, our goal is achieved. That is, since we are dealing with conjunctive clauses, simply including a single literal that evaluates to 0 makes the whole conjunction also evaluate to 0. In this manner, we guide the Tsetlin Automata towards eliminating false positive output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Type II Feedback -Combating False Positive Output</head><p>Together, Type I Feedback and Type II Feedback interact to reduce the output error rate to a minimal level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.5">The Tsetlin Machine Algorithm</head><p>The step-by-step procedure for learning conjunctive clauses can be found in Algorithm 1. The algorithm takes a set of training examples, (X,? i ) ? S, as input. Based on this, it produces a propositional formula in conjunctive normal form, ? i (X), for predicting the output, y i .</p><p>We will now take a closer look at the algorithm, line-by-line. Lines 2-3. From the perspective of game theory, each Tsetlin Automaton, TA i,j 2k /TA i,j 2k?1 , j = 1, . . . , m, k = 1, . . . , o, takes part in a large and complex game, consisting of multiple independent players. Every Tsetlin Automaton is assigned a user specified number of states, N , per action, for learning which action to perform. The start-up state is then randomly set to either N or N + 1. Each Tsetlin Automaton TA i,j 2k /TA i,j 2k?1 selects between two actions: Either to include or exclude a specific literal, x k /?x k , in a specific clause, C i j . These Tsetlin Automata are in turn organized into teams of 2o automata. Each team, G i j , is responsible for a specific clause C i j , forming a subgame. Line 5. As seen in the algorithm, the learning process is driven by a set of training examples, S, sampled from the joint input-output distribution P (X, y i ), as described in Section 2. Each single training example (X,? i ) is fed to the Tsetlin Machine, one at a time, facilitating online learning. Line 6. In each iteration, the Tsetlin Automata decide whether to include or exclude each literal from each of the conjunctive clauses. The result is a new set of conjunctive clauses, C i , capable of predicting y i .</p><p>Lines 7-17. The next step is to measure how well, C i , predicts the observed output? i in order to provide feedback the Tsetlin Automata teams G i j . As seen, feedback is generated directly based on the output of the summation function, f i (X), from Eqn. 3. This part of the algorithm is particularly intricate, yet critical for the learning process. We therefore go through this part in more detail in the following paragraphs.</p><p>In order to maximize pattern representation capacity, we use a threshold value T as target for the summation f i . This mechanism is inspired by the finite-state automaton based resource allocation scheme for solving the knapsack problem in unknown and stochastic environments <ref type="bibr" target="#b8">[9]</ref>. The purpose of the mechanism is to ensure that only a few of the available clauses are spent representing each specific sub-pattern. This is to effectively allocate sparse pattern representation resources among competing sub-patterns. To exemplify, assume that the correct output is y i = 1 for an input X. If the votes accumulate to a total of T or more, neither rewards nor penalties are provided to the involved Tsetlin Automata. This leaves the Tsetlin Automata unaffected. <ref type="table">Table 3</ref>: Type II Feedback -Feedback from the perspective of a single Tsetlin Automaton deciding to either Include or Exclude a given literal x k /?x k in the clause C i j . Type II Feedback is triggered to decrease the number of clauses that incorrectly evaluates to 1 for a given input X. </p><formula xml:id="formula_14">Truth Value of Target Clause C i j 1 0 Truth Value of Target Literal x k /?x k 1 0 1 0 Include Literal (k ? I i j /k ?? i j ) P (Reward) 0 NA 0 0 P (Inaction) 1.0 NA 1.0 1.0 P (Penalty) 0 NA 0 0 Exclude Literal (k / ? I i j /k / ?? i j ) P (Reward) 0 0 0 0 P (Inaction) 1.0 0 1.0 1.0 P (Penalty) 0 1.0 0 0</formula><formula xml:id="formula_15">G i 1 , . . . , G i m ? ProduceTsetlinMachine(m,o)</formula><p>Produce 2o TsetlinAutomata (TA) for each clause C i j , assigning TA i,j 2k?1 to x k and TA i,j 2k to ?x k . Both TA i,j 2k?1 and TA i,j 2k belong to G i j .</p><p>3:</p><formula xml:id="formula_16">G i ? {G i 1 , . . . , G i m } Collect each team G i j in G i 4: repeat 5:X,? ? GetNextTrainingExample(S)</formula><p>Mini-batches, random selection, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>C i ? ObtainConjunctiveClauses(G i ) The Tsetlin Automata Teams G i j ? G i make their decisions, producing the conjunctive clauses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for j ? 1, 3, . . . , m ? 1 do</p><p>Provide feedback for clauses with positive polarity. for j ? 2, 4, . . . , m do Provide feedback for clauses with negative polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>if? i = 1 then 20:</p><p>if Random() ? T ?max(?T,min(T,f i (X))) 2T then 21:</p><p>TypeIIFeedback(G i j ) Output? i = 1 activates Type II Feedback for clauses with negative polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>end if <ref type="bibr" target="#b22">23</ref>: until StopCriteria(S, C i ) = 1 <ref type="bibr">30:</ref> return PruneAllExcludeClauses(C i ) Return completely trained conjunctive clauses C i j ? C i for y i , after pruning clauses where all literals have been excluded. 31: end function</p><formula xml:id="formula_17">else if? i = 0 then 24: if Random() ? T +max(?T,min(T,f i (X))) 2T<label>then</label></formula><formula xml:id="formula_18">Algorithm 2 Type I Feedback -Combating False Negative Output Input Input X, Clause C i j , Tsetlin Automata team G i j , Number of inputs o 1: procedure GenerateTypeIFeedback(X, X i j , G i j , o) 2:</formula><p>for k ? 1, o do Reward/Penalize all Tsetlin Automata in G i j .</p><p>3:</p><formula xml:id="formula_19">x k ? X[k] 4:</formula><p>if Random() ? TypeIFeedback(Reward, Action(TA i,j 2k?1 ), x k , C i j (X)) then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Reward(TA i,j 2k?1 ) Reward TA controlling x k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>else if Random() ? TypeIFeedback(Penalty, Action(TA i,j 2k?1 ), x k , C i j (X)) then 7:</p><p>Penalize(TA i,j 2k?1 ) Penalize TA controlling x k . if Random() ? TypeIFeedback(Reward, Action(TA i,j 2k ), ?x k , C i j (X)) then 10:</p><p>Reward(TA i,j 2k ) Reward TA controlling ?x k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>else if Random() ? TypeIFeedback(Penalty, Action(TA i,j 2k ), ?x k , C i j (X)) then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Penalize(TA i,j 2k ) Penalize TA controlling ?x k . for k ? 1, o do 3:</p><formula xml:id="formula_20">x k ? X[k] 4:</formula><p>if Random() ? TypeIIFeedback(Penalty, Action(TA i,j 2k?1 ), x k , C i j (X)) then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Penalize(TA i,j 2k?1 ) Penalize TA controlling x k . if Random() ? TypeIIFeedback(Penalty, Action(TA i,j 2k ), ?x k , C i j (X)) then 8:</p><p>Penalize(TA i,j 2k ) Penalize TA controlling ?x k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>end if 10:</p><p>end for 11: end procedure Generating Type I Feedback. If the target output is y i = 1, we randomly generate Type I Feedback for each clause C i j ? C i . The probability of generating Type I Feedback is:</p><formula xml:id="formula_21">T ? max(?T, min(T, f i (X))) 2T .<label>(5)</label></formula><p>Generating Type II Feedback. If, on the other hand, the target output is y i = 0, we randomly generate Type II Feedback for each clause C i j ? C i . The probability of generating Type II Feedback is:</p><formula xml:id="formula_22">T + max(?T, min(T, f i (X))) 2T .<label>(6)</label></formula><p>Notice how the feedback vanishes as the number of triggering clauses correctly approaches T /?T . This is a crucial part of effective use of the available pattern representation capacity. Indeed, if the existing clauses already are able to capture the patternX faced, there is no need to adjust any of the clauses.</p><p>After Type I or Type II Feedback have been triggered for a clause, the invidual Tsetlin Automata within each clause is rewarded/penalized according to Algorithm 2 and Algorithm 3, respectively. In all brevity, rewarding/penalizing is directly based on <ref type="table">Table 2 and Table 3</ref>.</p><p>Lines 18-28. After the clauses with positive polarity have received feedback. The next step is to invert the role of Type I and Type II Feedback, and feed the resulting feedback to the clauses with negative polarity.</p><p>Line 29. The above steps are iterated until a stopping criteria is fulfilled (for instance a certain number of iterations over the dataset), upon which the current clauses C i are returned as the output of the algorithm.</p><p>The resulting propositional formula, returned by the algorithm, has been composed by the Tsetlin Automata with the goal of predicting the output y i with optimal accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation of Tsetlin Automata With Bit-wise Operators</head><p>Small memory footprint and speed of operation can be crucial in complex and large scale pattern recognition. Being based on propositional formula, the Tsetlin Machine architecture can naturally be represented with bits and operated upon using bit-wise operators. However, it is not straightforward how to represent and update the Tsetlin Automata themselves since the state of each Tsetlin Automaton is an integer, and the action of an automaton is decided upon using a smaller-than operator.</p><p>One approach is to represent the state of 32 Tsetlin Automata with eight 32-bit integers, as laid out in <ref type="table">Table 4</ref>. The benefit of such a representation is that the action of each Tsetlin Automaton is readily available from the integer that represents bit 8, highlighted in bold in the table. This means that a bit mask for calculating the output of a clause always is available, without further computation. Employing this bit-based representation reduces memory usage four times, compared to using whole integer to represent the state of a single Tsetlin Automaton.</p><p>More importantly, it is possible to increment/decrement the state of 32 automata at a time by customized increment/decrement procedures, further increasing learning speed. As an example, for the MNIST dataset (cf. Section 5), memory usage is approx. ten times smaller, learning speed 3.5 times higher, and classification speed 8 times higher, using this procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>The reader may now have recognized that we have designed the Tsetlin Machine with mathematical analysis in mind, in order to facilitate a deeper understanding of our learning scheme. In this section, we argue that the Tsetlin Machine converges towards solving the problem of TA ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Bit 2 b i,j <ref type="table">Table 4</ref>: Bit-based representation of the Tsetlin Automata states, allowing the actions of each automaton to be obtained directly from bit 8. <ref type="figure">Figure 6</ref>: The pertinent subsets X , X , X j , and X ? j for the proofs.</p><formula xml:id="formula_23">1,2 b i,j 2,2 ? ? ? b i,j 32,2 ? ? ? b i,j 33,2 b i,j 34,2 ? ? ? b i,j 64,2 ? ? ? Bit 1 b i,j 1,1 b i,j 2,1 ? ? ? b i,j 32,1 ? ? ? b i,j 33,1 b i,j 34,1 ? ? ? b i,j 64,1 ? ? ?</formula><formula xml:id="formula_24">X' X' X j ' X j ' ?</formula><p>Pattern Learning with Propositional Logic from Definition 1 with probability arbitrary close to unity. Let the propositional formula ? i * solve a given pattern recognition problem, per Definition 1. To simplify notation, we will for the remainder of this section omit the index i, and instead use y and ? * to respectively refer to any y i and ? i * , i ? {1, . . . , n}. Without loss of generality, we further limit ourselves to consider one of the underlying sub-patterns X u ? {X 1 , . . . , X q } described in Section 2. By definition, there exists at least one clause C * j in ? * such that C * j (X) = 1 for all inputs X ? X u . Finally, let l k be a literal (representing either x k or ?x k ). Notice that we in the following focus on the sub-patterns belonging to class 1. The reasoning would follow along the same lines for class 0.</p><p>Our overall strategy for the proof consists of three steps: <ref type="bibr" target="#b0">(1)</ref> show that C * j forms a Nash Equilibrium for the associated team of Tsetlin Automata; <ref type="bibr" target="#b1">(2)</ref> show that other candidate clauses C j = C * j do not form Nash Equilibria; and finally, (3) allude to the convergence properties of Tsetlin Automata games, combining multiple subgames into the full-blown Tsetlin Machine game.</p><p>Before we can complete the proof, we need to derive the expected reward of the actions. <ref type="figure">Figure 6</ref> illustrates pertinent pattern subsets that will help us do that. Firstly, let X = {X|? * (X) = 1, X ? X } be the subset of input, X ? X , that makes ? * evaluate to 1. Conversely, let X = {X|? * (X) = 0, X ? X } be the complement of X . Let further X j = {X|C j (X) = 1, X ? X } be the subset of X where a clause C j (X) evaluates to 1, and let X ? j = {X|C j (X) = 1 ? l k = 1, X ? X } be an even further constrained subset where l k also is 1. These four subsets are depicted in the figure, to guide the reader through the set calculations that follows. We will use the notation G j to refer to a subgame between the Tsetlin Automata that controls the composition of clause C j (X), that is, G j = {TA j k |1 ? k ? 2o}. Consider one of the Tsetlin Automata, TA j k ? G j , in the subgame G j . Notice that the payoffs it can receive are given in <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref> for the whole range of subgame G j outcomes, from the perspective of TA j k . Finally, let X, y be an input-output pair sampled from P (X, y).</p><p>Lemma 1. The expected payoff of the action Exclude Literal for automaton TA j k within the subgame G j is:</p><formula xml:id="formula_25">P (y = 1|X ? X \ X ? j )P (X ? X \ X ? j ) ? 1 s + P (y = 1|X ? X )P (X ? X ) ? 1 s ? P (y = 1|X ? X ? j )P (X ? X ? j ) ? s ? 1 s ? P (y = 0|X ? X j \ X ? j )P (X ? X j \ X ? j ) ? 1.0 (7)</formula><p>Proof. In brief, we receive an expected fractional reward 1 s every time y becomes 1, except when both l k and C j (X) evaluates to 1. In that case, we instead receive an expected fractional penalty of s?1 s (see <ref type="table">Table 2</ref>). Formally, we can reformulate this rewarding and penalizing as follows:</p><formula xml:id="formula_26">P (y = 1 ? ?(C j (X) = 1 ? l k = 1)) ? 1 s ? P (y = 1 ? C j (X) = 1 ? l k = 1) ? s ? 1 s = (8) P (y = 1 ? ?(X ? X ? j )) ? 1 s ? P (y = 1 ? X ? X ? j ) ? s ? 1 s = (9) P (y = 1 ? X ? X ? j ) ? 1 s ? P (y = 1 ? X ? X ? j ) ? s ? 1 s = (10) P (y = 1 ? X ? X \ X ? j ) ? 1 s + P (y = 1 ? X ? X ) ? 1 s ? P (y = 1 ? X ? X ? j ) ? s ? 1 s = (11) P (y = 1|X ? X \ X ? j )P (X ? X \ X ? j ) ? 1 s + P (y = 1|X ? X )P (X ? X ) ? 1 s ? P (y = 1|X ? X ? j )P (X ? X ? j ) ? s ? 1 s .<label>(12)</label></formula><p>Furthermore, selecting the Exclude Literal when l k = 0 and y = 0, while C j (X) evaluates to 1, provides a full penalty (see <ref type="table">Table 3</ref>). As further seen in the table, false positive output never triggers penalties or rewards for the Include Literal action. All this is by design in order to suppress the output 1 from C j (X) to combat false positive output. This additional effect can be formalized as follows:</p><formula xml:id="formula_27">P (y = 0 ? l k = 0 ? C j (X) = 1) = (13) P (y = 0|X ? X j \ X ? j )P (X ? X j \ X ? j )<label>(14)</label></formula><p>Lemma 2. The expected payoff of the action Include Literal for automaton TA j k within the subgame G j is:</p><formula xml:id="formula_28">P (y = 1|X ? X ? j )P (X ? X ? j ) ? s ? 1 s ? P (y = 1|X ? X \ X ? j )P (X ? X \ X ? j ) ? 1 s + P (y = 1|X ? X )P (X ? X ) ? 1 s .<label>(15)</label></formula><p>Proof. Using <ref type="table">Table 2</ref>, we now simply establish that the expected feedback of action Include Literal is symmetric to the expected feedback of action Exclude Literal, apart from not being affected by Type II Feedback:</p><formula xml:id="formula_29">P (y = 1 ? C j (X) = 1 ? l k = 1) ? s ? 1 s ? P (y = 1 ? ?(C j (X) = 1 ? l k = 1)) ? 1 s = (16) P (y = 1 ? X ? X ? j ) ? s ? 1 s ? P (y = 1 ? X ? X ? j ) ? 1 s = (17) P (y = 1 ? X ? X ? j ) ? s ? 1 s ? P (y = 1 ? X ? X \ X ? j ) ? 1 s ? P (y = 1 ? X ? X ) ? 1 s = (18) P (y = 1|X ? X ? j )P (X ? X ? j ) ? s ? 1 s ? P (y = 1|X ? X \ X ? j )P (X ? X \ X ? j ) ? 1 s ? P (y = 1|X ? X )P (X ? X ) ? 1 s .<label>(19)</label></formula><p>In other words, for the same reasons that Exclude Literal has a negative expected payoff, Include Literal has a positive one, and vice versa! This symmetry is by design to facilitate robust and fast learning in the game. Theorem 1. Let ? * be a solution to a pattern recognition problem per Definition 1. Further, let the probability of observing erroneous class information be a constant, ? &lt; 0.5. Then every clause C * j in ? * is a Nash equilibrium in the associated Tsetlin Machine subgame G j .</p><p>Proof. We start our proof by reformulating Eqn. 7 to incorporate the probability of erroneous class information, ?:</p><formula xml:id="formula_30">(1 ? ?) ? P (X ? X \ X ? j ) ? 1 s + ? ? P (X ? X ) ? 1 s ? (1 ? ?) ? P (X ? X ? j ) ? s ? 1 s ? ? ? P (X ? X j \ X ? j ) ? 1.0.<label>(20)</label></formula><p>Further, we note that the Tsetlin Machine can be self-balancing, ensuring that P (X ? X ) = 1 2 (due to how training examples are sampled for the Multi-Class Tsetlin Machine in Section 6):</p><formula xml:id="formula_31">(1 ? ?) ? P (X ? X \ X ? j ) ? 1 s + ? ? 1 2s ? (1 ? ?) ? P (X ? X ? j ) ? s ? 1 s ? ? ? P (X ? X j \ X ? j ) ? 1.0.<label>(21)</label></formula><p>Finally, we note that P (X ? X ? j ) = 1 2 ? P (X ? X \ X ? j ). This is because P (X ? X ) = 1 2 , again due to the self-balancing nature of the Tsetlin Machine, and because X ? j is a subset of X . In the following, let ? = P (X ? X ? j ). We can thus simplify the expected payoff of the Exclude Literal action to:</p><formula xml:id="formula_32">(1 ? ?) ? 1 2 ? ? ? 1 s + ? ? 1 2s ? (1 ? ?) ? ? ? s ? 1 s ? ? ? 1 2 ? ? .<label>(22)</label></formula><p>Similarly, the expected payoff of the Include Literal action can be simplified to:</p><formula xml:id="formula_33">(1 ? ?) ? ? ? s ? 1 s ? (1 ? ?) ? 1 2 ? ? ? 1 s ? ? ? 1 2s .<label>(23)</label></formula><p>Let us now consider an arbitrary Tsetlin Automaton, which controls, let us say, the inclusion or exclusion of literal l k . This produces two possible scenarios. Either the literal l k is part of the clause C * j (the action selected is Include Literal ). Otherwise, the literal is not part of the clause C * j (the selected action is Exclude Literal ). We will now consider each of these scenarios and verify that both scenarios qualify as Nash equilibria.</p><p>Scenario 1: Literal included. Let us first consider the situation where ? is zero. This means that the output y is free of noise, and the problem becomes purely the extraction of the underlying sub-patterns. We only need to verify that the expected payoff of the Exclude Literal action is negative. Multiplying by 2 leaves the polarity of the expression unchanged and we have:</p><formula xml:id="formula_34">(1 ? 2?) ? 1 s ? 2? ? s ? 1 s .<label>(24)</label></formula><p>We know that ? &gt; 1 2s by Definition 1 and due to the balancing effect of the Tsetlin Machine. Thus, clearly, (1 ? 2?) ? 1 s will always be smaller than 2? ? s?1 s . In other words, the expected payoff for Exclude Literal is always negative. Hence, due to the symmetry, Include Literal always has positive expected payoff, and is the preferred action, enforcing the equilibrium.</p><p>By allowing noisy output y, the analysis becomes somewhat more complex:</p><formula xml:id="formula_35">(1 ? ?) ? ( 1 2 ? ?) ? 1 s + ? ? 1 2s ? (1 ? ?) ? ? ? s ? 1 s ? ? ? 1 2 ? ? .<label>(25)</label></formula><p>Again, multiplying by 2 leaves the polarity of the expression unchanged and we have:</p><formula xml:id="formula_36">(1 ? ?) ? (1 ? 2?) ? 1 s + ? ? 1 s ? (1 ? ?) ? 2? ? s ? 1 s ? ? ? (1 ? 2?) .<label>(26)</label></formula><p>We now too note that ? &gt; 1 2s , and observe that (</p><formula xml:id="formula_37">1 ? ?) ? 2? ? s?1 s is larger than (1 ? ?) ? (1 ? 2?) ? 1 s</formula><p>and ? ? (1 ? 2?) is larger than ? ? 1 s . Hence, the expected payoff for Exclude Literal is negative and we have a Nash Equilibrium.</p><p>Recall that the whole purpose of the above Nash equilibrium is to make sure that the patterns captured by the clause C * j is of an appropriate granularity, decided by s, finely balancing Exclude Literal actions against Include Literal actions. This is combined with the combating of false positive output through targeted selection of Include Literal actions.</p><p>To conclude, due to the established symmetry in payoff, switching from Include Literal to Exclude Literal leads to a net loss in expected payoff, providing a Nash equilibrium for the action Include Literal.</p><p>Scenario 2: Literal excluded. Again, consider the expected payoff of Exclude Literal when ? is zero:</p><formula xml:id="formula_38">1 2 ? ? ? 1 s ? ? ? s ? 1 s .<label>(27)</label></formula><p>With l k excluded from C * j , we know that ? &lt; 1 2s by definition, otherwise, l k would have been included instead. In other words, the expected payoff of Exclude Literal is always positive, while the expected payoff of Include Literal becomes negative. In a similar manner, we can modify the above procedure for the noisy case. Hence, the Nash equilibrium! Theorem 2. A conjunctive clause C j that is not part of the solution ? * is not a Nash equilibrium.</p><p>Proof. This theorem follows from the proof for Theorem 1. By invalidating any of the required conditions that made the clause C * j a Nash equilibrium, it can no longer be a Nash equilibrium. That is, by including more literals, for instance, ? drops below 1 s . Conversely, starting with too many literals excluded, it becomes advantageous to include the literals (positive expected payoff). A final possibility is that a clause captures another class than its target class. Such a configuration is highly unstable since Type II Feedback will aggressively move the Tsetlin Automata out of that configuration.</p><p>We end this section by arguing that the Tsetlin Machine will converge to a solution ? * with probability arbirtrarily close to unity. Here follows a sketch for a proof. Any solution scheme that is capable of finding a single Nash equilibrium in a game will be able to solve each subgame G j due to Theorem 1 and Theorem 2. This is because each subgame G j is played independently of the other subgames, apart from the indirect interaction through the summation function f of the Tsetlin Machine. However, the feedback that connects the subgames only controls how often each game is activated. Indeed, a subgame is activated with probability T ? max(?T, min(T, f (X))) 2T <ref type="bibr" target="#b27">(28)</ref> for Type I Feedback, and with probability:</p><p>T + max(?T, min(T, f (X))) 2T <ref type="bibr" target="#b28">(29)</ref> for Type II Feedback. Together, these merely control the mixing factor of the two different types of feedback, as well as the frequency with which the subgame is played. Type II Feedback is self-defeating, eliminating itself by nature to a minimum. As an equilibrium is found in each subgame, eventually, all subgames are stopped being played, i.e. f (X) always evaluates to either T or ?T , and the Tsetlin Machine game has been solved.</p><p>The Tsetlin Automaton is one particularly robust mechanism for solving such coordination games, converging to a Nash equilibrium with probability arbitrarily close to unity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Results</head><p>In this section we evaluate the Tsetlin Machine empirically using five datasets:</p><p>? Binary Iris Dataset. This is the classical Iris Dataset, however, with features in binary form.</p><p>? Binary Digits Dataset. This is the classical digits dataset, again with features in binary form.</p><p>? Axis &amp; Allies Board Game Dataset. This new dataset involving optimal move prediction in a minimalistic, yet intricate, mini-game from the Axis &amp; Allies board game.</p><p>? Noisy XOR Dataset with Non-informative Features. This artificial dataset is designed to reveal particular "blind zones" of pattern recognition algorithms. The dataset captures the renowned XOR-relation. Furthermore, the dataset contains a large number of random non-informative features to measure susceptibility towards the curse of dimensionality <ref type="bibr" target="#b35">[36]</ref>. To examine robustness towards noise we have further randomly inverted 40% of the outputs.</p><p>? MNIST Dataset. The MNIST dataset is a larger scale dataset used extensively to benchmark machine learning algorithms. We have included this dataset to investigate the scalability of the Tsetlin Machine, as well as the behaviour of longer learning processes.</p><p>For these datasets, we form ensembles of 50 to 1000 independent replications with different random number streams. We do this to minimize the variance of the reported results and to provide the foundation for a statistical analysis of the merits of the different schemes evaluated.</p><p>Together with the Tsetlin Machine, we also evaluate several classical machine learning techniques using the same random number streams. This includes Multilayer Perceptron Networks, the Naive Bayes Classifier, Support Vector Machines, and Logistic Regression. Where appropriate, the different schemes are optimized by means of hyper-parameter grid searches. As an example, <ref type="figure" target="#fig_10">Figure 7</ref> captures the impact the s parameter of the Tsetlin Machine has on mean accuracy, for the Noisy XOR Dataset. Each point in the plot measures the mean accuracy of 100 different replications of the XOR-experiment for a particular value of s. Clearly, accuracy increases with s up to a certain point, before it degrades gradually. Based on the plot, for the Noisy XOR-experiment, we decided to use an s value of 3.9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Binary Iris Dataset</head><p>We first evaluate the Tsetlin Machine on the classical Iris dataset 3 . This dataset consists of 150 examples with four inputs (Sepal Length, Sepal Width, Petal Length and Petal Width), and three possible outputs (Setosa, Versicolour, and Virginica). We increase the challenge by transforming the four input values into one consecutive sequence of 16 bits, four bits per float. It is thus necessary to also learn how to segment the 16 bits into four partitions, and extract the numeric information. We refer to the new dataset as the The Binary Iris Dataset.</p><p>We partition this dataset into a training set and a test set, with 80 percent of the data being used for training. We here randomly produce 1000 training and test data partitions. For each ensemble, we also randomly reinitialize the competing algorithms, to gain information on stability and robustness. The results are reported in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>The Tsetlin Machine 4 used here employs 300 clauses, and uses an s-value of 3.0 and a threshold T of 10. Furthermore, the individual Tsetlin Automata each has 100 states. This Tsetlin Machine is run for 500 epochs, and it is the accuracy after the final epoch that is reported. Propositional formulas with higher test accuracy are often found in preceding epochs because of the random exploration of the Tsetlin Machine. However, to avoid overfitting to the test set by handpicking the best configuration found, we instead simply use the last configuration produced.</p><p>In <ref type="table" target="#tab_4">Table 5</ref>, we list mean accuracy with 95% confidence intervals, 5 and 95 percentiles, as well as the minimum and maximum accuracy obtained, across the 1000 experiment runs we executed. As seen, the Tsetlin Machine provides the highest mean accuracy. However, for the 95 %ile scores, most of the schemes obtain 100% accuracy. This can be explained by the small size of the test set, which merely contains 30 examples. Thus it is easier to stumble upon a random configuration that happens to provide fault free classification. Since the test set is merely a sample of the corresponding real-world problem, it is reasonable to assume that higher mean accuracy translates to more robust performance overall.</p><p>The training set, on the other hand, reveals subtler differences between the schemes. The results obtained on the training set are shown in <ref type="table" target="#tab_5">Table 6</ref>. As seen, the SVM here provides best performance on average, while the Tsetlin Machine provides the second best accuracy. However, the large drop in accuracy from the training data to the test data for the SVM indicates overfitting on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Binary Digits Dataset</head><p>We next evaluate the Tsetlin Machine on the classical Pen-Based Recognition of Handwritten Digits Dataset 5 . The original dataset consists of 250 handwritten digits from 44 different writers, for a total number of 10992 instances. We increase the challenge by removing the individual pixel value structure, transforming the 64 different input features into a sequence of 192 bits, 3 bits per pixel. We refer to the modified dataset as the The Binary Digits Dataset. Again we partition the dataset into training and test sets, keeping 80 percent of the data for training.</p><p>The Tsetlin Machine 6 used here contains 1000 clauses, uses an s-value of 3.0, and has a threshold T of 10. Furthermore, the individual Tsetlin Automata each has 1000 states. The Tsetlin machine is run for 300 epochs, and it is the accuracy after the final epoch that is reported. <ref type="table" target="#tab_7">Table 7</ref> reports mean accuracy with 95% confidence intervals, 5 and 95 percentiles, as well as the minimum and maximum accuracy obtained, across the 100 experiment runs we executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">UCI Machine</head><p>Learning Repository [http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits]. <ref type="bibr" target="#b5">6</ref> In this experiment, we used a Multi-Class Tsetlin Machine, described in Section 6.1. We also apply Boosting of True Positive Feedback to Include Literal actions as described in Section 3.5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technique/Accuracy (%)</head><p>Mean   As seen, the Tsetlin Machine again clearly provides the highest accuracy on average, also when taking the 95% confidence intervals into account. For this dataset, the Tsetlin Machine is also superior when it comes to the maximal accuracy found across the 100 replications of the experiment, as well as for the 95 %ile results.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Axis &amp; Allies Board Game Dataset</head><p>Besides the two classical datasets, we also have built a new dataset based on the board game Axis &amp; Allies 7 . We designed this dataset to exhibit intricate pattern structures, involving optimal move prediction in a minimalistic, yet subtle, subgame of Axis &amp; Allies. Indeed, superhuman performance for the Axis &amp; Allies board game has not yet been attained. In Axis &amp; Allies, every piece on the board are potentially moved each turn. Additionally, new pieces are introduced throughout the game, as a result of earlier decisions. This arguably yields a larger search tree than the ones we find in Go and chess. Finally, the outcome of battles are determined by dice, rendering the game stochastic. The Axis &amp; Allies Board Game Dataset consists of 10 000 board game positions, exemplified in <ref type="figure" target="#fig_12">Figure 8</ref>. Player 1 owns the "Caucasus" territory in the figure, while Player 2 owns "Ukraine" and "West Russia". At start-up, each player is randomly assigned 0-10 tanks and 0-20 infantry each. These units are their respective starting forces. For Player 2, they are randomly distributed among his two territories. The game consists of two rounds. First Player 1 attacks. This is followed by a counter attack by Player 2. In order to win, Player 1 needs to capture both of "Ukraine" and "West Russia". Player 2, on the other hand, merely needs to take "Caucasus".</p><p>To produce the dataset, we built an Axis &amp; Allies Board Game simulator. This allowed us to find the optimal attack for each assignment of starting forces. The resulting input and output variables are shown in <ref type="table" target="#tab_9">Table 9</ref>. The at start forces are to the left, while the optimal attack forces can be found to the right. In the first row, for instance, it is optimal for Player 1 to launch a preemptive strike against the armor in Ukraine (armor is better offensively than defensively), to destroy offensive power, while keeping the majority of forces for defense.</p><p>We use 25% of the data for training, and 75% for testing, randomly producing 100 different partitions of the dataset. The Tsetlin Machine employed here contains 10 000 clauses, and uses an s-value of 40.0 and a threshold T of 10. Furthermore, the individual Tsetlin Automata each has 1000 states. The Tsetlin machine is run for 200 epochs, and it is the accuracy after the final epoch that is reported. <ref type="table" target="#tab_10">Table 10</ref> reports the results from predicting output bit 5 among the 20 output bits (as representative for all of the bits). In the table, we list mean accuracy with 95% confidence intervals, 5 and 95 percentiles, as well as the minimum and maximum accuracy obtained, across the 100 experiment runs we executed. As seen in the table, apparently only the Tsetlin Machine and the neural network are capable of properly handling the complexity of the dataset, providing statistically similar performance. The Tsetlin Machine is quite stable performancewise, while the neural network performance varies more.</p><p>However, the number of clauses needed to achieve the above performance is quite high for the Tsetlin Machine, arguably due to its flat one-layer architecture. Another reason that can explain the need for a large number of clauses can be the intricate nature of the mini-game of Axis &amp; Allies. Since we need an s-value as large as 40, apparently, some of the pertinent sub-patterns must be quite fine-grained. Because the s-value is global, all patterns, even the coarser ones, must be learned at this fine granularity. A possible next step in the research on the Tsetlin Machine could therefore be to investigate the effect of having clauses with different s-values -some with smaller values for the rougher patterns, and some with larger values for the finer patterns.</p><p>As a final observation, <ref type="table" target="#tab_11">Table 11</ref> reports performance on the training data. Random Forest distinguishes itself by almost perfect predictions for the training data, thus clearly overfitting, but still performing well on the test set. The other techniques provide slightly improved performance on the training data, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>At Start</head><p>Optimal Attack Caucasus W. Russia Ukraine W. Russia Ukraine Inf Tnk <ref type="table" target="#tab_4">Inf Tnk Inf Tnk Inf Tnk Inf Tnk   16  4  11  4  5  4  0  0  3  4  19  3  6  1  6  3  7  2  12  1  9</ref> 1 1 3 0 5 0 0 0 0    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The Noisy XOR Dataset with Non-informative Features</head><p>We now turn to an artifical dataset, constructed to uncover "blind zones" caused by XOR-like relations. Furthermore, the dataset contains a large number of random non-informative features to measure susceptibility towards the curse of dimensionality <ref type="bibr" target="#b35">[36]</ref>. To examine robustness towards noise, we have further randomly inverted 40% of the outputs.  No. Sign Clause Learned The dataset consists of 10 000 examples with twelve binary inputs, X = [x 1 , x 2 , . . . , x 12 ], and a binary output, y. Ten of the inputs are completely random. The two remaining inputs, however, are related to the output y through an XOR-relation, y = XOR(x k 1 , x k 2 ). Finally, 40% of the outputs are inverted. <ref type="table" target="#tab_12">Table 12</ref> shows four examples from the dataset, demonstrating the high level of noise. We partition the dataset into training and test data, using 50% of the data for training.</p><formula xml:id="formula_39">1 + ?x 1 ? x 2 2 ? ?x 1 ? ?x 2 3 + x 1 ? ?x 2 4 ? x 1 ? x 2</formula><p>The Tsetlin Machine 8 used here contains 20 clauses, and used an s-value of 3.9 and a threshold T of 15. Furthermore, the individual Tsetlin Automata each has 100 states. The Tsetlin Machine is run for 200 epochs, and it is the accuracy after the final epoch, that we report. <ref type="table" target="#tab_13">Table 13</ref> contains four of the clauses produced by the Tsetlin Machine. Notice how the noisy dataset from <ref type="table" target="#tab_12">Table 12</ref> has been turned into informative propositional formulas that capture the structure of the dataset.</p><p>The empirical results are found in <ref type="table" target="#tab_15">Table 14</ref>. Again, we report mean accuracy with 95% confidence intervals, 5 and 95 percentiles, as well as the minimum and maximum accuracy obtained, across the 100 replications of the experiment. Note that for the test data, the output values are unperturbed. As seen, the XOR-relation, as expected, makes Logistic Regression and the Naive Bayes Classifier incapable of predicting the output value y, resorting to random guessing. Both the neural network and the Tsetlin Machine, on the other hand, see through the noise and captures the underlying XOR pattern. SVM performs slightly better than the Naive Bayes Classifier and Logistic Regression, however, is clearly distracted by the added noninformative features (the SVM performs much better with fewer non-informative features).     The Tsetlin Machine 10 used here contains 40 000 clauses, 4000 clauses per class, uses an s-value of 10.0, and a threshold T of 50. Furthermore, the individual Tsetlin Automata each has 256 states. The Tsetlin machine is run for 400 epochs, and it is the accuracy after the final epoch that is reported.</p><p>As seen in <ref type="figure" target="#fig_0">Figure 10</ref>, both mean test-and training accuracy increase almost monotonically across the epochs, however, affected by random fluctuation. Perhaps most notably, while the mean accuracy on the training data approaches 99.8%, accuracy on the test data continues to increase as well, hitting 98.2% after 400 epochs. This is quite different from what occurs with back-propagation on a neural network, where accuracy on test data starts to drop at some point due to overfitting, without proper regularization mechanisms.</p><p>The figure also show how varying the number of clauses and the threshold T affects accuracy and fluctuation. With more clauses available to express patterns, in combination with a higher threshold T , both learning speed, stability and accuracy increases, however, at the cost of larger computational cost. <ref type="table" target="#tab_4">Table 15</ref> reports the mean accuracy of the Tsetlin Machine, across the 50 experiment runs we executed. As points of reference, results for other well-known algorithms have been obtained from http://yann.lecun.com/exdb/mnist/ and included in the table (in italic). Only the vanilla version of the algorithms, that has been applied directly on unenhanced MNIST data, is included here. The purpose of this selection is to strictly compare algorithmic performance. In other words, we do not consider the effect of enhancing the dataset (warping, distortion, deskewing), combining different algorithms (e.g., neural network combined with nearest neighbor, convolution schemes), or applying meta optimization techniques (boosting, ensemble learning, etc.). With such techniques, it is possible to significantly increase accuracy, with the best currently reported results being an accuracy of 99.79% <ref type="bibr" target="#b37">[38]</ref>. Enhancing the vanilla Tsetlin Machine with such techniques is further work.</p><p>Additionally, as a further point of reference, we train and evaluate logistic regression, decision trees, and multinomial Naive Bayes on the binarized MNIST dataset used by the Tsetlin Machine.</p><p>As seen in the table, the Tsetlin Machine provides competitive accuracy, outperforming e.g. K-nearest neighbor and a 3-layer neural network. It is outperformed by a 2-layer neural network with 800 hidden nodes, using cross entropy loss. However, note that the Tsetlin Machine operates upon the binarized MNIST data (the grey tone value of each pixel is either set to 0 or 1) , and thus has a disadvantage. Improved binarization techniques for the Tsetlin Machine is further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Tsetlin Machine as a Building Block in More Advanced Architectures</head><p>We have designed the Tsetlin Machine to facilitate building of more advanced architectures. We will here exemplify different ways of connecting multiple Tsetlin Machines in more advanced architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Multi-Class Tsetlin Machine</head><p>In some pattern recognition problems the task is to assign one of n classes to each observed pattern, X. That is, one needs to decide upon a single output value, y ? {1, . . . , n}. Such a multi-class pattern recognition problem can be handled by the Tsetlin Machine by representing y as bits, using multiple outputs, y i . In this section, however, we present an alternative architecture that addresses the multi-class pattern recognition problem more directly. <ref type="figure" target="#fig_0">Figure 11</ref> depicts the Multi-Class Tsetlin Machine 11 which replaces the threshold function of each output y i , i = 1, . . . , n with a single argmax operator. With the argmax operator, the <ref type="bibr" target="#b10">11</ref>    </p><formula xml:id="formula_40">y = argmax i=1,...,n (f i (X)).<label>(30)</label></formula><p>In this manner, each propositional formula ? i , consisting of clauses C i , captures the pertinent aspects of the respective class i that it models. Training is done as described in Section 3.5, apart from one critical modification. Assume we have? = i for the current observation, (X,?). Then the Tsetlin Automata team behind C i is trained as per? i = 1 in the original Algorithm 1. Additionally, a random class q = i is selected. The Tsetlin Automata team behind C q is then trained in accordance with? i = 0 in the original algorithm (trained with opposite feedback, i.e., Type I Feedback becomes Type II Feedback, and vice versa).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Fully Connected Deep Tsetlin Machine</head><p>Another architectural family is the Fully Connected Deep Tsetlin Machine <ref type="bibr" target="#b38">[39]</ref>, illustrated in <ref type="figure" target="#fig_0">Figure 12</ref> combining the propositional formula composed at one layer into more complex formula at the next. As exemplified in the figure, we here connect multiple Tsetlin Machines in a sequence. The clause output from each Tsetlin Machine in the sequence is provided as input to the next Tsetlin Machine in the sequence. In this manner we build a multi-layered system. For instance, if layer t produces two clauses (P ? ?Q) and (?P ? Q), layer t + 1 can manipulate these further, treating them as inputs. Layer t + 1 could then form more complex formulas like ?(P ? ?Q) ? (P ? ?Q), which can be rewritten as (?P ? Q) ? (P ? ?Q).</p><p>One simple approach for training such an architecture is indicated in the figure. As illustrated, each layer is trained independently, directly from the output target y i , exactly as described in Section 3.5. The training procedure is thus similar to the strategy Hinton et al. used to train their pioneering Deep Belief Networks, layer-by-layer, in 2006 <ref type="bibr" target="#b39">[40]</ref>. Such an approach can be effective when each layer produces abstractions, in the form of clauses, that can be taken advantage of in the following layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">The Convolutional Tsetlin Machine</head><p>We next demonstrate how self-contained and independent Tsetlin Machines can interact to build a Convolutional Tsetlin Machine <ref type="bibr" target="#b40">[41]</ref>, illustrated in <ref type="figure" target="#fig_0">Figure 13</ref>. The Convolutional Tsetlin Machine is a deep architecture based on mathematical convolution, akin to Convolutional Neural Networks <ref type="bibr" target="#b36">[37]</ref>. For illustration purposes, consider 2D images of size 100 ? 100 as input. At the core of a Convolutional Tsetlin Machine we find a kernel Tsetlin Machine with a small receptive field. Each layer t of the Convolutional Tsetlin Machine operates as follows:</p><p>1. A convolution is performed over the input from the previous Tsetlin Machine layer, producing one feature map per output y i . Here, the Tsetlin Machine acts as a kernel in the convolution. In this manner, we reduce complexity by reusing the same Tsetlin Machine across the whole image, focusing on a small image patch at a time.</p><p>2. The feature maps produced are then down-sampled using a pooling operator, in a similar fashion as done in a Convolutional Neural Network, before the next layer and a new Tsetlin Machine takes over. Here, the purpose of the pooling operation is to gradually increase the abstraction level of the clauses, layer by layer.</p><p>A simple approach for training a Convolutional Tsetlin Machine is indicated in the figure. In brief, the feedback to the Tsetlin Machine kernel is directly provided from the desired end output y i , exactly as described in Section 3.5. The only difference is the fact that the input to layer t + 1 comes from the down-scaled feature map produced by layer t. Again, this is useful when each layer produces abstractions, in the form of clauses, that can be taken advantage of at the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">The Recurrent Tsetlin Machine</head><p>The final example is the Recurrent Tsetlin Machine <ref type="bibr" target="#b41">[42]</ref>  <ref type="figure" target="#fig_0">(Figure 14)</ref>. In all brevity, the same Tsetlin Machine is here reused from time step to time step. By taking the output from the Tsetlin Machine of the previous time step as input, together with an external input from the current time step, an infinitely deep sequence of Tsetlin Machines is formed. This is quite similar to the family of Recurrent Neural Networks <ref type="bibr" target="#b42">[43]</ref>. Again, the architecture can be trained</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>External input</head><p>Memory Input from memory Time step t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time step t-1</head><p>External input Input from memory <ref type="figure" target="#fig_0">Figure 14</ref>: The Recurrent Tsetlin Machine. layer by layer, directly from the target output y i (t) of the current time step t. However, to learn more advanced sequential patterns, there is a need for rewarding and penalizing that propagate back in time. How to design such a propagation scheme is presently an open research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Further Work</head><p>In this paper we proposed the Tsetlin Machine, an alternative to neural networks. The Tsetlin Machine solves the vanishing signal-to-noise ratio of collectives of Tsetlin Automata. This allows it to coordinate thousands of Tsetlin Automata. By equipping teams of Tsetlin Automata with the ability to express patterns in propositional logic, we have enabled them to recognize complex patterns. Furthermore, we proposed a novel decentralized feedback orchestration mechanism. The mechanism is based on resource allocation principles, with the intent of maximizing effectiveness of sparse pattern recognition capacity. This mechanism effectively provides the Tsetlin Machine with the ability to capture unlabelled sub-patterns.</p><p>Our theoretical analysis reveals that the Tsetlin Machine forms a set of subgames where the Nash equilibria maps to propositional formulas that maximize pattern recognition accuracy. In other words, there are no local optima in the learning process, only global ones. This explains how the collectives of Tsetlin Automata are able to accurately converge towards complex propositional formulas that capture the essence of five diverse pattern recognition problems. Overall, the Tsetlin Machine is particularly suited for digital computers, being merely based on simple bit manipulation with AND-, OR-, and NOT gates. Both input, hidden patterns, and output are expressed with easy-to-interpret bit patterns. In our empirical evaluations on five distinct benchmarks, the Tsetlin Machine provided competitive accuracy with respect to both Multilayer Perceptron Networks, Support Vector Machines, Decision Trees, Random Forests, the Naive Bayes Classifier and Logistic Regression. It further turns out that the Tsetlin Machine requires much less data than neural networks, even outperforming the Naive Bayes Classifier in data sparse environments.</p><p>The Tsetlin Machine is a completely new tool for machine learning. Based on its solid anchoring in automata-and game theory, promising empirical results, and its ability to act as a building block in more advanced systems, we believe the Tsetlin Machine has the potential to impact the AI field as a whole, opening a wide range of research paths ahead.</p><p>By demonstrating that the longstanding problem of vanishing signal-to-noise ratio can be solved, the Tsetlin Machine further provides a novel game theoretic framework for recasting the problem of pattern recognition. Such a framework can provide new opportunities for introducing bandit algorithms into large-scale pattern recognition. It could for instance be interesting to investigate the effect of replacing the Tsetlin Automaton with alternative bandit algorithms, such as algorithms based on Thompson Sampling <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> or Upper Confidence Bounds <ref type="bibr" target="#b47">[48]</ref>.</p><p>Further, the more advanced Fully Connected Deep Tsetlin Machine, the Convolution Tsetlin Machine, and the Recurrent Tsetlin Machine architectures form a starting point for further exploration. These architectures can potentially improve pattern representation compactness and even learning speed. However, it is currently unclear how these architectures can be effectively trained.</p><p>Lastly, the high accuracy and robustness of the Tsetlin Machine, combined with its ability to produce self-contained easy-to-interpret propositional formulas for pattern recognition, makes it attractive for applied research, such as in the safety-critical medical domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A Tsetlin Automaton for two-action environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The Tsetlin Automata team for composing a clause.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The basic Tsetlin Machine architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The extended Tsetlin Machine architecture, introducing clause polarity, a summation operator collecting "votes", and a threshold function arbitrating the final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1</head><label>1</label><figDesc>The Tsetlin Machine Input Training data (X,? i ) ? S ? P (X, y i ), Number of clauses m, Output index i, Number of inputs o, Precision s, Threshold T Output Completely trained conjunctive clauses C i j ? C i for y i 1: function TrainTsetlinMachine(S, m, i, o, s, T ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>8 :i = 1 then 9 :</head><label>89</label><figDesc>if? if Random() ? T ?max(?T,min(T,f i (X)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>The mean accuracy of the Tsetlin Machine (y-axis) on the Noisy XOR Dataset for different values of the parameter s (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>The Axis &amp; Allies mini game.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>x 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8 x 9 x 10 x 11 x 12</head><label>12</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Accuracy (y-axis) for the Noisy XOR Dataset for different training dataset sizes (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>The mean test-and training accuracy per epoch for the Tsetlin Machine on the MNIST Dataset. We next evaluate the Tsetlin Machine on the MNIST Dataset of Handwritten Digits 9 [37], also investigating how learning progresses, epoch-by-epoch, in terms of accuracy. Note that the experimental results reported here can be reproduced with the demo found at https://github.com/cair/fast-tsetlin-machine-with-mnist-demo. The original dataset consists of 60 000 training examples, and 10 000 test examples. We binarize this dataset by replacing pixel values larger than 0.3 with 1 (with the original pixel grey tones ranging from 0.0 to 1.0). Pixel values below or equal to 0.3 are replaced with 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>The Multi-Class Tsetlin Machine. index i of the largest sum f (C i (X)) is outputted as the final output of the Tsetlin Machine:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>. The purpose of this architecture is to build composite propositional formulas, The fully connected Deep Tsetlin Machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :</head><label>13</label><figDesc>The Convolutional Tsetlin Machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The Binary Iris Dataset -accuracy on test data.</figDesc><table><row><cell>Technique</cell><cell>Mean</cell><cell cols="4">5 %ile 95 %ile Min. Max.</cell></row><row><cell cols="2">Tsetlin Machine 96.6 ? 0.05</cell><cell>95.0</cell><cell>98.3</cell><cell>94.2</cell><cell>99.2</cell></row><row><cell cols="2">Naive Bayes 92.4 ? 0.08</cell><cell>90.0</cell><cell>94.2</cell><cell>85.8</cell><cell>97.5</cell></row><row><cell cols="2">Logistic Regression 93.8 ? 0.07</cell><cell>92.5</cell><cell>95.8</cell><cell>90.0</cell><cell>97.5</cell></row><row><cell cols="2">Multilayer Perceptron Network 95.0 ? 0.07</cell><cell>93.3</cell><cell>96.7</cell><cell>92.5</cell><cell>98.3</cell></row><row><cell cols="2">SVM 96.7 ? 0.05</cell><cell>95.8</cell><cell>98.3</cell><cell>95.8</cell><cell>99.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>The Binary Iris Dataset -accuracy on training data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The Binary Digits Dataset -accuracy on test data.Performing poor on the test data and well on the training data indicates susceptibility to overfitting.Table 8reveals that the other techniques, apart from the Naive Bayes Classifier, perform significantly better on the training data, unable to transfer this performance to the test data.</figDesc><table><row><cell>Technique</cell><cell>Mean</cell><cell cols="4">5 %ile 95 %ile Min. Max.</cell></row><row><cell cols="2">Tsetlin Machine 100.0 ? 0.01</cell><cell>99.9</cell><cell>100.0</cell><cell>99.8</cell><cell>100.0</cell></row><row><cell>Naive Bayes</cell><cell>92.9 ? 0.07</cell><cell>92.4</cell><cell>93.5</cell><cell>91.3</cell><cell>93.7</cell></row><row><cell>Logistic Regression</cell><cell>99.6 ? 0.02</cell><cell>99.4</cell><cell>99.7</cell><cell>99.3</cell><cell>99.9</cell></row><row><cell>Multilayer Perceptron Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>The Binary Digits Dataset -accuracy on training data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>The Axis &amp; Allies Board Game Dataset.</figDesc><table><row><cell>Technique/Accuracy (%)</cell><cell>Mean</cell><cell cols="4">5 %ile 95 %ile Min. Max.</cell></row><row><cell cols="2">Tsetlin Machine 87.7 ? 0.0</cell><cell>87.4</cell><cell>88.0</cell><cell>87.2</cell><cell>88.1</cell></row><row><cell cols="2">Naive Bayes 80.1 ? 0.0</cell><cell>80.1</cell><cell>80.1</cell><cell>80.1</cell><cell>80.1</cell></row><row><cell cols="2">Logistic Regression 77.7 ? 0.0</cell><cell>77.7</cell><cell>77.7</cell><cell>77.7</cell><cell>77.7</cell></row><row><cell cols="2">Multilayer Perceptron Network 87.6 ? 0.1</cell><cell>87.1</cell><cell>88.1</cell><cell>86.6</cell><cell>88.3</cell></row><row><cell cols="2">SVM 83.7 ? 0.0</cell><cell>83.7</cell><cell>83.7</cell><cell>83.7</cell><cell>83.7</cell></row><row><cell cols="2">Random Forest 83.1 ? 0.1</cell><cell>82.3</cell><cell>83.8</cell><cell>81.6</cell><cell>84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>The Axis &amp; Allies Dataset -accuracy on test data.</figDesc><table><row><cell>Technique/Accuracy (%)</cell><cell>Mean</cell><cell cols="4">5 %ile 95 %ile Min. Max.</cell></row><row><cell cols="2">Tsetlin Machine 96.2 ? 0.1</cell><cell>95.7</cell><cell>96.8</cell><cell>95.5</cell><cell>97.0</cell></row><row><cell cols="2">Naive Bayes 81.2 ? 0.0</cell><cell>81.2</cell><cell>81.2</cell><cell>81.2</cell><cell>81.2</cell></row><row><cell cols="2">Logistic Regression 78.8 ? 0.0</cell><cell>78.8</cell><cell>78.8</cell><cell>78.8</cell><cell>78.8</cell></row><row><cell cols="2">Multilayer Perceptron Network 92.6 ? 0.1</cell><cell>91.5</cell><cell>93.6</cell><cell>90.7</cell><cell>94.2</cell></row><row><cell cols="2">SVM 85.2 ? 0.0</cell><cell>85.2</cell><cell>85.2</cell><cell>85.2</cell><cell>85.2</cell></row><row><cell cols="2">Random Forest 99.1 ? 0.0</cell><cell>98.8</cell><cell>99.4</cell><cell>98.6</cell><cell>99.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table /><note>The Axis &amp; Allies Dataset -accuracy on training data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>The Noisy XOR Dataset with Non-informative Features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Example of four clauses composed by the Tsetlin Machine for the XOR Dataset with Non-informative Features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>The Noisy XOR Dataset with Non-informative Features -accuracy on test data.Figure 9 shows how accuracy degrades with less data, when we vary the dataset size from 1000 examples to 20 000 examples. As expected, Naive Bayes and Logistic Regression guess blindly for all the different data sizes. The main observation, however, is that the accuracy advantage the Tsetlin Machine has over neural networks increases with less training data. Indeed, it turns out that the Tsetlin Machine performs robustly with small training data sets in all of our experiments.</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tsetlin Machine</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Neural Networks</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SVM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Logistic Regression</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Naive Bayes</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>2000</cell><cell>4000</cell><cell>6000</cell><cell>8000</cell><cell>10000</cell><cell>12000</cell><cell>14000</cell><cell>16000</cell><cell>18000</cell><cell>20000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3"># Training Examples</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc></figDesc><table /><note>A comparison of vanilla machine learning algorithms with the Tsetlin Machine, directly on the original unenhanced MNIST dataset (NN -Neural Network).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we have decided to use the binary representation 0/1 to refer to the truth values False/True. These can be used interchangeably throughout the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We found the Tsetlin Machine on propositional logic, which can be mapped to Boolean algebra, and vice versa.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/iris].<ref type="bibr" target="#b3">4</ref> In this experiment, we use a Multi-Class Tsetlin Machine, described in Section 6.1. We also apply Boosting of True Positive Feedback to Include Literal actions as described in Section 3.5.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://avalonhill.wizards.com/games/axis-and-allies</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">In this experiment, we used a Multi-Class Tsetlin Machine, described in Section 6.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">http://www.pymvpa.org/datadb/mnist.html<ref type="bibr" target="#b9">10</ref> In this experiment, we used a Multi-Class Tsetlin Machine, described in Section 6.1. We also applied Boosting of True Positive Feedback to Include Literal actions, as described in Section 3.5.3.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>I thank my colleagues from the Centre for Artificial Intelligence Research (CAIR), Lei Jiao, Xuan Zhang, Geir Thore Berge, Bernt Viggo Matheussen, Saeed Rahimi Gorji, Darshana Abeyrathna, Sondre Glimsdal, Morten Goodwin, Jivitesh Sharma, Ahmed Abouzeid, and Rahele Jafari, for comments that greatly improved the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Availability</head><p>Source code and datasets for the Tsetlin Machine, available under the MIT Licence, can be found at https://github.com/cair/TsetlinMachine and https://github.com/cair/fasttsetlin-machine-with-mnist-demo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On behaviour of finite automata in random medium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Tsetlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avtomat. i Telemekh</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1354" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning Automata: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L</forename><surname>Thathachar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Some aspects of the sequential design of experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bandit processes and dynamic allocation indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gittins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="177" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Theory of Finite Automata With an Introduction to Formal Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving Stochastic Nonlinear Resource Allocation Problems Using a Hierarchy of Twofold Resource Allocation Automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="560" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using Finite State Automata to Produce Self-Optimization and Self-Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kleinrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="47" to="61" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Automata-based Solutions to the Nonlinear Fractional Knapsack Problem with Applications to Optimal Resource Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Myrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="175" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic Searching on the Line and its Applications to Parameter Learning in Nonlinear Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="733" to="739" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A novel strategy for solving the stochastic point location problem using a hierarchical searching scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">John</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2202" to="2220" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A solution to the stochastic point location problem in metalevel nonstationary environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="466" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Solving the Satisfiability Problem Using Finite Learning Automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouhmala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic Learning for SAT-Encoded Graph Coloring Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouhmala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Metaheuristic Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Service selection in stochastic environments: A learning-automaton based solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="617" to="637" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A two-armed bandit collective for hierarchical examplar based mining of frequent itemsets with applications to intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haugland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kj?lleberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computational Collective Intelligence XIV</title>
		<imprint>
			<biblScope unit="volume">8615</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal sampling for estimation with constrained resources using a learning automaton-based solution for the nonlinear fractional knapsack problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning-Automaton-Based Online Discovery and Tracking of Spatiotemporal Event Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1118" to="1130" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deterministic Learning Automata Solutions to The Equipartitioning Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A streaming sampling algorithm for social activity networks using fixed structure learning automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Meybodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Routing bandwidth-guaranteed paths in MPLS traffic engineering: A multiple race track learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the analysis of a random walk-jump chain with treebased transitions and its applications to faulty dichotomous search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B. John</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sequential Analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Symmetrical Hierarchical Stochastic Searching on the Line in Informative and Deceptive Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="635" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Routing bandwidth-guaranteed paths in MPLS traffic engineering: a multiple race track learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="959" to="976" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Networks of Learning Automata: Techniques for Online Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L</forename><surname>Thathachar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pattern-Recognizing Stochastic Learning Automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="375" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Induction of Decision Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interpretable Two-Level Boolean Rule Learning for Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><forename type="middle">M</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malioutov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Human Interpretability in Machine Learning (WHI 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Bayesian Framework for Learning Rule Sets for Interpretable Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macneille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morgenstern</surname></persName>
		</author>
		<title level="m">Theory of Games and Economic Behavior</title>
		<imprint>
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Automata-based Solutions to the Nonlinear Fractional Knapsack Problem withApplications to Optimal Resource Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Myrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="175" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pattern Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining Finite Learning Automata with GSAT for the Satisfiability Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouhmala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence (EAAI)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="715" to="726" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The complexity of theorem-proving procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third annual ACM symposium on Theory of computing -STOC &apos;71</title>
		<meeting>the third annual ACM symposium on Theory of computing -STOC &apos;71</meeting>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons, Inc</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Fully Connected Deep Tsetlin Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preparation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Convolutional Tsetlin Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preparation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Recurrent Tsetlin Machine</title>
	</analytic>
	<monogr>
		<title level="m">Preparation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian Learning Automaton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Computing and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="234" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimistic Bayesian sampling in contextual-bandit problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Korda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Leslie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2069" to="2106" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An Empirical Evaluation of Thompson Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
