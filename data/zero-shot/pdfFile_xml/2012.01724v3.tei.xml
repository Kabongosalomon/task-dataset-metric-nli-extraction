<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARALLEL RESIDUAL BI-FUSION FEATURE PYRAMID NETWORK FOR ACCURATE SINGLE-SHOT OBJECT DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-08">September 8, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Yang</forename><surname>Chen</surname></persName>
							<email>pingyang.cs08@nycu.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
							<email>mchang2@albany.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Wei</forename><surname>Hsieh</surname></persName>
							<email>jwhsieh@nycu.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
							<email>yschen@nycu.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Yang Ming Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science University at Albany, SUNY</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Artificial Intelligence and Green Energy National Yang Ming</orgName>
								<orgName type="institution">Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Yang Ming Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PARALLEL RESIDUAL BI-FUSION FEATURE PYRAMID NETWORK FOR ACCURATE SINGLE-SHOT OBJECT DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-08">September 8, 2022</date>
						</imprint>
					</monogr>
					<note>Code is available at https://github.com/pingyang1117/PRBNet_PyTorch</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) for fast and accurate single-shot object detection. Feature Pyramid (FP) is widely used in recent visual detection, however the top-down pathway of FP cannot preserve accurate localization due to pooling shifting. The advantage of FP is weakened as deeper backbones with more layers are used. In addition, it cannot keep up accurate detection of both small and large objects at the same time. To address these issues, we propose a new parallel FP structure with bi-directional (top-down and bottom-up) fusion and associated improvements to retain high-quality features for accurate localization. We provide the following design improvements: (1) A parallel bifusion FP structure with a bottom-up fusion module (BFM) to detect both small and large objects at once with high accuracy.</p><p>(2) A concatenation and re-organization (CORE) module provides a bottom-up pathway for feature fusion, which leads to the bi-directional fusion FP that can recover lost information from lower-layer feature maps. (3) The CORE feature is further purified to retain richer contextual information. Such CORE purification in both top-down and bottom-up pathways can be finished in only a few iterations. (4) The adding of a residual design to CORE leads to a new Re-CORE module that enables easy training and integration with a wide range of deeper or lighter backbones. The proposed network achieves state-of-the-art performance on the UAVDT17 and MS COCO datasets. Code is available at In general, detecting small objects is more difficult than detecting large objects. Both high-level and low-level features are required to discriminate and localize objects among background and other objects. YOLOv3 [2] maintains detailed grid features to retain detection accuracy of small objects. However, the effectiveness is limited, as accurate detection of both small and large objects cannot be kept together at the same time. The best performing method from LPIRC 2019 challenge <ref type="bibr" target="#b6">[7]</ref> shows improvement on detecting general-sized objects but not small objects on the COCO dataset <ref type="bibr" target="#b7">[8]</ref>.</p><p>How to design fast and accurate network that can effectively detect all object sizes is still an open question. One solution to retain accurate feature localization is to add a bottom-up pathway to offset the lost information from low-level feature maps. In <ref type="bibr" target="#b8">[9]</ref>, the adding of a gating module on the SSD frame leads to a gated bi-directional FP; however such gated network is not easily trainable. In <ref type="figure">[10]</ref>, a bottom-up path aggregation network was proposed for object segmentation. The bi-directional network of [6] can efficiently circulate both low-level and high-level semantic information for small object detection. In [11], a BiFPN was proposed based on NAS-FPN [12] to better detect small objects with high efficiency. In YOLOv4 [13], path aggregation [10] was modified by replacing the addition with concatenation for better detection of small objects. However, such BiFPN structure still cannot keep up accurate detections of both small and large objects all together.</p><p>We propose a new Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) with a parallel design and multiple improvements that can retain both deeper and shallower features for fast and accurate single-shot object detection. Different from other bi-fusion FPN structures such as PANet [10], NAS-FPN [12], and BiFPN [11], we create a parallel bi-fusion structure to fuse three-layers of feature maps in parallel to generate three prediction maps at the same time, see <ref type="figure">Fig. 1</ref>. Without losing efficiency, these three-way prediction maps can retain more accurate semantic and localization information to better detect both tiny and large objects. In this parallel structure, we introduce a new concatenation and re-organization (CORE) module for data fusion, where output features can be further purified to retain contextual information. We introduce a "residual" design (motivated from the spirit of ResNet [5]) into our bi-fusion pipeline, which enables easy training and integration with a number of popular backbones. We will show that our residual FP design outperforms other bi-directional methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> in Section 4. In comparison, methods based on traditional FPs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> can only learn un-referenced features, thus they are not suitable for detecting both large and small objects. Our residual FP retains semantic richer features in higher layers that can better detect small objects.</p><p>A key novelty in our design is the adding of parallelization to the bi-fusion FPN architecture. This parallel design is more effective in feature representation, i.e. for capturing features to identify and localize objects in either small or large sizes without losing efficiency. In comparison, most existing bi-directional FP methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref> directly concatenate large feature maps in a memory-consuming way, which ends up with an even larger feature map.</p><p>The proposed PRB-FPN is simple, efficient, and suitable for generic object detection for multiple object classes and sizes (small, mid, and large). We will show in Section 4 that our approach is generalizable in combining with mainstream backbones including Pelee [17] and DarkNet53 <ref type="bibr" target="#b1">[2]</ref>. It can run in real-time and is easily deployable to edge devices. Main contributions of this paper are summarized in the following:</p><p>? We propose a new Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) that can effectively fuse both deep and shallow feature layers in parallel for fast and accurate one-shot object detection.</p><p>? The parallel design of PRB-FPN makes it well-suited for detecting objects in both small and large sizes with higher accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual object detection has improved significantly in the state-of-the-art (SoTA) models. Recent deep models including FPN <ref type="bibr" target="#b0">[1]</ref>, YOLOv3 <ref type="bibr" target="#b1">[2]</ref>, and SSD <ref type="bibr" target="#b2">[3]</ref> typically consist of three components: (1) a deep feature extraction backbone e.g. DarkNet-53 <ref type="bibr" target="#b3">[4]</ref> or ResNet-101 <ref type="bibr" target="#b4">[5]</ref>, (2) a feature pyramid (FP), and (3) an object classifier. To ensure high detection accuracy, most SoTA object detectors adopt deep CNN structures that can achieve impressive performance in detecting large and medium sized objects. However the performance for detecting smaller objects are still inferior <ref type="bibr" target="#b5">[6]</ref>. This is mainly because the feature map resolution is reduced after simple pooling in the FP. Tiny objects (&lt; 32 ? 32 pixels) can turn into about a single-pixel feature vector in the last layer of FP, causing insufficient spatial resolution for accurate discrimination. On the other hand, using a shallow backbone increases the computational efficiency. This comes with the drawback of reduced detection performance, as the capability to retain contextual and semantic features also decreases directly. ? The PRB-FPN can be easily trained and integrated with different backbone thanks to the residual design. A newly proposed bottom-up fusion module (BFM) can improve the detection accuracy of both small and large objects. ? Extensive experiments on Pascal VOC <ref type="bibr" target="#b17">[18]</ref> and MS COCO <ref type="bibr" target="#b7">[8]</ref> datasets show that PRB-FPN achieves the SoTA results for accurate and efficient object detection. Results also show great generalization ability on various object sizes and types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Object detection is a very active field in computer vision since the blooming of deep learning. The extensive amount of literature can be organized into two categories based on their network architectures: two-stage proposal-driven and one-stage (single-shot) approaches. In general, two-stage methods can achieve high detection accuracy but with longer computation time, while one-stage methods run faster with inferior accuracy. We focus on the survey of one-stage object detectors. RefineDet <ref type="bibr" target="#b18">[19]</ref> employs an encode-decode structure in the deeper network with the use of up-sampling deeper scale features to enrich contextual information. PeLee <ref type="bibr" target="#b16">[17]</ref> is a variant of DenseNet <ref type="bibr" target="#b19">[20]</ref> that outperforms SSD+MobileNet by 6.53% on the Stanford Dogs dataset <ref type="bibr" target="#b20">[21]</ref> based on a much shallower network. However, PeLee <ref type="bibr" target="#b16">[17]</ref> does not detect small objects well on MS COCO <ref type="bibr" target="#b7">[8]</ref>. PFPN <ref type="bibr" target="#b14">[15]</ref> adopts the VGGNet-16 backbone <ref type="bibr" target="#b21">[22]</ref> and SPP to generate a feature pyramid by concatenating multi-scale features.</p><p>One-stage object detectors mostly consist of a backbone network and a predictor. The backbone is a stacked feature map representing the input image in high feature resolution (but low spatial resolution for abstraction). The backbone network can be pre-trained as an image classifier on a large dataset such as ImageNet. OverFeat <ref type="bibr" target="#b8">[9]</ref> was the first CNN-based one-stage object detector developed in 2013 with a sliding-window paradigm. Two years later, the first version of YOLO <ref type="bibr" target="#b22">[23]</ref> achieved state-of-the-art performance by integrating bounding box proposals and subsequent feature re-sampling in a single stage. SSD <ref type="bibr" target="#b2">[3]</ref> employed in-network multiple feature maps for detecting objects with varying shapes and sizes. The multi-map design enabled SSD with better robustness over YOLOv1 <ref type="bibr" target="#b22">[23]</ref>. For better detection of small objects, The Feature Pyramid Network (FPN) <ref type="bibr" target="#b0">[1]</ref> based on FP can achieve higher detection accuracy for small objects. YOLOv3 <ref type="bibr" target="#b1">[2]</ref> was developed by adopting the concept of FPN. By changing the backbone from DarkNet-19 <ref type="bibr" target="#b3">[4]</ref> to DarkNet-53, YOLOv3 achieves superior performance in 2018. Similarly, RetinaNet <ref type="bibr" target="#b23">[24]</ref> combines FPN <ref type="bibr" target="#b0">[1]</ref> and ResNet <ref type="bibr" target="#b4">[5]</ref> as the backbone. RetinaNet used focal loss to significantly reduce false positives in a single stage, such that the weights of each anchor box can be dynamically adjusted. Shift-invariance in CNNs was originally achieved using sub-sampling layers. The work of <ref type="bibr" target="#b24">[25]</ref> evaluated the effect of small geometry perturbations on CNN and suggested that max-pooling is more effective in object detection and classification. In <ref type="bibr" target="#b25">[26]</ref>, a pooling-after-blurred technique was proposed by combing blurring and sub-sampling techniques to ensure shift-invariance.</p><p>Feature pyramid (FP) is widely used in SoTA detectors for detecting objects at different scales, where spatial and contextual features are extracted from the last layer of the top-down path for accurate object detection. This top-down aggregation is now a common practice for improving scale invariance in both two-stage and one-stage detectors. Popular FPs used for this purpose include the pyramidal feature hierarchy (bottom-up), hourglass (bottom-up and top-down), FPN <ref type="bibr" target="#b0">[1]</ref>, SPP <ref type="bibr" target="#b26">[27]</ref>, and PFPN <ref type="bibr" target="#b14">[15]</ref>. It is also well-known that the top-down pathway in FP cannot preserve accurate object localization due to the shift-effect of pooling.</p><p>Bi-directional FP can recover lost information from shallow layers to improve small object detection in several works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. A gating module was used to control the feature flow direction in <ref type="bibr" target="#b13">[14]</ref>. A light-weight scratch network and a bi-directional network were constructed in <ref type="bibr" target="#b5">[6]</ref> to efficiently circulate both low-and high-level semantic information. M2Det <ref type="bibr" target="#b27">[28]</ref> is a one-stage detector that outperforms most 2019 methods on all multi-scale categories on MS COCO <ref type="bibr" target="#b7">[8]</ref>. However, the M2Det model is complicated and time-consuming, thus is not suitable for real-time object detection. Inspirited by NAS-FPN <ref type="bibr" target="#b11">[12]</ref>, a BiFPN was proposed in <ref type="bibr" target="#b10">[11]</ref> to better detect small objects with higher efficiency. The recent YOLOv4 <ref type="bibr" target="#b12">[13]</ref> modified the path aggregation method <ref type="bibr" target="#b9">[10]</ref> by replacing the addition with concatenation to better detect small objects. However, this BiFPN structure still cannot keep up accurate detection of both small and large objects all together.</p><p>Multi-Scale Object Detectors face the challenge of small-size false positives due to the inadequacy of low-level features, which result in small receptive field size and weak semantic capabilities. The work of <ref type="bibr" target="#b28">[29]</ref> demonstrates that independent predictions from different feature layers on the same region are beneficial in reducing false positives. In <ref type="bibr" target="#b29">[30]</ref>, a novel paradigm of multi-scale deep network is developed to model the spatial contexts surrounding different pixels at various scales. In <ref type="bibr" target="#b30">[31]</ref>, deep convolutional networks are used to obtain multi-scaled features, where deformable convolutional structures are added to overcome geometric transformations.</p><p>Anchor-free methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> do not reply on handcrafted anchors, thus are free of issues commonly associated with anchor-based designs. In <ref type="bibr" target="#b31">[32]</ref>, corner features are detected for object detection. By inheriting the architecture of R-CNN, ME R-CNN <ref type="bibr" target="#b34">[35]</ref> used multiple stream pipelines for accurate anchor-free object detection, where one pipeline is an expert for processing a certain type of ROIs and controlled by an expert assignment network. A cascade anchor refinement module is proposed in <ref type="bibr" target="#b15">[16]</ref> to refine pre-designed anchors. This is then injected into a bidirectional FP, which can detect objects with highly accurate localization. However, one pass of regression during training is not accurate enough for detection in this anchor-free approach. In <ref type="bibr" target="#b35">[36]</ref>, an attention CoupleNet was proposed by designing a cascade attention structure to generate class-agnostic attention maps of target regions so that a discriminative feature representation can be formulated for part-based object detection. In <ref type="bibr" target="#b36">[37]</ref>, Jin et al. used an adaptive anchor generator to generate all possible anchor boxes. They then proposed a semi-anchor-free network for object detection with an enhanced feature pyramid which consists of two modules, i.e., adaptive feature fusion module (AFFM) and self-enhanced module (SEM). In <ref type="bibr" target="#b32">[33]</ref>, a number of low-quality bounding boxes are predicted and further verified with a centerness branch that can detect objects without using any pre-defined anchor boxes. In <ref type="bibr" target="#b37">[38]</ref>, a hierarchical shot detector is used to predict detection bounding boxes via regression. These regression based methods are more accurate but less efficient. Compared with CornerNet <ref type="bibr" target="#b31">[32]</ref>, FoveaBox <ref type="bibr" target="#b38">[39]</ref> does not require any embedding or grouping techniques at post-processing stage to locate real bounding boxes. However, its latency is higher and results in lower efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Transferring</head><p>The above detectors can be trained well enough from a large set of sufficiently representative data. However, as there exists numerous application scenarios in which only a few training samples (e.g. tumor images in medical applications) are available, transfer learning can be used to customize the model and adopt to the tasks <ref type="bibr" target="#b39">[40]</ref>. For example, in <ref type="bibr" target="#b40">[41]</ref>, a weakly-shared Deep Transfer Network (DTN) was proposed to hierarchically learn and transfer semantic knowledge from web texts to images for image classification. In <ref type="bibr" target="#b41">[42]</ref>, a novel generalized DTNs was proposed to solve the problem of insufficient training images by transferring label information across heterogeneous domains, such as transferring from the textual to visual domain for image classification. Moreover, in <ref type="bibr" target="#b42">[43]</ref>, a transfer learning system named GAIA was proposed to provide powerful pre-trained weights, select models, and collect relevant data for object detection when only a few training samples were given. However, although network transferring methods can provide performance improvements, they cannot outperform ordinary methods trained with sufficient samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We first motivate the design of our proposed network architecture by addressing the limitation of the Feature Pyramid (FP) for visual object detection. In Section 3.1, details of our new parallel bi-fusion scheme are described. The adding of parallelization to the bi-fusion FPN architecture can better capture features for both small and large objects </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parallel Concatenation and Re-organization Feature Bi-Fusion Architecture</head><p>Feature Pyramid (FP) is widely-used in top-down feature aggregation that can collect semantically rich features to effectively discriminate objects with scale invariance. However, it is well-known that FP cannot preserve accurate localization for small objects due to pooling and quantization. The winning methods of LPIRC 2019 challenge <ref type="bibr" target="#b6">[7]</ref> show improvements on detecting general-sized objects but not on small objects. There object prediction was carried out using information from both each pyramid layer and the respective lower layers. This coincide with thoughts from several SoTA bi-directional methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> in leveraging new feature streams from lower feature layers (or the raw image itself) to keep track of features from smaller objects and achieve more accurate localization. Such bi-fusion modules specially designed for improving small object detection still lack capabilities in detecting larger objects.</p><p>In this paper, we propose an effective parallel FP fusion design to tackle this difficult problem of object detection considering all object scales. This is done by creating multiple bi-fusion paths to keep tracks of features that are suitable to detect objects of all sizes (including tiny and large objects). Each bi-fusion path keeps track of size-dependent features to represent objects at a specific scale. Assume that there are N prediction maps (where N = 3 for YOLOv4), we propose to execute the N different concurrent fusion paths to generate N fused feature maps for the N prediction maps. We use n to index the bi-fusion modules (thus n ? N ). Let L be the level of the top layer in the FP. As shown in <ref type="figure">Fig. 1</ref>, the n th bi-fusion module will bi-fuse feature maps from the (L ? n + 1) th layer to the (L ? n ? N + 2) th layer in the backbone. The s th output will be fed into the s th prediction map for object detection, which will integrate feature maps from the s th layer of all bi-fusion modules. Noticeably, the 1 st bi-fusion module in our model corresponds to the sole bi-fusion module in SoTA bi-directional methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Concatenation and Re-organization for Feature Bi-Fusion</head><p>In <ref type="figure" target="#fig_0">Fig. 2</ref>, each bi-fusion module consists of three concatenation and re-organization (CORE) blocks and two skip connections. Details of the bi-fusion module are shown in <ref type="figure" target="#fig_0">Fig. 2</ref> To avoid using too many dithering operations (i.e., point-wise convolutions) and to avoid computationally expensive operations (i.e., pooling and addition), we adopt an 1 ? 1 depth-wise convolution in the CORE module. This enables effective fusion of pathways coming from deeper and shallower layers in each layer of the FP. Our 1 ? 1 depth-wise convolution in CORE is very different from most of SoTA bi-directional methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, where feature fusion is carried out by concatenating all feature maps. Their simple concatenations result in a large feature 5 map proportional to the total feature size. In contrast, our 1 ? 1 conv filter in CORE is automatically learned, such that features can be fused more effectively via a feature map of fixed size.</p><p>In each layer of the used backbone, CORE fuses features of each layer with its two adjacent (immediately shallower and deeper) layers. In other words, feature bi-fusion is performed in the feature pyramid of CORE. In the bottom-up fusion with the shallower layer, similar to YOLOv2 <ref type="bibr" target="#b3">[4]</ref>, a Re-Org block from <ref type="figure" target="#fig_1">Fig. 3</ref>(b) is adopted in <ref type="figure" target="#fig_0">Fig. 2</ref>(b) to re-organize the feature map into 4 channels. However, instead of using a concatenation operation, the 1 ? 1 convolution filter is then performed to fuse all feature maps as the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Residual Bi-Fusion Feature Pyramid</head><p>We further adopt the residual concept inspired from ResNet <ref type="bibr" target="#b4">[5]</ref> to the CORE block in our design, and created a new Residual CORE (Re-CORE) block. Re-CORE enables the fusion of four adjacent scales (namely, the shallow, current, deep, and deeper layers) for better detection of small objects. Specifically, by recursively injecting the output of the (i + 1) th CORE module to the i th CORE module, the Bi-Fusion FP becomes a fully-featured Residual Bi-Fusion FPN as in <ref type="figure" target="#fig_1">Fig. 3</ref>(a). <ref type="figure" target="#fig_0">Fig. 2</ref>(c) depicts the connection between the Re-CORE and Convolution modules, in which F i and ?F i denote the outputs of the i-th Re-CORE and Convolution modules, respectively. Working with popular backbones: Similar to ResNet <ref type="bibr" target="#b4">[5]</ref>, the residual nature of our Re-CORE module enables easy training and integration of the FP with a wide range of backbones that works particularly well for small object detection. Instead of learning un-referenced features, Re-CORE obtains better accuracy from the largely increased feature depths when compared with traditional FPs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>. Note that SoTA FPs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15]</ref> often learn redundant features and perform poorly on small object detection.</p><p>The Re-CORE module provides a new effective fusion approach for collecting localization information from bottom layers that can improve the accuracy of small object detection. In comparison, the naive approach in <ref type="bibr" target="#b43">[44]</ref> detects small objects by generating high-resolution images as inputs to the detection module, which comes with a cost of large computational burden. Another approach for small object detection is to leverage contextual information, by sending semantic features from a top-down way via a FP as in YOLOv3 <ref type="bibr" target="#b1">[2]</ref>. However, in these methods without the use of residual property, the learning will include un-referenced features and thus bound the number of FP layers that can actually contribute to object detection. In comparison to the FP proposed in <ref type="bibr" target="#b6">[7]</ref>, our Re-CORE module can capture richer semantic features from deeper layers that can directly improve small object detection.</p><p>In summary, our residual design and bi-directional fusion make the Re-CORE module suitable for detecting small and even tiny objects without notable computation overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bottom-up Feature Fusion</head><p>As aforementioned, the top winning method in LPIRC 2019 <ref type="bibr" target="#b6">[7]</ref> improved detection on large and medium-sized objects, but not able to keep up the performance for small objects. To address this issue, we propose the adding of a bottom-up fusion module (BFM) to the PRB-FPN network to further improve the localization of both small and large objects. <ref type="figure" target="#fig_1">Fig. 3</ref>(c) depicts the proposed BFM architecture. Instead of using convolution with stride 2 (adopted in PANet <ref type="bibr" target="#b9">[10]</ref>, Bi-FPN <ref type="bibr" target="#b10">[11]</ref>, or YOLOv4 <ref type="bibr" target="#b12">[13]</ref>), the BFM adopts a Re-Org block to split C channels of feature map into 4C channels to better preserve spatial information and generate robust semantic features via 1 ? 1 convolution, which improves small object detection. As for the bidirectional FPN-BPN work <ref type="bibr" target="#b15">[16]</ref>, convolutions with stride 2 are used for down-sampling, while de-convolutions are adopted for up-sampling. However, this design results in lower accuracy for small object detection due to the stride 2 operator, and the use of de-convolution leads to low efficiency in object detection.</p><p>In summary, in our design: (1) Section 3.1 describes the use of parallel bi-fusion paths that run concurrently for effective detection of both small and large objects; (2) Section 3.3 describes the adding of the Re-CORE module for improving the detection of small objects; and finally, (3) the BFM in Section 3.4 can bring specific local information from a bottom-up pathway to localize the objects more accurately. The BFM pathway works particularly well for detecting both large and mid-sized objects. Experimental results in this regard are shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate the PRB-FPN against SoTA object detection methods on MS COCO benchmark <ref type="bibr" target="#b7">[8]</ref> and UAVDT <ref type="bibr" target="#b44">[45]</ref> using machines with nVidia Titan X GPU and V100. Accuracy is evaluated in the metric of Average Precision (AP). Computational efficiency is evaluated in the processing frames per second (FPS).</p><p>Backbones. Our pipeline is not limited to any feature extraction backbone. We evaluated the following backbones: PeLee <ref type="bibr" target="#b16">[17]</ref>, MobileNet-V2 <ref type="bibr" target="#b45">[46]</ref>, DarkNet-53 <ref type="bibr" target="#b1">[2]</ref>, VGG16 <ref type="bibr" target="#b21">[22]</ref>, ResNet-50 <ref type="bibr" target="#b4">[5]</ref>, DenseNet <ref type="bibr" target="#b19">[20]</ref>, CSPnet <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details and Evaluation Configures</head><p>Implementation details: For performance evaluations on MS COCO dataset, the default hyper-parameters are set as follows. Total training steps are 500, 500 with the step decay learning rate 0.001. The learning rate is further multiplied by a factor 0.01 at the 400, 000 steps and 450, 000 steps, respectively. Momentum and weight decay rate are set to be 0.9 and 0.0005, respectively. All various PRB models were trained on a single V100 with batch size 64, and mini-batch size 16, 8, or 4 depended on the used model size for fitting the limitation of the available GPU RAM.</p><p>Evaluation details: We next evaluate the newly introduced designs of PRB-FPN in terms of how each design effectively fuse both deep and shallow feature layers in parallel for fast and accurate one-shot object detection. The first major design in the parallel structure of PRB-FPN is the new residual concatenation and re-organization </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BFM Accuracy Improvements</head><p>Since the BFM in PRB-FPN is designed to detect both small and large objects, we evaluate the effectiveness of BFM on object detection based on the MS COCO dataset across four backbones, namely PeLee <ref type="bibr" target="#b16">[17]</ref>, DarkNet-53 <ref type="bibr" target="#b1">[2]</ref>, VGG16 <ref type="bibr" target="#b21">[22]</ref>, and DenseNet <ref type="bibr" target="#b19">[20]</ref>), to evaluate the generalization capability of BFM. <ref type="table" target="#tab_1">Table 1</ref> shows this BFM ablation study results. Observe that the BFM computational load is very light and can be ignored for all backbones. Also observe the generalizability of BFM in maintaining high AP across these backbones for detecting different object sizes. <ref type="table" target="#tab_1">Table 1</ref> also shows another important observation that BFM can improve the detection accuracy of a shallower backbone more than deeper backbone. Specifically, the improvements on AP50 with BFM for DarkNet <ref type="bibr" target="#b1">[2]</ref>, Pelee <ref type="bibr" target="#b16">[17]</ref>, and DenseNet <ref type="bibr" target="#b19">[20]</ref> are 6.5%, 3.5%, and 0.2%, respectively. This indicates that BFM improves the detection of large objects better than  <ref type="table">Table 3</ref>: Ablation study of Re-CORE and BFM; RB denotes the proposed Residual Bi-Fusion design as in <ref type="figure" target="#fig_1">Fig.3(a)</ref>. smaller objects. Thus, BFM can provide a good solution for applications that demands the detection of arbitrary-sized objects.</p><p>In addition to the ablation study of our BFM method among different backbones, <ref type="table" target="#tab_2">Table 2</ref> shows the comparisons against other SoTA bifusion methods in terms of accuracy and efficiency. When the backbones ResNet-50 and EfficientNet are adopted, our BFM method outperforms EfficientDet-D0 <ref type="bibr" target="#b10">[11]</ref> and NAS-FPN <ref type="bibr" target="#b11">[12]</ref>. As for the bidirectional FPN-BPN <ref type="bibr" target="#b15">[16]</ref>, their convolutions with stride 2 for down-sampling result in lower accuracy in small object detection. In addition, their de-convolution for upsampling results in lower efficiency for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BFM with Re-CORE Accuracy Improvements</head><p>We evaluate the effectiveness of BFM with Re-CORE for Residual Bi-Fusion object detection. <ref type="table" target="#tab_0">Table 4</ref>.1 shows the ablation study of the PRB-FPN vs. YOLOv3 and YOLOv4 with or without BFM Re-CORE. As a result, PRB-FPN outperforms YOLOv3 in all categories. Note that the frame rates with or without BFM are very similar. For input size 512 ? 512, YOLOv3 with BFM also outperforms YOLOv3 alone on all categories. BFM improves the detection of small objects significantly, with an increasing trend as the input image size increases. On the contrary, improvements on the large objects have a decreasing trend as the input size increases.   <ref type="figure">4</ref> shows the ablation study comparisons of object detectors regarding the effects of BFM and Re-CORE modules on a selected image from COCO-test-dev. <ref type="figure">Fig. 4(b)</ref> shows detections obtained by YOLOv3. <ref type="figure">Fig. 4(c) and (d)</ref> show detections of YOLOv3 with BF and Re-CORE modules, respectively. <ref type="figure">Fig. 4(e)</ref> shows detections of the proposed PRB-FPN. In comparison, <ref type="figure">Fig. 4(f)</ref> shows detections obtained by M2Det <ref type="bibr" target="#b27">[28]</ref>. Observe clearly that the proposed PRB-FRN outperforms YOLOv3, YOLOv4 and M2Det. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the comparisons of object detection against LRFNet <ref type="bibr" target="#b5">[6]</ref> on a 1024 ? 540 image from the UAVDT17 benchmark <ref type="bibr" target="#b44">[45]</ref>. Note that the black masks in <ref type="figure" target="#fig_5">Fig. 5</ref> come with the original images in UAVDT for privacy protection. LRFNet fails to detect the tiny far-away vehicles from the camera view, while PRB-FPN can successfully detect most of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PRB-FPN vs the Original FPN</head><p>We compare the performance the proposed PRB-FPN vs. the original FPN on the UAVDT <ref type="bibr" target="#b44">[45]</ref> benchmark. Performance evaluation on the MS COCO dataset is omitted, as it contains very few samples of small objects. <ref type="table" target="#tab_0">Table 4</ref> shows the performance comparisons with and without the proposal parallel or the residual bi-fusion modules. We adopted two backbones, namely MobileNet-V2 <ref type="bibr" target="#b45">[46]</ref> and CSPDarknet-53 <ref type="bibr" target="#b46">[47]</ref> in this evaluation. CSPDarknet-53 was created in our previous framework and is now adopted in YOLOv4. The baseline of FPN is the single bi-fusion module adopted in SoTA bi-fusion methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>.  When the MobileNet-V2 backbone is used, accuracy of the baseline method (i.e. single bi-fusion module) is 29.7%. In comparison, as the Re-CORE module is added, the accuracy improves from 29.7% to 34.2%. However, the score is still lower than LRFNet <ref type="bibr" target="#b5">[6]</ref>, SpotNet <ref type="bibr" target="#b66">[67]</ref>, and CenterNet <ref type="bibr" target="#b67">[68]</ref>, since MobileNet-V2 is a very lightweight network. Finally, after the parallel FP bi-fusion design is included, the accuracy improves significantly from 34.3% to 65.47%, which outperform all comparison methods. Note that our PRB-FPN achieves double the amount of accuracy and triple amount of efficiency over RetinaNet <ref type="bibr" target="#b23">[24]</ref>.</p><p>Evaluation with the CSPDarknet-53 backbone is shown in the last three rows of <ref type="table" target="#tab_0">Table 4</ref>. Accuracy improvement is significant from 64.52 % to 76.55 %. These evaluations suggest that our parallel FP bi-fusion design is general for accuracy improvement. <ref type="table" target="#tab_6">Table 5</ref> shows the effects on accuracy and FPS of the number of feature pyramid layers. Two backbones, i.e., ResNet-50 and CSPDarknet-53 were adopted in this evaluation. Clearly, with more FP layers, higher accuracy can be obtained for both backbones. Also the use of more FP layers results in lower FPS.    <ref type="figure">6</ref> shows visual comparisons of object detection between YOLOv4 and PRB-FPN. YOLOv4 cannot detect both large and small objects well enough. The enlargement of input image to detect small objects in YOLOv4 often fails in detecting large objects and undesired inefficiency. In <ref type="figure">Fig. 6(a)</ref>, the cargo truck was missed by YOLOv4 but successfully detected by our PRB-FPN. In addition, YOLOv4 often detects a large object as several small ones. As shown <ref type="figure">Fig. 6(c)</ref>, a truck was detected as two cars, the PRB-FPN can detect it successfully in <ref type="figure">Fig. 6(d)</ref>. Note that the stop sign in <ref type="figure">Fig. 6</ref>(c) was missed by YOLOv4. Without the enlargement, YOLOv4 will further miss-detect or incorrectly classify small objects. For example, the small persons on the playground (highlighted in a red circle) in <ref type="figure">Fig. 7</ref>(a) were missed by YOLO 4, while PRB-FPN can successfully detect them in <ref type="figure">Fig. 7(b)</ref>. Also, in <ref type="figure">Fig. 7(a)</ref>, a small building was wrongly detected as a bus by YOLOv4. High detection rate and high recall rate for small objects are the major characteristics of our PRB model. <ref type="figure" target="#fig_8">Fig. 8(b)</ref> shows a failure case of our method. A false detection of a pedestrian (shown with a red circle) occurred due to the dark background. In comparison, YOLOv4 detection results in <ref type="figure" target="#fig_8">Fig. 8(a)</ref> is visually better without a false detection, however this is due to the weakness of YOLOv4 in identifying small objects. Such weakness of YOLOv4 can explain the miss detection of small vehicles and trucks in <ref type="figure" target="#fig_8">Fig. 8(a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons with State-of-The-Art Models</head><p>Tables 6 and 7 compare the PRB-FPN with and without parallelization design against other SoTA object detectors in terms of accuracy and efficiency. Here experiments are conducted on two backbones of CSPDarknet-53 <ref type="bibr" target="#b46">[47]</ref> and ResNet-50 <ref type="bibr" target="#b4">[5]</ref> for the performance comparison of PRB-FPN. To make fair comparisons, we did not evaluate the performance of anchor-free methods as their efficiency scores were not reported. Inference time is calculated as the average of execution time of the network with Non-Maximum Suppression (NMS) from 999 random images. <ref type="figure" target="#fig_9">Fig. 9</ref> plots the inference time vs. mean Average Precision small (APs) <ref type="bibr" target="#b7">[8]</ref> for many evaluated models, where the PRB-FPN was tested on nVidia Titan X. Observe that PRB-FPN (green curve in <ref type="figure" target="#fig_9">Fig. 9</ref>) achieves outstanding speed-accuracy performance compared to other SoTA models. We highlight two advantages of the PRB-FPN: (1) the parallel bi-fusion design for multi-scale feature extraction can detect both small and large objects at the same time with higher accuracy , and (2) the fusion module can effectively fuse both deep and shallow feature layers in parallel for fast and accurate one-shot object detection, specially for small objects. Observe that PRB-FPN outperforms the other SoTA one-stage object detectors (YOLOv4, YOLOv3, EfficientDet, ATSS, SM_NAS, and NAS-FPN), when taking both the accuracy and speed into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone</head><p>Input size FPS AP AP50 AP75 APS APM APL YOLOv4 <ref type="bibr" target="#b12">[13]</ref> CSPDarknet-53 <ref type="bibr" target="#b46">[47]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present a new PRB-FPN model that can effectively fuse deep and shallow pyramid features for fast and accurate object detection. Our novel bi-directional residual FP design enables easy training and integration with popular backbones. The proposed bottom-up fusion improves the detection of both small and large objects. Extensive evaluations show that PRB-FPN outperforms other bi-directional methods and SoTA one-stage methods, in terms of accuracy and speed.</p><p>Future work includes the development of anchor-free methods that can avoid handcrafted anchors, which might further improves detection accuracy. Finally, Network Architecture Search (NAS) can potentially be adopted to find the better architecture, considering both the backbone and FP structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Input size FPS AP AP50 AP75 APS APM APL  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Detailed network architecture for the proposed modules (refer to Fig. 1 for the overall architecture of PRB-FPN): (a) the Bi-Fusion module for concurrent fusion of contextual features from adjacent layers, (b) the concatenation and re-organization (CORE) module for recursively fusion of contextual features from adjacent layers, and (c) the Residual CORE (Re-CORE) design in combining CORE with the residual design inspired from ResNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Details of the proposed Re-CORE architecture. (b) The Re-Org block for feature re-organization. (c) The bottom-up fusion module (BFM).without degrading efficiency. Section 3.2 describes our new feature concatenation and re-organization scheme that can effectively circulate semantic and localization information. Section 3.3 further adopts a residual recursive formulation into our pipeline, which enables easier training and better performance for small object detection. Finally, Section 3.4 adds one more design of bottom-up location feature fusion that can further improve object localization. Figs. 1, 2 and 3 depict the complete pipeline of our proposed network architecture. Details are provided in the following sessions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a). The CORE design in Fig. 2(b) brings a major advantage that feature fusion can be recursively applied in both top-down and bottom-up fashions to: (1) concatenate semantic features from top layers (top-down), and (2) re-organize spatially rich localization features from bottom layers (bottom-up).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 (</head><label>3</label><figDesc>a) shows the detailed Re-CORE architecture. The Re-CORE module performs bi-fusion to integrate features from the four input layers with residual design. The output of the previous Re-CORE module becomes the input of the current Re-CORE module via a skip connection, which is depicted as a red line inFig. 2(c) andFig. 3(a), respectively. Features from the i th , (i ? 1) th , and (i + 1) th layers are fused by an 1 ? 1 convolution and then added to an up-sampled version of the skip connection to produce a new feature map. This map is then fed into a convolution block to produce the final output of this Re-CORE module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(ReCORE) module proposed for effective and efficient data fusion. The second major design is the bottom-up fusion module (BFM) added after ReCORE in PRB-FPN as shown in Fig. 2, which can further improve the localization of both small and large objects. To evaluate the effect of each module, accuracy improvement based on BFM is first evaluated in ? 4.2. The effect of BFM with ReCORE module for accuracy improvements is evaluated in ? 4.3. Evaluation of the RB-FPN module against the original RPN is provided in ? 4.4. Finally, comparisons between PRB-FPN and other SoTA methods are provided in ? 4.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Small object detection results on the UAVDT17 benchmark [45]. (a) and (c): LRFNet [6]. (b) and (d): The proposed PRB-FPN. Black boxes indicate don't-care regions that come with the original UAVDT17 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Comparisons of object detection between YOLOv4 and our PRB-FPN.(a) and (b) are the results of our home pictures taken from aerial cameras in Suao, Taiwan; (c) and (d) are the results on the AI City Challenge [49]. (a) : YOLOv4 512 ? 512, (b) : PRB-FPN w/o 512 ? 512. (c) : YOLOv4 512 ? 512, (d) : PRB-FPN w/o 512 ? 512. Comparisons of object detection between YOLOv4 and our PRB-FPN.(a) and (b) are the results of our home pictures taken from aerial cameras in Suao, Taiwan. (a) : YOLOv4 512 ? 512, (b) : PRB-FPN 512 ? 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig. 4 shows the ablation study comparisons of object detectors regarding the effects of BFM and Re-CORE modules on a selected image from COCO-test-dev. Fig. 4(b) shows detections obtained by YOLOv3. Fig. 4(c) and (d) show detections of YOLOv3 with BF and Re-CORE modules, respectively. Fig. 4(e) shows detections of the proposed PRB-FPN. In comparison, Fig. 4(f) shows detections obtained by M2Det [28]. Observe clearly that the proposed PRB-FRN outperforms YOLOv3, YOLOv4 and M2Det.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of a failure case of PRB-FPN when compared with (a) YOLOv4 512 ? 512. (b) shows the result of PRB-FPN 512 ? 512, where a false negative detection is shown in a red circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>AP vs. inference time on MS COCO detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4</head><label>4</label><figDesc>Backbone BFM FPS AP AP50 AP75 APs AP M AP L DarkNet53 28.9 28.6 50.7 29.6 15.5 30.4 35.3 512x512 28.4 34.9 57.2 37.7 18.6 37.1 45.3 Pelee 85.8 26.7 49.9 26.2 13.5 27.8 33.5 512x512 84.5 28.3 51.8 28.4 14.0 30.1 35.6 VGG16 31.8 34.1 58.3 35.8 17.9 35.9 44.1 512x512 31.4 34.6 58.6 36.7 18.6 36.5 44.3 DenseNet201 30.5 30.1 54.5 32.5 15.7 33.8 40.8 512x512 39.4 31.5 54.7 33.3 15.9 33.9 41.1</figDesc><table><row><cell>.1.</cell></row></table><note>(f) M2Det [28] 512 ? 512 Figure 4: Small object detection results on the MS COCO test set.7</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of BFM among different backbones.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="8">Input size FPS AP AP50 AP75 APs AP M AP L</cell></row><row><cell>GBFPN-SSD [14]</cell><cell>VGG16</cell><cell>512?512</cell><cell>-</cell><cell>-</cell><cell>33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FPN-BPN [16]</cell><cell>VGG16</cell><cell cols="8">320?320 32.4 29.6 48.4 32.3 9.6 32.5 44.3</cell></row><row><cell>FPN-BPN [16]</cell><cell>VGG16</cell><cell cols="8">512?512 18.9 33.1 53.1 36.3 15.7 37 44.2</cell></row><row><cell cols="3">EfficientDet-D0 [11] EfficientNet [48] 512?512</cell><cell cols="5">-34.6 53.0 37.1 -</cell><cell>-</cell><cell>-</cell></row><row><cell>NAS-FPN [12]</cell><cell>ResNet-50</cell><cell cols="4">1024?1024 -44.2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BFM [Ours]</cell><cell>VGG16</cell><cell cols="8">512?512 31.4 34.6 58.6 36.7 18.6 36.5 44.3</cell></row><row><cell>RB-FPN [Ours]</cell><cell>ResNet-50</cell><cell cols="8">512?512 32.1 44.3 65.1 48.2 25.1 47.3 56.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparisons between our BFM and other SoTA bi-directional fusion methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MethodBackbone Re-CORE BFM FPS AP AP50 AP75 APs AP M AP L 85.<ref type="bibr" target="#b7">8</ref> 26.7 49.9 26.2 13.5 27.8 33.5 Pelee with BFM Pelee 84.5 28.3 51.8 28.4 14.0 30.1 35.6 Pelee with RB-FPN Pelee 84.2 29.5 52.9 30.2 14.9 33.1 36.7 Yolov3 ? Darknet53 ? 28.9 32 56.5 33 17.4 34 41.4 Yolov3-SPP ? Darknet53 ? 28.7 35.3 59.2 37.4 16.9 37.1 48 Yolov3 Trained and tested by ourselves according to the paper. Test results with weights provided in the YOLOv3 website. Trained and tested by ourselves according to the instruction.</figDesc><table><row><cell>Pelee  ?</cell><cell>Darknet53  ?</cell><cell>28.9 28.6 50.7 29.6 15.5 30.4 35.3</cell></row><row><cell cols="2">Yolov3 with Re-CORE Darknet53</cell><cell>27.6 36 59.5 38.2 18.9 37.3 47.1</cell></row><row><cell>Yolov3 with BFM</cell><cell>Darknet53</cell><cell>28.4 34.9 57.2 37.7 18.6 37.1 45.3</cell></row><row><cell cols="2">Yolov3 with RB-FPN Darknet53</cell><cell>27.2 36.8 59.7 39.6 19 39.5 48</cell></row><row><cell>Yolov4</cell><cell>CSPDarknet53</cell><cell>31 43 64.9 46.5 24.3 46.1 55.2</cell></row><row><cell cols="2">Yolov4 with Re-CORE CSPDarknet53</cell><cell>28.5 44.8 66.5 47.3 26.9 46.3 55.8</cell></row><row><cell cols="2">Yolov4 with BFM CSPDarknet53</cell><cell>30.5 43.7 65.3 47.1 24.5 48.2 55.3</cell></row><row><cell cols="2">Yolov4 with RB-FPN CSPDarknet53</cell><cell>27.3 45.1 67.2 48.2 27.1 48.5 57</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* Pelee** The input size for all backbones is 512x512.*??</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Improvements by Parallel and Residual FPNs on UAVDT<ref type="bibr" target="#b44">[45]</ref> Benchmark.</figDesc><table><row><cell></cell><cell>Number of FP layers</cell><cell>Method</cell><cell>Backbone</cell><cell>FPS</cell><cell>mAP</cell></row><row><cell>3</cell><cell>4</cell><cell>5</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>PRB</cell><cell>ResNet-50</cell><cell>31.26</cell><cell>70.71</cell></row><row><cell></cell><cell></cell><cell>PRB</cell><cell>ResNet-50</cell><cell>27.15</cell><cell>72.32</cell></row><row><cell></cell><cell></cell><cell>PRB</cell><cell>ResNet-50</cell><cell>22.30</cell><cell>74.19</cell></row><row><cell></cell><cell></cell><cell>PRB</cell><cell>CSPDarknet-53</cell><cell>19.2</cell><cell>76.55</cell></row><row><cell></cell><cell></cell><cell>PRB</cell><cell>CSPDarknet-53</cell><cell>12.2</cell><cell>77.82</cell></row><row><cell></cell><cell></cell><cell>PRB</cell><cell>CSPDarknet-53</cell><cell>4.7</cell><cell>79.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of the Number of feature pyramidal layers for PRB with ResNet50 and RB with CSPDarknet53 on UAVDT [45] benchmark.</figDesc><table /><note>Fig.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>512x512 83 43 64.9 46.5 24.3 46.1 55.2</figDesc><table><row><cell>EfficientDet-D0 [11]</cell><cell>Efficient-B0 [48]</cell><cell cols="3">512x512 97.0 33.8 52.2 35.8 12 38.3 51.2</cell></row><row><cell>EfficientDet-D1 [11]</cell><cell>Efficient-B1 [48]</cell><cell cols="3">640x640 74.0 39.6 58.6 42.3 17.9 44.3 56</cell></row><row><cell>EfficientDet-D2 [11]</cell><cell>Efficient-B2 [48]</cell><cell cols="2">768x768 57.0 43 62.3 46.2 22.5 47</cell><cell>58.4</cell></row><row><cell>EfficientDet-D3 [11]</cell><cell>Efficient-B3 [48]</cell><cell cols="3">896x896 36.0 47.5 66.2 51.5 27.9 51.4 62.0</cell></row><row><cell>SM-NAS: E2 [50]</cell><cell></cell><cell cols="3">800x600 25.3 40 58.2 43.4 21.1 42.4 51.7</cell></row><row><cell>SM-NAS: E3 [50]</cell><cell></cell><cell cols="3">800x600 19.7 42.8 61.2 46.5 23.5 45.5 55.6</cell></row><row><cell>SM-NAS: E5 [50]</cell><cell></cell><cell cols="3">1333x800 9.3 45.9 64.6 49.6 27.1 49.0 58.0</cell></row><row><cell>NAS-FPN [12]</cell><cell>ResNet-50 [5]</cell><cell>640</cell><cell>24.4 39.9</cell><cell></cell></row><row><cell>NAS-FPN [12]</cell><cell>ReNet-50 [5]</cell><cell>1024</cell><cell>12.7 44.2</cell><cell></cell></row><row><cell>ATSS [51]</cell><cell>ResNet-101 [5]</cell><cell>800x</cell><cell>17.5 43.6 62.1 47.4 26.1 47</cell><cell>53.6</cell></row><row><cell>ATSS [51]</cell><cell>ReNet-101 [5]</cell><cell>800x</cell><cell cols="2">13.7 46.3 64.7 50.4 27.7 49.8 58.4</cell></row><row><cell>YOLOv7 [52]</cell><cell>E-ELAN [52]</cell><cell>640x640</cell><cell cols="2">51.4 69.7 55.9 31.8 55.5 65.0</cell></row><row><cell>RB-FPN [Ours]</cell><cell cols="4">CSPDarknet-53 [47] 512x512 76.9 45.1 67.2 48.2 27.1 48.5 57</cell></row><row><cell>PRB-FPN [Ours]</cell><cell cols="4">CSPDarknet-53 [47] 800x800 37.5 48.9 69.5 55.9 30.8 55.9 60.2</cell></row><row><cell cols="3">PRB-FPN-CSP  *  [Ours] CSPDarknet-53 [47] 640x640</cell><cell cols="2">51.8 70.0 56.7 32.6 55.5 64.6</cell></row><row><cell>PRB-FPN  *  [Ours]</cell><cell>E-ELAN [52]</cell><cell>640x640</cell><cell cols="2">52.5 70.4 57.2 33.4 56.2 65.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* We further investigated our performance on YOLOv7 [52] with our parallelization design. We didn't adopt the Re-Org block to save training time.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparisons on the MS COCO test-dev set with SoTA models on nVidia Volta V100.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Comparisons on the MS COCO test-dev set with SoTA models on nVidia Geforce Titan X. 14</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>The authors sincerely appreciate Mr. Yuwei Chen for proofreading and improving the English writing of this paper. We thank to National Center for High-performance Computing (NCHC) for providing computational and storage resources. We sincerely appreciate Wing-Kit, Chan (CUHK AIST major, year 4 in 2022-2023.) and Hao-Yuan, Yue (CUHK CS major, year 3 in 2022-23.) for expanding our PRBNet in the YOLO PyTorch family <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b51">52]</ref> and oriented object detection [70].</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">YOLOv3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning rich features at high-speed for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-power computer vision: Status, challenges, and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alyamkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journa on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="421" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientdet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gated bidirectional feature pyramid network for accurate one-shot detection. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonmin</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="543" to="555" />
		</imprint>
	</monogr>
	<note>Ho-Deok Jang, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parallel feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Wook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single-shot bidirectional pyramid networks for high-quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pelee: A real-time object detection system on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">C</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN (3)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6354</biblScope>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">M2Det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qijie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Previewer for multi-scale object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchically gated deep networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2267" to="2275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An improved object detection algorithm based on multi-scaled and deformable convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ME R-CNN: Multi-expert R-CNN for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1030" to="1044" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention couplenet: Fully convolutional attention coupling network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SAFNet: A semi-anchor-free network with enhanced feature pyramid for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9445" to="9457" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transfer learning for visual categorization: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1019" to="1034" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly-shared deep transfer networks for heterogeneous-domain knowledge propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalized deep transfer networks for knowledge propagation in heterogeneous domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4s</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A transfer learning system of object detection that fits your needs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The unmanned aerial vehicle benchmark: Object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CSPNet: A new backbone that can enhance learning capability of cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1571" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The 4th AI City Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="2665" to="2674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SM-NAS: Structural-to-modular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12661" to="12668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun. R-Fcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Coupling global structure with local parts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness NMS and bounded IoU loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">DSSD : Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficient featurized image pyramid network for single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">SpotNet: Self-attention multi-task network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?ritier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">ultralytics/yolov5: v6.0 -YOLOv5n &apos;Nano&apos; models, Roboflow integration, TensorFlow export, OpenCV DNN support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Jocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
