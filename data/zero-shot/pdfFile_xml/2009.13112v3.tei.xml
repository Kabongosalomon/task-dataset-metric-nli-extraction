<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology of China</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Xin</roleName><forename type="first">Eric</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
						</author>
						<title level="a" type="main">Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in realworld environments. A key challenge is to recognize and stop at the correct location, especially for complicated outdoor environments. Existing methods treat the STOP action equally as other actions, which results in undesirable behaviors that the agent often fails to stop at the destination even though it might be on the right path. Therefore, we propose Learning to Stop (L2STOP), a simple yet effective policy module that differentiates STOP and other actions. Our approach achieves the new state of the art on a challenging urban VLN dataset TOUCHDOWN, outperforming the baseline by 6.89% (absolute improvement) on Success weighted by Edit Distance (SED).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision-and-language navigation (VLN) aims at training an agent to navigate in real environments by following natural language instructions. Compared to indoor VLN <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>, navigation in urban environments <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> is particularly challenging, since urban environments are often more diverse and complex. Several research studies <ref type="bibr" target="#b2">(Mirowski et al., 2018;</ref><ref type="bibr" target="#b8">Li et al., 2019;</ref> have been conducted to solve the problem. In this paper, we also focus on the urban VLN task. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, given a natural language instruction, the agent perceives local visual scene and chooses actions at every time step, learning to match the instruction with the produced trajectory and navigate to the destination. Existing VLN models <ref type="bibr">Tan et al., 2019;</ref><ref type="bibr" target="#b8">Ke et al., 2019;</ref><ref type="bibr">Ma et al., 2019b,a;</ref><ref type="bibr" target="#b5">Fried et al., 2018;</ref><ref type="bibr" target="#b17">Wang et al., 2018)</ref> seem to neglect the importance of the STOP action and treat all actions equally. However, this can lead to undesirable behaviors, also noticed in ; <ref type="bibr" target="#b1">Blukis et al. (2018)</ref>, that the agent fails to stop at the target although it might be on the right path, because the STOP action is severely underestimated.</p><p>We argue that the STOP action in the urban VLN tasks is crucially important and deserves special treatment. First, in contrast to errors on other actions that are likely to be fixed later in the journey, the price of stopping at a wrong location is higher, because producing STOP terminates the episode, and there will be no chance to fix a wrong stop. Second, the statistical count of STOP is much lower than other actions as it only appears once per episode. Thus STOP will receive less attention if we treat all actions equally and ignore the difference of occurrence frequency. Moreover, STOP and other actions need different understandings of the dynamics between the instruction and the visual scene. Both require the alignment between trajectories and instructions, but STOP would emphasize the completeness of the instruction and the matching between the inferred target and the sur-rounding scene, while choosing directions requires a planning ability to imagine the future trajectory.</p><p>Therefore, we introduce a Learning to Stop (L2STOP) module to address the issues. L2STOP is a simple and model-agnostic module, which can be easily plugged into VLN models to improve their navigation performance. As we demonstrate in <ref type="figure" target="#fig_0">Fig. 1</ref>, the L2STOP module consists of a Stop Indicator to determine whether to stop and a Direction Decider to choose directions when at key points. Besides, we weigh STOP action more than other actions in the loss function, forcing the agent to pay more attention to the STOP action. We conduct experiments on a language-grounded street-view navigation dataset TOUCHDOWN 1 <ref type="bibr" target="#b3">(Chen et al., 2019)</ref>. Extensive results show that our proposed approach significantly improves the performance over the baseline model on all metrics and achieves the new state-of-the-art on the TOUCHDOWN dataset. 2 2 Approach <ref type="figure" target="#fig_2">Fig. 2</ref> illustrates the framework of our L2STOP model. Specifically, a text encoder and visual encoder are used to get the text and visual representations. Then the trajectory encoder uses the representations to compute the hidden context state, which is the input of the policy module. Unlike previous VLN models, which use one branch policy module, we use our proposed L2STOP module, a two-branch policy module that separates the policies for STOP and other actions. We detail each component below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual and Text Encoder</head><p>As shown in the <ref type="figure" target="#fig_2">Fig. 2</ref>, we use two encoders as used in <ref type="bibr" target="#b3">Chen et al. (2019)</ref> for encoding visual scene and language instruction respectively. For visual part, we apply a CNN <ref type="bibr" target="#b10">(Krizhevsky et al., 2012)</ref> as the visual encoder to extract visual representation v t from current visual scene at time step t. For text, we adopt an LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref> as the text encoder to get the instruction representation X = {x 1 , x 2 , ..., x l }. We then use a soft-attention <ref type="bibr">(Vaswani et al., 2017)</ref> to get the grounded textual feature x t at time step t:</p><formula xml:id="formula_0">? t,l = sof tmax((W x h t?1 ) T x l )</formula><p>(1)  where W x denotes parameters to be learnt, ? t,l denotes attention weight over l-th feature vector at time t, and h t?1 denotes the hidden context at previous time step. Then the agent produces the hidden context at the current step:</p><formula xml:id="formula_1">x t = l ? t,l x l<label>(2)</label></formula><formula xml:id="formula_2">h t = LST M ([x t , v t , a t?1 ]).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning to Stop Policy Module</head><p>Unlike the existing methods that view all the actions equally important, we propose the L2STOP module that helps the agent to learn whether to stop and where to go next with separate policy branches, Stop Indicator and Direction Decider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stop Indicator</head><p>The stop indicator produces stop or non-stop signals at every time step. At time step t, the stop indicator takes the hidden context h t and the time embedding t as input and outputs the probabilities of stop and non-stop signals:</p><formula xml:id="formula_3">s t,1 , s t,2 = sof tmax(g 2 ([h t , t]))<label>(3)</label></formula><p>where g 2 is a linear layer, and s t,1 as well as s t,2 are the probabilities of non-stop and stop signals at time step t, respectively. If the stop indicator produces stop signal, the agent will stop immediately. Otherwise, the direction decider will choose a direction to go next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Direction Decider</head><p>The direction decider is employed to select actions from a subset of the original action space. Specifically, the action subset includes all actions except STOP action (go forward, turn left, and turn right). Empirically, we observe that when navigating in urban environments, the agent only needs to choose directions at the intersections (nodes with more than two neighbors) it encounters in the journey. Therefore, we view these intersections as key points on the road and assume that the direction decider only needs to choose directions at key points and always goes forward otherwise. So at time step t, if the agent is at a key point, it will be activated and takes the hidden context h t as well as a learned time embedding t as input and outputs the probability of each action in its action space:</p><formula xml:id="formula_4">p t,k = sof tmax(g 1 ([h t , t]))<label>(4)</label></formula><p>where g 1 is a linear layer and p t,k is the probability of each action at time step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning</head><p>We use Teacher-Forcing <ref type="bibr" target="#b12">(Luong et al., 2015)</ref> method to train the model. We have two loss functions, L direction and L stop , for direction decider and stop indicator, respectively. L direction is a regular cross-entropy loss function,</p><formula xml:id="formula_5">L direction = ? t k q t,k log(p t,k )<label>(5)</label></formula><p>Where q t,k denotes ground truth label for each action at time step t. For the stop indicator, we use a weighted cross-entropy loss, where we assign a greater weight for the stop signal in the loss function and therefore force the agent to pay more attention to the stop action, in formula,</p><formula xml:id="formula_6">L stop = t ?o t log(s t,1 ) ? ?(1 ? o t )log(s t,2 ) (6)</formula><p>where o t are the ground-truth non-stop signals, and ? is the weight for the stop signal. Finally, the agent is optimized with a weighted sum of two loss functions:</p><formula xml:id="formula_7">L loss = ?L direction + (1 ? ?)L stop<label>(7)</label></formula><p>where ? is the weight balancing the two losses.  <ref type="formula" target="#formula_1">2019)</ref>, we report three evaluation metrics for the VLN task in urban environments: Task Completion (TC), Shortest-path Distance (SPD), and Success weighted by Edit Distance (SED). We also add another two metrics evaluating the alignment between the trajectories and the instructions: Coverage weighted by Length Score (CLS) 3  and Success weighted by normalized Dynamic Time Warping (SDTW) 3 .  kernels with stride 4, and the second layer uses 64 4 ? 4 kernels with stride 4, applying ReLu nonlinearities after each convolutional operation. Then a single fully-connected layer including biases of size 256 follows. An action embedding layer of size 16 is learned to map the previous action at every time step. Then, we concatenate the text representation, the visual representation, and the action embedding to get the input of the trajectory encoder. The trajectory encoder is a single-layer RNN with 256 hidden states. The time embedding layer is a single fully-connected layer including biases of size 32. Both of the stop indicator and the direction decider consist of a single-layer perceptron with biases and a SOFTMAX operation to compute the action probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>We compare the performance of our approach with the baselines: (1) Random: randomly take actions at each time step. (2) GA and RCONCAT: the baseline models reported in the original dataset paper <ref type="bibr" target="#b3">(Chen et al., 2019)</ref>. We adapt the RCONCAT model by equipping it with an attention mechanism on instruction representation to get our Attention-RConcat (ARC) model that outperforms RCON-CAT. Then we integrate ARC with the proposed L2STOP module, which further boosts the performances on all metrics and achieves the best results on both development and test sets.</p><p>In <ref type="table">Table 1</ref>, our approach substantially outperforms the baseline models, improving SED from 9.45% to 16.34%. Significant improvements on both goal-oriented metrics (TC, SED) and path alignment metrics (CLS, SDTW) demonstrate the effectiveness of L2STOP model in instruction following and goal achievement, which also validate that L2STOP learns not only where to go but also where to stop better.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modularity</head><p>We compare the performance between the baseline models with and without L2STOP module. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. Integrated with the L2STOP module, both of the baseline models show improvements on all the metrics. It demonstrates that our approach is model-agnostic and generalizable: the L2STOP module can be plugged into other VLN models and enhance their navigation performance in the urban environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Effect of Individual Components We conduct an ablation study to illustrate each component's effect on the development set in <ref type="table" target="#tab_5">Table 3</ref>. Row 2-4 shows the influence of each component by removing them respectively from the final model (ARC with L2STOP module). Removing any of the components results in worse performance, proving the indispensability of all components in our model. Row 2 shows the results of ARC with only one policy module, which will disable turn left and turn right actions when the agent is not at key points. The results evaluate the effectiveness of the two-branch structure for providing different subpolicies for STOP and other actions. Row 3 shows the results of the model whose Direction Decider makes decisions at every time step instead of only at key points. The results validate the effectiveness of only choosing directions at key points. Row 4 shows the results where the stop signal's weight is the same as the non-stop signal in the loss function of Stop Indicator. The worst results validate the importance of STOP action. When stop and non-stop signals are treated equally, the agent will prefer non-stop because of its higher occurrence <ref type="figure">Figure 3</ref>: Case study. We choose two cases from the development set, where our proposed model is successful, but the baseline stops either too late or too early. Red boxes show the key items to recognize the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>frequency.</head><p>Which Is More Important, Stop or Direction?</p><p>In <ref type="table" target="#tab_6">Table 4</ref> First, Row 2 shows the stop branch has about 30% chance to stop at the right position when the agent is on the right path. Second, The performance in Row 3 is much greater than that in Row 1, indicating that although our approach improves agent's stop ability, the performance is still seriously limited by the wrong stop problem. This indicates that the wrong stop problem in VLN deserves more attention and further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study</head><p>We provide visualizations for two qualitative examples to further illustrate how our L2STOP model learns to stop better in <ref type="figure">Figure 3</ref>. In both cases, our model and the baseline model are on the right path to the target. However, the baseline stops either too late or too early. Specifically, In (a), the baseline agent fails to recognize the black fire hydrant on the target but stops at a place where another black fire hydrant is visible. In (b), the baseline agent successfully recognize the parking pay station on the right, but it ignores the instruction "slightly past it" and just stops immediately. In contrast, our agent stops in the right place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We investigate the importance of the STOP action and study how to learn a policy that can not only make better decisions on where to go but also stop more accurately. We propose the L2STOP module for the vision-language navigation task situated in urban environments. Experiments illustrate that L2STOP is modular and can be plugged into other VLN models to further boost their performance in urban environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>A.1 Analysis of Model Structure</p><p>In <ref type="figure" target="#fig_3">Fig. 4</ref>, we examine four model structures to evaluate the interactions between the two branches:</p><p>(1) Separate Enc-Dec model, where two encoderdecoder models are trained separately for two branches.</p><p>(2) Shared Enc model, which has a shared encoder but uses two different decoders for two branches.</p><p>(3) Shared Dec model, which has different encoders for both linguistic and visual input but shared trajectory decoder. <ref type="formula" target="#formula_4">(4)</ref> shared Enc-Dec model, which shares both the encoder and the decoder. Note that this is the final architecture we use, which is demonstrated in Sec. 2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-Parameters Sensitivity Analysis</head><p>Threshold for Stop Signal We study the sensitivity of the threshold for stop signals on the development set. The result is shown in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>. Task-Completion (TC) is consistent in a large range of thresholds, with a slight drop when the thresh- old is getting higher than 0.7 and sharp decreases when the threshold is close to 0 and 1. The results demonstrate that our approach is insensitive to the change of threshold for stop signals. The consistency of the performance means that the scores of stop signals are either low or high, rarely intermediate. This proves that our approach enables the agent to pay more attention to STOP; that is, the agent is cautious about deciding to stop and only stop when it is highly confident it reaches the goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Direction Branch Weight</head><p>We study the sensitivity of direction branch weight ? on the development set. The optimal value for ? is 0.6, as depicted in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>, which demonstrates that the balance between the loss functions of two branch enables the agent to not only select correct directions at key points but also stop at the right place. As shown in the figure, smaller ? (0-0.5) results in relatively worse performance than higher ?, indicating that small ? enforces the agent to concentrate too much on STOP but ignore the choice for direction. Consistently good performance with larger ? (0.6-0.85) shows that only a small weight for the stop branch can significantly improve the agent's stop ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stop Signal Weight</head><p>We study the sensitivity of stop signal weight ? on the development set. As shown in the <ref type="figure" target="#fig_4">Fig. 5 (c)</ref>, the optimal value for ? is 20. We can see that when ? = 0, our model's perfor- mance is similar to the ARC model (15.53 as shown in <ref type="table">Table 1</ref>). However, when setting greater ?, the TC shows fluctuations, but is consistently better than ARC's performance. Only when ? increases to a large number of 80 does the performance decline sharply. This demonstrates the effectiveness of our proposed Weighted Cross-Entropy loss function, which consistently improves the agent's stop ability with a large range of ?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Vision-and-language navigation task in an urban environment. Our L2STOP agent chooses directions at key points and leverages a stop indicator to produce stop or non-stop signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>https://github.com/lil-lab/touchdown 2 The previous version of this work<ref type="bibr" target="#b18">(Xiang et al., 2019)</ref> was presented at the NeurIPS 2019 ViGIL workshop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our L2STOP model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Two-branch VLN models. Input includes language instruction and local visual scene. one Encoder consists of a Visual Encoder and a Text Encoder in Fig. 2, and Decoder represents Trajectory Encoder in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a) Task Completion (TC) scores with different thresholds for the stop signal (s t,2 in Equation 6). TC shows insensitivity to different thresholds. (b) TC scores with different direction branch weights ? in Equation 7. ? = 0.6 gives the highest TC. (c) TC scores with different stop signal weight ? in Equation 6. ? = 20 gives the highest TC. All the experiments are done on the development set,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Implementation DetailsThe proposed framework and the baselines are implemented inPy- Torch (Paszke et al., 2019), and the training of the models costs at average 6 hours. We use Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.00025 to train the model. The text encoder consists of a word embedding layer of size 32, and a bi-directional single-layer RNN with 256 hidden units. A single-layer fully connected layer of size 512 is used to map the previous hidden states, which is then used to compute the soft-attention to19.76 12.18 50.10 12.18   RCONCAT  11.14 19.87 10.77 46.61 10.76  RCONCAT + L2STOP 13.01 19.28 12.69 50.86 12.66    </figDesc><table><row><cell>Method</cell><cell cols="2">TC? SPD? SED? CLS? SDTW?</cell></row><row><cell>GA</cell><cell>9.85 21.43 9.50 46.86</cell><cell>9.44</cell></row><row><cell>GA + L2STOP</cell><cell>12.58</cell><cell></cell></row></table><note>get the text representation. The visual encoder is a three-layer CNN. The first layer uses 32 8 ? 8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of the baseline models with and without L2STOP module on the development set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ARC +L2STOP 19.48 17.05 19.02 55.68 18.97 2 -one branch 15.40 18.33 14.92 52.00 14.86 3 -no key points 15.18 18.17 14.55 51.67 14.44 4 -no weighting 12.65 21.60 12.22 47.91 12.20</figDesc><table><row><cell># Model</cell><cell>TC? SPD? SED? CLS? SDTW?</cell></row><row><cell>1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results for individual components on the development set.</figDesc><table><row><cell cols="2"># Model</cell><cell>TC (Dev Set)</cell></row><row><cell cols="2">1 ARC + L2STOP</cell><cell>19.48</cell></row><row><cell>2</cell><cell>w/ Oracle Direction</cell><cell>30.63</cell></row><row><cell>3</cell><cell>w/ Oracle Stop</cell><cell>61.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Effect of oracle direction and stop.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, we study the effect of making either Direction Decider or Stop Indicator an Oracle to see to what extent the model can be improved. Oracle Direction means the Direction Decider always chooses correct directions, and Oracle Stop means the Stop Indicator always produces ground truth stop signals as long as the agent reaches there.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Dec 19.48 17.05 19.02 55.68 18.97</figDesc><table><row><cell></cell><cell>Table</cell></row><row><cell cols="2">5 shows the performance of four architectures on</cell></row><row><cell cols="2">the development and test set. First, despite worse</cell></row><row><cell cols="2">performance on other metrics, Separate Enc-Dec</cell></row><row><cell cols="2">can achieve competitive performance on SPD and</cell></row><row><cell cols="2">CLS against other two-branch shared models. The</cell></row><row><cell cols="2">results show that the Separate Enc-Dec agent can</cell></row><row><cell cols="2">produce high-fidelity trajectory matched with in-</cell></row><row><cell cols="2">struction but fail to stop at the correct location. This</cell></row><row><cell cols="2">shows that to stop better, the stop indicator requires</cell></row><row><cell cols="2">the information from the direction branch. Second,</cell></row><row><cell cols="2">compared with Shared Enc model, Shared Dec per-</cell></row><row><cell cols="2">forms competitively on SPD and CLS while much</cell></row><row><cell cols="2">worse on other metrics, indicating that the stop</cell></row><row><cell cols="2">branch learns better from the direction branch in</cell></row><row><cell cols="2">the encoder phase. Third, both Shared Enc and</cell></row><row><cell cols="2">Shared Dec show stronger ability to learn to stop;</cell></row><row><cell cols="2">thus we use Shared Enc-Dec model, which requires</cell></row><row><cell cols="2">fewer parameters. Improved performance shows</cell></row><row><cell cols="2">the Shared Enc-Dec model learns to stop better</cell></row><row><cell cols="2">than other architectures.</cell></row><row><cell>Method</cell><cell>TC? SPD? SED? CLS? SDTW?</cell></row><row><cell cols="2">Separate Enc-Dec 13.71 17.67 13.35 55.24 13.32</cell></row><row><cell>Shared Dec</cell><cell>14.43 18.45 14.05 52.90 14.00</cell></row><row><cell>Shared Enc</cell><cell>18.75 18.19 18.32 52.42 18.27</cell></row><row><cell>Shared Enc-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison for four different architectures of the two-branch model on the development set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/google-research/ google-research/tree/master/r4r</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was done when the first author was interning at UC Santa Barbara. The authors would like to thank the anonymous reviewers for the constructive feedback, and Yongji Wu and Yiheng Xu for their helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mapping navigation instructions to continuous control actions with position-visitation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valts</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04179</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning deployable navigation policies at kilometer scale from a single traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Milford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05211</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Touchdown: Natural language navigation and spatial reasoning in visual street environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12538" to="12547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Following formulaic map instructions in a street simulation environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Visually Grounded Interaction and Language</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speaker-follower models for vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3314" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12255</idno>
		<title level="m">Stay on the path: Instruction fidelity in vision-and-language navigation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tactical rewind: Self-correction via backtracking in visionand-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyiming</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6741" to="6749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05930</idno>
		<title level="m">Cross-view policy learning for street navigation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-monitoring navigation agent via auxiliary progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Al-Regib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03035</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The regretful agent: Heuristic-aided navigation through progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6732" to="6740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective and general evaluation for instruction conditioned navigation using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05446</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6629" to="6638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-andlanguage navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="37" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Not all actions are equal: Learning to stop in language-grounded urban navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ViGIL@ NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
