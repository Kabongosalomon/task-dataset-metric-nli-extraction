<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inter-channel Conv-TasNet for multichannel speech enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongheon</forename><surname>Lee</surname></persName>
							<email>dongheon0115@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Seongrae</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jung-Woo</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inter-channel Conv-TasNet for multichannel speech enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. 1 ?</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech enhancement in multichannel settings has been realized by utilizing the spatial information embedded in multiple microphone signals. Moreover, deep neural networks (DNNs) have been recently advanced in this field; however, studies on the efficient multichannel network structure fully exploiting spatial information and inter-channel relationships is still in its early stages. In this study, we propose an end-to-end time-domain speech enhancement network that can facilitate the use of interchannel relationships at individual layers of a DNN. The proposed technique is based on a fully convolutional time-domain audio separation network (Conv-TasNet), originally developed for speech separation tasks. We extend Conv-TasNet into several forms that can handle multichannel input signals and learn interchannel relationships. To this end, we modify the encoder-maskdecoder structures of the network to be compatible with 3-D tensors defined over spatial channels, features, and time dimensions. In particular, we conduct extensive parameter analyses on the convolution structure and propose independent assignment of the depthwise and 1?1 convolution layers to the feature and spatial dimensions, respectively. We demonstrate that the enriched inter-channel information from the proposed network plays a significant role in suppressing noisy signals impinging from various directions. The proposed inter-channel Conv-TasNet outperforms the state-of-the-art multichannel variants of neural networks, even with one-tenth of their parameter size. The performance of the proposed model is evaluated using the CHiME-3 dataset, which exhibits a remarkable improvement in SDR, PESQ, and STOI.</p><p>Index Terms-Temporal Convolutional Networks, Conv-TasNet, multichannel speech enhancement Jung-Woo Choi.) The authors are with the School of Electrical Engineering,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ultichannel speech enhancement is a task to recover clean target speech from noise mixtures recorded by multichannel microphones. In the past, clean speech was estimated using various spatial filters designed using beamforming techniques, such as EB-MVDR <ref type="bibr" target="#b0">[1]</ref> and EB-MUSIC <ref type="bibr" target="#b1">[2]</ref>. In recent years, deep learning (DL)-based speech enhancement techniques have exhibited acceptable performance in single-channel speech enhancement tasks <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. The DL-based speech enhancement technique directly estimates the target sound source with a reduced noise through incorporated in the DL-based approach, which can model complex nonlinear relationships.</p><p>The DL-based speech enhancement technique produces a clean speech signal by estimating a spectrogram or timedomain waveform <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In the spectrogram estimation approach, a DNN is configured to learn the mapping between the input and output spectrograms. The spectrogram handled by the DNN can be either a magnitude or a complex spectrogram. When only the magnitude spectrogram is used, the noisy phase information can be combined with the estimated magnitude spectrogram <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, or the clean phase information can be reconstructed using a separate network or post-processing technique <ref type="bibr" target="#b9">[10]</ref>. Furthermore, DNNs using the complex spectrogram as input and output features <ref type="bibr" target="#b10">[11]</ref> have been proposed to mitigate the phase reconstruction issue. However, both spectrogram-based approaches require the short-time Fourier transform (STFT) <ref type="bibr" target="#b11">[12]</ref> for spectrogram conversion, which is not optimized for speech enhancement tasks. Therefore, end-to-end approaches have been proposed to directly receive and generate time-domain signals <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Training and inference using temporal waveforms automatically consider both the magnitude and phase information and enable the learning of appropriate characteristics for noise reduction.</p><p>The actual speech enhancement can be realized using the direct estimation of a target speech signal or noise suppression mask estimation. In the latter approach, a real or complex mask is trained by a DNN such that the element-wise multiplication of a noisy speech with a mask function reconstructs a clean speech <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. The mask estimation approach originates from the ideal binary mask of computational auditory scene analysis <ref type="bibr" target="#b16">[17]</ref>, which was subsequently extended to the ideal ratio mask <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In recent years, many DNNs have been introduced for mask estimation <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>The success of the single-channel DNN in the speech enhancement task has evolved into multichannel DNN models. Unlike the single-channel case, more spatial information is available for multichannel speech enhancement tasks. Therefore, the extraction of spatial features is key to the success of multichannel speech enhancement tasks. Many neural beamforming techniques have been proposed that use beamforming as the front end of a single-channel neural network <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Neural beamforming techniques were designed to combine the benefits of spatial filtering and DL; however, a fundamental performance limit set exists in the linear spatial filtering of the beamforming process.</p><p>Therefore, many DNN-based techniques have been developed that can exploit nonlinear relationships between microphones and overcome this limit <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. To date, Channel-Attention Dense U-Net (CA Dense U-Net) <ref type="bibr" target="#b26">[27]</ref> has exhibited the best performance among the models tested using the CHiME-3 dataset <ref type="bibr" target="#b27">[28]</ref>. CA Dense U-Net introduced the channel-attention network into the conventional U-Net structure to mimic beamforming. Despite its high performance, several issues <ref type="bibr" target="#b20">[21]</ref> arise in the STFT procedure and U-Net <ref type="bibr" target="#b28">[29]</ref> structure. First, it is uncertain whether the Fourier transform is the optimal transformation candidate for a speech enhancement task. Second, because STFT converts the time-domain signal into a complex domain, the network needs to handle both the magnitude and phase of the signal. Although a complex ratio mask has been proposed to utilize the phase information <ref type="bibr" target="#b10">[11]</ref>, the upper bound of performance exists because the reconstruction artifact induced by the up-sampling process. Third, the U-Net-based network typically has a large parameter size. Thus, the signal should be directly modeled in the timedomain using structures such as TasNet <ref type="bibr" target="#b20">[21]</ref> to overcome these problems. Among DNN models employing the TasNet structure, Conv-TasNet <ref type="bibr" target="#b29">[30]</ref> is an advanced end-to-end architecture appropriate for speech separation tasks. Conv-TasNet utilizes a temporal convolutional network (TCN) <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> consisting of dilated one-dimensional convolution (1-D Conv) layers. This can reduce the checkerboard artifact induced by down-and upsampling blocks of U-Net structures, which might be beneficial to the speech enhancement task. Its computation time can be reduced by performing parallel convolution, which allows its extension to a larger-scale model, such as the multichannel structure presented in this study.</p><p>Following the success of Conv-TasNet in single-channel speech enhancement <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, its extensions to the multichannel speech enhancement task <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> have been attempted. One representative example is the multichannel Conv-TasNet (MC Conv-TasNet) <ref type="bibr" target="#b34">[35]</ref>, which has a multichannel encoder to handle multichannel input data, and adds up encoded multichannel features into a single-channel. However, the spatial information is lost by the addition; thus, it cannot fully utilize the multichannel information in the TCN layers.</p><p>In this study, we propose a multichannel DNN model that can enhance noisy speech by fully exploiting the inter-channel relationship in the multichannel data. The base architecture of the proposed model is the MC Conv-TasNet; however, the encoder-mask-decoder structure of the conventional network is substantially redesigned to extract the spatial relationship using the network. The proposed model, referred to as inter-channel Conv-TasNet (IC Conv-TasNet) in this study, has the following advanced characteristics:</p><p>1. We constitute a 3-D tensor by stacking multichannel encoder outputs in the channel dimension. This changes the 2-D input feature of MC Conv-TasNet to a 3-D feature with various spatial information.</p><p>2. We separate the roles of the depthwise and 1-D convolutions of the TCN such that the depthwise convolution extracts inter-channel relationships only, whereas the 1-D Conv layer focuses on the extraction of spectral and temporal features. To this end, the 1-D Conv layer is replaced by a 2-D Conv functioning in the feature and time dimensions. Additionally, an extensive parameter study was conducted to build a small-sized DNN model. The model with the best performance was determined using various parameter studies with different model parameter sizes. The superior performance of the proposed model is compared with that of the state-of-theart (SOTA) models <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b36">[37]</ref> developed for multichannel speech enhancement tasks. The remainder of this paper is organized as follows. Section II introduces the proposed model architecture and provides details of the IC Conv-TasNet. In Sections III and IV, performance evaluation is presented with parameter studies. A downsized version of the IC Conv-TasNet is also introduced using a detailed analysis of the parameter size. Finally, Section V summarizes the results and draws the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MODEL ARCHITECTURE</head><p>The proposed DNN model is based on Conv-TasNet. The conventional single-channel (SC) Conv-TasNet for the sound separation task consists of an encoder, mask(separation), and decoder structures. The input waveform is divided into L overlapping segments of window length K, which are represented as</p><formula xml:id="formula_0">L K ? ? x ?</formula><p>. The 1-D Conv layer in the encoder module converts each segment data into a feature vector of length F . The encoder output</p><formula xml:id="formula_1">L F ? ? w ?</formula><p>can be written as ReLU( ),</p><formula xml:id="formula_2">U ? w x (1) where K F U ? ? ?</formula><p>is the encoding matrix, and ReLU <ref type="bibr" target="#b37">[38]</ref> is the rectified linear unit to guarantee the nonnegative representation of w .</p><p>The encoder output is subsequently fed into the mask estimation network (separation module) and analyzed to estimate the source separation mask. The separation of the th i source ( 1, ..., i S ? ) is realized by the element-wise product of the estimated mask L F i ? ? m ? and the encoder output w . The masked encoder outputs of the th i source can be obtained as ,</p><formula xml:id="formula_3">i i ? d w m ?<label>(2)</label></formula><p>where ? denotes the element-wise product. In the sound enhancement task, the source index i is discarded, and we obtain ? d w m ? . A decoder subsequently takes the masked encoder outputs and reconstructs them into enhanced sound segments. The segments of the enhanced sound can be expressed as ?, ? s dV</p><formula xml:id="formula_4">(3) where F K ? ? V ?</formula><p>is the decoding matrix. The reconstructed segments can be transformed into the final waveform using an overlap-and-add operation <ref type="bibr" target="#b38">[39]</ref>. The overall structure of Conv-TasNet is presented in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>The source separation module of Conv-TasNet consists of a single 1?1 Conv layer compressing the number of features from F to N ( F N ? ), followed by several stacks of 1-D Conv blocks. Here, we denote the 1?1 Conv layer before the stacks of 1-D Conv blocks as a 'bottleneck layer.' Each 1-D Conv block ( <ref type="figure">Fig.  2(a)</ref>) includes a 1?1 Conv layer upsizing the number of features from N to the number of hidden layers H, dilated depthwise convolution (D-Conv) layer <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and two 1?1 Conv layers for the residual path and skip connection, respectively. Each 1-D Conv block implements its own expansion and compression of features before and after the D-Conv. The nonlinear activation function (PReLU <ref type="bibr" target="#b41">[42]</ref>) and layer normalization are applied after 1?1 Conv and D-Conv, respectively. The serial connection of 1-D Conv blocks constituting a single TCN stack increases the receptive field by using a higher dilation factor d . In each stack of TCN, D 1-D Conv blocks are located. Multiple stacks are used to extract different information from multiple skip connections, which are subsequently added and mixed to generate a source separation mask. The generated mask is multiplied with the single-channel encoder output to produce a separated waveform through the transposed convolution layer.</p><p>MC Conv-TasNet <ref type="bibr" target="#b34">[35]</ref> incorporated one modification into SC Conv-TasNet to handle multichannel data. The encoder is extended in the channel dimension and 1-D Conv operations are applied to individual microphone channels. The 3-D tensor output of size (time, feature, channel) = ( , , ) L F M , obtained from individual encoders is then superposed along the microphone channel dimension to produce a 2-D encoder output feature of L F ? ? w ? , which is compatible with other layers of the SC Conv-TasNet, as shown in <ref type="figure">Fig. 3</ref>(a). However, this superposition of multichannel output causes the interchannel relationship to be lost in the following convolutional layers.</p><p>Here, we introduce and compare three different models using a modified encoder and TCN structures to seek a network structure that can effectively extract inter-channel relationship throughout all convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2-D Conv-TasNet</head><p>To avoid the loss of spatial information accompanied by the addition operation of the MC Conv-TasNet, we concatenate the multichannel encoder outputs, which yields a 2-D tensor</p><formula xml:id="formula_5">L FM ? ? w ?</formula><p>. Additionally, the bottleneck layer is modified to downsize the number of features (FM) to the number of features N. Consequently, the channel and feature information are mixed by the 1?1 Conv layer before TCN. Because the rest of the architecture is the same as that of Conv-TasNet, no difference exists between the channel dimension M and feature dimension F in TCN.</p><p>We denote this network with a modified encoder and bottleneck as 2-D Conv-TasNet. The block diagrams of 1-D Conv block in TCN and the encoder module are shown in Figs. 2(a) and 3(b), respectively. One extra modification required for the multichannel structure is the mask multiplication step. As the encoder outputs</p><formula xml:id="formula_6">L FM ? ? w ? are multichannel data, we select one microphone channel feature F ref L? ? w ? among w</formula><p>and multiply the generated mask to that channel only. The selected channel is indicated as the reference encoder output in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3-D Conv-TasNet</head><p>In the next architecture, indicated as 3-D Conv-TasNet, the channel dimension is separately handled from the feature dimension, as presented in <ref type="figure">Fig. 3(c)</ref>. A 3-D tensor of size (L, F, M) is constructed by stacking the outputs of the multichannel encoder and changing its size to (L, N, C) through two 1?1 Conv layers positioned before TCN. By constructing a 3-D tensor and applying separate 1?1 Conv operations along the channel and feature dimensions, we attempt to independently treat the channel and feature information throughout the entire TCN layer.</p><p>The 3-D tensor is subsequently split into C slices of (L, N) matrices, which are sent to the C individual 1-D Conv blocks of TCN, as shown in <ref type="figure">Fig. 2(b)</ref>. Consequently, different channels are independently convolved in individual TCN layers, and the mix of channel information occurs only at the final mask </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inter-channel Conv-TasNet (IC Conv-TasNet)</head><p>IC Conv-TasNet is the final model proposed in this study, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. Unlike the 3-D Conv-TasNet that utilizes the inter-channel relationship at the mask generation stage, the IC Conv-TasNet extracts inter-channel features within the TCN layers to fully exploit the available spatial information.</p><p>Similar to 3-D Conv-TasNet, 1-D Conv is applied to each microphone channel in the encoder module, and the encoder outputs of individual channels are stacked in the channel dimension, as shown in <ref type="figure">Fig. 3</ref>(c). The major difference between IC Conv-TasNet is the TCN blocks of the mask estimation network.</p><p>In the 2-D Conv block in the TCN of IC Conv-TasNet ( <ref type="figure">Fig.  2(c)</ref>), only the size of the channel dimension is increased by 1?1 Conv from C to the number of hidden layers H. This modification promotes channel-wise diversity without changing feature and time dimensions. The increased channel size is set as four times the input channel size in this study (H = 4C).</p><p>Subsequently, the D-Conv layer is modified to apply 2-D depthwise convolutions in the feature and time dimensions. The D-Conv layer receives a feature map of size (L, N, C) as an input, and extracts feature-and time-dependent information from 2-D dilated convolution of C slices of (L, N) matrices. Furthermore, 2-D zero padding is employed in the D-Conv layer to maintain the size of the feature map.</p><p>The inter-channel relationships are then extracted by two 1?1 Conv layers located before the skip and residual paths. Unlike the other Conv-TasNets that apply a 1?1 convolution in the feature dimension, the 1?1 Conv layers of the IC Conv-TasNet only extract the information in the channel dimension to focus on inter-channel relationships.</p><p>After the skip connection is connected to the PReLU activation function, the channel information is compressed through a 1?1 Conv (C) layer to make the feature map into a single-channel. Subsequently, another 1?1 Conv (N) layer adjusts the number of features from N to that of the encoder output F. A sigmoid function is used to constrain the values of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The three models (2-D, 3-D, and IC Conv-TasNet) discussed in Section II were tested using the publicly available CHiME-3 dataset <ref type="bibr" target="#b27">[28]</ref>. The dataset was designed for speech enhancement and recognition tasks. Moreover, it has been commonly used for testing multichannel speech enhancement techniques <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The dataset was created by mixing a speech pronounced using a speaker positioned at different locations from the tablet device and outdoor noise. The outdoor noises were recorded in four different outdoor environments, including caf?s, buses, pedestrian areas, and streets. The speeches were measured using 6-channel microphones installed on the tablet device at a sampling rate of 16 kHz. This dataset contains 7,138, 1,640, and 1,320 simulated utterances, which correspond to the training, development, and test data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment procedure</head><p>We used the MC Conv-TasNet as the baseline model for the performance comparison. In the first experiment was conducted to compare the performance of the three variants of Conv-TasNet. We chose the best model architecture from a comparison to the baseline. The second experiment was designed to search for the best model parameters through various parameter studies. The parameter studies were conducted by changing the number of layers in each stack of TCN (D), number of TCN stacks (S), number of features (F, N), and size of channel dimension (C) as listed in <ref type="table" target="#tab_0">Table 1</ref>. The performance and parameter size of the best model were also compared to those of the SOTA models designed for multichannel speech enhancement. Finally, the third experiment was designed to explore the possibility of downsizing the IC Conv-TasNet. By designing a downsized model with a reduced input tensor size, we attempted to reduce the training time with a minimal effect on the performance.</p><p>Additionally, the networks were trained in the experiments using the signal-to-distortion ratio (SDR) loss for 200 epochs. The encoder and decoder processed the temporal waveform using a window with a length of 256 and overlap of 50%. A 3?3 kernel size was used for the 1-D or 2-D Conv block of TCN. Channel 5 was selected as the reference encoder output, based on suggestions from a previous study <ref type="bibr" target="#b42">[43]</ref>. The network was trained with an ADAM <ref type="bibr" target="#b43">[44]</ref> optimizer with a learning rate of 10 -3 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation metrics</head><p>The performance of the proposed models was evaluated in terms of three metrics typically used in the speech enhancement task: SDR <ref type="bibr" target="#b44">[45]</ref>, perceptual evaluation of speech quality (PESQ; the wideband version recommended in ITU-T P.862.2 <ref type="bibr" target="#b45">[46]</ref>, and the short-time objective intelligibility measure (STOI) <ref type="bibr" target="#b46">[47]</ref>. A short description of these metrics is presented below: 1) Signal-to-Distortion Ratio (SDR) SDR is an objective metric designed to express the physical quality of the reconstructed signal. That is, SDR represents the distance between the ground truth and recovered signals, which </p><p>where s is the ground truth and ? is the recovered signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Perceptual Evaluation of Speech Quality (PESQ)</head><p>PESQ is a popular measure for speech quality evaluation as experienced by a user of a telephony system. PESQ compares clean and noisy speech signals and returns a value between -0.5 and 4.5, with higher values indicating better quality.</p><p>3) Short-Time Objective Intelligibility (STOI) STOI is a function that well represents the average intelligibility of the degraded speech. It provides a value from 0 to 1, with higher values indicating better intelligibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>A. Changing the model architecture <ref type="table" target="#tab_2">Table 2</ref> shows the performance evaluation results of the baseline (MC Conv-TasNet) and the three models with their parameter sizes. The hyperparameters and parameter sizes of all models are presented in <ref type="table" target="#tab_0">Tables 1 and 2</ref>, respectively. The size in the feature dimension (F) was fixed at 2048 for guaranteeing a reasonable comparison. In addition, the product of the channel and feature sizes for the TCN input was kept constant ( 512 C N ? ? ) to provide input tensors of the same total size irrespective of the models. The other parameters were determined using the values of a conventional Conv-TasNet <ref type="bibr" target="#b29">[30]</ref>, as presented in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="table" target="#tab_2">Table 2</ref> illuminates that the 2-D Conv-TasNet significantly outperforms the baseline MC Conv-TasNet considering all performance indices. This improvement can be explained by the difference in encoder outputs. In the MC Conv-TasNet, the spatial information in the multichannel output from the encoder is lost by summing all channel outputs into a single-channel, whereas the 2-D Conv-TasNet preserves it by concatenating all encoder outputs. This limitation of the 3-D Conv-TasNet can be resolved using the IC Conv-TasNet. According to the data presented in <ref type="table" target="#tab_2">Table  2</ref>, the performance of IC Conv-TasNet surpasses all other models in every performance criterion. The IC Conv-TasNet has an improved TCN structure that aggregates the C channel data into H hidden layers, and subsequently the inter-channel relationships between them are extracted by the 1?1 Conv layers. Consequently, the IC Conv-TasNet makes use of spatial information more aggressively throughout the entire convolution process. Here, the parameter of IC Conv-TasNet that has the significant effect on the performance indices needs to be determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameter study</head><p>Parameter studies were conducted on the representative parameters summarized in <ref type="table" target="#tab_0">Table 1</ref> to investigate the sensitivity of the IC Conv-TasNet parameters. The variations in the model parameters are listed in <ref type="table">Table 3</ref> with the corresponding performance indices and total parameter sizes.</p><p>In the first part of the parameter study <ref type="figure" target="#fig_1">(Models 1-3)</ref>, we examined the performance change with an increasing number of TCN stacks (S). The other model parameters were fixed as D = 8, F = 2048, N = 64, and C = 8. The performance gradually increases with an increasing S. However, when S is 3 or higher, the degree of improvement is not considerable. Although we can have a deeper TCN with a higher number of stacks, <ref type="bibr" target="#b2">3</ref> S ? was chosen for the next study regarding the increase in the parameter size in relationship to the change in C and N . The performance can be further enhanced by increasing the number of dilated convolution layers D. When we changed D from 6 to 10 (Models 4, 1, and 5, respectively), an increase in SDR and STOI could be observed. However, the best PESQ  ).</p><p>In the next study, we investigated the influence of the number of encoder outputs (F). In Model 6 of <ref type="table">Table 3</ref>, F was decreased to 512 from 2048 in Model 1. Even with a 25% feature size, performance degradation is not considerable. The TCN features rather than the encoder outputs strongly influenced the performance of the proposed network. When the number of features in the output of the bottleneck layer (N), that is, the number of features for the TCN input, was increased to 128 from 64 of Model 6, noticeable improvements in SDR, PESQ, and STOI could be obtained (Model 7). This result reveals that the number of features considered in the dilated convolution layers is more important than the number of encoder outputs that are eventually downsized by the bottleneck layer. In addition, the significance of N can be seen from the performance of Model 8, which has a higher F than that of Model 7 with exhibiting no remarkable improvement. From the insights gained in the aforementioned parameter studies, we configured a small-sized encoder output ( 512 F ? ) with a high number of features for the TCN input ( 128 N ? ). With this configuration, we continued the parameter study for the number of channels (C) in TCN. It was found that the number of channels is of paramount importance in enhancing the IC Conv-TasNet. Model 9, which has four times of C than Model 7, yields dramatic improvements in SDR and PESQ. When C was further increased to 64 (Model 10), a considerably higher performance could be obtained. These results demonstrate the importance of utilizing the channel and spatial information underlying multichannel speech data.</p><p>The spectrograms of a speech sample before and after enhancement by Model 10 are presented in <ref type="figure" target="#fig_5">Fig. 5</ref>. The spectrogram of the enhanced speech shows noise reduction over most frequency bands. Comparing Figs. 5 (b) and (c), however, the harmonics from 3-5 kHz are suppressed in enhanced speech compared to clean speech. This is attributable to the extremely low SNR of high frequency harmonics that cannot provide enough information for the reconstruction. Also, 1.7-2.1 kHz noise presents in the enhanced speech around 0.4 s, 1.1 s, and 3.7 s. Despite these observations, the enhanced speech quality is excellent and comparable to the clean speech in the informal listening test.</p><p>Additionally, the proposed model performance was compared with that of the SOTA models <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b36">[37]</ref>, which are based on different architectures for multichannel speech enhancement, and have the highest performance in SDR, PESQ, and STOI for the CHiME-3 dataset. According to the results summarized in <ref type="table" target="#tab_4">Table 4</ref>   . To superpose all skip connections from the 2-D Conv blocks, the size of each skip is increased to the original size of the first stack (L, N, C) in terms of two additional 1?1 Conv layers.</p><p>This progressive reduction structure is inspired by that of U-Net; however, we applied downsizing in channel and feature dimensions, whereas U-Net architecture pursuits the same in the time dimension. With the downsized model, sizes of the input and output tensors from each stack can be reduced by more than half compared with that of Model 10. This also reduces the parameter size and shortens the training time. In the experiment conducted with the same amount of GPU memory and machine, the training time was reduced by 70%. Two other models were designed and compared with those of models D and 10. First, the upsizing model (Model U) increases from <ref type="bibr">1 1</ref> ( , ) (64,16) N C ? to <ref type="bibr">3 3</ref> ( , ) (128, 64) N C ? as opposed to Model D. Sizes of the skip connections are also increased to match with the largest one (128, 64). Second, a small-sized IC Conv-TasNet model (Model S) of size ( , ) (64, <ref type="bibr" target="#b15">16)</ref> N C ? was examined without any progressive output size reduction to inspect the importance of progressive downsizing. In the results summarized in <ref type="table" target="#tab_5">Table 5</ref>, Model 10 exhibits the highest performance; however, the downsized Model D also has a comparable performance. The performance of the downsized Model D is extremely higher than that of the small-sized Model <ref type="figure">Fig. 6</ref>. Block diagram of overall structure of downsized IC Conv-TasNet S; thus, it is necessary to have large-sized entry stacks in the progressive downsizing. This is in contrast to the upsizing Model U, which does not perform as well as Model D, implying that the size of the first stack is the most significant one, and loss of information in the first stack causes performance degradation in the following stacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this study, we proposed IC Conv-TasNet for a multichannel speech enhancement task. The proposed model introduced three major modifications to the conventional Conv-TasNet to effectively learn spatial information. First, in the encoder part, encoded multichannel signals were stacked along the new channel dimension to form a 3-D tensor, rather than summing to a single output channel. Therefore, the spatial information could be analyzed using the channel dimension, independent of the feature dimension. Second, the 1-D dilated convolution with respect to the feature dimension in TCN was changed to a 2-D dilated convolution for the feature and time dimensions. Thus, the channel-dependent information could be separately processed, which was implemented by 1?1 Conv layers that operated in the channel dimension rather than the feature dimension of the original Conv-TasNet. The performance of the proposed model was compared with that of the SOTA models tested using the same CHiME-3 dataset, which demonstrated excellent performance of the proposed model compared to that of the comparison models. In addition, a downsized IC Conv-TasNet model with small sizes of input and output tensors was proposed to reduce memory usage. For the speech enhancement task, the size of the first TCN stack was important; thus, the downsized model that progressively decreased the output tensor size with an increasing stack index exhibited a comparable performance to that of the full-sized IC Conv-TasNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>nonlinear mapping operations realized by linear transform and nonlinear activation units. Deep neural networks (DNNs) are This work was supported by the BK21 Four program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea and conducted by Center for Applied Research in Artificial Intelligence (CARAI) grant funded by DAPA and ADD (UD190031RD). (Corresponding author:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overall structure of Conv-TasNet generation step by two 1?1 Conv layers positioned after the parametric rectified linear unit (PReLU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 2 .</head><label>32</label><figDesc>Comparison of encoder structures of (a) MC Conv-TasNet (b) 2-D Conv-Conv-TasNet (c) 3-D and IC Conv-TasNet Structures of 1-D Conv blocks in TCN of (a) SC Conv-TasNet, MC Conv-TasNet and 2-D Conv-TasNet (b) 3-D Conv-TasNet, and 2-D Conv block of (c) IC Conv-TasNet [0, 1]. The generated mask of size (L, F) is multiplied with the encoder output in terms of an element-wise product, resulting in the masked encoder outputs. In summary, the 2-D Conv block in TCN aggregates the information of each channel through 1?1 Conv; thus, each Conv layer actively exploits inter-channel relationship to extract features. The entire model architecture of the IC Conv-TasNet and 2-D Conv block in TCN are illustrated in Figs. 4 and 2(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Overall structure of IC Conv-TasNetThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The performance was further improved using the 3-D Conv-TasNet. The main difference between the 2-D and 3-D Conv-TasNets is the separation of the channel and feature dimensions. Because the channel-feature size product ( C N ? ) of the 3-D Conv-TasNet was equal to the total feature size ( N ) of the 2-D Conv-TasNet, the performance enhancement is solely attributable to the separation of the channel dimension. Unlike the simple concatenation of 2-D Conv-TasNet, the 3-D Conv-TasNet creates a new channel dimension by stacking the encoder outputs. Therefore, it is possible to maintain channeldependent information through convolution operations. The significantly improved PESQ score of the 3-D Conv-TasNet implies that the separation of the channel dimension and 2-D dilated convolutions for time and feature dimensions help improve the sound quality. Nevertheless, the 3-D Conv-TasNet has an intrinsic limitation in that different channel data are not mixed by the 1-D Conv layer. The TCN structure of the 3-D Conv-TasNet is merely a parallel connection of multiple TCNs for individual channel data. The inter-channel relationships are only utilized at the final mask generation step by the 1?1 Conv (C) layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Spectrogram of (a) noisy speech, (b) enhanced speech using IC Conv-TasNet, and (c) clean speech parameter size for training. The parameter size of the proposed model is 1.67 M, which is only approximately 6.4% that of the CA Dense U-Net with the highest reported SDR performance. The small parameter size of the proposed model originates from the separation of the channel dimensions. In the conventional Conv-TasNet, the number of filters in the 1?1 Conv layers of TCN is determined by 128 512 H N ? ? ? . Similarly, the number of filters in the D-Conv layer is H = 512. In contrast, in the IC Conv-TasNet, the number of filters of 1?1 Conv layers in TCN is 64 256 C H ? ? ?, and that of D-Conv is H = 256. Therefore, the parameter size can be significantly reduced by separating the channel dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 HYPERPARAMETERS</head><label>1</label><figDesc></figDesc><table><row><cell>OF THE NETWORK</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 COMPARISON</head><label>2</label><figDesc>OF MODEL ARCHITECTURES This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</figDesc><table><row><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 PERFORMANCE</head><label>4</label><figDesc>Despite its small parameter size, training of the IC Conv-TasNet requires a considerable amount of GPU memory because size of the input and output tensors is greater than that of the conventional models. A downsized version of IC Conv-TasNet with smaller input tensors is developed (downsized IC Conv-TasNet) to reduce the memory consumption. The model architecture is shown inFig. 6.The downsized model (Model D) is characterized by a progressive output size reduction across individual TCN stacks. Since Model 10 shows the best performance among nondownsized models, Model D is based on Model 10. The input and output tensor sizes of the first TCN stack were the same as</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">channels is halved ( 2 C C ?</cell><cell>/ 2</cell><cell>), and the number of features is</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>reduced to 2 N</cell><cell>? ? ?</cell><cell cols="2">N</cell><cell>/ 2</cell><cell>? ? . In the last (S = 3) stack, they are</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">further reduced to</cell><cell cols="2">S C C ?</cell><cell>/ 4</cell><cell>and</cell><cell>N</cell><cell>S</cell><cell>?</cell><cell>N</cell><cell>/ 2</cell></row><row><cell cols="6">COMPARISON OF IC CONV-TASNET</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">WITH THAT OF THE STATE-OF-THE-ART MODELS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">SDR PESQ STOI</cell><cell>Param. Size</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MVDRGC [37]</cell><cell cols="2">-</cell><cell>-</cell><cell>0.952</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CA Dense U-Net [27] 18.64</cell><cell>2.44</cell><cell>-</cell><cell>26 M</cell><cell></cell><cell></cell><cell></cell></row><row><cell>IC Conv-TasNet</cell><cell cols="2">19.67</cell><cell>2.67</cell><cell cols="2">0.973 1.67 M</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">C. Downsizing the IC Conv-TasNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">the previous case ( 1 N N ? ?</cell><cell>128</cell><cell cols="3">, 1 C C ? ? ). However, as 64</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">the stack index increases, the number of channels and features</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">is progressively reduced. In the second stack, the number of</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 PERFORMANCE</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">COMPARISONS OF DOWNSIZED</cell></row><row><cell cols="5">VERSIONS OF INTER-CHANNEL CONV-TASNET</cell></row><row><cell></cell><cell>SDR</cell><cell>PESQ</cell><cell>STOI</cell><cell>Param. Size</cell></row><row><cell>Model D</cell><cell>19.39</cell><cell>2.61</cell><cell>0.972</cell><cell>1.01 M</cell></row><row><cell>Model U</cell><cell>18.09</cell><cell>2.41</cell><cell>0.964</cell><cell>0.954 M</cell></row><row><cell>Model 10</cell><cell>19.67</cell><cell>2.67</cell><cell>0.973</cell><cell>1.67 M</cell></row><row><cell>Model S</cell><cell>17.43</cell><cell>2.31</cell><cell>0.958</cell><cell>0.427 M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimal modal beamforming for spherical microphone arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hovem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="371" />
			<date type="published" when="2011-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spherical harmonics MUSIC versus conventional MUSIC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="646" to="652" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks to enhance coded speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="663" to="678" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. LVA/ICA</title>
		<meeting>Int. Conf. LVA/ICA<address><addrLine>Liberec, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single channel speech enhancement using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kounovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop ECMSM</title>
		<meeting>IEEE Int. Workshop ECMSM<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="436" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end source separation with adaptive front-ends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casebeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 52 nd Asilomar Conference on Signals, Systems, and Computers</title>
		<meeting>52 nd Asilomar Conference on Signals, Systems, and Computers<address><addrLine>Pacific Grove, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fully convolutional neural network for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1993" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Raw waveform-based speech enhancement by fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal and Information Process. Assoc. Annu. Summit and Conf. (APSIPA ASC)</title>
		<meeting>Asia-Pacific Signal and Information ess. Assoc. Annu. Summit and Conf. (APSIPA ASC)<address><addrLine>Kuala Lumpur, Malaysia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An algorithm that improves speech intelligibility in noise for normal-hearing listeners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On ideal binary mask as the computational goal of auditory scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech separation by humans and machines</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Computational auditory scene analysis: Principles, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Hoboken, NJ, USA, Wiley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the optimality of ideal binary time-frequency masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="239" />
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A convolutional recurrent neural network for realtime speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3229" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Calgary, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved mvdr beamforming using single-channel mask prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1981" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural network based time-frequency masking and steering vector estimation for twochannel mvdr beamforming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristjansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Calgary, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6717" to="6721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised speech enhancement based on multichannel NMF-informed beamforming for noise-robust automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="960" to="971" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous optimization of forgetting factor and timefrequency mask for block online multi-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2702" to="2706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Channel-attention dense u-net for multichannel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tolooshams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="836" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The third &apos;CHiME&apos; speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop Autom. Speech Recognit. Understanding</title>
		<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Med. Image Comput. Comput.-Assisted Intervention</title>
		<meeting>Med. Image Comput. Comput.-Assisted Intervention<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<meeting><address><addrLine>Amsterdam, Netherland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exploring the best loss function for DNN-based low-latency speech enhancement with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11611</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Time-domain speech enhancement with generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16149</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">End-to-end multi-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enhancing end-to-end multi-channel speech separation via spatial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7319" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel method to correct steering vectors in MVDR beamformer for noise robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4280" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units (relu)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conv-TasSAN: Separative adversarial network based on Conv-TasNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2647" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Speech enhancement using beamforming and non negative matrix factorization for robust speech recognition in the CHiME-3 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bigot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop Autom. Speech Recognit. Understanding</title>
		<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech, Signal ess<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
