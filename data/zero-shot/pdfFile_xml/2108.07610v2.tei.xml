<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DRAEM -A discriminatively trained reconstruction embedding for surface anomaly detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitjan</forename><surname>Zavrtanik</surname></persName>
							<email>vitjan.zavrtanik@fri.uni-lj.si</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
							<email>matej.kristan@fri.uni-lj.si</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
							<email>danijel.skocaj@fri.uni-lj.si</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DRAEM -A discriminatively trained reconstruction embedding for surface anomaly detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual surface anomaly detection aims to detect local image regions that significantly deviate from normal appearance. Recent surface anomaly detection methods rely on generative models to accurately reconstruct the normal areas and to fail on anomalies. These methods are trained only on anomaly-free images, and often require hand-crafted post-processing steps to localize the anomalies, which prohibits optimizing the feature extraction for maximal detection capability. In addition to reconstructive approach, we cast surface anomaly detection primarily as a discriminative problem and propose a discriminatively trained reconstruction anomaly embedding model (DRAEM). The proposed method learns a joint representation of an anomalous image and its anomaly-free reconstruction, while simultaneously learning a decision boundary between normal and anomalous examples. The method enables direct anomaly localization without the need for additional complicated post-processing of the network output and can be trained using simple and general anomaly simulations. On the challenging MVTec anomaly detection dataset, DRAEM outperforms the current state-of-theart unsupervised methods by a large margin and even delivers detection performance close to the fully-supervised methods on the widely used DAGM surface-defect detection dataset, while substantially outperforming them in localization accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Surface anomaly detection addresses localization of image regions that deviate from a normal appearance <ref type="figure">(Figure 1)</ref>. A closely related general anomaly detection problem considers anomalies as entire images that significantly differ from the non-anomalous training set images. In contrast, in surface anomaly detection problems, the anomalies occupy only a small fraction of image pixels and are typically close to the training set distribution. This is a particu-M o <ref type="figure">Figure 1</ref>. DRAEM estimates the decision boundary between the normal an anomalous pixels solely by training on synthetic anomalies automatically generated on anomaly-free images (left) and generalizes to a variety of real-world anomalies (right). The result (Mo) closely matches the ground truth (GT). larly challenging task, which is common in quality control and surface defect localization applications.</p><p>In practice, anomaly appearances may significantly vary, and in applications like quality control, images with anomalies present are rare and manual annotation may be overly time consuming. This leads to highly imbalanced training sets, often containing only anomaly-free images. Significant effort has thus been recently invested in designing robust surface anomaly detection methods that preferably require minimal supervision from manual annotation.</p><p>Reconstructive methods, such as Autoencoders <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26]</ref> and GANs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>, have been extensively explored since they enable learning of a powerful reconstruction subspace, using only anomaly-free images. Relying on poor reconstruction capability of anomalous regions, not observed in training, the anomalies can then be detected by thresholding the difference between the input image and its re- <ref type="bibr">Figure 2</ref>. Autoencoders over-generalize to anomalies, while discriminative approaches over-fit to the synthetic anomalies and do not generalize to real data. Our approach jointly discriminatively learns the reconstruction subspace and a hyper-plane over the joint original and reconstructed space using the simulated anomalies and leads to substantially better generalization to real anomalies. construction. However, determining the presence of anomalies that are not substantially different from normal appearance remains challenging, since these are often well reconstructed, as depicted in <ref type="figure">Figure 2</ref>, top-left.</p><p>Recent improvements thus consider the difference between deep features extracted from a general-purpose network and a network specialized for anomaly-free images <ref type="bibr" target="#b3">[4]</ref>. Discrimination can also be formulated as a deviation from a dense clustering of non-anomalous textures within the deep subspace <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref>, as forming such a compact subspace prevents anomalies from being mapped close to anomaly-free samples. A common drawback of the generative methods is that they only learn the model from anomaly-free data, and are not explicitly optimized for discriminative anomaly detection, since positive examples (i.e., anomalies) are not available at training time. Synthetic anomalies could be considered to train discriminative segmentation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, but this leads to over-fitting to synthetic appearances and results in a learned decision boundary that generalizes poorly to real anomalies ( <ref type="figure">Figure  2</ref>, top-right).</p><p>We hypothesize that over-fitting can be substantially reduced by training a discriminative model over the joint, reconstructed and original, appearance along with the reconstruction subspace. This way the model does not overfit to synthetic appearance, but rather learns a local-appearanceconditioned distance function between the original and reconstructed anomaly appearance, which generalizes well over a range of real anomalies (see <ref type="figure">Figure 2</ref>, bottom).</p><p>To validate our hypothesis, we propose, as our main contribution, a new deep surface anomaly detection network, discriminatively trained in an end-to-end manner on synthetically generated just-out-of-distribution patterns, which do not have to faithfully represent the target-domain anomalies. The network is composed of a reconstructive subnetwork, followed by a discriminative sub-network <ref type="figure">(Figure 3)</ref>. The reconstructive sub-network is trained to learn anomaly-free reconstruction, while the discriminative subnetwork learns a discriminative model over the joint appearance of the original and reconstructed images, producing a high-fidelity per-pixel anomaly detection map ( <ref type="figure">Figure 1)</ref>.</p><p>In contrast to related approaches that learn surrogate generative tasks, the proposed model is trained discriminatively, yet does not require the synthetic anomaly appearances to closely match the anomalies at test time and outperforms the recent, more complex, state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Many surface anomaly detection methods focus on image reconstruction and detect anomalies based on image reconstruction error <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. Auto-encoders are commonly used for image reconstruction <ref type="bibr" target="#b4">[5]</ref>. In <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26]</ref> auto-encoders are trained with adversarial losses. The anomaly score of the image is then based on the image reconstruction quality or in the case of adversarially trained auto-encoders, the discriminator output. In <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref> a GAN <ref type="bibr" target="#b12">[13]</ref> is trained to generate images that fit the training distribution. In <ref type="bibr" target="#b22">[23]</ref> an encoder network is additionally trained that finds the latent representation of the input image that minimizes the reconstruction loss when used as the input by the pretrained generator. The anomaly score is then based on the reconstruction quality and the discriminator output. In <ref type="bibr" target="#b28">[29]</ref> an interpolation auto-encoder is trained to learn a dense representation space of in-distribution samples. The anomaly score is then based on a discriminator, trained to estimate the distance between the input-input and input-output joint distributions, however the approach to surface anomaly detection remains generative as the discriminator evaluates the reconstruction quality.</p><p>Instead of the commonly used image space reconstruction, the reconstruction of pretrained network features can also be used for surface anomaly detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. Anomalies are detected based on the assumption that features of a pre-trained network will not be faithfully reconstructued by another network trained only on anomaly-free images. Alternatively <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref> propose surface anomaly detection as identifying significant deviations from a Gaussian fitted to anomaly-free features of a pre-trained network. This requires a unimodal distribution of the anomaly-free visual features which is problematic on diverse datasets. <ref type="bibr" target="#b15">[16]</ref> propose a one-class variational auto-encoder gradient-based attention maps as output anomaly maps. However the method is sensitive to subtle anomalies close to the normal sample distribution.</p><p>Recently Patch-based one-class classification methods have been considered for surface anomaly detection <ref type="bibr" target="#b29">[30]</ref>. These are based on one-class methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref> which attempt to estimate a decision boundary around anomaly-free data that separates it from anomalous samples by assuming a unimodal distribution of the anomaly-free data. This assumption is often violated in surface anomaly data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DRAEM</head><p>The proposed discriminative joint reconstructionanomaly embedding method (DRAEM) is composed from a reconstructive and a discriminative sub-networks (see <ref type="figure">Figure 3</ref>). The reconstructive sub-network is trained to implicitly detect and reconstruct the anomalies with semantically plausible anomaly-free content, while keeping the non-anomalous regions of the input image unchanged. Simultaneously, the discriminative sub-network learns a joint reconstruction-anomaly embedding and produces accurate anomaly segmentation maps from the concatenated reconstructed and original appearance. Anomalous training examples are created by a conceptually simple process that simulates anomalies on anomaly-free images. This anomaly generation method provides an arbitrary amount of anomalous samples as well as pixel-perfect anomaly segmentation maps which can be used for training the proposed method without real anomalous samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reconstructive sub-network</head><p>The reconstructive sub-network is formulated as an encoder-decoder architecture that converts the local patterns of an input image into patterns closer to the distribution of normal samples. The network is trained to reconstruct the original image I from an artificially corrupted version I a obtained by a simulator (see Section 3.3).</p><p>An l 2 loss is often used in reconstruction based anomaly detection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, however this assumes an independence between neighboring pixels, therefore a patch based SSIM <ref type="bibr" target="#b26">[27]</ref> loss is additionally used as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref>:</p><formula xml:id="formula_0">L SSIM (I, I r ) = 1 N p H i=1 W j=1 1 ? SSIM I, I r (i,j) ,<label>(1)</label></formula><p>where H and W are the height and width of image I, respectively. N p is equal to the number of pixels in I. I r is the reconstructed image output by the network. SSIM (I, I r ) (i,j) is the SSIM value for patches of I and I r , centered at image coordinates (i, j). The reconstruction loss is therefore:</p><formula xml:id="formula_1">L rec (I, I r ) = ?L SSIM (I, I r ) + l 2 (I, I r ),<label>(2)</label></formula><p>where ? is a loss balancing hyper-parameter. Note that an additional training signal is acquired from the downstream discriminative network (Section 3.2), which performs anomaly localization by detecting the reconstruction difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminative sub-network</head><p>The discriminative sub-network uses U-Net [21]-like architecture. The sub-network input I c is defined as the channel-wise concatenation of the reconstructive subnetwork output I r and the input image I. Due to the normality-restoring property of the reconstructive subnetwork, the joint appearance of I and I r differs significantly in anomalous images, providing the information necessary for anomaly segmentation. In reconstruction-based anomaly detection methods anomaly maps are obtained using similarity functions such as SSIM <ref type="bibr" target="#b26">[27]</ref> to compare the original image to its reconstruction, however a surface anomaly detection-specific similarity measure is difficult to hand-craft. In contrast, the discriminative sub-network learns the appropriate distance measure automatically. The network outputs an anomaly score map M o of the same size as I. Focal Loss <ref type="bibr" target="#b13">[14]</ref> (L seg ) is applied on the discriminative sub-network output to increase robustness towards accurate segmentation of hard examples.</p><p>Considering both the segmentation and the reconstructive objectives of the two sub-networks, the total loss used in training DRAEM is</p><formula xml:id="formula_2">L(I, I r , M a , M ) = L rec (I, I r ) + L seg (M a , M ), (3)</formula><p>where M a and M are the ground truth and the output anomaly segmentation masks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Simulated anomaly generation</head><p>DRAEM does not require simulations to realistically reflect the real anomaly appearance in the target domain, but rather to generate just-out-of-distribution appearances, which allow learning the appropriate distance function to recognize the anomaly by its deviation from normality. The proposed anomaly simulator follows this paradigm.</p><p>A noise image is generated by a Perlin noise generator <ref type="bibr" target="#b17">[18]</ref> to capture a variety of anomaly shapes <ref type="figure" target="#fig_0">(Figure 4</ref>, P ) and binarized by a threshod sampled uniformly at random ( <ref type="figure" target="#fig_0">Figure 4</ref>, M a ) into an anomaly map M a . The anomaly texture source image A is sampled from an anomaly source image dataset which is unrelated to the input image distribution ( <ref type="figure" target="#fig_0">Figure 4, A)</ref>. Random augmentation sampling, inspired by RandAugment <ref type="bibr" target="#b9">[10]</ref>, is then applied by a set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anomaly generation</head><p>Reconstructive sub-network Discriminative sub-network <ref type="figure">Figure 3</ref>. The anomaly detection process of the proposed method. First anomalous regions are implicitly detected and inpainted by the reconstructive sub-network trained using Lrec. The output of the reconstructive sub-network and the input image are then concatenated and fed into the discriminative sub-network. The segmentation network, trained using the Focal loss L f ocal <ref type="bibr" target="#b13">[14]</ref>, localizes the anomalous region and produces an anomaly map. The image level anomaly score ? is acquired from the anomaly score map. of 3 random augmentation functions sampled from the set: {posterize, sharpness, solarize, equalize, brightness change, color change, auto-contrast}. The augmented texture image A is masked with the anomaly map M a and blended with I to create anomalies that are just-out-ofdistribution, and thus help tighten the decision boundary in the trained network. The augmented training image I a is therefore defined as</p><formula xml:id="formula_3">I a = M a I + (1 ? ?)(M a I) + ?(M a A), (4)</formula><p>where M a is the inverse of M a , is the element-wise multiplication operation and ? is the opacity parameter in blending. This parameter is sampled uniformly from an interval, i.e., ? ? [0.1, 1.0]. The randomized blending and augmentation afford generating diverse anomalous images from as little as a single texture (see <ref type="figure" target="#fig_1">Figure 5</ref>).</p><p>The above described simulator thus generates training sample triplets containing the original anomaly-free image I, the augmented image containing simulated anomalies I a and the pixel-perfect anomaly mask M a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Surface anomaly localization and detection</head><p>The output of the discriminative sub-network is a pixellevel anomaly detection mask M o , which can be interpreted in a straight-forward way for the image-level anomaly score estimation, i.e., whether an anomaly is present in the image.</p><p>First, M o is smoothed by a mean filter convolution layer to aggregate the local anomaly response information. The final image-level anomaly score ? is computed by taking the maximum value of the smoothed anomaly score map:</p><formula xml:id="formula_4">? = max M o * f s f ?s f ,<label>(5)</label></formula><p>where f s f ?s f is a mean filter of size s f ? s f and * is the convolution operator. In a preliminary study, we trained a classification network for the image-level anomaly classification, but did not observe improvements over the direct score estimation method <ref type="bibr" target="#b4">(5)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>DRAEM is extensively evaluated and compared with the recent state-of-the-art on unsupervised surface anomaly detection and localization. Additionally, individual components of the proposed method and the effectiveness of training on simulated anomalies are evaluated by an ablation study. Finally, the results are placed in a broader perspective by comparing DRAEM with state-of-the-art weaklysupervised and fully-supervised surface-defect detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with unsupervised methods</head><p>DRAEM is evaluated on the recent challenging MVTec anomaly detection dataset <ref type="bibr" target="#b2">[3]</ref>, which has been established as a standard benchmark dataset for evaluating unsupervised surface anomaly detection methods. We evaluate DRAEM on the tasks of surface anomaly detection and localisation. The MVTec dataset contains 15 object classes with a diverse set anomalies which enables a general evaluation of surface anomaly detection methods. Anomalous examples of the MVTec dataset are shown in <ref type="figure" target="#fig_4">Figure 8</ref>. For evaluation, the standard metric in anomaly detection, AU-ROC, is used. Image-level AUROC is used for anomaly detection and a pixel-based AUROC for evaluating anomaly localization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. The AUROC, however, does not reflect the localization accuracy well in surface anomaly detection setups, where only a small fraction of pixels are anomalous. The reason is that false positive rate is dominated by the a-priori very high number of non-anomalous pixels and is thus kept low despite of false positive detections. We thus additionally report the pixel-wise average precision metric (AP), which is more appropriate for highly imbalanced classes and in particular for surface anomaly detection, where the precision plays an important role.</p><p>In our experiments, the network is trained for 700 epochs on the MVTec anomaly detection dataset <ref type="bibr" target="#b2">[3]</ref>. The learning rate is set to 10 ?4 and is multiplied by 0.1 after 400 and 600 epochs. Image rotation in the range of (?45, 45) degrees is used as a data augmentation method on anomaly free images during training to alleviate overfitting due to the relatively small anomaly-free training set size. The Describable Textures Dataset <ref type="bibr" target="#b8">[9]</ref> is used as the anomaly source dataset.</p><p>A number of obtained qualitative examples are presented in <ref type="figure" target="#fig_4">Figure 8</ref>. As one can observe, the obtained anomaly masks are very detailed and resemble the given ground truth labels to a high degree of accuracy. Consequently, DRAEM achieves state-of-the-art quantitative results across all MVTec classes for surface anomaly detection as well as localization.</p><p>Surface Anomaly Detection. This makes such anomalies difficult to distinguish from anomaly-free regions. An example of this can be seen in <ref type="figure" target="#fig_2">Figure 6</ref>, where some of the transistor leads had been cut. The ground truth marks the area where the broken lead should be as anomalous. DRAEM only detects anomalous features in a small region of the cut lead, as the background features are common during training. Anomaly Localization. <ref type="table" target="#tab_1">Table 2</ref> compares DRAEM to the recent state-of-the-art on the task of pixel-level surface anomaly detection. DRAEM achieves comparable results to the previous best-performing methods in terms of AUROC scores and surpasses the state-of-the-art by 13.4 percentage points in terms of AP. A better AP score is achieved in 11 out of 15 classes and is comparable to the state-of-the-art in other classes. A qualitative comparison with the state-ofthe-art method Uninformed Students <ref type="bibr" target="#b3">[4]</ref> and PaDim <ref type="bibr" target="#b10">[11]</ref> is shown in <ref type="figure" target="#fig_3">Figure 7</ref>. DRAEM achieves a significant improvement in anomaly segmentation accuracy.</p><p>A detailed inspection showed that some of the detection errors can be attributed to the inaccurate ground truth labels on ambiguous anomalies. An example of this is shown in <ref type="figure" target="#fig_2">Figure 6</ref>, where the ground truth covers the entire surface of the pill, yet only the yellow dots are anomalous. DRAEM produces an anomaly map that correctly localizes the yellow dots, but the discrepancy with the ground truth mask  increases the performance error. These annotation ambiguities also impact the AP score of the evaluated methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>The DRAEM design choices are analyzed by groups of experiments evaluating (i) the method architecture, (ii) the choice of anomaly appearance patterns and (iii) low perturbation example generation. Results are visually grouped by shades of gray in <ref type="table">Table 3</ref>.</p><p>Architecture. The DRAEM reconstructive sub-network impact on the downstream surface anomaly detection performance is evaluated by removing it from the pipeline and training the discriminative sub-network alone. The results are shown in <ref type="table">Table 3</ref>, experiment Disc. Note a reduction in performance in comparison to the full DRAEM architecture ( <ref type="table">Table 3</ref>, experiment DRAEM). The performance drop is due to overfitting of the discriminative sub-network to the simulated anomalies, which are not a faithful representation of the real ones.</p><p>Next, the discriminative power of the reconstructive subnetwork alone is analyzed by evaluating it as an autoencoder-based surface anomaly detector. The reconstructed image output of the sub-network is compared to the input image using the SSIM function <ref type="bibr" target="#b26">[27]</ref> to generate the anomaly map. The results of this approach are shown in <ref type="table">Table 3</ref>, experiment Recon.-AE. Recon.-AE outperforms the recent auto-encoder-based surface anomaly detection method AE-SSIM <ref type="bibr" target="#b4">[5]</ref> (see results in <ref type="table" target="#tab_1">Table 2</ref>) This suggests that simulated anomaly training introduces additional information into the auto-encoder-based training, but judging by the performance gap to DRAEM, the SSIM similarity function may not be optimal for extraction of the anomaly information. Indeed, using the recently proposed similarity function MS-GMS <ref type="bibr" target="#b30">[31]</ref> (Recon.-AE M SGM S ) improves the performance, but the results are still significantly worse than when using the entire DRAEM architecture, which indicates that both reconstructive and discriminative parts are required for optimal results.</p><p>To further emphasize the contribution of the DRAEM backbone, we replace it entirely by the recent state-of-theart supervised discriminative surface anomaly detection network <ref type="bibr" target="#b5">[6]</ref> and re-train with the simulated anomalies ( <ref type="table">Table 3</ref>, Bo?i? et al.). Performance substantially drops, which further supports the power of learning the anomaly deviation extent from normality rather than the anomaly or normality appearance.</p><p>Anomaly appearance patterns. DRAEM is re-trained using ImageNet <ref type="bibr" target="#b11">[12]</ref> as the texture source in the anomaly simulator to study the influence of the anomaly generation dataset (DRAEM ImageN et in <ref type="table">Table 3</ref>). Results are comparable to using the much smaller DTD <ref type="bibr" target="#b8">[9]</ref> dataset. <ref type="figure" target="#fig_5">Figure 9</ref> shows the performance at various anomaly source dataset sizes. Results suggest that the augmentation and opacity randomization substantially contribute to performance allowing remarkably small number of texture images (less than 10). As an extreme case, the anomaly textures are   <ref type="table">Table 3</ref>. Surface anomaly detection (Det.) and localization (Loc.) experiments of the ablation study grouped by shades of gray into (i) method architecture, (ii) anomaly source dataset, (iii) hard simulated anomaly generation, (iv) simulated anomaly shape, and (v) the performance of DRAEM for reference. generated as homogeneous regions of a randomly sampled color (DRAEM color ). Note that DRAEM color still achieves state-of-the-art results, further suggesting that DRAEM does not require simulations to closely match the real anomalies. The impact of the anomaly shape generator is evaluated by replacing the Perlin noise generator by a rectangular region generator. The anomaly mask is thus generated by sampling multiple rectangular areas for the anomalous regions (DRAEM rect in <ref type="table">Table 3</ref>). Training on rectangular anomalies causes only a slight performance drop and suggests that the simulated anomaly shape does not have to be realistic to generalize well to real world anomalies. Examples of anomalies generated in anomaly appearance ablation experiments are shown in <ref type="figure" target="#fig_6">Figure 10</ref>.</p><p>Low perturbation examples. The anomaly source image augmentation and the opacity randomization are re-sponsible for tightening the decision boundary around the anomaly-free training distribution. <ref type="table">Table 3</ref> reports the results of DRAEM variants trained (i) without image augmentation and opacity randomization (DRAEM no aug ), (ii) using only image augmentation (DRAEM img aug ) and (iii) using only opacity randomization (DRAEM ? ). There is a significant localization performance gap between DRAEM no aug and DRAEM, however, this can be significantly narrowed by using the opacity randomization in training even without image data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with supervised methods</head><p>Supervised methods require anomaly annotations at training time and cannot be evaluated on MVTec. We thus compare DRAEM with the supervised methods on the DAGM dataset <ref type="bibr" target="#b27">[28]</ref> that contains 10 textured object classes  with small anomalies visually very similar to the background, which makes the dataset particularly challenging for the unsupervised methods.</p><p>DRAEM is trained only on anomaly-free training samples using the same parameters as in previous experiments. The standard evaluation protocol on this dataset <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref> is used -the challenge is to classify whether the image contains the anomaly; localization accuracy is not measured, since the anomalies are only coarsely labeled. <ref type="table">Table 4</ref> shows that the best fully supervised methods nearly perfectly classify anomalous images, while the stateof-the-art unsupervised methods like RIAD <ref type="bibr" target="#b30">[31]</ref> and US <ref type="bibr" target="#b3">[4]</ref> struggle with subtle anomalies on highly textured regions 1 . DRAEM significantly outperforms these methods, and even the weakly supervised CADN <ref type="bibr" target="#b31">[32]</ref> by a large margin, obtaining classification performance close to the best fullysupervised methods, which is a remarkable result.</p><p>Furthermore, DRAEM outperforms all supervised methods in terms of anomaly localization accuracy on this dataset. Since the training images are only coarsely annotated with ellipses that approximately cover the surface defects and contain background, the supervised methods produce inaccurate localization in test images as well. In contrast, DRAEM does not use the labels at all, and thus produces more accurate anomaly maps, as shown in <ref type="figure">Figure 11</ref>. <ref type="bibr" target="#b0">1</ref> Please see the supplementary material for additional qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>AUROC TPR TNR CA Unsup.</p><p>RIAD <ref type="bibr" target="#b30">[31]</ref> 78.6 79.  <ref type="bibr" target="#b18">[19]</ref> 99.6 99.9 99.5 -Lin et al. <ref type="bibr" target="#b14">[15]</ref> 99.0 99.4 99.9 -Bo?i? et al. <ref type="bibr" target="#b5">[6]</ref> 100 100 100 100 <ref type="table">Table 4</ref>. DRAEM outperforms unsupervised methods on DAGM dataset and performs on par with fully supervised ones. <ref type="figure">Figure 11</ref>. Supervised methods replicate the approximate ground truth training annotations, leading to a low localization accuracy. DRAEM does not use the ground truth, yet produces far better localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>A discriminative end-to-end trainable surface anomaly detection and localization method DRAEM was presented. DRAEM outperforms the current state-of-the-art on the MVTec dataset [3] by 2.5 AUROC points on the surface anomaly detection task and by 13.5 AP points on the localization task. On the DAGM dataset <ref type="bibr" target="#b27">[28]</ref>, DRAEM delivers anomalous image classification accuracy close to fully supervised methods, while outperforming them in localization accuracy. This is a remarkable result since DRAEM is not trained on real anomalies. In fact, a detailed analysis shows that our paradigm of learning a joint reconstruction-anomaly embedding through a reconstructive sub-network significantly improves the results over standard methods and that an accurate decision boundary can be well estimated by learning the extent of deviation from reconstruction on simple simulations rather than learning either the normality or real anomaly appearance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Simulated anomaly generation process. The binary anomaly mask Ma is generated from Perlin noise P . The anomalous regions are sampled from A according to Ma and placed on the anomaly free image I to generate the anomalous image Ia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>The original anomaly source image (left) can be augmented several times (center) to generate a wide variety of simulated anomalous regions (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>The original image (a) contains anomalies which are difficult to mark in the ground truth mask (b) which causes a discrepancy between the ground truth and the output anomaly map (c,d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>The anomalous images are shown in the first row. The middle three rows show the anomaly maps generated by our implementation of Uninformed Students<ref type="bibr" target="#b3">[4]</ref>, PaDim<ref type="bibr" target="#b10">[11]</ref> and DRAEM, respectively. The last row shows the direct anomaly map output of DRAEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative examples. The original image, the anomaly map overlay, the anomaly map and the ground truth map are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>DRAEM achieves a remarkable detection and localization performance already at as low as 10 texture source images in the simulator when augmentation is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Anomalies simulated using the DTD<ref type="bibr" target="#b8">[9]</ref> (DRAEM), ImageNet<ref type="bibr" target="#b11">[12]</ref> (DRAEMImageNet), homogeneous color regions (DRAEM color ) and rectangular masks (DRAEMrect), from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>quantitatively</cell></row><row><cell>compares DRAEM with recent approaches on the task</cell></row><row><cell>of image-level surface anomaly detection. DRAEM sig-</cell></row><row><cell>nificantly outperforms all recent surface anomaly detec-</cell></row></table><note>tion methods, achieving the highest AUROC in 9 out of 15 classes and achieving comparable results in the other classes. It surpasses the previous best state-of-the-art ap- proach by 2.5 percentage points. The reduced performance in some classes could be explained by particularly diffi- cult anomalies that are close to the normal image distri- bution. The absence of a part of the object is especially difficult to detect. Regions, where the object features are missing, usually contain other commonly occurring fea- tures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Class</cell><cell>US[4]</cell><cell>RIAD[31] PaDim[11]</cell><cell>DRAEM</cell></row><row><cell>bottle</cell><cell cols="3">97.8 / 74.2 98.4 / 76.4 98.2 / 77.3 99.1 / 86.5</cell></row><row><cell>capsule</cell><cell cols="3">96.8 / 25.9 92.8 / 38.2 98.6 / 46.7 94.3 / 49.4</cell></row><row><cell>grid</cell><cell cols="3">89.9 / 10.1 98.8 / 36.4 97.1 / 35.7 99.7 / 65.7</cell></row><row><cell>leather</cell><cell cols="3">97.8 / 40.9 99.4 / 49.1 99.0 / 53.5 98.6 / 75.3</cell></row><row><cell>pill</cell><cell cols="3">96.5 / 62.0 95.7 / 51.6 95.7 / 61.2 97.6 / 48.5</cell></row><row><cell>tile</cell><cell cols="3">92.5 / 65.3 89.1 / 52.6 94.1 / 52.4 99.2 / 92.3</cell></row><row><cell cols="4">transistor 73.7 / 27.1 87.7 / 39.2 97.6 / 72.0 90.9 / 50.7</cell></row><row><cell>zipper</cell><cell cols="3">95.6 / 36.1 97.8 / 63.4 98.4 / 58.2 98.8 / 81.5</cell></row><row><cell>cable</cell><cell cols="3">91.9 / 48.2 84.2 / 24.4 96.7 / 45.4 94.7 / 52.4</cell></row><row><cell>carpet</cell><cell cols="3">93.5 / 52.2 96.3 / 61.4 99.0 / 60.7 95.5 / 53.5</cell></row><row><cell>hazelnut</cell><cell cols="3">98.2 / 57.8 96.1 / 33.8 98.1 / 61.1 99.7 / 92.9</cell></row><row><cell cols="4">metal nut 97.2 / 83.5 92.5 / 64.3 97.3 / 77.4 99.5 / 96.3</cell></row><row><cell>screw</cell><cell cols="3">97.4 / 7.8 98.8 / 43.9 98.4 / 21.7 97.6 / 58.2</cell></row><row><cell cols="4">toothbrush 97.9 / 37.7 98.9 / 50.6 98.8 / 54.7 98.1 / 44.7</cell></row><row><cell>wood</cell><cell cols="3">92.1 / 53.3 85.8 / 38.2 94.1 / 46.3 96.4 / 77.7</cell></row><row><cell>avg</cell><cell cols="3">93.9 / 45.5 94.2 / 48.2 97.4 / 55.0 97.3 / 68.4</cell></row></table><note>. Results for the task of anomaly localization on the MVTec dataset (AUROC / AP).</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skip-GANomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Ak?ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MVTec AD -A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Joint Conference on Computer Vision, Imaging and Computer Graphics theory and Applications</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-toend training of a two-stage neural network for defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Bo?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domen</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition ICPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Anomaly detection using one-class neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06360</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Padim: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelique</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Industrial Machine Learning, ICPR 2020</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An efficient network for surface defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zesheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards visually explaining variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8642" to="8651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image synthesizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Siggraph Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="296" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A compact convolutional neural network for textured surface anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domen</forename><surname>Ra?ki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Toma?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1331" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly segmentation via deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">424</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="22" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Anomaly detection neural network with dual auto-encoders gan and its industrial inspection applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ta-Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Han</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Fang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jauh-Hsiang</forename><surname>Nad Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakiem</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Tsu</forename><surname>Young</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly supervised learning for industrial optical inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wieler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mirrored autoencoders with simplex interpolation for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhanukiran</forename><surname>Vinzamuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconstruction by inpainting for visual anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitjan</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cadn: A weakly supervised learningbased category-aware object detection network for surface defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">107571</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
