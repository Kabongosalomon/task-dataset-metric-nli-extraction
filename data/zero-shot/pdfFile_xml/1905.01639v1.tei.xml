<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Video Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Video Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video inpainting aims to fill spatio-temporal holes with plausible content in a video. Despite tremendous progress of deep neural networks for image inpainting, it is challenging to extend these methods to the video domain due to the additional time dimension. In this work, we propose a novel deep network architecture for fast video inpainting. Built upon an image-based encoder-decoder model, our framework is designed to collect and refine information from neighbor frames and synthesize still-unknown regions. At the same time, the output is enforced to be temporally consistent by a recurrent feedback and a temporal memory module. Compared with the state-of-the-art image inpainting algorithm, our method produces videos that are much more semantically correct and temporally smooth. In contrast to the prior video completion method which relies on time-consuming optimization, our method runs in near realtime while generating competitive video results. Finally, we applied our framework to video retargeting task, and obtain visually pleasing results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video inpainting can help numerous video editing and restoration tasks such as undesired object removal, scratch or damage restoration, and retargeting. More importatnly, and apart from its converntional demands, video inpainting can be used in combination with Augmented Reality (AR) for a greater visual experience; Removing existing items gives more opportunities before overlaying new elements in a scene. Therefore, as a Diminished Reality (DR) technology, it opens up new opportunities to be paired with recent real-time / deep learning-based AR technologies. Moreover, there are several semi-online streaming scenarios such as automatic content filtering and visual privacy filtering. Only a small wait will lead to a considerable latency, thus making the speed itself an important issue.</p><p>Despite tremendous progress on deep learning-based inpainting of a single image, it is still challenging to extend these methods to video domain due to the additional time * Both authors have contributed equally to this work. <ref type="figure">Figure 1</ref>. Input video with mask boundaries in red (row-1). Video inpainting results by per-frame image inpainting <ref type="bibr" target="#b32">[33]</ref> (row-2), optimization-based method <ref type="bibr" target="#b10">[11]</ref> (row-3), and our method (row-4). Best viewed when zoomed-in.</p><p>dimension. The difficulties coming from complex motions and high requirement on temporal consistency make video inpainting a challenging problem. A straightforward way to perform video inpainting is to apply image inpainting on each frame individually. However, this ignores motion regularities coming from the video dynamics, and is thus incapable of estimating non-trivial appearance changes in image-space over time. Moreover, this scheme inevitably brings temporal inconsistencies and causes severe flickering artifacts. The second row in <ref type="figure">Fig. 1</ref> shows an example of directly applying the state-of-the-art feed-forward image inpainting <ref type="bibr" target="#b32">[33]</ref> in a frame-by-frame manner.</p><p>To address the temporal consistency, several methods have been developed to fill in the missing motion fields; using a greedy selection of local spatio-temporal patches <ref type="bibr" target="#b23">[24]</ref>, a per-frame diffusion-based technique <ref type="bibr" target="#b15">[16]</ref>, or an iterative optimization <ref type="bibr" target="#b10">[11]</ref>. However, the first two methods treat flow estimation to be independent of color estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref> and the last relies on time-consuming optimization <ref type="bibr" target="#b10">[11]</ref> (3rd row in <ref type="figure">Fig. 1</ref>), which is effective but limits their practicality and flexibility in general scenarios.</p><p>One might attempt to maintain temporal consistency by applying a post-processing method. Recently, Lai et al. <ref type="bibr" target="#b13">[14]</ref> proposed a deep CNN model that takes both original and per-frame processed videos as input and produces a temporally consistent video. However, their method is only applicable when those two input videos have a pixel-wise correspondences (e.g.colorization), which is not the case for video inpainting.</p><p>In this paper, we investigate whether a feed-forward deep network can be adapted to the video inpainting task. Specifically, we attempt to train a model with two core functions: 1) temporal feature aggregation and 2) temporal consistency preserving. For the temporal feature aggregation, we cast the video inpainting task as a sequential multi-tosingle frame inpainting problem. In particular, we introduce a novel 3D-2D feed-forward network which is built upon a 2D-based (image based) encoder-decoder model. The network is designed to collect and refine potential hints from neighbor frames and synthesize semantically-coherent video content in space and time. For the temporal consistency, we propose to use a recurrent feedback and a memory layer (e.g. convoutional LSTM <ref type="bibr" target="#b27">[28]</ref>). In addition, we use a flow loss to learn a warping of the previously synthesized frame and a warping loss to enforce both short-term and long-term consistency in results. Finally, we come up with a single, unified deep CNN model called VINet.</p><p>We conduct extensive experiments to validate the contributions of our design choices. We show that our multi-to-single frame formulation produces videos that are much more accurate and visually pleasing than the method of <ref type="bibr" target="#b32">[33]</ref>. An example result of our method is shown in the last row of <ref type="figure">Fig. 1</ref>. Our model sequentially processes video frames of arbitrary length and requires no optical flow computation at the test time, thus runs at a near real-time rate.</p><p>Contribution. In summary, our contribution is as follow.</p><p>1. We cast video inpainting as a sequential multi-tosingle frame inpainting task and present a novel deep 3D-2D encoder-decoder network. Our method effectively gathers features from neighbor frames and synthesizes missing content based on them.</p><p>2. We use a recurrent feedback and a memory layer for the temporal stability. Along with the effective network design, we enforce strong temporal consistency via two losses: flow loss and warping loss.</p><p>3. Up to our knowledge, it is the first work to provide a single, unified deep network for the general video inpainting task. We conduct extensive subjective and objective evaluations and show its efficacy. Moreover, we apply our method to video retargeting and superresolution tasks, demonstrating favorable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Significant progress has been made on image inpainting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>, to a point of where commercial solutions are now available <ref type="bibr" target="#b1">[2]</ref>. However, video inpainting algorithms have been under-investigated. This is due to the additional time dimension which introduces major challenges such as severe viewpoint changes, temporal consistency preserving, and high computational complexity. Most recent methods found in the literature address these issues using either object-based or patch-based approaches.</p><p>In object-based methods, a pre-processing is required to split a video into foreground objects and background, and it is followed by an independent reconstruction and merging step at the end of algorithms. Previous efforts which fall under this category are homography-based algorithms that are based on the graph-cut <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. However, the major limitation of these object-based methods is that the synthesized content has to be copied from the visible regions. Therefore, these methods are mostly vulnerable to abrupt appearance changes such as scale variations, e.g. when an object moves away from the camera.</p><p>In patch-based methods, the patches from known regions are used to fill in a mask region. For example, Patwardhan et al. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> extend the well-known texture synthesis technique <ref type="bibr" target="#b7">[8]</ref> to video inpainting. However, these methods either assume static cameras <ref type="bibr" target="#b18">[19]</ref> or constrained camera motion <ref type="bibr" target="#b19">[20]</ref> and are based on a greedy patch-filling process where the early errors are inevitably propagated, yielding globally inconsistent outputs.</p><p>To ensure the global consistency, patch-based algorithms have been cast as a global optimization problem. Wexler et al. <ref type="bibr" target="#b26">[27]</ref> present a method that optimizes a global energy minimization problem for 3D spatio-temporal patches by alternating between patch search and reconstruction steps. Newson et al. <ref type="bibr" target="#b16">[17]</ref> extend this by developing a spatiotemporal version of PatchMatch <ref type="bibr" target="#b1">[2]</ref> to strengthen the temporal coherence and speed up the patch matching. Recently, Huang et al. <ref type="bibr" target="#b10">[11]</ref> modify the energy term of <ref type="bibr" target="#b26">[27]</ref> by adding an optical flow term to enforce temporal consistency. Although these methods are effective, their biggest limitations are high computational complexity and the absolute dependence upon the pre-computed optical flow which cannot be guaranteed to be accurate in complex sequences.</p><p>To tackle these issues, we propose a deep learning based method for video inpainting. To better exploit temporal information coming from multiple frames and be highly efficient, we construct a 3D-2D encoder-decoder model, that can provide traceable features revealed from the video dynamics. It takes total 6 frames as input; 5 source frames and 1 reference frame (i.e.the frame to be inpainted). We learn the feature flow between frames to deal with both hole-filling and coherence. The still-unknown regions are synthesized in a semantically natural way based on the sur-rounding context. We argue that our method provides a better prospect than the previous optimization-based techniques in that deep CNNs are excellent at learning spatial semantics and temporal dynamics from an ever-growing vast amount of video data. To our best knowledge, this is the first work that deeply addresses the general video inpainting problem via a deep CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Video inpainting aims to fill in arbitrary missing regions in video frames X T 1 := {X 1 , X 2 , ..., X T }. The reconstructed regions should be either accurate as in the groundtruth frames Y T 1 := {Y 1 , Y 2 , ..., Y T } and consistent in space and time. We formulate the video inpainting problem as learning a mapping function from X T 1 to the output</p><formula xml:id="formula_0">Y T 1 := {? 1 ,? 2 , ...,? T } such that the conditional distribu- tion p(? T 1 |X T 1 ) is identical to p(Y T 1 |X T 1 )</formula><p>. Through matching the conditional distributions, the network learns to generate realistic and temporally-consistent output sequences. To simplify the problem, we make a Markov assumption where we factorize the conditional distribution to a product form. In this form, the naive frame-by-frame inpainting can be formulated as</p><formula xml:id="formula_1">p(? T 1 |X T 1 ) = T t=1 p(? t |X t ).<label>(1)</label></formula><p>However, to obtain visually pleasing video results, we argue that the generation of t-th frame? t should be consistent with 1) spatio-temporal neighbor frames X t+N t?N where N denotes a temporal radius, 2) the previously generated frame? t?1 and 3) all previous history encoded in a recurrent memory M t . Thus, we propose to learn the conditional distribution of</p><formula xml:id="formula_2">p(? T 1 |X T 1 ) = T t=1 p(? t |X t+N t?N ,? t?1 , M t ).<label>(2)</label></formula><p>In our experiments, we set N to 2, taking two lagging and two leading frames to recover the current frame. We sample frames with a temporal stride 3, such that X t+N t?N := {X t?6 , X t?3 , X t , X t+3 , X t+6 }. We want to recover the current frame by both aggregating information from neighbor frames and synthesizing totally blind regions jointly. At the same time, the output is enforced to be temporally consistent with the past predictions by the recurrent feedback (? t?1 ) and the memory (M t ). We train a deep network D to model the conditional distribution p</p><formula xml:id="formula_3">(? t |X t+N t?N ,? t?1 , M t ) as? t = D(X t+N t?N ,? t?1 , M t ).</formula><p>We obtain the final output Y T 1 by applying the function D in an autoregressive manner. Our multi-to-single frame formulation outperforms a single-frame baseline and even produces results comparable with the optimization-based method, as described in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Design</head><p>Our full model (VINet) jointly learns to inpaint the video and maintain temporal consistency. The overview of VINet is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Multi-to-Single Frame Video Inpainting</head><p>In videos, the occluded or removed parts in a frame are often revealed in the past/future frames as the objects move and the viewpoint changes. If such hints exist in the temporal radius, those disclosed content can be borrowed to recover the current frame. Otherwise, the still-unknown regions should be synthesized. To achieve this, we construct our model as an encoder-decoder network that learns such temporal feature aggregation and single-frame inpainting simultaneously. The network is designed to be fully convolutional, which can handle arbitrary size input.</p><p>Source and reference encoders. The encoder is a multipletower network with source and reference streams. The source stream takes past and future frames with the inpainting masks as input. For the reference stream, the current frame and its inpainting mask are provided. We concatenate the image frames and the masks along the channel axis, and feed into the encoder. In practice, we use a 6-tower encoder: 5 source streams with weight-sharing that take two lagging (X t?6 , X t?3 ) and two leading frames (X t+3 , X t+6 ), and the previously generated frame (? t?1 ), and 1 reference stream. The source features that are non-overlapping with the reference features can be borrowed to inpaint the missing regions by the following feature flow learning and learnable feature composition.</p><p>Feature flow learning. Before directly combining the source and reference features, we propose to explicitly align the feature points. This strategy helps our model easily borrow traceable features from the neighbor frames. To achieve this, we insert flow sub-networks to estimate the flows between the source and reference feature maps in four different spatial scales (1/8, 1/4, 1/2, and 1). We adopt the coarse-to-fine structure of PWCNet <ref type="bibr" target="#b24">[25]</ref>. The explicit flow supervision is only given at the finest scale (i.e. 1) and only between the consecutive two frames, where we extract the pseudo-ground-truth flow W t?t?1 between Y t and Y t?1 using FlowNet2 <ref type="bibr" target="#b12">[13]</ref>.</p><p>Learnable Feature Composition. Given the aligned feature maps from the five source streams, they are concatenated along the time dimension and fed into a 5 ? 3 ? 3 (THW) convolution layer that produces a spatio-temporally aggregated feature map F s with the time dimension of 1. This is designed to dynamically select source feature points across the time axis, by highlighting the features complementary to the reference features and ignoring otherwise. For each 4 scales, we employ a mask sub-network to com- <ref type="figure">Figure 2</ref>. The overview of VINet. Our network takes in multiple frames (Xt?6, Xt?3, Xt, Xt+3, Xt+6) and the previously generated frame (?t?1), and generates the inpainted frame (?t) as well as the flow map (?t?t?1). We employ both flow sub-networks and mask sub-networks at 4 scales (1/8, 1/4, 1/2, and 1) to aggregate and synthesize feature points progressively. For temporal consistency, we use a recurrent feedback and a temporal memory layer (ConvLSTM) along with two losses: flow loss and warp loss. The orange arrows denote the ?2 upsampling for residual flow learning as in <ref type="bibr" target="#b24">[25]</ref> for 5 streams, while the thinner orange arrow is for only the stream from?t?1. The mask sub-networks are omitted in the figure for the simplicity.</p><p>bine the aggregated feature map F s with the reference feature map F r . The mask sub-network consists of three convolution layers and takes the absolute difference of the two feature maps |F s ? F r | as input and produces single channel composition mask m, as suggested in <ref type="bibr" target="#b5">[6]</ref>. By using the mask, we can gradually combine the warped features and the reference features. At the scale of 1/8, the composition is done by</p><formula xml:id="formula_4">F c 1/8 = (1 ? m 1/8 ) F r 1/8 + m 1/8 F s 1/8 ,<label>(3)</label></formula><p>where is the element-wise product operator. Decoder. To pass image details to the decoder, we employ skip connections as in U-net <ref type="bibr" target="#b22">[23]</ref>. To prevent the concern raised by [32] that skip connections contain zero values at the masked region, our skip-connections pass the composite features similarly to Eq. (3), as</p><formula xml:id="formula_5">F c 1/4 = (1 ? m 1/4 ) F r 1/4 + m 1/4 F s 1/4 ,<label>(4)</label></formula><formula xml:id="formula_6">F c 1/2 = (1 ? m 1/2 ) F r 1/2 + m 1/2 F s 1/2 .<label>(5)</label></formula><p>At the finest scale, the estimated optical flow? t?t?1 is used to warp the previous output? t?1 to the current raw output? t . We then blend this warped image and the raw output with the composition mask m 1 , to obtain our final output? t a?</p><formula xml:id="formula_7">Y t = (1 ? m 1 ) ? t + m 1 ? t?t?1 (? t?1 ). (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Recurrence and Memory</head><p>To strongly enforce the temporal coherence on the video output, we propose to use the recurrent feedback loop (? t?1 ) and the temporal memory layer (M t ) as formulated in Eq. (2).</p><p>Our formulation encourages the current output to be conditional to the previous output frame. The knowledge from the previous output encourages the traceable features to be kept unchanged, while the untraceable (e.g. occlusion) points to be synthesized. This not only helps the output to be consistent along the motion trajectories but also avoids ghosting artifacts at occlusions or motion discontinuities.</p><p>While the recurrent feedback connects the consecutive frames, filling in the large holes requires more long-term (e.g. 5 frames) knowledge. At this point, the temporal memory layer can help to connect internal features from different time steps in the long term. We adopt a convolutional LSTM (ConvLSTM) layer and a warping loss as suggested in <ref type="bibr" target="#b13">[14]</ref>. In particular, we feed the composite feature F c at the scale 1/8 to the ConvLSTM at every time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Losses</head><p>We train our network to minimize the following loss function,</p><formula xml:id="formula_8">L = ? R L R + ? F L F + ? W L W ,<label>(7)</label></formula><p>where L R is the reconstruction loss, L F is the flow estimation loss, and L W is the warping loss. The balancing weights ? R , ? F , ? W are set to 1, 10, 1 respectively throughout the experiments. For the temporal losses L F and L W , we set the number of recurrences as 5 (T = 5).</p><p>L R consists of two terms, L 1 and L ssim ,</p><formula xml:id="formula_9">L 1 = ? t ? Y t 1 ,<label>(8)</label></formula><formula xml:id="formula_10">L ssim = ( (2?? t ? Yt + c 1 )(2?? tYt + c 2 ) (? 2? t + ? 2 Yt + c 1 )(? 2 Yt + ? 2 Yt + c 2 ) ),<label>(9)</label></formula><formula xml:id="formula_11">L R = L 1 + L ssim ,<label>(10)</label></formula><p>where? t , Y t denote the predicted frame and the groundtruth frame respectively. ?, ? denote the average, variance, respectively. c 1 , c 2 denote the stabilization constants which are respectively set to 0.01 2 , 0.03 2 .</p><p>The flow loss L F is defined as</p><formula xml:id="formula_12">T t=2 ( W t?t?1 ?? t?t?1 1 + Y t ?? t?t?1 (Y t?1 ) 1 ),<label>(11)</label></formula><p>where W t?t?1 is the pseudo-ground-truth backward flow between the target frames, Y t and Y t?1 , extracted by FlowNet2 <ref type="bibr" target="#b12">[13]</ref>. In Eq. (11), the first term is the endpoint error between the groundturth and the estimated flow, and the second is the warping error when the flow is used to warp the previous target frame to the next target frame.</p><p>The warping loss L W includes L st and L lt as,</p><formula xml:id="formula_13">L st = T t=2 M t?t?1 ? t ? W t?t?1 (Y t?1 ) 1 ,<label>(12)</label></formula><formula xml:id="formula_14">L lt = T t=2 M t?1 ? t ? W t?1 (Y 1 ) 1 ,<label>(13)</label></formula><formula xml:id="formula_15">L W = L st + L lt .<label>(14)</label></formula><p>We follow the protocol in <ref type="bibr" target="#b13">[14]</ref> that uses FlowNet2 <ref type="bibr" target="#b12">[13]</ref> to obtain M t?t?1 and W t?1 , which respectively denote the binary occlusion mask and the backward optical flow between the target frames Y t and Y t?1 . We adopt both short-term and long-term temporal losses. Note that we use ground-truth target frames in the warping operation since the synthesizing ability is imperfect during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Two-Stage Training</head><p>We employ a two-stage training scheme that gradually learns the core functionalities for video inpainting; 1) We first train the model without the recurrent feedback and memory to focus on learning the temporal feature aggregation. At this stage, we only use the reconstruction loss L R ; 2) We then add the recurrent feedback and the Con-vLSTM layer, and fine-tune the model using the full loss function (Eq. (7)) for temporally coherent predictions. We use videos in the Youtube-VOS dataset <ref type="bibr" target="#b28">[29]</ref> as ground-truth for the training. It is a large-scale dataset for video object segmentation containing 4000+ YouTube videos with 70+ common objects. All video frames are resized to 256 ? 256 pixels for training and testing.</p><p>Video mask dataset. In general video inpainting, the spatio-temporal holes consist in diverse motion and shape changes. To simulate this complexity during training, we create the following four types of video masks.</p><p>1. Random square: We randomly mask a square box in each frame. The visible regions each of input frames are mostly complementary so that the network can clearly learn how to align, copy, and paste neighboring feature points.</p><p>2. Flying square: The motion of the inpainting holes is rather regularized than random in real scenarios. To simulate such regularity, we shift a square by a uniform step size in one direction across the input frames.</p><p>3. Arbitrary mask: To simulate diverse hole shapes and sizes, we use the irregular mask dataset <ref type="bibr" target="#b14">[15]</ref> which consists of random streaks and holes of arbitrary shapes. During training, we apply random transformations (translation, rotation, scaling, sheering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Video object mask:</head><p>In the context of the video object removal task, masks with the most realistic appearance and motion can be obtained from video object segmentation datasets. We use the foreground segmentation masks of the YouTube-VOS dataset <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference</head><p>We assume that the inpainting masks for all video frames are given. To avoid any data overlap between training and testing, we obtain object masks from the DAVIS dataset <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, the public benchmark dataset for video object segmentation. It contains dynamic scenes, complex camera movements, motion blur effects, and large occlusions. The inpainting mask is constructed by dilating the ground-truth segmentation mask. Our method processes frames recursively in a sliding window manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Implementation Details</head><p>Our model is implemented using Pytorch v0.4, CUDNN v7.0, CUDA v9.0. It run on the hardware with Intel(R) Xeon(R) (2.10GHz) CPU and NVIDIA GTX 1080 Ti GPU. The model runs at 12.5 fps on a GPU for frames of 256?256 pixels. We use Adam optimizer with ? = (0.9, 0.999) and a fixed learning rate 1e-4. We train our model from scatch. The first and second training stage takes about 1 day each using four NVIDIA GTX 1080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct experiments to analyze our two major design choices. Specifically, we visualize the learned multi-to-single mechanism and show the impact of the added recurrence and memory. We then evaluate our video results both quantitatively and qualitatively, (a) (b) <ref type="figure">Figure 3</ref>. Visualization of the learned feature composition. Input frames are on the odd rows, and corresponding feature flows referential to the center, and the inpainted frame are on the even rows. Our network successfully aligns and integrates the source features to fill in the large and complex hole in the reference frame. compared with the state-of-the-art baselines. Finally, we demonstrate the applicability of our framework on video retargeting and video super-resolution tasks.</p><p>Baselines. We compare our approach to two state-of-the-art baselines in the literature by running their test codes with our testing videos and masks.</p><p>? Yu et al. <ref type="bibr" target="#b32">[33]</ref>: A feed-forward CNN based method, which is designed for single image inpainting. We processes videos frame-by-frame without using any temporal information.</p><p>? Huang et al. <ref type="bibr" target="#b10">[11]</ref>: An optimization-based video completion method, which jointly estimates global flow and color. It requires on-the-fly optical flow computation and is extremely time-consuming. <ref type="figure">Fig. 3</ref> shows that the proposed model explicitly borrows visible neighbor features to synthesize the missing content. For the visualization, we take the model of the first training stage and plot the learned feature flow from each of the four source streams to the reference stream, at 128 ? 128 pixel resolution. We observe that even with a large and complex hole in the reference (center) frame, our network is able to align the source feature maps to the reference and integrate them to fill in the hole. Even without an explicit flow supervision, our flow sub-network is able to warp the feature points in visible regions and shrink the unhelpful zero features in masked regions. Moreover, these potential hints are DAVIS masks on Sintel frames Frame-by-frame <ref type="bibr" target="#b32">[33]</ref> 0.0429 Optimization <ref type="bibr" target="#b10">[11]</ref> 0.0343 VINet (agg. only) 0.0383 VINet (agg. + T.C.) 0.0015 <ref type="table">Table 1</ref>. Flow warping errors. We evaluate the flow warping errors on the Sintel dataset using 21 videos and ground truth flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visualization of Learned Feature Composition</head><p>DAVIS masks on DAVIS frames Frame-by-frame <ref type="bibr" target="#b32">[33]</ref> 0.0080 Optimization <ref type="bibr" target="#b10">[11]</ref> 0.0053 VINet (agg. only) 0.0073 VINet (agg. + T.C.) 0.0046 <ref type="table">Table 2</ref>. FID scores. We evaluate the FID scores on the DAVIS dataset using 20 videos.</p><p>adjusted according to the spatio-temporal semantics, rather than copied-and-pasted in a fixed manner. One example is shown in <ref type="figure">Fig. 3-(b)</ref> where the eyes of the hamster are synthesized half-closed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Improvement on Temporal Consistency</head><p>We compare the temporal consistencies of our video results before and after adding the recurrent feedback and the convLSTM. To validate the effectiveness of our method, we also compare with the two representative baselines mentioned above <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>. Since the Sintel dataset <ref type="bibr" target="#b3">[4]</ref> provides ground-truth optical flows, we use it to quantitatively measure the flow warping errors <ref type="bibr" target="#b13">[14]</ref>. We use the object masks in the DAVIS dataset <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> as our inpainting mask sequences. We take 32 frames each from 21 videos in Sintel to constitute our inputs and experiment for five trials. For each trial, we randomly select 21 videos of length 32+ from DAVIS to create corresponding mask sequences and keep them unchanged for all the methods.</p><p>In <ref type="table">Table.</ref> 1, we report the flow warping errors averaged over the videos and trials. It shows that our full model outperforms other baselines by large margins. Even the global (heavy) optimization method <ref type="bibr" target="#b10">[11]</ref> performs marginally better than our 1st-stage method and has a much larger error than our full model. Not surprisingly, Yu et al.'s method turns out to be the least temporally consistent. Note that the error of our full model is reduced by a factor of 10 after adding the recurrent feedback and the convLSTM layer, implying that they significantly improve the temporal stability in the short and long term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Spatio-Temporal Video Quality</head><p>Wang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a video version of the inception score (FID) to quantitatively evaluate the quality of video generation. We take this metric to evaluate the quality of video inpainting as it measures the spatio-temporal For each input sequence, we show representative frames with mask boundaries in red. We show the inpainted results using our method in even rows. quality in a perceptual level. As in <ref type="bibr" target="#b25">[26]</ref>, we follow the protocol that uses the I3D network <ref type="bibr" target="#b4">[5]</ref> pretrained on a video recognition task to measure the distance between the spatiotemporal features extracted from the output videos and the ground-truth videos.</p><p>For this experiment, we take 20 videos in the DAVIS dataset. For each video, we ensure to choose a different video out of the other 19 videos to make a mask sequence, so that we have the setting where our algorithm is supposed to recover the original videos rather than remove any parts. We use the first 64 frames for both input and mask videos. We run five trials as in Sec. 4.2 and average the FID scores over the videos and trials. <ref type="table">Table.</ref> 2 summarizes the results. Our method has the smallest FID among the compared methods. This implies that our method achieves both better visual quality and temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">User Study on Video Object Removal</head><p>We apply our approach to remove dynamically moving objects in videos. We use 24 videos from the DAVIS dataset <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> of which the names are listed in <ref type="figure" target="#fig_2">Fig. 6</ref>. Examples of our results are in <ref type="figure" target="#fig_0">Fig. 4</ref>. We perform a human subjective test for evaluating the visual quality of inpainted videos. We compare our method with the strong optimization baseline <ref type="bibr" target="#b10">[11]</ref> which is specifically aimed for the video completion task.</p><p>In each testing case, we show the original input video, our removal result and the result of Huang et al. on the same screen. The order of the two removal video results is shuffled. To ensure that a user has enough time to distinguish the difference and make a careful judge, we play all the video results once at the original speed and then once at 0.5? speed. Also, a user allows seeing videos multiple times. Each participant is asked to choose a preferred  result or tie. A total of 30 users participated in this study. We specifically ask each participant to check for both image quality and temporal consistency. The user study results are summarized in <ref type="figure" target="#fig_2">Fig. 6</ref>. It shows that, while there are different preferences across video samples, our method is preferred more often by the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Application to Video Retargeting</head><p>Video retargeting aims to adjust the aspect ratio (or size) of frames to fit the target aspect ratio while maintaining salient content in a video. We propose to solve video retargeting by removing and then adding, which is a potential pipeline where our framework would run in combination with other AR (i.e.overlaying) technologies. Specifically, we first remove the salient content by inpainting the background, resize the inpainted frames into the target aspect ratio, and then overlay the salient content after the desirable rescaling. To simplify the settings, we target to horizontally or vertically shrink the frames while keeping the original aspect ratio of the moving object. The saliency masks can be automatically estimated, for example, by a feed-forward CNN <ref type="bibr" target="#b6">[7]</ref>, however we assume a more constrained scenario where the saliency masks are given as the object segmentation masks for all frames. Our method yields little warble and jittering over time and produces natural video sequences. <ref type="figure" target="#fig_1">Fig. 5</ref> shows examples of the retargeted frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitation</head><p>We observe color saturation artifacts when there is a large and long occlusion in a video. The discrepancy error of the synthesized color propagates over time, causing inaccurate warping. The regions that have not been revealed in the temporal radius is synthesized blurry. Also, due to the limited memory footprint, we only experimented with 256 ? 256 px frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel framework for video inpainting. Based on the multi-to-single encoder-decoder network, our model learns to aggregate and align the feature maps from neighbor frames to inpaint videos. We use the recurrent feedback and the temporal memory to encourage temporally coherent output. Our extensive experiments demonstrate that our method achieves superior visual quality than the state-of-the-art image inpainting solution and performs favorably against an optimization method both qualitatively and quantitatively. Despite some limitations, we argue that a well-posed feed-forward network has a great potential to avoid computation-heavy optimization method and boosts its applicability in many related vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Object removal from DAVIS video sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>(a) First input frame (b) Horizontally shrunk frames (c) Vertically shrunk frames Extension to video retargeting. (a) Original first frame. (b) Horizontally shrunk frames. (c) Vertically shrunk frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>User study results.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coherent online video style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Computer Vision (ICCV</title>
		<meeting>Intl. Conf. Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly-and self-supervised learning for content-aware deep image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4568" to="4577" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">iccv</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">1033</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Background inpainting for videos with dynamic objects and a free-moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="682" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How not to be seenobject removal from videos of crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Computer Vision (ECCV)</title>
		<meeting>of European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07723</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Full-frame video stabilization with motion inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1150" to="1163" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video inpainting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1993" to="2019" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video inpainting of occluding and occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing, 2005. ICIP 2005. IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video inpainting under constrained camera motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalm?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="545" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video completion by motion field transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Space-time video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="127" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Youtube-vos: Sequenceto-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00461</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
