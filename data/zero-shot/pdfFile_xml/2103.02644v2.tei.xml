<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compute and memory efficient universal sound source separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-14">14 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
							<email>etzinis2@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compute and memory efficient universal sound source separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-14">14 Jul 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) Paris Smaragdis University of Illinois at Urbana-Champaign &amp; Adobe Research 2 Efthymios Tzinis et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Audio source separation ? low-cost neural networks ? deep learning ? real-time processing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in audio source separation led by deep learning has enabled many neural network models to provide robust solutions to this fundamental estimation problem. In this study, we provide a family of efficient neural network architectures for general purpose audio source separation while focusing on multiple computational aspects that hinder the application of neural networks in real-world scenarios. The backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. This mechanism enables our models to obtain high fidelity signal separation in a wide variety of settings where a variable number of sources are present and with limited computational resources (e.g. floating point operations, memory footprint, number of parameters and latency). Our experiments show that SuDoRM-RF models perform comparably and even surpass several state-of-the-art benchmarks with significantly higher computational resource requirements. The causal variation of SuDoRM-RF is able to obtain competitive performance in real-time speech separation of around 10dB scale-invariant signal-to-distortion ratio improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a laptop device.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of the deep learning era has enabled the effective usage of neural networks towards single-channel source separation with mask-based architectures <ref type="bibr" target="#b11">[12]</ref>. Recently, end-to-end source separation in time-domain has shown state-of-the-art results in a variety of separation tasks such as speech separation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref>, universal sound separation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref> and music source separation <ref type="bibr" target="#b4">[5]</ref>. The separation module of ConvTasNet <ref type="bibr" target="#b23">[24]</ref> and its variants <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref> consist of multiple stacked layers of depth-wise separable convolutions <ref type="bibr" target="#b32">[33]</ref> which can aptly incorporate long-term temporal relationships. Building upon the effectiveness of a large temporal receptive field, a dual-path recurrent neural network (DPRNN) <ref type="bibr" target="#b21">[22]</ref> has shown remarkable performance on speech separation. Demucs <ref type="bibr" target="#b4">[5]</ref> has a refined U-Net structure <ref type="bibr" target="#b30">[31]</ref> and has shown strong performance improvement on music source separation. Specifically, it consists of several convolutional layers in each a downsampling operation is performed in order to extract high dimensional features. A two-step approach has been introduced in <ref type="bibr" target="#b35">[36]</ref> and showed that universal sound separation models could be further improved when working directly on the latent space and learning the ideal masks on a separate step.</p><p>Despite the dramatic advances in source separation performance, the computational complexity of the aforementioned methods might hinder their extensive usage across multiple devices. Specifically, many of these algorithms are not amenable to, e.g., embedded systems deployment, or other environments where computational resources are constrained. Additionally, training such systems is also an expensive computational undertaking which can amount to significant costs.</p><p>Several studies, mainly in the image domain, have introduced more efficient architectures in order to overcome the growing concern of large models with high computational requirements. Models with depth-wise separable convolutions <ref type="bibr" target="#b32">[33]</ref> have shown strong potential for several image-domain tasks <ref type="bibr" target="#b3">[4]</ref> while significantly reducing the computational requirements. Thus, several variants such as MobileNets <ref type="bibr" target="#b10">[11]</ref> have been proposed for deep learning on edge devices. However, convolutions with a large dilation factor might inject several artifacts and thus, lightweight architectures that combine several dilation factors in each block have been proposed for image tasks <ref type="bibr" target="#b25">[26]</ref>. More recent studies propose meta-learning algorithms for optimizing architecture configurations given specific computational resource and accuracy requirements <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Despite the recent success of low-resource architectures in the image domain, little progress has been made towards proposing efficient architectures for audio tasks and especially source separation. In <ref type="bibr" target="#b12">[13]</ref> a WaveRNN is used for efficient audio synthesis in terms of floating point operations (FLOPs) and latency. Other studies have introduced audio source separation models with reduced number of trainable parameters <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23]</ref> and binarized models <ref type="bibr" target="#b15">[16]</ref>. Modern approaches, mainly in speech enhancement and music source separation, have been focusing on developing models which are capable of real-time inference. Specifically, a temporal convolutional network for real time speech enhancement has been proposed in <ref type="bibr" target="#b26">[27]</ref> while the latest state-of-the-art performance has been obtained by a real-time variation of Demucs for online speech denoising <ref type="bibr" target="#b5">[6]</ref>. In a similar sense real-time music source separation models have been proposed in <ref type="bibr" target="#b8">[9]</ref> and a system capable of real-time speech separation from background music has been implemented in <ref type="bibr" target="#b13">[14]</ref>.</p><p>In this study, we propose a novel efficient neural network architecture for audio source separation while following a more holistic approach in terms of computational resources that we take into consideration (FLOPs, latency and total memory requirements). Our proposed model performs SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) using depth-wise convolutions. By doing so, SuDoRM-RF exploits the effectiveness of iterative temporal resampling strategies <ref type="bibr" target="#b6">[7]</ref> and avoids the need for multiple stacked dilated convolutional layers <ref type="bibr" target="#b23">[24]</ref>. We also propose improved versions of the aforementioned architecture with significant benefits in terms of computational resource requirements as well as causal variations where online inference is available. We report a separation performance comparable to or even better than several recent state-of-the-art models on speech, environmental and universal sound separation tasks with significantly lower computational requirements. Our experiments suggest that SuDoRM-RF models a) could be deployed on devices with limited resources, b) be trained significantly faster and achieve good separation performance and c) scale well when increasing the number of parameters. Our code is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sudo rm -rf network architecture</head><p>On par with many state-of-the-art approaches in the literature <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5]</ref>, SuDoRM-RF performs end-to-end audio source separation using a mask-based architecture with adaptive encoder and decoder basis. We have extended our basic model in order to also remove the mask estimation process by introducing SuDoRM-RF++ that directly estimates the latent representations of the sources in the adaptive front-end domain. First, we describe all the modules which are needed for both architectures and describe extensively the inference path for our basic SuDoRM-RF architecture. In <ref type="figure" target="#fig_0">Figure 1</ref>, both architectures are shown. Consequently we also present the extensions of our original SuDoRM-RF model including: its improved version SuDoRM-RF++ (Section 2.4), a variant of SuDoRM-RF++ including group communication <ref type="bibr" target="#b22">[23]</ref> (Section 2.5) as well as its causal variation C-SuDoRM-RF++ (Section 2.6).</p><p>The input is the raw signal from a mixture x ? R T with T samples in the time-domain. First, we feed the input mixture x to an encoder E in order to obtain a latent representation for the mixture v</p><formula xml:id="formula_0">x = E (x) ? R C E ?L .</formula><p>Consequently the latent mixture representation is fed through a separation module S which estimates the corresponding masks m i ? R C E ?L for each one of the N sources s 1 , ? ? ? , s N ? R T which constitute in the mixture. The estimated latent representation for each source in the latent space v i is retrieved by multiplying element-wise an estimated mask m i with the encoded mixture representation v x . Finally, the reconstruction for each source s i is obtained by using a decoder D to transform the latent-space v i source estimates back into the time-domain s i = D ( v i ). An overview of the SuDoRM-RF architecture is displayed in <ref type="figure" target="#fig_0">Figure 1a</ref>. The encoder, separator and decoder modules are described in Sections 2.1, 2.2 and 2.3, respectively. For simplicity of our notation we will describe the whole architecture assuming that the processed batch size is one. Moreover, we are going to define some useful operators of the various convolutions which are used in SuDoRM-RF.</p><formula xml:id="formula_1">S e p a r a t o r ? U - C o n v B l o c k s ? 1 ? 2 E n c o d e r ? D e c o d e r ? ? 1 ? 2 ? 1 ? 2 ( 0 ) ( ) C o n v 1 D , , ? ? ? 2 C o n v 1 D , 1 , 1 L a y e r N o r m C o n v 1 D , , 1 ? C o n v T r 1 D , , ? ? ? 2 T ( 1 ) ( 2 ) T T T T r a n s p o s e S o f t m a x (a) SuDoRM-RF architecture. (b) SuDoRM-RF++ architecture.</formula><p>Definition 1 Conv1D C,K,S : R Cin?Lin ? R C?L defines a kernel W ? R C?Cin?K and a bias vector b ? R C . When applied on a given input x ? R Cin?Lin it performs a one-dimensional convolution operation with stride equal to S as shown next:</p><formula xml:id="formula_2">Conv1D C,K,S (x) i,l = b i + Cin j=1 K k=1 W i,j,k ? x j,S?l?k ,<label>(1)</label></formula><p>where the indices i, j, k, l denotes the output channel, the input channel, the kernel sample and, the temporal index, respectively. Note that without loss of generality and performing appropriate padding, the last dimension of the output representation would be L = Lin /S .</p><p>Definition 2 ConvTr1D C,K,S : R Cin?Lin ? R C?L defines a one-dimensional transpose convolution. Since any convolution operation could be expressed as a matrix multiplication, transposed convolution can be directly understood as the gradient calculation for a regular convolution w.r.t. its input <ref type="bibr" target="#b33">[34]</ref>.</p><p>Definition 3 DWConv1D C,K,S : R Cin?Lin ? R C?L defines a one-dimensional depth-wise convolution operation <ref type="bibr" target="#b32">[33]</ref>. In essence, this operator defines G = C in separate one-dimensional convolutions</p><formula xml:id="formula_3">F i = [Conv1D C G ,K,S ] i with i ? {1, ? ? ? , G} where C G = C /G .</formula><p>Given an input x ? R Cin?Lin the ith onedimensional convolution contributes to C G = C /G output channels by considering as input only the ith row of the input as described below:</p><formula xml:id="formula_4">DWConv1D C,K,S (x) = Concat ({F i (x i ) , ?i}) ,<label>(2)</label></formula><p>where Concat(?) performs the concatenation of all individual one-dimensional convolution outputs across the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder</head><p>The encoder E architecture consists of a one-dimensional convolution with kernel size K E and stride equal to K E /2 similar to <ref type="bibr" target="#b23">[24]</ref>. Each convolved input audio segment of K E samples is transformed to a C E -dimensional vector representation where C E is the number of output channels of the 1D-convolution. We force the output of the encoder to be strictly non-negative by applying a rectified linear unit (ReLU) activation on top of the output of the 1D-convolution. Thus, the encoded input mixture representation could be expressed as:</p><formula xml:id="formula_5">v x = E (x) = ReLU Conv1D C E ,K E , K E/2 (x) ? R C E ?L ,<label>(3)</label></formula><p>where the activation ReLU (?) is applied element-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Separator</head><p>In essence, the separator S module performs the following transformations to the encoded mixture representation v x ? R C E ?L :</p><p>1. Projects the encoded mixture representation v x ? R C E ?L to a new channel space through a layer-normalization (LN) <ref type="bibr" target="#b0">[1]</ref> followed by a point-wise convolution as shown next:</p><formula xml:id="formula_6">y 0 = Conv1D C,1,1 (LN (v x )) ? R C?L ,<label>(4)</label></formula><p>where LN (v x ) denotes a layer-normalization layer in which the moments used are extracted across the temporal dimension for each channel separately.</p><p>2. Performs repetitive non-linear transformations provided by B U-convolutional blocks (U-ConvBlocks) on the intermediate representation y 0 . In other words, the output of the ith U-ConvBlock would be denoted as y i ? R C?L and would be used as input for the (i + 1)th block. Each U-ConvBlock extracts and aggregates information from multiple resolutions which is extensively described in Section 2.2.1. 3. Aggregates the information over multiple channels by applying a regular one-dimensional convolution for each source on the transposed feature representation y T B ? R L?C . Effectively, for the ith source we obtain an intermediate latent representation as shown next:</p><formula xml:id="formula_7">z i = Conv1D C,C E ,1 y T B T ? R C E ?L<label>(5)</label></formula><p>This step has been introduced in <ref type="bibr" target="#b35">[36]</ref> and empirically shown to make the training process more stable rather than using the activations from the final block y B to estimate the masks. 4. Combines the aforementioned latent codes for all sources z i ?i ? {1, ? ? ? , N } by performing a softmax operation in order to get mask estimates m i ? [0, 1] C E ?L which add up to one across the dimension of the sources. Namely, the corresponding mask estimate for the ith source would be:</p><formula xml:id="formula_8">m i = vec ?1 exp (vec (z i )) N j=1 exp (vec (z j )) ? R C E ?L ,<label>(6)</label></formula><p>where vec (?) : R K?N ? R K?N and vec ?1 (?) : R K?N ? R K?N denotes the vectorization of an input tensor and the inverse operation, respectively. 5. Estimates a latent representation v i ? R C E ?L for each source by multiplying element-wise the encoded mixture representation v x with the corresponding mask m i :</p><formula xml:id="formula_9">v i = v x m i ? R C E ?L ,<label>(7)</label></formula><p>where a b is the element-wise multiplication of the two tensors a and b assuming that they have the same shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">U-convolutional block (U-ConvBlock)</head><p>U-ConvBlock uses a block structure which resembles a depth-wise separable convolution <ref type="bibr" target="#b32">[33]</ref> with a skip connection as in ConvTasNet <ref type="bibr" target="#b23">[24]</ref>. However, instead of performing a regular depth-wise convolution as shown in <ref type="bibr" target="#b3">[4]</ref> or a dilated depth-wise which has been successfully utilized for source separation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36]</ref> our proposed U-ConvBlock extracts information from multiple resolutions using Q successive temporal downsampling and Q upsampling operations similar to a U-Net architecture <ref type="bibr" target="#b30">[31]</ref>. More importantly, the output of each block leaves the temporal resolution intact while increasing the effective receptive field of the network multiplicatively with each temporal sub-sampling operation <ref type="bibr" target="#b20">[21]</ref>. We postulate that this whole resampling procedure of extracting</p><formula xml:id="formula_10">( 0 ) ( 1 ) ( 2 )<label>( 3 ) ( 4 ) ( 1 )</label></formula><formula xml:id="formula_11">( 0 ) ? ? ? ? ( ) ? ? ? ( + 1 ) ? ? ( 3 ) ( 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel expansion</head><p>Temporal Upsampling Temporal downsampling</p><formula xml:id="formula_12">Copy , ? ( ) ( ) ? ? ? 2 ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel contraction</head><p>Output transformation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: U-ConvBlock forward pass</head><formula xml:id="formula_13">Input: y (i) ? R C?L Output: y (i+1) ? R C?L // Expand channel dimensions q ? PReLU C U LN Conv1D C U ,1,1 y (i) ; d (0) ? PReLU C U LN DWConv1D C U ,K U ,1 (q) ; for i = 1; i++; while i &lt;= Q do // Successive depth-wise downsampling d (i) ? LN DWConv1D C U ,K U ,S U d (i?1) ; d (i) ? PReLU C U d (i) ; end u (Q) ? d (Q) ; for i = Q ? 1; i??; while i &gt;= 0 do // Upsampling and adding multi-resolution features u (i) ? d (i) + I S U u (i+1) ; end o ? LN Conv1D C,1,1 PReLU C LN u (0) ; return PReLU C y (i) + o ;</formula><p>features at multiple scales combined with the efficient increase of the effective receptive field enables SuDoRM-RF models to outperform several convolutional architectures and perform in par with much more expensive recurrent and self-attention architectures <ref type="bibr" target="#b34">[35]</ref>. An abstract view of the ith U-ConvBlock is displayed in <ref type="figure" target="#fig_1">Figure 2</ref> while a detailed description of the operations is presented in Algorithm 1.</p><p>Definition 4 PReLU C : R C?L ? R C?L defines a parametric rectified linear unit (PReLU) <ref type="bibr" target="#b7">[8]</ref> with C learnable parameters a ? R C . When applied to an input matrix y ? R C?L the non-linear transformation could be defined element-wise as:</p><formula xml:id="formula_14">PReLU C (y) i,j = max (0, y i,j ) + a i ? min (0, y i,j )<label>(8)</label></formula><p>Definition 5 I M : R C?L ? R C?M ?L defines a nearest neighbor temporal interpolation by a factor of M . When applied on an input matrix y ? R C?L this upsampling procedure could be formally expressed element-wise as:</p><formula xml:id="formula_15">I M (u) i,j = u i, j /M</formula><p>Definition 6 LN : R C?L ? R C?L defines a parametric normalization layer <ref type="bibr" target="#b0">[1]</ref> with learnable parameters ? ? R C and ? ? R C . When applied to an input matrix y ? R C?L the normalization could be defined element-wise as:</p><formula xml:id="formula_16">LN (y) i,j = y i,j ? ? i ? i ? i + ? i , ? i = j y i,j , ? i = j (y i,j ? ? i ) 2 (9)</formula><p>Definition 7 GLN : R C?L ? R C?L defines a parametric normalization layer with learnable parameters ? ? R C and ? ? R C . When applied to an input matrix y ? R C?L the normalization could be defined element-wise as:</p><formula xml:id="formula_17">GLN (y) i,j = y i,j ? ? ? ? i + ? i , ? = i,j y i,j , ? = i,j (y i,j ? ?) 2<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decoder</head><p>Our decoder module D is the final step in order to transform the latent space representation v i for each source back to the time domain. In our proposed model we follow a similar approach as in <ref type="bibr" target="#b35">[36]</ref> where each latent source representation v i is fed through a different transposed convolution decoder ConvTr1D C E ,K E , K E/2 . The efficacy of dealing with different types of sources using multiple decoders has also been studied in <ref type="bibr" target="#b1">[2]</ref>. Ignoring the permutation problem, for the ith source we have the following reconstruction in time:</p><formula xml:id="formula_18">s i = D i ( v i ) = ConvTr1D C E ,K E , K E/2 ( v i )<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Improved version with no mask estimation SuDoRM-RF++</head><p>In the improved version of the proposed architecture, namely, SuDoRM-RF++ , the model estimates directly the latent representation for each target signal v i ? R C E ?L and uses only one decoder module. Our intuition lies in the aspect that a highly parameterized neural network could potentially estimate those targets without the need of the hard-regularized element-wise multiplication process of the masks on top of the mixture encoded representation v x ? R C E ?L . Essentially, SuDoRM-RF++, which is presented in <ref type="figure" target="#fig_0">Figure 1b</ref>, can be derived from our initial SuDoRM-RFmodel by applying the following alternations to the architecture:</p><p>-We replace the mask estimation and element-wise multiplication process with a direct estimation of the latent target signals v i after the final output of the model y <ref type="bibr">(B)</ref> . We have validated experimentally that by removing the mask estimation layer leads to similar or slightly improved results.</p><p>-We use only one trainable decoder in order to transform the latent representation back to the time domain instead of two separate ones, namely,</p><formula xml:id="formula_19">s i = D ( v i ) = ConvTr1D C E ,K E , K E/2 ( v i ). -</formula><p>We replace the layer normalization layers (Equation 9) with global layer normalization (GLN) layers as defined in <ref type="formula" target="#formula_2">(Equation 10</ref>). This change significantly improves the convergence of our models probably because of the interdependence of the gradient statistics between the channels. -For each intermediate representation with C channels, we simplify the activation layers and we use PReLU activation layers with only one learnable parameter instead of C as it was initially defined in Equation <ref type="bibr" target="#b7">8</ref>. In this way, we are able to achieve similar results as before with less parameters.</p><p>We would like to underline that the structure of the initial SuDoRM-RF models could potentially outperform the improved SuDoRM-RF++ variation in cases where the direct estimation of the latent targets would be more difficult than estimating the masks (e.g. unconstrained optimization in those latent spaces might be worse than estimating a bounded mask with values in the [0, 1] region). Moreover, the alternation of containing two decoders proposed in the initial version might be more useful in cases where one wants to solve an audio source separation problem containing two distinct classes of sounds (e.g. speech enhancement) where each decoder could be fine-tuned towards decoding the class-specific characteristics of each estimated latent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Group communication variation</head><p>We also propose a new variation of our model, namely, SuDoRM-RF++ GC, where we combine group communication (GC) with our improved version of our model SuDoRM-RF++. GC is a novel way to significantly reduce the parameters of an audio processing network which has been recently proposed in <ref type="bibr" target="#b22">[23]</ref>. In the proposed architecture, the intermediate representations are being processed in groups of sub-bands of channels. We divide the channels of each 1 ? 1 convolutional block into 16 groups and we process them first independently by sharing the parameters across all groups of sub-bands. At a second step, we apply a self-attention module <ref type="bibr" target="#b31">[32]</ref> to combine them. The resulting architecture leads to a significant improvement in the number of trainable parameters which is mainly dominated by the bottleneck dense layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Causal version C-SuDoRM-RF++</head><p>Our latest extension of the proposed model is to be able to run online and enable streamable extensions for real-time applications. To this end, we propose C-SuDoRM-RF++ which is a more shallow and efficient model which has the following differences against the improved model SuDoRM-RF++ .</p><p>-We replace all non-causal convolutions with causal counterparts. Now the architecture does not depend on future samples in order to produce the estimated signal up to the current time-frame. A depiction of those two convolutional modules is presented in <ref type="figure">Figures 3a and 3b</ref> for the non-causal and the causal convolutional layers, respectively. -In order to simplify the implementation and make those models more efficient in terms of memory footprint and execution time, we also remove all the normalization layers. 3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Audio source separation tasks</head><p>Speech separation (2 active speakers): We perform speech separation experiments using the publicly available WSJ0-2mix dataset <ref type="bibr" target="#b9">[10]</ref> by following a similar setup with other studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b19">20]</ref>. Speaker mixtures are generated by randomly mixing speech utterances with two active speakers at random signal to noise ratios (SNR)s between ?5 and 5dB from the Wall Street Journal (WSJ0) corpus <ref type="bibr" target="#b28">[29]</ref>.</p><p>Non-speech sound separation (2 active sources): For our non-speech sound separation experiments we follow the exact same setup as in <ref type="bibr" target="#b35">[36]</ref> and utilize audio clips from the environmental sound classification (ESC50) data collection <ref type="bibr" target="#b29">[30]</ref> which consists of a wide variety of sounds (non-speech human sounds, animal sounds, natural soundscapes, interior sounds and. urban noises). For each data sample, two audio sources are mixed with a random SNR between ?2.5 and 2.5dB where each source belongs to a distinct sound category from a total of 50.</p><p>Universal sound separation (variable number of sources 1-4): We also evaluate our models under a purely universal sound separation setup where multiple sound classes might be present and also we do not know how many sources are active in each input mixture. To that end, we use the FUSS benchmark dataset presented in <ref type="bibr" target="#b39">[39]</ref>. FUSS contains sound clips that might contain at least one and up to four active sources per input mixture. Moreover, the sound clips represent a wide variety of real-world sounds including (speech, engine sounds, music, wind, rain, and many others). Also, the SNR distribution of the input sound mixtures is more realistic by capturing a wide range approximately from ?40dB to 40dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data pre-processing and generation</head><p>We follow the same data augmentation process which was firstly introduced in <ref type="bibr" target="#b35">[36]</ref> and it has been shown beneficial in other recent studies <ref type="bibr" target="#b43">[43]</ref>. We also normalize all processed audio clips by subtracting their mean and divide with their standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Fixed number of sources</head><p>The process for generating a training mixture is the following: A) random choosing two sound classes (for non-speech sound separation) or speakers (for speech separation) B) random cropping of 4sec segments from two sources audio files C) mixing the source segments with a random SNR (as specified in Section 3.1). For each epoch, 20, 000 new training mixtures are generated.</p><p>Validation and test sets are generated once with each one containing 3, 000 mixtures. We also downsample each audio clip to 8kHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Variable number of sources</head><p>In order to be consistent with the state-of-the-art results on FUSS, we use the same dataset splits as the ones provided in <ref type="bibr" target="#b39">[39]</ref>. The augmentation pipeline for each training mixture includes mixing sources from different training samples by sampling them uniformly over the batch. For each epoch, 20, 000 new training mixtures are generated. Validation and test sets contain 5, 000 and 3, 000 mixtures, respectively. Moreover, we also train and test keeping the same length of 10secs at all clips as well as their sampling frequency which is 16kHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Fixed number of sources</head><p>All models are trained for 120 epochs using a batch size equal to 4. As a loss function we use the negative permutation-invariant <ref type="bibr" target="#b41">[41]</ref> scale-invariant signal to distortion ratio (SI-SDR) <ref type="bibr" target="#b18">[19]</ref>. The total loss for N sources is computed as the average loss across each source as follows: For the ith source we define the loss between the clean signal s and the estimates s as:</p><formula xml:id="formula_20">L = ? 1 N N i=1 SI-SDR(s * i , s i ) = ? 1 N N i=1 10 log 10 ? i s * i 2 ?s * i ? s i 2 ,<label>(12)</label></formula><p>where s * denotes the permutation of the sources that maximizes SI-SDR and ? i = s i s * i / s i 2 is a scalar used for making the loss invariant to the scale of the ith estimated source s i . During training, we use the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with an initial learning rate set to 0.001 and we decrease it by a factor of 5 every 50 epochs. By training the model for more epochs, using the same learning rate scheduler, does not yield any significant gains on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variable number of sources</head><p>All models are trained for 40 epochs using a batch size equal to 4. In this case we assume that we know the maximum number of sources in each input mixture x, e.g. N , but in reality the mixture might be comprised of only N &lt; N active sources and N ? N inactive sources. We follow a similar training procedure with the one presented in <ref type="bibr" target="#b39">[39]</ref> and we force the network to produce zero outputs for the inactive slots after inferring the permutation that maximizes the total SNR. The loss for the active sources and inactive sources is defined in a permutation invariant sense as follows:</p><formula xml:id="formula_21">L = min ??? ? 1 N N i=1 10 log 10 s i 2 + s i ? s (?) i 2 + + 1 N ? N N i=N +1 10 log 10 s (?) i 2 + ? x 2 + ,<label>(13)</label></formula><p>where ? is the set of all possible permutations of the estimated sources s and we assume that the first N target signals represent the active sources. The first part of the loss function forces the model to maximize the reconstruction fidelity of the N active sources while the second part forces it to produce close to zero energy estimates for the last N ? N , assuming that the best permutation is already in place. The constants = 10 ?9 and ? = 10 ?3 solve numerical stability issues created by zero target signals s. During training, we use the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with an initial learning rate set to 0.001 and we decrease it by a factor of 2 every 10 epochs. Using the aforementioned scheduler we are able to obtain good performance with only 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation details</head><p>In order to evaluate the performance of our models we use a stable version of the permutation invariant SI-SDR improvement (SI-SDRi) as proposed in <ref type="bibr" target="#b39">[39]</ref>. The SI-SDRi metric has also been chosen in several studies in the source separation literature <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">39]</ref>. There are several other SNR-based metrics which also reflect the fidelity of the reconstructed sources such as signal to distortion ratio (SDR), signal to interference ratio (SIR) and signal to artifacts ratio (SAR) <ref type="bibr" target="#b37">[38]</ref>. SDR incorporates the errors from both artifacts and interference but is an overly optimistic performance measure compared to SI-SDR <ref type="bibr" target="#b18">[19]</ref>. The improvement is defined as the gain that we get on the SI-SDR measure using the estimated signal instead of the mixture signal x, as shown next:</p><formula xml:id="formula_22">SI-SDRi ( s, s) =max ??? ? ? 1 N N i=1 10 log 10 ? i s i 2 + ? i s i ? s (?) i 2 + ? ? ? 1 N N i=1 10 log 10 ? i s i 2 + ? i s i ? x 2 + ,<label>(14)</label></formula><p>where ? i = s i s (? * ) i / s i 2 , = 10 ?9 and ? * is the permutation that maximizes the average SI-SDRi over the active sources. For the case where non-active sources exist, thus, N &lt; N , we omit to compute the aforementioned metric as it provides infinity values. However, we follow a stricter evaluation metric than the one proposed in <ref type="bibr" target="#b39">[39]</ref> in the sense that we do not exclude pairs of estimates-targets with low energy estimated sources, as we believe that the evaluation metric should also reflect a penalty for the cases where the model under-separates the input mixture, leading to M &lt; N non-zero estimated sources. For the variable number of sources case and specifically for the singlesource mixtures we simply report the maximum absolute SI-SDR obtained by each one of the estimated sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">SuDoRM-RF configurations</head><p>We describe the default values for all proposed architectures SuDoRM-RF, SuDoRM-RF++and, C-SuDoRM-RF++. In the following experimental sections, all those values are going to be described as such, unless otherwise specified. For the encoder E and decoder modules D we use a kernel size K E = 21 for input mixtures sampled at 8kHz and K E = 41 for 16kHz. Also, the number of basis is equal to C E = 512. For the configuration of each U-ConvBlock we set the input number of channels equal to C = 128, the number of successive resampling operations equal to Q = 4 and, the expanded number of channels equal to C U = 512. In each subsampling operation we reduce the temporal dimension by a factor of 2 and all depth-wise separable convolutions have a kernel length of K U = 5 and a stride of S U = 2. Only for the causal variation C-SuDoRM-RF++, we increase the number of input channels to C = 256 and the default kernel length to K U = 11 in order to increase the receptive field in shallower and more efficient architectures needed for real-time applications. For simplicity we use the following naming convention based on the number B of U-ConvBlocks inside the separator module S. Namely, SuDoRM-RF 2.0x , SuDoRM-RF 1.0x , SuDoRM-RF 0.5x , SuDoRM-RF 0.25x consist of 32, 16, 8 and 4 blocks, respectively. The same applies to the improved version SuDoRM-RF++ and its causal variation C-SuDoRM-RF++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Literature models configurations</head><p>We compare against the best configurations of some of the latest state-of-theart approaches for speech <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref>, universal <ref type="bibr" target="#b35">[36]</ref> and music <ref type="bibr" target="#b4">[5]</ref> source separation. For a fair comparison with the aforementioned models we use the authors' original code, the best performing configurations for the proposed models as well as the suggested training process. For Demucs <ref type="bibr" target="#b4">[5]</ref>, 80 channels are used instead of 100 in order to be able to train it on a single graphical processing unit (GPU). For the universal sound separation experiments with a variable number of sources, we compare against the reported numbers in <ref type="bibr" target="#b39">[39]</ref>, where an enhanced variation of the ConvTasNet is used, namely, TDCN++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Measuring computational resources</head><p>One of the main goals of this study is to propose models for audio source separation which could be trained using limited computational resources and deployed easily on a mobile or edge-computing device <ref type="bibr" target="#b17">[18]</ref>. Specifically, we consider the following aspects which might cause a computational bottleneck during inference or training: We are using various sampling profilers in Python using Pytorch <ref type="bibr" target="#b27">[28]</ref> (version 1.7.1.) for tracing all the requirements of the non-causal models on a server with an Intel(R) Core(TM) i7-3820 @ 3.60GHz CPU and a GeForce GTX TITAN X GPU. For the causal models, we focus on the computational requirements on a much more resource-constrained hardware in order to show the applicability of our models for real-time source separation on devices used by typical users. Thus, we evaluate our causal models on a laptop with an Intel(R) Core(TM) i7-8750H @ 2.20GHz CPU. We measure the inference pass as a simple forward pass always on CPU while we consider that a backward pass is comprised of a forward pass in order to compute the estimated signals and the full back-propagation of the gradient and we measure its computational requirements on GPU.  <ref type="table">Table 1</ref>: SI-SDRi separation performance for the proposed models and models in the literature on both separation tasks (speech and non-speech) alongside their computational requirements for performing inference on an Intel(R) Core(TM) i7-3820 @ 3.60GHz CPU for one second of input audio or equivalently 8000 samples. * We assign the maximum SI-SDRi performance obtained by our runs and the reported number on the corresponding paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results &amp; Discussion</head><p>In <ref type="table">Tables 1, and 2</ref> we show the separation performance under the speech and non-speech separation tasks for some of the most recent state-of-the-art models in the literature and the proposed SuDoRM-RF configurations alongside computational requirements. Specifically, in <ref type="table">Table 1</ref>, we focus on the computational aspects required during a forwards pass of those models on a CPU while in <ref type="table">Table 2</ref>, the same computational resource requirements are shown for a backward pass on GPU as well as the number of trainable parameters. It is easy to see that the proposed models can match and even outperform the separation performance of other several state-of-the-art models using orders of magnitude less computational requirements across the board.</p><p>In Sections 4.1, 4.1.1, 4.3, we focus on specific computational aspects for all the models presented in this study. Moreover, a better visualization for understanding the Pareto efficiency of the proposed architectures is displayed in <ref type="figure" target="#fig_4">Figure 4</ref>. Specifically, we show for each model, its performance on non-speech sound separation vs a specific computational requirement. In Section 4.4, we conduct a small ablation study to shed light on the most important aspects of the proposed SuDoRM-RF models and how the performance is affected by changing the corresponding hyperparameters. Experiments for universal sound separation with a variable number of sources are presented in Section 4.5. Finally, we present the causal setup of our model and evaluate it for different configurations under a two-speaker separation task in 4.6.  <ref type="table">Table 2</ref>: SI-SDRi separation performance for the proposed models and models in the literature on both separation tasks (speech and non-speech) alongside their computational requirements for performing a backward pass on a GeForce GTX TITAN X GPU for one second of input audio or equivalently 8000 samples. * We assign the maximum SI-SDRi performance obtained by our runs and the reported number on the corresponding paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Floating point operations (FLOPs)</head><p>Different devices (CPU, GPU, mobiles, etc.) have certain limitations on their FLOPs throughput capacity. In the case of an edge device, the computational resource one might be interested in is the number of FLOPs required during inference. On the other hand, training on cloud machines might be costly if a huge number of FLOPs is needed in order to achieve high separation performance. As a result, it is extremely important to be able to train and deploy models which require a low number of computations <ref type="bibr" target="#b10">[11]</ref>. We see from the first column of <ref type="figure" target="#fig_4">Figure 4</ref> that SuDoRM-RF models scale well as we increase the number of U-ConvBlocks B from 4 ? 8 ? 16. Furthermore, we see from <ref type="table">Tables 1 and 2</ref> that for both forward and backward passes, correspondingly, the family of the proposed SuDoRM-RF models appear more Pareto efficient in terms of SI-SDRi performance vs Giga-FLOPs (GFLOPs) and time required compared to the other state-of-the-art models which we take into account.</p><p>Specifically, the DPRNN model <ref type="bibr" target="#b21">[22]</ref> which performs sequential matrix multiplications (even with a low number of parameters) requires at least 45 times more FLOPs for a single pass compared to SuDoRM-RF 0.25x while performing worse when trained for the same number of epochs under the non-speech separation task.</p><p>Moreover, we see from both <ref type="table">Tables 1 and 2</ref> that the improved version SuDoRM-RF++ achieves similar or even better results than the original version of the proposed model SuDoRM-RF with a lower number of FLOPs both in forward and backward for a similar number of parameters and execution time. A significant drop in the absolute number of FLOPs is also obtained by combining the group communication mechanism proposed in <ref type="bibr" target="#b22">[23]</ref> with SuDoRM-RF++ . However, that does not automatically entail a lower execution time. The causal variations C-SuDoRM-RF++ perform competitively with the same number of FLOPs but they are still performing much worse than all the other non-causal models showing that there is much room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Cost-efficient training</head><p>Usually one of the most detrimental factors for training deep learning models is the requirement of allocating multiple GPU devices for several days or weeks until an adequate performance is obtained on the validation set. This huge power consumption could lead to huge cloud services rental costs and carbon dioxide emissions <ref type="bibr" target="#b2">[3]</ref>. In <ref type="figure" target="#fig_5">Figure 5</ref>, we show the validation SI-SDRi performance for the speech separation task which is obtained by each model versus the total amount of FLOPs performed. For each training epoch, all models perform updates while iterating over 20, 000 audio mixtures. Notably, even the original SuDoRM-RF models outperform all other models in terms of cost-efficient training as they obtain better separation performance while requiring significantly fewer training FLOPs. For instance, SuDoRM-RF 1.0x obtains ? 16dB in terms of SI-SDRi compared to ? 10dB of DPRNN <ref type="bibr" target="#b21">[22]</ref> which manages to complete only 3 epochs given the same number of training FLOPs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Trainable parameters</head><p>From <ref type="table">Table 2</ref> it is easy to see that SuDoRM-RF architectures are using orders of magnitude fewer parameters compared to the U-net architectures like Demucs <ref type="bibr" target="#b4">[5]</ref> where each temporal downsampling is followed by a proportional increase to the number of channels. Moreover, the upsampling procedure inside each U-ConvBlock does not require any additional parameters. The SuDoRM-RF models seem to increase their effective receptive field with significantly fewer parameters compared to dilated convolutional architectures <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Memory requirements</head><p>In most of the studies where efficient architectures are introduced <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">42]</ref> authors are mainly concerned with the total number of trainable parameters of the network. The same applies to efficient architectures for source separation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. However, the trainable parameters comprise only a small portion of the total amount of memory required for a single forward or backward pass. The space complexity could easily be dominated by the storage of intermediate representations and not the actual memory footprint of the model itself. The latter could become even worse when multiple skip connections are present, gradients from multiple layers have to be stored or implementations require augmented matrices (dilated, transposed convolutions, etc.). From <ref type="table">Tables 1 and 2</ref>, we see that SuDoRM-RF++ and the initial SuDoRM-RF models have almost the same memory footprint. However, when combining the SuDoRM-RF++ with the group communication (GC) mechanism we see that even if the number of trainable parameters is significantly reduced then still the actual memory requirement increases which positively validates our hypothesis that the actual memory requirement in GBs is a very important metric to show when proposing new efficient models. The causal variation of our models C-SuDoRM-RF++ is the more light-weight model that we propose which also secures a very competitive inference time.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, we see that even the original SuDoRM-RF models are more Pareto efficient in terms of the memory required compared to the dilated convolutional architectures of ConvTasNet <ref type="bibr" target="#b23">[24]</ref> and Two-Step TDCN <ref type="bibr" target="#b35">[36]</ref> where they require an increased network depth in order to increase their receptive field. Although SuDoRM-RF models do not perform downsampling in every feature extraction step as Demucs <ref type="bibr" target="#b4">[5]</ref> does, we see that the proposed models require orders of magnitude less memory especially during a backward update step as the number of parameters in Demucs is significantly higher. Finally, SuDoRM-RF models have a smaller memory footprint because the encoder E performs temporal downsampling by a factor of div (K E , 2) = 10 compared to DPRNN <ref type="bibr" target="#b21">[22]</ref> which does not reduce the temporal resolution at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study on WSJ0-2mix</head><p>We perform an ablation study in order to show how different parameter choices in SuDoRM-RF++ models affect the separation performance. In order to be directly comparable with the numbers reported by several other studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b19">20]</ref>, we train our models for 200 epochs and test them using the given data splits from WSJ0-2mix dataset <ref type="bibr" target="#b9">[10]</ref>. The results are shown in <ref type="table">Table 3</ref>.</p><p>We see that a significant aspect is the stride of the encoder and decoder which is always defined as K E //2. By decreasing the size of the stride we force the model to perform more computations and also estimate the signal in a more fine-grained scale closer to the time-domain resolution leading to better results which is also consistent with other studies <ref type="bibr" target="#b14">[15]</ref>. Moreover, we see that the GLN significantly helps our model to reach a better solution compared to the simple layer norm, presumably acting as a better regularizer in between the intermediate activations. Furthermore, when keeping all the other parameters the same except for the number of U-ConvBlocks B and the number of resampling procedures Q we see one of the most important aspects of the SuDoRM-RF++ model which is the benefit in the receptive field that we are getting by analyzing the signal at multiple scales. Specifically, we see that a model with B = 18 and Q = 7 outperforms a deeper model in terms of U-ConvBlocks B = 20 which only processes the signal at Q = 2 more  <ref type="table">Table 3</ref>: SI-SDRi separation performance on WSJ0-2mix for various parameter configurations of SuDoRM-RF++ models. GLN corresponds to the global layer normalization as described in <ref type="figure" target="#fig_0">Equation 10</ref> and LN corresponds to the classic layer normalization layer proposed in <ref type="bibr" target="#b0">[1]</ref> and explained in Equation 9.</p><p>All the other parameters have the same values as described in Section 3.6.</p><p>scales. We need to underline that increasing the parameter Q does not lead to significant computational requirements mainly because of the downsampling operation which is relatively a cheap way to increase the receptive field of the model without carrying its whole information end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Variable number of sources</head><p>In <ref type="table" target="#tab_4">Table 4</ref> we report the performance of the proposed SuDoRM-RF models under a universal sound separation task with a varying number of sources in each mixture. We always assume that the maximum number of active sources in each mixture is N = 4 and we measure the performance on the same dataset splits where mixtures with N ? {1, 2, 3, 4} active sources. We see that by increasing our SuDoRM-RF and SuDoRM-RF++ model sizes to match the size of TDCNN++ <ref type="bibr" target="#b39">[39]</ref>, we can match its performance which is also the current state-of-the-art performance in universal sound separation with a variable number of sources. For the single source mixtures we see that our models perform worse than the TDCNN++, however, above 25dB it is really difficult even for a human being to understand the nuance artifacts which are barely audible. Our SuDoRM-RF++ 2.0x outperforms the state-of-the-art for the most difficult case where N = 4 sources are active. Moreover, by using the SuDoRM-RF++ 1.0x GC, we match the performance obtained by the stateof-the-art with a significantly fewer parameters. We also mention that our models perform worse than the state-of-the-art for the cases where N = 2 and N = 3 because we also penalize the performance of our models in the cases of under-separation which has been reported as 25% in <ref type="bibr" target="#b39">[39]</ref>. Namely, for the pairs of corresponding estimates-targets with low-energy estimates, the state-of-the-art numbers only consider the pairs with estimated sources which have power higher than 20 dB above the power of the quietest non-zero reference source. However, we always include all pairs of estimates-targets if a target source is non-zero as we believe that this is a more appropriate metric   <ref type="table">Table 5</ref>: SI-SDRi separation performance on WSJ0-2mix for various parameter configurations of causal C-SuDoRM-RF++ models alongside their inference time for 8000 input samples on a laptop with an Intel(R) Core(TM) i7-8750H @ 2.20GHz CPU. For reference, one of the most current state-of-the-art speech denoisers, namely, real-time Demucs <ref type="bibr" target="#b5">[6]</ref> runs on the same setup at 92.3ms.</p><p>to use. We also like to note that we did not notice any significant performance improvement when we use mixture consistency <ref type="bibr" target="#b40">[40]</ref> at the output estimates of our models where the separated sources are forced to sum up to the input mixture using a projection layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Causal setup</head><p>In <ref type="table">Table 5</ref> we report the performance of the proposed causal version of SuDoRM-RF models under a separation setup where two speakers are active using the WSJ0-2mix dataset <ref type="bibr" target="#b9">[10]</ref> alongside the inference time on a laptop for 1 second of input audio sampled at 8kHz. We see that we are able to obtain competitive separation performance for all configurations while remaining ? 10 to 20 times faster than real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this study, we have introduced the SuDoRM-RF network, a novel architecture for efficient universal sound source separation. Moreover, we have presented several improvements on the original model including SuDoRM-RF++ which directly estimates the of the latent representations of the models, a variation which shares parameters across sub-bands as well as C-SuDoRM-RF++ which is causal and enables real-time inference. The proposed model is based on the U-ConvBlock which is capable of extracting multi-resolution temporal features through successive depth-wise convolutional downsampling of intermediate representations and aggregates them using a non-parametric interpolation scheme. In this way, SuDoRM-RF models are able to significantly reduce the required number of layers in order to effectively capture long-term temporal dependencies. We show that these models can perform similarly or even better than recent state-of-the-art models while requiring significantly less computational resources in FLOPs, memory and, time for experiments with a fixed and a variable number of sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>SuDoRM-RF and SuDoRM-RF++ architectures for separating two sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>U-ConvBlock architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Non-causal convolution. Output signal Kernel Input signal (b) Causal convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .</head><label>1</label><figDesc>Number of executed floating point operations (FLOPs). 2. Number of trainable parameters. 3. Memory allocation required on the device for a single pass. 4. Time for completing each process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>SI-SDRi non-speech sound separation performance on ESC50 vs computational resources with an input audio recording of 8000 samples for all models. (Top row) computational requirements for a single forward pass on CPU (Bottom) for a backward pass on GPU. All x-axis are shown in logscale while the 3 connected blue stars correspond to the three SuDoRM-RF configurations from Section 3.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Validation SI-SDRi separation performance for speech-separation vs the number of FLOPs executed during training. All models are trained using batches of 4 mixtures with 32, 000 time-samples each. Each point corresponds to a completed training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Notably, our largest model SuDoRM-RF 1.0x matches the relatively low number of parameters of the DPRNN [22] model which is based on stacked RNN layers. Group communication combined with SuDoRM-RF++ is one of the most effective ways to reduce the number of trainable parameters caused by the bottleneck dense layers between the U-ConvBlocks. Essentially, the SuDoRM-RF++ 1.0x GC model with B = 16 number of U-ConvBlocks has less than half of the parameters of a shallow original SuDoRM-RF 0.25x model with only B = 4 U-ConvBlocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>SI-SDR and SI-SDRi separation performance on the FUSS dataset for a variable number of sources. Specifically, we report the absolute SI-SDR (N = 1) for the reconstruction of single-source mixtures. For mixtures with multiple active sources N &gt; 1 we measure the SI-SDRi performance of the reconstructed sources wrt the input mixture. *Metrics do not consider pairs of corresponding estimates-target sources with low energy estimates, where a 25% of under-separation is reported in the cases of N ? {2, 3, 4}.</figDesc><table><row><cell>B</cell><cell>K U</cell><cell>SI-SDRi</cell><cell>Time (ms)</cell></row><row><cell></cell><cell>3</cell><cell>8.4</cell><cell>50.8</cell></row><row><cell>4</cell><cell>5</cell><cell>9.1</cell><cell>50.4</cell></row><row><cell></cell><cell>11</cell><cell>9.5</cell><cell>88.7</cell></row><row><cell></cell><cell>3</cell><cell>9.6</cell><cell>90.6</cell></row><row><cell>8</cell><cell>5</cell><cell>10.1</cell><cell>88.2</cell></row><row><cell></cell><cell>11</cell><cell>10.3</cell><cell>165.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: https://github.com/etzinis/sudo rm rf</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monaural music source separation using a resnet latent separator network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICTAI</title>
		<meeting>ICTAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1124" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real Time Speech Enhancement in the Waveform Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3291" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spleeter: a fast and efficient music source separation tool with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Voituret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page">2154</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hydranet: A real-time waveform separation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Kaspersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kounalakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Erkut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4327" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="175" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient source separation using bitwise neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="187" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepx: A software accelerator for low-power deep learning inference on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forlivesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qendro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPSN</title>
		<meeting>IPSN</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ultra-lightweight speech separation via group communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08397</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lightweight online separation of the sound source of interest through blstm-based binary masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11241</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tcnn: Temporal convolutional neural network for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6875" to="6879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at</title>
		<meeting><address><addrLine>Harriman, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Proc. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving universal sound separation using sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">What&apos;s all the fuss about free universal sound separation data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00803</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Differentiable consistency constraints for improved deep speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Slimmable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
