<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Image-based Automatic Dial Meter Reading: Dataset and Baselines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Salomon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran? (UFPR)</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<region>PR</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayson</forename><surname>Laroca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran? (UFPR)</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<region>PR</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Menotti</surname></persName>
							<email>menotti@inf.ufpr.br</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran? (UFPR)</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<region>PR</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Image-based Automatic Dial Meter Reading: Dataset and Baselines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-automatic meter reading</term>
					<term>dial meters</term>
					<term>pointer- type meters</term>
					<term>deep learning</term>
					<term>public dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Smart meters enable remote and automatic electricity, water and gas consumption reading and are being widely deployed in developed countries. Nonetheless, there is still a huge number of non-smart meters in operation. Image-based Automatic Meter Reading (AMR) focuses on dealing with this type of meter readings. We estimate that the Energy Company of Paran? (Copel), in Brazil, performs more than 850,000 readings of dial meters per month. Those meters are the focus of this work. Our main contributions are: (i) a public real-world dial meter dataset (shared upon request) called UFPR-ADMR; (ii) a deep learning-based recognition baseline on the proposed dataset; and (iii) a detailed error analysis of the main issues present in AMR for dial meters. To the best of our knowledge, this is the first work to introduce deep learning approaches to multidial meter reading, and perform experiments on unconstrained images. We achieved a 100.0% F1-score on the dial detection stage with both Faster R-CNN and YOLO, while the recognition rates reached 93.6% for dials and 75.25% for meters using Faster R-CNN (ResNext-101).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Measuring residential energy consumption is known to be a laborious task <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Although smart meters are gradually replacing old meters, there are still many old mechanical meters in operation around the world since their replacement is time-consuming and costly. In many regions, such as remote areas and developing nations, manual on-site readings are still prevalent <ref type="bibr" target="#b3">[4]</ref>. Even in developed countries, replacements are still far for complete. For example, in the end of 2018, there were still more than 26 million non-automatic meters in the United States <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the literature, Automatic Meter Reading (AMR) is usually associated with digital and smart meters <ref type="bibr" target="#b5">[6]</ref>. In this work, we use this designation exclusively for image-based automatic readings. AMR allows the employees of the service company (electricity/gas/water) or, preferably, the consumers themselves to capture meter images using a mobile device, which is cheaper and more feasible than manual on-site reading, and easier to deploy -in the short/medium term -than the replacement of old meters.</p><p>There are two main categories of residential energy meters <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>: (i) analog (with cyclometer and dial displays) and (ii) digital (with electronic display and smart meters), as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. This work focuses on dial meters since, although there are numerous dial meters in operation, there are still many open challenges in this context (as detailed further). The Energy Company of Paran? (Copel) <ref type="bibr" target="#b8">[9]</ref> measures electricity consumption in more than 4 million consuming units (i.e., meters) per month in the Brazilian state of Paran. From the images they provided us (see Section III), we estimate that 21% of those devices are dial meters, resulting in more than 840,000 dial meter readings carried out every month.</p><p>Most of the dial meter recognition literature is focused on industrial applications, e.g., pressure meters <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, voltmeter <ref type="bibr" target="#b12">[13]</ref> and ammeter <ref type="bibr" target="#b13">[14]</ref>. As the meters are generally fixed and indoors, the image quality is strictly controlled.</p><p>Although in some cases the conditions are indeed realistic, they are not as unconstrained as in images obtained in outdoor environments, with challenging conditions, e.g., severe lighting conditions (low light, glares, uneven illumination, reflections, etc.), dirt in the region of interest, and taken at a distance. In addition, most approaches are based on handcrafted features <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and were evaluated exclusively on private datasets <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. To the best of our knowledge, there are no public datasets containing dial meter images in the literature.</p><p>Taking into account the above discussions, we introduce a real-world fully-labeled dataset (shared upon request) contain-ing 2,000 meter images, acquired in unconstrained scenarios by Copel employees, with 9,097 individual dials and a welldefined evaluation protocol to assist the development and assessment of new approaches for this task <ref type="bibr" target="#b0">1</ref> . In addition, we conducted experiments using deep learning models in our dataset images to serve as baselines for future work, investigating problems related to dial meter reading and providing guidance for further research through a detailed quantitative and qualitative error analysis.</p><p>The remainder of this work is organized as follows. In Section II, we discuss approaches designed for AMR as well as deep learning techniques. The proposed dataset is described in Section III. Section IV presents the evaluated deep learningbased approaches for automatic reading of dial meters, while the results (with a detailed error analysis) are reported in Section V. Lastly, in Section VI, we state the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There are many works in the literature that dealt with AMR. Most of them focus on the recognition of cyclometers and digital meters using Optical Character Recognition (OCR) methods. Recently, deep learning approaches have received great attention in this context <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Dial meter recognition research, on the other hand, is more scarce. Most methods focus on gauges for industrial application <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Although gauges may look similar to energy dial meters, they usually only contain a single dial and one type of dial template, and the image conditions tend to be much more controlled in terms of lighting, dirt, and image quality. In this section, we describe some relevant works on AMR as well as state-of-the-art deep learning approaches for object detection and recognition <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Digit-based Meter Reading</head><p>Gallo et. al. <ref type="bibr" target="#b1">[2]</ref> proposed a method that uses Multilayer Perceptron (MLP) to locate the Region of Interest (ROI) of the meters (also denoted as counter region <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>), Maximally Stable Extremal Regions (MSER) to segment the digits, Histogram of Oriented Gradients (HOG) for feature extraction, and Support Vector Machine (SVM) for digit recognition.</p><p>Nodari and Gallo <ref type="bibr" target="#b26">[27]</ref> proposed a method named MultiNOD for gas cyclometers reading. It consists of a neural network tree, sharing and resizing features to perform counter detection and digit segmentation. The digit recognition stage was handled using Tesseract. This approach was later improved in <ref type="bibr" target="#b0">[1]</ref>, with the addition of a Fourier analysis applied to the segmented image, in order to avoid false positives. Finally, SVM was employed for digit classification.</p><p>Tsai et. al. <ref type="bibr" target="#b17">[18]</ref> employed Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b27">[28]</ref>, a deep learning object detector, to locate the counter region in energy meters. The authors reported an accuracy rate of 100% on their experiments, but did not address the recognition stage.</p><p>Yang et. al. <ref type="bibr" target="#b18">[19]</ref> proposed a Fully Convolutional Sequence Recognition Network (FCSRN) for water meter analog digit reading, with a novel loss function entitled Augmented Loss (AugLoss). AugLoss addresses the "middle-state" that can occur when the digit accumulator is changing from one display digit to the next one, usually outputting the old displayed digit. Their approach outperformed Recurrent Neural Networks (RNN) and attention-based models on the task of sequence recognition, but the experiments were made in controlled images, with cropped and aligned meters.</p><p>Gmez et. al. <ref type="bibr" target="#b16">[17]</ref> introduced a segmentation-free approach to perform meter reading. They trained a Convolutional Neural Network (CNN) to yield readings directly from the input images, without the need to detect the counter region. Although their approach has achieved promising results, the authors used a private dataset in the experiments, and only compared their method with traditional algorithms that rely on handcrafted features, which are easily affected by noise and may not be robust to images acquired in adverse conditions <ref type="bibr" target="#b3">[4]</ref>.</p><p>Laroca et. al. <ref type="bibr" target="#b3">[4]</ref> designed a two-stage approach for AMR. The Fast-YOLOv2 model <ref type="bibr" target="#b28">[29]</ref> was employed for counter detection and three CNN-based models were evaluated in the counter recognition stage. The authors considerably improved their recognition results when balancing the training set in terms of digit classes through data augmentation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dial Meter Reading</head><p>Tang et. al. <ref type="bibr" target="#b14">[15]</ref> proposed a complete framework for dial energy meter reading based on binarization, line intersection, and morphological operations. Despite being an interesting approach, the dataset used in the experiments was not published, and the images were obtained in a controlled environment.</p><p>In <ref type="bibr" target="#b15">[16]</ref>, the authors also employed handcrafted features for dial recognition. In addition to binarization and line intersection, the counter region was detected using Scale-Invariant Feature Transform (SIFT) features. Their method was evaluated on a private dataset containing only 141 images taken in a controlled environment.</p><p>The following approaches dealt only with single-dial meters (commonly known as gauges) and not with energy meters. Although the problems are similar, there is a fundamental difference: a small error in a multi-dial meter can result in a completely wrong measurement (especially if the error occurs in recognizing the most significant dials). Such a fact needs to be taken into account when evaluating recognition methods.</p><p>Several approaches explored handcrafted features, such as Hough Transform (HT), in order to locate the dials <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>. The steps in such works are very similar: image binarization on the preprocessing stage, Hough Circle Transform (HCT) for dial location, and pointer angle detection using Hough Line Transform (HLT) or similar methods. These approaches generally work well in constrained environments, but may not be suitable for real-world outdoor scenarios with uneven lighting and the presence of noise.</p><p>Mask R-CNN was proposed for pointer recognition in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Fang et. al. <ref type="bibr" target="#b13">[14]</ref> used it to find reference key points and the pointer in a gauge scale marks, while He et. al. <ref type="bibr" target="#b20">[21]</ref> focused on segmenting the meter dial and pointer. In both works, the angle between the pointer and the dial was explored to retrieve the reading. The datasets used in the experiments were not provided in both works.</p><p>Region-based Fully Convolutional Networks (R-FCNs) were used for meter detection <ref type="bibr" target="#b11">[12]</ref>. Although the authors used deep learning for detection, the meter reading was performed with handcrafted methods such as binarization, line detection, and skeleton extraction. Liu et. al. <ref type="bibr" target="#b9">[10]</ref> evaluated Fast R-CNN, Faster R-CNN, YOLO and SSD for meter detection and concluded that even though Faster R-CNN outperforms the others, YOLO is the fastest. Nevertheless, the recognition was performed by a handcrafted method (i.e., HT) and the images used have not been made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Learning Methods</head><p>ResNet <ref type="bibr" target="#b21">[22]</ref> is one of the recent breakthroughs in deep networks. The introduction of residual blocks enabled deeper network architectures while having fewer parameters than shallower networks, such as VGG19 <ref type="bibr" target="#b22">[23]</ref>. ResNet also performs better and converges faster. The residual learning process introduces lower level features directly to higher abstraction layers, preserving information. ResNet was later upgraded to ResNeXt <ref type="bibr" target="#b29">[30]</ref>. The main difference between them is the concept of "cardinality"; instead of going deeper, ResNeXt uses a multi-branch architecture (cardinality refers to the number of branches used) to increase the transformations and achieve a higher representation power. ResNet and ResNeXt can be employed for recognition (classification) problems.</p><p>In order to detect the dials on each image, object detection deep networks will be explored. Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> is a stateof-the-art approach that uses attention mechanisms and the sharing of convolutional features between the Region Proposal Network (RPN) and the detection network (originally VGG16) to enhance speed and accuracy. First, the RPN generates region proposals that may contain known objects; then, the detection network evaluates the boundaries and classifies the objects.</p><p>Redmon et al. <ref type="bibr" target="#b24">[25]</ref> proposed YOLO (You Only Look Once), an object detector that focuses on an extreme speed/accuracy trade-off by dividing the input image into regions and predicting bounding boxes and probabilities for each region. YOLOv2 <ref type="bibr" target="#b28">[29]</ref>, an improved version of YOLO, adopts a series of concepts (e.g., anchor boxes, batch normalization, etc.) from existing works along with novel concepts to improve YOLO's accuracy while making it faster <ref type="bibr" target="#b30">[31]</ref>. Similarly, Redmon and Farhadi <ref type="bibr" target="#b25">[26]</ref> introduced YOLOv3 (the latest version of YOLO), which uses various tricks to improve training and increase performance, such as residual blocks, shortcut connections, and upsampling. YOLO-based models have been successfully applied in several research areas <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Datasets</head><p>Most of the referred works do not provide a public dataset to enable a fair comparison of results. There are a few publicly available meter datasets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref>, however, none of them have images containing pointer-type meters, only digit-based ones. As far as we know, there is no publicly available dataset containing images of dial meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE UFPR-ADMR DATASET</head><p>We acquired the meter images from Copel, a company of the Brazilian electricity sector that serves more than 4 million consuming units per month <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The images of the meters were obtained at the consuming units by Copel employees using cell phone cameras (note that cell phones of many brands and models were used). All images had already been resized and compressed for storage, resulting in images of 640 ? 480 or 480 ? 640 pixels (depending on the orientation in which the image was taken). To create the UFPR-ADMR dataset, we selected 2,000 images where it was possible for a human to recognize the correct reading of the meter, as the images were acquired in uncontrolled environments and it would not be possible to label the correct reading in many cases.</p><p>In each image, we manually labeled the position (x, y) of each corner of an irregular quadrilateral that contains all the dials. These corner annotations can be used to rectify the image patch containing the dials. <ref type="figure" target="#fig_1">Fig. 2</ref> shows some images selected for the dataset as well as illustrations of the annotations. All meters have 4 or 5 dials, being 903 meters (45%) with 4 dials and 1,097 meters (55%) with 5. The values pointed on each dial have an almost uniform distribution of digits, having slightly more 0s than other digits. Information about the dimensions of the meters and dials in the dataset are shown in <ref type="table" target="#tab_0">Table I</ref>. Note the great variability in the size of both meters and dials, for example, the smallest dial (20 ? 29 pixels) is almost 10 times smaller than the largest one (206?201 pixels).  <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the distribution of digits per dial. The most prominent bar indicates that the most frequent digit in the first position is 0. Nevertheless, it should be noted that the distribution is not as unbalanced as datasets with digit-based meter images, such as the UFPR-AMR dataset <ref type="bibr" target="#b3">[4]</ref>, in which the number of 0s in the first position is equal to the sum of 0s in the other positions. This is probably due to the fact that dial meters stopped being manufactured and deployed decades ago, which implies that each dial might have completed many cycles since the installation and may be indicating any value.  <ref type="table" target="#tab_0">Table II</ref> shows the frequency of digits in the UFPR-ADMR dataset. Unlike datasets containing digit-based meters <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which were manufactured/deployed more recently, the distribution of the digits is almost uniform across our dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Challenges</head><p>The main challenge of the proposed dataset is the quality of the images. Low-end cameras, challenging environmental conditions and high compression are factors that have a high impact on the final image quality. The challenging environmental conditions include: reflections, dirt, and broken glass, and low-quality acquisition may result in: noisy, blurred and low-contrast images. <ref type="figure">Fig. 4</ref> illustrates the main image-quality issues described above.</p><p>In addition to the aforementioned quality issues, there are several types of meter templates and each manufacturer has its own dial model (with variations on the marks) and pointer design. This variations combined with the image capture angle make it difficult to determine the exact pointed value.</p><p>Another challenge arises from the presence of clockwise and counter-clockwise dials -for design purposes, each meter has alternating clock directions -, and the direction of the dials may differ depending on the meter model and manufacturer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Protocol and Metrics</head><p>An evaluation protocol is necessary to enable fair comparison between different approaches. The dataset was randomly divided in three disjoints subsets: 1200 images for training (60%), 400 images for validation (20%) and the remaining 400 images for testing (20%). Following recent works in which datasets were introduced <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the subsets generated are explicitly available along with the UFPR-ADMR dataset.</p><p>To assess the recognition, three metrics are proposed: (i) dial recognition rate, (ii) meter recognition rate, and (iii) mean absolute error. As the main task is to correctly recognize the meter reading, which is a sequence of digits, the meter recognition rate consists of the comparison between the predicted sequence (pred m ) and the ground-truth sequence (gt m ), for each of the N meters:</p><formula xml:id="formula_0">M R rate = 1 N N m=1 match(pred m , gt m ) (1) match(x, y) = 1, if x = y, 0, if x = y.</formula><p>For the dial recognition rate, we employed the Levenshtein distance (also known as edit distance), a common measurement for computing distance between two sequences of characters. The Levenshtein distance measures the minimum number of edits (addition, removal or replacement of characters) required to transform one sequence in the other. Levenshtein distance is suitable for our evaluation since it can handle small sequence errors in sequences that other metrics would treat as a big error. For instance, if we have a ground-truth sequence a = "1234" and a prediction sequence b = "234", a per-character evaluation metric would consider the error equal to 4, while Levenshtein distance is equal to 1, as the difference between them is a single digit prediction. The Levenshtein distance between the sequences a and b can be determined using:</p><formula xml:id="formula_1">lev a,b (i, j) = max(i, j), if min(i, j) = 0, lev a,b (i, j) otherwise,</formula><p>where:  <ref type="figure">Fig. 4</ref>. Samples of the challenging scenarios present on the provided images. We selected for the UFPR-ADMR dataset 2,000 images in which it was possible for a human to recognize the correct reading of the meter. We blurred the region containing the consumer unit number in each image due to privacy constraints.</p><formula xml:id="formula_2">lev a,b (i, j) = min ? ? ? ? ? lev a,b (i ? 1, j) + 1 lev a,b (i, j ? 1) + 1 lev a,b (i ? 1, j ? 1) + 1 (ai =bj ).</formula><p>The Levenshtein distance between the prediction and the ground truth is computed and then divided by the longest sequence size between them. This gives us the error. Subtracting the error from 1 gives us the dial recognition rate for each meter. Finally, the mean of all recognition rates yields the total dial recognition error:</p><formula xml:id="formula_3">DR rate = 1 N N m=1 1? lev (predm,gtm) (|pred m |, |gt m |) max(|pred m |, |gt m |)<label>(2)</label></formula><p>Considering that the sequence of digits that composes the meter reading is, in fact, a number (integer), correctly predicting the last digit in the sequence is not as important as correctly predicting the first one (i.e., the most significant digit). In order to differentiate and penalize errors in the most significant digits, the mean absolute error is simple yet effective. After converting the sequences to integers (pred m and gt m become the integers p m and g m , respectively), the mean absolute error can be obtained using:</p><formula xml:id="formula_4">M A error = 1 N N m=1 |p m ? g m |<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATED APPROACH</head><p>We chose two deep networks to evaluate: Faster R-CNN and YOLO. The reason for treating dial meter reading as a detection problem arises from the previous successful approaches to AMR using detection networks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Faster R-CNN presented accurate results in several detection and recognition problems in the literature, while YOLO achieved reasonable results with a high rate of frames per second (FPS), improving the viability of mobile applications. As illustrated in <ref type="figure">Fig. 5</ref>, the proposed pipeline consists of (i) image acquisition, (ii) dial detection and recognition, and (iii) final reading. <ref type="figure">Fig. 5</ref>. The main steps to perform dial meter reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Acquisition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dial Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dial Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Reading</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dial Detection</head><p>We perform dial detection directly in the input images, that is, without first detecting the ROI. According to our experiments, presented in section V, this approach achieves the highest F-score value. In other words, our recognition results are not significantly influenced by minor errors on the detection stage, making ROI detection avoidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dial Recognition</head><p>Faster R-CNN is evaluated with the following residual networks as backbones replacing VGG <ref type="bibr" target="#b22">[23]</ref>: ResNet-50 <ref type="bibr" target="#b21">[22]</ref> (with 50 convolutional layers), ResNet-101 <ref type="bibr" target="#b21">[22]</ref> (with 101 convolutional layers) and ResNeXt-101 <ref type="bibr" target="#b29">[30]</ref>. According to <ref type="bibr" target="#b21">[22]</ref>, ResNets outperform VGG and other several networks in classification tasks; therefore, they are used in our experiments.</p><p>For the YOLO-based models, we use the classifiers proposed along with the networks in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>. YOLOv2 uses the Darknet-19 model as its backbone, which has 19 convolutional layers (hence the name) and 5 max-pooling layers. YOLOv3, on the other hand, uses a network called Darknet-53 (with 53 convolutional layers) for feature extraction; Darknet-53 can be seen as a hybrid approach between Darknet-19 and residual networks <ref type="bibr" target="#b25">[26]</ref>. We employed both YOLOv2 and YOLOv3 models in our experiments in order to assess their speed/accuracy trade-off for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Final Reading</head><p>The final reading is generated according to the position of the detected dial on the image (from leftmost to the rightmost dial). Non-maximum suppression is performed using the Intersection over Union (IoU) metric (IoU &gt; 0.5) and considering a maximum of 5 dials per image, keeping only the dials predicted with higher confidence in order to avoid false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We evaluated the performance of the models based on YOLO and Faster R-CNN to detect and recognize the dials simultaneously (note that we used pre-trained weights when fine-tuning both networks). We performed our experiments on a CPU with a Quad-Core AMD Opteron 8387 2.8GHz processor, 64GB of RAM and an NVIDIA Titan Xp GPU. In order to stop the training process and select the best model for testing, we chose the mean Average Precision (mAP) evaluation metric, which has been commonly employed on object detection tasks <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. The mAP can be calculated as follows:</p><formula xml:id="formula_5">mAP = 1 c c i=1 AP i ,<label>(4)</label></formula><p>where AP i stands for the average precision value (for recall values from 0 to 1) of the i-th class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Augmentation</head><p>We generated new images by creating small variations to the training images to increase the generalization power of the networks. Based on preliminary experiments carried out on the validation set, we generated seven times the number of training images (the combined number of original and augmented images was 9,600). The following transformations were randomly chosen for each image: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>First, we investigate the performance of the models in the dial detection task. The results are listed in <ref type="table" target="#tab_0">Table III</ref>. For comparison, a common method proposed in the literature was evaluated: HCT <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>. F-score was chosen as the evaluation metric, as it is often used to assess detection tasks.</p><p>As expected, deep learning-based methods (i.e., YOLO and Faster R-CNN) outperformed HCT, reaching very high F-score values. HCT did not cope well with the large variations on lighting, contrast and perspective found in our dataset images. We performed the recognition (reading) by combining the recognized digits (from the leftmost to the rightmost) and comparing them with the pointed values, using the metrics described in Section III. The recognition results, as well as the FPS rates obtained, are displayed in <ref type="table" target="#tab_0">Table IV</ref>. The best performing method was Faster R-CNN (ResNext-101) followed by YOLOv3. Faster R-CNN obtained a 75.25% recognition rate per meter and 93.60% per dial, using 800 ? 800-pixel images. After YOLOv3, Faster R-CNN with ResNet-101 performed better than ResNet-50 considering the recognition rate per dial. Interestingly, ResNet-101 presented a lower hit rate considering the recognition at meter level. The lower hit rate is caused by the fact that ResNet-101 errors were better distributed across the images, while ResNet-50 concentrated the errors on fewer images.</p><p>The faster method was Fast-YOLOv2, using 416 ? 416 images, achieving 244 FPS. Although YOLOv3 did not surpass Faster R-CNN (ResNext-101) in recognition rates, the FPS rates obtained were three times higher (20 FPS and 6 FPS, respectively). Considering that the recognition rates achieved by YOLOv3 were not far behind, this model showed a promising trade-off between accuracy and speed.</p><p>The best method regarding mean absolute error was Faster R-CNN (ResNet-101) with an error of 1343. <ref type="bibr" target="#b28">29</ref>. This means that the method's errors occurred less frequently (or were smaller) on the most significant digits.   <ref type="figure" target="#fig_5">Fig. 6</ref> presents some correct prediction results. Note that the Levenshtein distance between every correct prediction and its respective ground-truth annotation always equals 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Error Analysis</head><p>The most common errors in the presented approach are caused by:</p><p>? Symmetry: as there are clockwise and counterclockwise dials, when the digits are blurred, the method can not differentiate the direction and thus may output the mirrored value of the real prediction. ? Neighbor value: the most common error. Variables such as angle, lighting, shadows and occlusion (when the pointer is in front of the dial scale mark) can hinder the reading of a dial. Even between the authors, there were some disagreements regarding the correct pointed value in such situations. ? Severe lighting conditions/Dirt: shadows, glares, reflections and dirt may confuse the networks, especially in low-contrast images, where those artifacts may emerge more than the pointer, fooling the network to think that it is a pointer border, resulting in an incorrect prediction. ? Rotation: rotated images are harder to predict, as the pointed value is not in the usual position. The predictions may be assigned to neighbor digits that would be in the current angle of the pointer if the image was not rotated.</p><p>To illustrate all of the aforementioned causes of errors, some samples are presented in <ref type="figure" target="#fig_6">Fig. 7</ref>. <ref type="table" target="#tab_0">Table VI</ref> summarizes the errors and their frequency on the best two methods: YOLOv3 and Faster R-CNN (ResNeXt-101). Note that most errors are caused by the neighbor values issue, when the pointer is in front of the mark, making it hard to determine if the pointed value is the one after or before the mark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Imaged-based AMR is a faster and less laborious solution than manual on-site reading, and easier to deploy than the replacement of old meters. In this work, we presented the issues and challenges regarding the automatic reading of dial meters since there are many open challenges in this context.</p><p>We introduced a public real-world dataset (shared upon request), called UFPR-ADMR, for automatic dial meter reading, that includes 2,000 fully annotated images acquired on site by employees of one of the largest companies of the Brazilian electricity sector <ref type="bibr" target="#b8">[9]</ref>. As far as we know, this is the first public dataset containing images of dial meters. The proposed dataset contains a well-defined evaluation protocol, which enables a fair comparison of different methods in future works. Considering that the image scenario is challenging in most cases, the deep networks Faster R-CNN and YOLO achieved promising results. This straightforward approach, without ROI detection or image preprocessing, simplified the traditional AMR pipeline <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, reducing the number of steps required to obtain the dial meter readings.</p><p>There is a lot of room for improvement, such as new methods to address the boundaries issues between the markers, which should solve most of the errors. In addition, a new loss function that penalizes errors on the leftmost dials should help to reduce the absolute error (minimizing the absolute error is of paramount importance to the service company).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The most common types of energy meters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of the images chosen for the dataset. In the bottom row, there are examples of the annotations provided for each image: in green the irregular surrounding quadrilateral, in blue the bounding boxes around the dials and in red the maximal ellipse contained in the bounding box. Note that the customer meter identification is blurred for keeping subject privacy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The distribution of digits according to the dial position on the meter. As 45% of the meters in the proposed dataset do not have a 5th dial, the values of the 5th dial quantities were interpolated proportionally for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>random scaling [?20%, 20%], random translation [?20%, 20%], random rotation [?15 ? , 15 ? ] and random shear [?12%, 12%]. The values, which are relative to the original size and position of the images, were chosen randomly within the defined intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Ground-truth and prediction examples of correctly recognized meters, with their respective Levenshtein distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Ground-truth and prediction examples with their respective Levenshtein distance. The errors are marked in red and include: a) neighbor values, b) severe lighting conditions, c) neighbor value (second dial) and symmetry (first dial); and d) rotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>ABOUT THE SIZE OF METERS AND INDIVIDUAL DIALS.</figDesc><table><row><cell></cell><cell>Min (px) W ? H</cell><cell>Max (px) W ? H</cell><cell>Mean (px) W ? H</cell><cell>Mean Area (px 2 )</cell></row><row><cell>Meters</cell><cell>96 ? 37</cell><cell>632 ? 336</cell><cell>326 ? 121</cell><cell>42,296</cell></row><row><cell>Dials</cell><cell>20 ? 29</cell><cell>206 ? 201</cell><cell>88 ? 86</cell><cell>8,328</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II FREQUENCY</head><label>II</label><figDesc>DISTRIBUTION OF DIGITS IN THE UFPR-ADMR DATASET.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Frequency / Digit Distribution</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell cols="6">996 913 899 906 929 936</cell><cell>942</cell><cell>872</cell><cell cols="2">818 886</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III DIAL</head><label>III</label><figDesc>DETECTION RESULTS ACHIEVED ON THE UFPR-ADMR DATASET.</figDesc><table><row><cell>Detection Model</cell><cell>Backbone</cell><cell>Prec.</cell><cell cols="2">(%) Recall F-score</cell></row><row><cell>Hough Circle Transform</cell><cell>-</cell><cell>53.27</cell><cell>55.28</cell><cell>54.25</cell></row><row><cell>Fast-YOLOv3</cell><cell>Darknet</cell><cell>99.94</cell><cell>100.0</cell><cell>99.97</cell></row><row><cell>YOLOv3</cell><cell>Darknet-53</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>Faster R-CNN</cell><cell>ResNet-50</cell><cell>100.0</cell><cell>99.94</cell><cell>99.97</cell></row><row><cell>Faster R-CNN</cell><cell>ResNet-101</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>Faster R-CNN</cell><cell cols="2">ResNeXt-101 100.0</cell><cell>100.0</cell><cell>100.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV RECOGNITION</head><label>IV</label><figDesc>RATE RESULTS OBTAINED ON THE UFPR-ADMR DATASET.</figDesc><table><row><cell>Method</cell><cell>Input Size</cell><cell>FPS</cell><cell cols="3">Recognition (%) Mean Abs. Dial Meter Error</cell></row><row><cell>Fast-YOLOv2</cell><cell>416 ? 416</cell><cell>244</cell><cell>79.61</cell><cell>42.25</cell><cell>5382.06</cell></row><row><cell>Fast-YOLOv2</cell><cell>608 ? 608</cell><cell>145</cell><cell>85.24</cell><cell>51.75</cell><cell>3810.34</cell></row><row><cell>Fast-YOLOv3</cell><cell>416 ? 416</cell><cell>220</cell><cell>83.27</cell><cell>47.75</cell><cell>6098.27</cell></row><row><cell>Fast-YOLOv3</cell><cell>608 ? 608</cell><cell>120</cell><cell>86.60</cell><cell>54.25</cell><cell>5183.82</cell></row><row><cell>YOLOv2</cell><cell>416 ? 416</cell><cell>67</cell><cell>91.42</cell><cell>68.00</cell><cell>2615.23</cell></row><row><cell>YOLOv2</cell><cell>608 ? 608</cell><cell>40</cell><cell>92.51</cell><cell>71.25</cell><cell>1924.98</cell></row><row><cell>YOLOv3</cell><cell>416 ? 416</cell><cell>35</cell><cell>93.00</cell><cell>73.75</cell><cell>1685.98</cell></row><row><cell>YOLOv3</cell><cell>608 ? 608</cell><cell>20</cell><cell>93.38</cell><cell>74.75</cell><cell>1591.16</cell></row><row><cell>FR-CNN R-50</cell><cell>800 ? 800</cell><cell>13</cell><cell>92.56</cell><cell>72.25</cell><cell>1451.81</cell></row><row><cell>FR-CNN R-101</cell><cell>800 ? 800</cell><cell>11</cell><cell>92.62</cell><cell>71.75</cell><cell>1343.29</cell></row><row><cell cols="2">FR-CNN X-101 800 ? 800</cell><cell>6</cell><cell>93.60</cell><cell>75.25</cell><cell>1591.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table V confirms this statement, as Faster R-CNN (ResNet-101) had fewer errors in the most significant dial (the leftmost).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V DISTRIBUTION</head><label>V</label><figDesc>OF ERRORS BY DIAL POSITION</figDesc><table><row><cell>Dial Position</cell><cell>1</cell><cell>2</cell><cell>Frequency (%) 3</cell><cell>4</cell><cell>5</cell></row><row><cell>YOLOv3</cell><cell cols="5">25.84 15.44 16.94 25.99 15.79</cell></row><row><cell>FR-CNN (R-101)</cell><cell cols="5">20.70 18.66 23.91 19.83 16.91</cell></row><row><cell>FR-CNN (X-101)</cell><cell cols="5">23.50 18.55 22.25 18.04 17.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI TYPE</head><label>VI</label><figDesc>AND FREQUENCY OF ERRORS OBTAINED ON EVALUATION.</figDesc><table><row><cell>Type of Error</cell><cell cols="2">Frequency YOLOv3 FR-CNN (X-101)</cell></row><row><cell>Symmetry</cell><cell>2%</cell><cell>3%</cell></row><row><cell>Neighbor value</cell><cell>82%</cell><cell>85%</cell></row><row><cell>Lighting conditions / Dirt</cell><cell>14%</cell><cell>9%</cell></row><row><cell>Rotation</cell><cell>2%</cell><cell>3%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The UFPR-ADMR dataset is publicly available (but upon request) to the research community at web.inf.ufpr.br/vri/databases/ufpr-admr/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by grants from the National Council for Scientific and Technological Development (CNPq), Grants #313423/2017-2 and #428333/2016-8) and the Coordination for the Improvement of Higher Education Personnel (CAPES) (Social Demand Program). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. We also thank the Energy Company of Paran? (Copel)  for providing the images for the UFPR-ADMR dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gas meter reading from real world images using a multi-net system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vanetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nodari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="519" to="526" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust angle invariant GAS meter reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2015-11" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Light-weight spliced convolution network-based automatic water meter reading in smart city</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="174" to="359" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for automatic meter reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13023</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Electric power annual 2018</title>
		<ptr target="https://www.eia.gov/electricity/annual/pdf/epa.pdf" />
		<editor>U. E. I. Administration.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on smart metering and smart grid communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kabalci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renewable and Sustainable Energy Reviews</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="302" to="318" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Types of meters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ausgrid</surname></persName>
		</author>
		<ptr target="https://www.ausgrid.com.au/Your-energy-use/Meters/Type-of-meters" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Types of electricity meters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callmepower</surname></persName>
		</author>
		<ptr target="https://callmepower.com/useful-information/electricity-meter-types" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Energy Company Of Paran?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copel</surname></persName>
		</author>
		<ptr target="http://www.copel.com/hpcopel/english/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A detection and recognition system of pointer meters in substations based on computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page">107333</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Development of an automatic reading method and software for pointer instruments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Electronics Instrumentation Information Systems</title>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An automatic detection and recognition method for pointer-type meters in natural gas stations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Control Conference</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="7866" to="7871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading recognition method of analog measuring instruments based on improved hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bingjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Electronic Measurement Instruments</title>
		<imprint>
			<date type="published" when="2011-08" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A mask RCNN based automatic reading method for pointer meter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Control Conference</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="8466" to="8471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extraction of energy information from analog meters using image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2032" to="2040" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image processing for automatic reading of electro-mechanical utility meters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ocampo-Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican International Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013-11" />
			<biblScope unit="page" from="164" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cutting sayre&apos;s knot: Reading scene text without segmentation. Application to utility meters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusiol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR Intern. Workshop on Document Analysis Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Use SSD to detect the digital region in electricity meter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Cybernetics (ICMLC)</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional sequence recognition network for water meter number reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="679" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer-type meter automatic reading from complex environment based on visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Wavelet Analysis and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="264" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A value recognition algorithm for pointer meter based on improved Mask-RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Science and Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="108" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A multi-neural network approach to image detection and segmentation of gas meter counter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nodari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="239" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An efficient and layout-independent automatic license plate recognition system based on the YOLO detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01754</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A benchmark for iris location and a deep learning detector evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Bezerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weingaertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A robust real-time automatic license plate recognition based on the YOLO detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
