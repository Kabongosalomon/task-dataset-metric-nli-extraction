<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decomposed Soft Actor Critic Method for Cooperative Multi-Agent Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Pu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaochen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
							<email>xinyao@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
							<email>binli@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Decomposed Soft Actor Critic Method for Cooperative Multi-Agent Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep reinforcement learning ? Multi-agent ? Actor-critic</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep reinforcement learning methods have shown great performance on many challenging cooperative multi-agent tasks. Two main promising research directions are multi-agent value function decomposition and multi-agent policy gradients. In this paper, we propose a new decomposed multi-agent soft actor-critic (mSAC) method, which effectively combines the advantages of the aforementioned two methods. The main modules include decomposed Q network architecture, discrete probabilistic policy and counterfactual advantage function (optinal). Theoretically, mSAC supports efficient off-policy learning and addresses credit assignment problem partially in both discrete and continuous action spaces. Tested on StarCraft II micromanagement cooperative multiagent benchmark, we empirically investigate the performance of mSAC against its variants and analyze the effects of the different components. Experimental results demonstrate that mSAC significantly outperforms policy-based approach COMA, and achieves competitive results with SOTA value-based approach Qmix on most tasks in terms of asymptotic perfomance metric. In addition, mSAC achieves pretty good results on large action space tasks, such as 2c vs 64zg and M M M 2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many real-world tasks can be modeled as multi-agent systems. Developing AI system for playing multiagent games has raised much attention. Recent years, deep multi-agent reinforcement learning (MARL) algorithms <ref type="bibr" target="#b0">[1]</ref> have presented impressive results in many challenging multi-agent systems, such as the coordination of autonomous vehicles <ref type="bibr" target="#b1">[2]</ref>, the challenging StarCraft II game <ref type="bibr" target="#b2">[3]</ref>, etc. Maybe the simplest way to solve multi-agent system problems is, treating everything else as the environment for each individual agent, and learning concurrently based on the global reward. However this will face the issues <ref type="bibr" target="#b3">[4]</ref>: <ref type="bibr" target="#b0">(1)</ref> non-stationarity: when an agent is learning, the policies of other agents are also changing simultaneously, which means that the dynamic of environments is non-stationary; (2) scalability: the joint state and action space grows exponentially as the number of agents increases. To cope with these issues, most recent advanced algorithms adopted the paradigm of centralized training with decentralized execution (CTDE) <ref type="bibr" target="#b4">[5]</ref>, in which they learn a centralized critic conditioned on joint action and observation history and take decentralized execution by learning different local actor (value functions or policies) for each individual agents.</p><p>Following CTDE paradigm, there are two main popular and promising research lines in MARL, one is the value function decomposition approach, another is multi-agent policy gradients. Value Decomposition Network (VDN) <ref type="bibr" target="#b5">[6]</ref> represented joint Q value Q tot as a sum of individual Q-values q i that condition only on individual actions and observations. Each decentralized policy arise simply from its local Q values q i (selects actions greedily by q i ). Afterwards, QMIX <ref type="bibr" target="#b6">[7]</ref> employed a network to estimate joint action-values as a non-linear combination of per-agent values that condition on local observations. The representative work of multi-agent policy gradient method is COMA <ref type="bibr" target="#b7">[8]</ref> method, which explicitly used a counterfactual baseline to address the challenges of multi-agent credit assignment and a critic representation to compute the counterfactual baseline efficiently.</p><p>Recent work <ref type="bibr" target="#b15">[16]</ref> points out that multi-agent Q-learning with linear value decomposition implicitly implements a classical multi-agent credit assignment method called counterfactual difference rewards, which draws a connection with COMA. However, value function decomposition is hard to apply in offpolicy training and potentially suffers from the risk of unbounded divergence. In single-agent problems, to achieve sample efficiency and robust performance, <ref type="bibr" target="#b5">[6]</ref> proposed the soft actor-critic algorithm, which is an off-policy actor-critic RL algorithm based on the maximum entropy reinforcement learning framework and achieves state-of-the-art performance on many challenging continuous control benchmarks.</p><p>To attain both stability and good final performance in CTDE paradigm, how to effectively incorporate soft actor critic paradigm with multi-agent value function decomposition would be important. Following the research line of <ref type="bibr" target="#b13">[14]</ref>, our key insight is, to efficiently compute the expected joint Q values, only when this linear condition -the joint Q value Q tot is the linear mixture of the individual Q value q i satisfy, the following equation holds (Detailed proof can be found in Appendix),</p><formula xml:id="formula_0">E ? Q tot (s, ? , a) = i k i (s)E ? i q i ? i , a i + b(s) (1) = q mix (s, E ? i q i ? i , a i )<label>(2)</label></formula><p>where the Q tot represents neural networks that consist of agent networks q i and the mixing network q mix . Note that, in our method, to make the aforementioned equation holds, the mixing network q mix is not a complex non-linear way but linear weights that is produced by the hyper-network only conditioned on global state information. Motivated by these insights, in this paper, we present a novel multi-agent soft actor-critic (mSAC) method, which is based on the following assumption: the joint Q value Q tot is the linear mixture of the individual Q values q i . mCSAC contains three main components: decomposed soft Q network architecture, decentralized probabilistic policy, and counterfactual advantage function. This method incorporates the idea of the soft actor-critic and multi-agent value function decomposition effectively.</p><p>We empirically investigate the performance of our algorithm mSAC and analyze the influence of these components by ablation studies in StarCraft II micromanagement cooperative multi-agent tasks. Experiment results demonstrate that mSAC significantly outperforms current advanced policy-based algorithms (e.g. COMA) and achieves comparable performance with value-based approaches (e.g. Qmix) on most tasks. In addition, the variant method mSAC achieves pretty good results in large action space tasks, like 2c vs 64zg and M M M 2 task.</p><p>To sum up, here are our contributions:</p><p>-We propose the novel mSAC method to effectively incorporate soft actor critic with value function decomposition method and investigate its practical performance on StarCraft II cooperative multiagent benchmark. -We conduct extensive performance test of different mSAC variants to show the effect of soft value iteration, counterfactual advantage function, probabilistic policy, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Soft Actor-Critic</head><p>Before introducing the Soft Actor-Critic method (SAC) <ref type="bibr" target="#b10">[11]</ref>, we first briefly present the deep reinforcement learning (RL) problem definition. RL problem is often formulated as a Markov Decision Process (MDP), M = (S, A, p, r, ?). When the RL agent interacting with the environment, at each step, the agent observes a state s t ? S, where S is the state space, and chooses an action a t ? A, according to the policy ?(a t |s t ), where A is the state space, then the agent receives a reward r (s t , a t ) and the environment transforms to a next state s t+1 ? p(s t+1 |s t , a t ).</p><p>The objective of reinforcement learning is to maximize the discounted expected total reward. However, in a maximum entropy RL framework, the goal is not only to optimize the cumulative expected rewards, but also maximizes the expected entropy of the policy:</p><formula xml:id="formula_1">J(?) = T t=0 E (st,at)??? [r (s t , a t ) + ?H (? (?|s t ))]<label>(3)</label></formula><p>, where ? is the discounted factor, ? ? (s t , a t ) denotes the state-action marginal distribution of the trajectory induced by the policy ?(a t |s t ). SAC is a popular single-agent off-policy actor-critic method using the maximum entropy reinforcement learning framework. It utilizes an actor-critic architecture with separate policy and value networks, an off-policy paradigm that enables reuse of previously collected data, and entropy maximization to enable effective exploration. In contrast to other off-policy algorithms, SAC is quite stable and has been considered as a state-of-the-art baseline for a diverse range of RL problems with continuous actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Value Function Decomposition</head><p>Value function decomposition (VDN ) <ref type="bibr" target="#b8">[9]</ref> methods learn local Q value functions for each individual agent, and then these local Q values are combined with a learnable mixing neural network to produce joint Q values.</p><formula xml:id="formula_2">Q tot (?, a) = q mix (s, q i ? i , a i )<label>(4)</label></formula><p>In VDN, the mixing function q mix is a simple algorithmic summation. While in QMIX, it's a non-linear monotonic factorization structure, which can achieve a much richer function class at the the same time satisfy the principle of the Individual-Global Maximization (IGM): a global argmax performed on Q tot yields the same result as a set of individual argmax operations performed on each local q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Agent Policy Gradients</head><p>The centralized training with decentralized execution (CTDE) paradigm has recently attracted attention for its ability to address non-stationarity problems Learning a centralized critic with decentralized actors (CCDA) is an efficient approach that exploits the CTDE paradigm. COMA and MADDPG are two representative examples. COMA uses a centralised critic to estimate the Q function and decentralised actors to optimise the agents' policies. To address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. In addition, COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. And it updates stochastic policies using the gradients:</p><formula xml:id="formula_3">g = E ? i ? ?i log ? i a i | ? i A i (?, a)<label>(5)</label></formula><p>where,</p><formula xml:id="formula_4">A i (?, a) = Q tot (?, a) ? a ,i ? i (a ,i | ? i )Q tot ?, a ?i , a ,i</formula><p>is a counterfactual advantage and a ?i is the joint action other than agent i. MADDPG <ref type="bibr" target="#b9">[10]</ref> is an adaptation of actor-critic methods which learns deterministic policies in continuous action spaces, considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we first introduce the definition and notation of the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) and then introduce the multi-agent policy gradient decomposed architecture. Afterwards we present the three variant method: multi-agent soft actor-critic (mSAC) method, multi-agent counterfactual soft actor-critic (mCSAC) method, multi-agent counterfactual actorcritic (mCAC) method, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>The fully cooperative multi-agent tasks can be modelled as a decentralized partially observable Markov decision process (Dec-POMDP) <ref type="bibr" target="#b11">[12]</ref> G = S, A, P, R, ?, O, n, ? , where s ? S is the global state and o ? ? is a local observation. At each time-step, each agent i receives an observation o i drawn according to the observation function O(s, i) and selects an action a i ? A i , forming a joint action a ? A ? A n , and the environment transitions to the next state s according to the transition function P (s | s, a) and receiving a reward r = R(s, a) shared by all agents. Each agent learns a policy ? i a i | ? i ; ? i , which is parameterized by ? i and conditioned on the local observation-action history ? i ? T ? (? ? A) * . The joint policy ?, with parameters ? = ? 1 , ? ? ? , ? n , constitute of the joint Q function: </p><formula xml:id="formula_5">Q tot ? (?, a) = E s0:?,a0:? [ ? t=0 ? t R (s t , a t ) | s 0 = s, a 0 = a, ?]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Agent Decomposed Policy Gradient Architecture</head><p>In this part, we first present the common multi-agent decomposed policy gradient architecture for all the algorithm variants we will introduce in the next three subsections. We use function approximators (neural networks) for both the centralized critic: Q-function and the decentralized actor: policy, and alternate between optimizing both networks with stochastic gradient descent. We will consider a parameterized Q function Q ? (s t , ? t , a t ) and a tractable policy ? ? (a t |? t )., where ? and ? are referred to the parameters of the Q networks and policy networks, respectively.</p><p>Policy Network Also called decentralized actor. For simplicity, our decentralized actor (policy) network structure is the same as the agent i's local Q network except that one clamp(?5, 2) operation and a sof tmax layer is added after the local Q network. At the beginning of training, if the policy networks have improper initialization parameters which would result in policy distribution becomes too sharp potentially, and thereby constrain the degree of exploration. Empirically, we found this clamp operation relieves this issue and accelerates training. The softmax layer is to convert probabilistic logits to the categorical distribution. The policy network parameter is shared among all agents, and different agents are distinguished by utilizing a one-hot identity vector, in order to be consistent with Qmix and fair comparing.</p><p>Value Network The centralized critic's network structure is modified from Qmix's Q network structure, and is comprised of agent i's local Q network q i and mixing network q mix in which the weights and biases are produced by the separate hyper-networks. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates detailed structure of local Q network and the mixing network. For each agent i, there is one local Q network that represents its local Q value function q i (? i , a i ). We represent local Q networks as GRUs <ref type="bibr" target="#b16">[17]</ref> that receive the current individual observation o i t and the last action a i t?1 as input at each time step. The mixing network is a feed-forward neural network that takes the agent's local Q network outputs as input and mixes them linearly and followed by an absolute activation function, producing the values of Q tot , as shown in <ref type="figure" target="#fig_0">Figure  1</ref>. To make the equation <ref type="formula">(1)</ref> holds, the weights and the biases of the mixing network are restricted to be linear functions of s, and the parameters are produced by the separate hyper-networks same as in Qmix, which allows us to effectively calculate the expected Q values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">multi-agent Soft Actor-Critic (mSAC)</head><p>Before introducing the counterfactual multi-agent soft actor-critic method, we first present multi-agent soft actor-critic method (we refer it as mSAC), which adopts the practical approximation to soft policy iteration as in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Similar with <ref type="bibr" target="#b10">[11]</ref>, the critic loss function of the mSAC method in multi-agent setting is,</p><formula xml:id="formula_6">L(?) = E D r t + ? * min j?1,2Q targ ? j ? Q tot ? (s t , ? t , a t ) 2<label>(6)</label></formula><p>Based on original soft actor-critic algorithm, our methods also utilize two soft Q-value networks Q tot ?j (s, ? , a), for j ? {1, 2}, and take the min values as the target. In equation <ref type="formula">(7)</ref>,</p><formula xml:id="formula_7">Q targ ? j = E? ? Q tot ? j (st+1, ?t+1, at+1) ? ? log ? (at+1 | ?t+1) (7) = q mix st+1, E ? i q i ? i t+1 , a ,i t+1 ? ? log ? i a ,i t+1 | ? i t+1 (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 mSAC</head><p>Initialize network parameters: ?, ?1,2,?1,2, and replay buffers: D for training policy and value networks. max training episodesM , replay buffer size rlbs 1: for episode = 1 to M do 2: for each agent i, observe the global state s and its individual observations o i 3:</p><p>for t = 1 to max-episode-length do 4:</p><p>for each agent i, , select action a i t according to the current policy ? ? (a i t |? i t ) 5:</p><p>Execute actionsa = (a 1 , a 2 , ..., a N ) and interact with the environment and obtain the global rewardr, and the environment transitions to the next global state s 6:</p><p>add the experience st, o i t , at, rt, st+1, o i t+1 to the replay buffer D 7:</p><p>for each rl training step do 8:</p><p>Sample a random minibatch of B uniformaly from D 9:</p><p>Update critic network w.r.t the equation <ref type="formula" target="#formula_6">(6)</ref> ?i ? ?i ? ?Q? ? i JQ (?i), for i ? {1, 2} 10:</p><p>Update actor network w.r.t the equation <ref type="formula" target="#formula_9">(9)</ref> ? ? ? ? ??? ? J?(?) 11:</p><p>Update hyper-parameter? w.r.t the equation <ref type="formula" target="#formula_11">(11)</ref> Update target value network parameters for each agent i:</p><formula xml:id="formula_8">?i ? ? ?i + (1 ? ? )?i, for i ? {1, 2} 12:</formula><p>end for 13: end for 14: end for 15: return Q ? , ? ? and D is a replay buffer containing previously sampled transitions (states, local observations, actions, rewards, next states, next local observations): and Q tot ? j is the target Q network, with parameters ? j that are obtained as an exponentially moving average of the current Q network weights ? j , which has been shown to stabilize training.</p><p>Note that, in equation <ref type="formula" target="#formula_9">(9)</ref>, a ,i t+1 is sampled from agent i's current policy ? i rather than sampled from the replay buffer. Compared with Qmix, we adopted the additional policy network that outputs the probabilistic policy (which is a probability mass function for discrete domain), which exactly represents the probabilistic value of each agent selects each discrete action. therefore we can calculate the expectation values exactly.</p><p>Recent work theoretically proved that the the soft (or call Boltzmann) policy iteration is guaranteed to improve and can converge to the optimal policy. Derived from the soft policy iteration procedure, the objective for policy update is below:</p><formula xml:id="formula_9">L(?) = E D ? log ? (a t | ? t ) ? Q tot ? (s t , ? t , a t )<label>(9)</label></formula><formula xml:id="formula_10">= q mix (s t , E ? i q i ? i t , a i t ? ? log ? i a i t | ? i t )<label>(10)</label></formula><p>? is a hyper-parameter that controls the trade-off between maximizing the entropy of policy and the expected discounted return. However, ? need to be set as different values at different stages of training or on different tasks. Because in different states, the degree of exploration needed is different. In some states, good policy have been learned, and the corresponding ? value should be reduced to very samll to weaken the degree of exploration, but in other states, it's not sure which action is good and which action is bad, so we need to increse the degree of exploration. The SAC algorithm proposes to reconstruct the original soft policy iterative process as a constrained optimization problem, that is, when optimizing the policy to maximize cumulative discount returns, the algorithm should keep the average entropy of policy a fixed value (usually ?|A|) and the action entropy in different states can be variable. Specifically, ? is automatically updated by optimizing the following loss <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_11">L(?) = E at??t ?? log ? t (a t | ? t ) ? ?H<label>(11)</label></formula><p>The details of the mSAC algorithm are summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">multi-agent Counterfactual Soft Actor-Critic (mCSAC)</head><p>One of the most important problems in multi-agent reinforcement learning is credit assignment. For partially solving this issue, we adopted the insight in COMA that is using the counterfactual advantage function when we optimize the individual policy in the multi-agent decomposed policy gradient paradigms. The loss function of policy in multi-agent counterfactual soft actor-critic (mCSAC) method is as following:</p><formula xml:id="formula_12">E (st,?t,rt,)?D,at?? ? log ? a a i t | ? i t A i (s t , ? t , a t )<label>(12)</label></formula><p>where,</p><formula xml:id="formula_13">A i (st, ?t, at) = ? ? log ? i a i t | ? i t + Q tot ? (st, ?t, at) (13) ? q mix st, E ? i q i ? i t , a i, t , q ?i ? ?i t , a ?i t<label>(14)</label></formula><p>Note that on the right side of the above equation, a t = a i t , a ?i t , a t refers to the joint actions at time t, and a i t refers to the local action of the agent i, and a ?i t refers to the (partial) joint actions of the agents other than the agent i. a ?i t is sampled from the current policy ? i of agent i, and a ?i t is sampled from the replay buffer D , q mix (.) calculates the counterfactual baseline, which measures the expected action value under the individual policy of the agent i when fix the actions of other agents except agent i. If the joint action value of the samples sampled from the replay buffer (s t , ? t , a t ) is greater than the previous baseline, then we update the policy network parameters of the agent i to increase the action probability of a i t , and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">multi-agent Counterfactual Actor-Critic (mCAC)</head><p>In order to probe the effect of the soft policy iteration paradigm on multi-agent policy optimization, in this section we introduce another variant of the mSAC method: the multi-agent counterfactual actor critic (mCAC) method. which can be obatained after deletes the entropy augment item corresponding to ?log? from all loss functions of the mCSAC method. Because it does not satisfy the condition of soft policy iteration paradigm, mCAC becomes an on-policy algorithm. The capacity of the replay buffer used to update the policy and the value network is set to a small value. The behavior strategy used to collect trajectory experience is not based on the categorical distribution that outputed by the policy network, but a strategy similar to -greedy exploration. The specific implementation of mCAC is similar to the mSAC algorithm. Action probabilities are produced from the final layer, z, via a bounded softmax distribution that lower-bounds the probability of any given action by /|A|: P (a) = (1 ? ) * sof tmax(z) a + /|A|). We anneal linearly from 0.5 to 0.02 across 20000 training episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our experimental results and some analysis. First, we describe the decentralised cooperated StarCraft II micromanagement benchmark to which we apply our proposed method mSAC and the variant methods we consider. Then we present the performance comparison between mSAC, the ablation variant algorithms mCSAC, mCAC and the representative value decomposed algorithm-Qmix and policy gradient algorithm-COMA in aforementioned discrete action environments. We used the Qmix implementation from this open-source code 1 with the same hyper-parameters as <ref type="bibr" target="#b6">[7]</ref>. To be consistent with previous work, our implementation 2 almost use the same network architecture and hyper-parameters across all the tasks. More experimental details can be found in the Appendix.</p><p>Experimental Setup We focus on the StarCraft II decentralized micromanagement tasks <ref type="bibr" target="#b12">[13]</ref> 3 , in which each of the agents controls an individual army unit and each agent receives a global shared reward. We use StarCraft Multi-Agent Challenge (SMAC) environment <ref type="bibr" target="#b14">[15]</ref> as our APIs, which has become a common-used benchmark for evaluating state-of-the-art MARL approaches such as COMA, QMIX and other baseline algorithms. In this paper, our algorithm learns multiple agents (or called policies) to control allied units to beat the enemy, while the enemy units are controlled by a built-in handcrafted AI, which make use of the handcrafted heuristics. Two representative StarCraftII micromanagement scenarios (3m and 2c vs 64zg) are shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>For comparing each method's performance justly as much as possible, our Qmix implementation also use target Q networks that are obtained as an exponentially moving average of the Q function weights, which was different from the hard update manner in the original paper <ref type="bibr" target="#b6">[7]</ref>. In addition, we adopt the same evaluation procedure as in <ref type="bibr" target="#b6">[7]</ref>. For each run of a method, we pause training every 100 episodes and run 20 independent episodes where each agent performing greedy decentralised action selection (for Qmix chosen the action with the largest local Q values, for other methods chosen the action with the largest probability value). The percentage of these episodes in which the method defeats all enemy units within the (different) time limit is referred as the test win rate.</p><p>The magnitude of x-axis is 100 episodes, and for different maps, there are different episode length limits according to the difficulty level of different tasks. The shaded region indicates the one quarter of standard deviation.</p><p>Algorithm Details of Variant Methods The policy network of all agents includes a recurrent layer composed of GRUs with 64-dimensional hidden states, and a fully connected MLP layer before and after this. After the team is defeated or the time step limit is reached, one episode ends. The mixed network part of the value function consists of a single hidden layer of 64 units, and the ELU nonlinear activation function is not used. Its weights and biases are generated by an additional hyper-network composed of a single hidden layer of 64 units without the ReLU nonlinear activation function.</p><p>Similar to Qmix, the mSAC algorithm training is also carried out in mini-batch mode, the batch size is 32, the target smoothing coefficient used to update the two target Q networks is 0.005, and the discount coefficient is set to 0.95. Due to parameter sharing, all agents will be processed in parallel, and the information of each agent at each time step of each episode occupies one entry of the mini-batch. Once a new episode of trajectory is added to the replay buffer, the algorithm will update the network parameters of the actor and critic. Specifically, after collecting a episode of trajectory, 32 episodes were sampled from the replay buffer as a mini-batch to train the actor and the critic, fully expand the recurrent network part of the actor and the critic at all time steps and backpropagate the gradients, then apply the summarized gradient update to the neural network. For clarity, the hyperparameter settings of the mSAC algorithm are summarized in <ref type="table" target="#tab_0">Table 1</ref>. The learning performance of the mSAC method and its variant methods mCSAC and mCAC on the StarCraft II micro-operation task were tested separately to study the impact of off-policy update, soft Q value, probability distribution policy, counterfactual advantage function and other modules on the multi-agent policy gradient algorithm. In all maps, all algorithms used reward standardization techniques for stability purposes, r standard = 10 * (r ? mean)/(r ? std + 1e ? 6)</p><p>For clarity, we briefly outline the key differences of our different variant methods in <ref type="table" target="#tab_1">Table 2</ref>.  <ref type="bibr" target="#b17">[18]</ref>, so we don't plot the COMA curves in these graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this part, we compared the performance of our method mSAC, its variant methods mCSAC, mCAC, and advanced Qmix and COMA methods in the different maps, including maps with homogeneous agents (3m, 8m), and maps with heterogeneous agents (1c3s5z, 3s5z, 3s vs 5z), maps that agent contains a large action space (2c vs 64zg, M M M 2, bane vs bane, 27m vs 30m). The performance test win rate learning curves are shown in <ref type="figure">Figure 3</ref>, 4 and 5 respectively. Through the experimental results, it can be found that the method mSAC proposed in this chapter is similar to policy-based method COMA on the map of homogeneous agents, and is significantly better than COMA on other maps. In the maps 8m, 1c3s5z, 3m, 3s5z, compared with the current value-based method Qmix, it has similar asymptotic performance. In the map with a large action space for the agent (2c vs 6 4zg, M M M 2, bane vs bane), its performance is significantly better than Qmix. In some relatively difficult tasks, such as 3s vs 5z, the performance of all policy-based methods is worse than Qmix but not much. After carefully analyzing the experimental results, the following observations and conclusions can be drawn:  1. Soft policy iteration paradigm is also effective in multi-agent scenarios. From the comparison of the results of all maps we found that mCAC behaves worse than the other methods both in stability and asymptotic performance, which indicates that soft policy iteration paradigm is usually beneficial to the robust policy improvement in multi-agent policy gradient setting. We conjecture that this is because simultaneously maximizing expected return and entropy can make the agent explore more widely and efficiently, and can capture multiple modes of near-optimal behaviours.</p><p>2. It is important to jointly optimize the entire policy distribution On tasks where the agent has a relatively large action space. For example, in the map 2c vs 64zg, the Colossi unit has a large action space |A| = 70. In the Qmix method, all agents are executed in a decentralized manner. Each agent greedily selects actions based on its local action value function. In a certain state, there is only one action that maximizes the local action value, and the others actions are given the same selection probability . At a certain time, in map 2c vs 64zg, the probability of ally unit attacking a specific enemy among all the 64 enemy units is very high, while the The probability of attacking the other 63 units is the same . While in the mSAC method, each agent executes actions according to its own strategy, that is, the learned categorical distribution, and can choose different areas of the action space in a planned way. By jointly optimizing the entire probability distribution to maximize the sum of expected returns and strategy entropy, intuitively speaking, this is more reasonable and effective than Qmix's -greedy paradigm exploration on tasks with large action spaces.</p><p>3. Counterfactual advantage functions are not always effective, and are more important in relatively complex tasks. For easy environments, like map 8m, 1c3s5z, 3s5z, the performance of mCSAC and mSAC is similar, but in harder environment, like map 3s vs 5z , the performance gap of mCSAC and mSAC is larger, which indicates that attribution of global reward is critical for solving this harder task, the counterfactual advantage function partially addresses the issue, could gradually learn a reasonable credit assignment during training in some tasks but is not always effective. Moreover, we carefully analyzed the performance of each seed and found that after training of 80000 episodes, in some seeds the test win rate can perform up to 90%, and some seeds are zero. We speculate that this may because it's more difficult to explore good strategies in difficult maps, which indicates that effective exploration would be a important research problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Works</head><p>In this paper, we presented the new decomposed multi-agent soft actor-critic method (mSAC) that incorporates value function decomposition, soft policy iteration, and counterfactual advantage function (optional), which supports efficient off-policy learning and addresses the issue of credit assignment partially. mSAC learns the distributional policy for each agent simultaneously which seems like a guided distributional exploration implicitly, which is especially important in large action space task through the experimental results.</p><p>In addition, we empirically investigate the performance of mSAC and its variant methods in StarCraft II micromanagement cooperative multi-agent benchmark. Experimental results demonstrate that mSAC can achieve relatively stable and efficient multi-agent off-policy learning and outperforms, or is competitive with, current main policy-based algorithms and value-based approaches (e.g. COMA, and Qmix) on most tasks, and achieves very good results in large action space task like 2c vs 64zg and M M M 2.</p><p>However, in this paper, we only study the effect of counterfactual multi-agent soft actor-critic paradigm on the discrete domain. Experiments under continuous domain need to be studied. In addition, The more solid theoretical analysis of the algorithm will be needed, and at the same time, how to explore more efficiently will be a valuable future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some Equation Proof Details</head><p>The important equation for efficiently calculating the expectation of the joint Q values using the expectation of the local Q values following local policies:</p><formula xml:id="formula_15">E ? Q tot (s, ? , a) = i k i (s)E ? i q i ? i , a i + b(s) = q mix (s, E ? i q i ? i , a i )<label>(16)</label></formula><p>The detailed proof is as follows: </p><formula xml:id="formula_16">k i (s) a ? i (a i |? i )? ?i (a ?i |? ?i )q i ? i , a i = i k i (s) a i ? i (a i |? i )q i ? i , a i a ?i ? ?i (a ?i |? ?i ) = i k i (s) a i ? i (a i |? i )q i ? i , a i = i k i (s)E ? i q i ? i , a i<label>(17)</label></formula><p>E ? Q tot (s, ? , a) ? ? log ? (a | ? ) = i k i (s)E ? i q i ? i , a i + b(s) + ?H(?) = q mix (s, E ? i q i ? i , a i ) + ?H(?)</p><p>we could approximate the above equation using following equation, q mix (s, E ? i q i ? i , a i ? ? log ? i a i | ? i ) = i k i (s)E ? i q i ? i , a i + i k i (s)E ? i ?? log ? i a i |? i + b(s)</p><formula xml:id="formula_18">= i k i (s)E ? i q i ? i , a i + b(s) + ? i k i (s)H(? i )<label>(19)</label></formula><p>if we use a different mixing network for entropy term, the equation becomes:</p><p>q mix1 (s, E ? i q i ? i , a i ) + q mix2 (s, E ? i ?? log ? i a i | ? i )</p><formula xml:id="formula_19">= i k i 1 (s)E ? i q i ? i , a i + b(s) + ? i k i 2 (s)H(? i )<label>(20)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Left: mixing network structure. Red figures are the hyper-networks that produce the weights and biases for the mixing network layers. Middle: the overall Qmix architecture. Right: agent's local Q network, which is in green, the i means the corresponding one-hot vector to distinguish different agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Visualizations of the two representative StarCraftII micromanagement scenarios (3m and 2c vs 64zg).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) 2c vs 64zg (b) M M M 2 (c) bane vs bane</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>The performance curves for mSAC and QMIX on large action space tasks: 2c vs 64zg, M M M 2 and bane vs bane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>E</head><label></label><figDesc>? Q tot (s, ? , a) = a ?(a|? )Q tot (s, ? , a) = a ?(a|? ) i k i (s)q i ? i , a i + b(s) = a ?(a|? ) i k i (s)q i ? i , a i + a ?(a|? )b(s) = i a ?(a|? )k i (s)q i ? i , a i + b(s) below omit b(s) for simplicity = i k i (s) a ?(a|? )q i ? i , a i = i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameters</figDesc><table><row><cell>Parameter Name</cell><cell>Value</cell></row><row><cell>leaning rate</cell><cell>5e-4</cell></row><row><cell cols="2">target smoothing coefficient (? ) 0.005</cell></row><row><cell>discount factor</cell><cell>0.99</cell></row><row><cell>optimizer</cell><cell>RMSprop</cell></row><row><cell>activation function</cell><cell>ReLU</cell></row><row><cell cols="2">replay buffer size (Off-policy) 5000 episodes</cell></row><row><cell cols="2">replay buffer size (On-policy) 32 episodes</cell></row><row><cell>RL batch size</cell><cell>32 episodes</cell></row><row><cell>KL lambda</cell><cell>automated adoptiing</cell></row><row><cell>entropy target</cell><cell>dim(A) (e.g. , -9 for 3m)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Variant Methods The performance curves for mCSAC, mSAC, mCAC, and QMIX, COMA on different StarCraft II micromanagement combat maps with heterogeneous agents. In 3s5z and 3s vs 5z, COMA method achieves zero test win-rate according to the experimental results in</figDesc><table><row><cell cols="2">method on/off policy buffer size</cell><cell cols="2">counterfactual advantage function soft Q values</cell></row><row><cell>mSAC off-policy</cell><cell cols="2">5000 episodes no</cell><cell>yes</cell></row><row><cell>mCSAC off-policy</cell><cell cols="2">5000 episodes yes</cell><cell>yes</cell></row><row><cell>mCAC on-policy</cell><cell cols="2">32 episodes yes</cell><cell>no</cell></row><row><cell></cell><cell>(a) 8m</cell><cell>(b) 3m</cell></row><row><cell cols="4">Fig. 3: The performance curves for mCSAC, mSAC, mCAC, and QMIX, COMA on different StarCraft</cell></row><row><cell cols="3">II micromanagement combat maps with homogeneous agents.</cell></row><row><cell cols="2">(a) 1c3s5z</cell><cell>(b) 3s5z</cell></row><row><cell></cell><cell></cell><cell>(c) 3s vs 5z</cell></row><row><cell>Fig. 4:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>StarCraftII Micromanagement Maps Parameters</figDesc><table><row><cell>Name</cell><cell>Ally Units</cell><cell>Enemy Units</cell><cell cols="3">Episode Length Obs. Dim Action Dim</cell></row><row><cell>Easy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3m</cell><cell>3 Marines</cell><cell>3 Marines</cell><cell>60</cell><cell>30</cell><cell>9</cell></row><row><cell>8m</cell><cell>8 Marines</cell><cell>8 Marines</cell><cell>120</cell><cell>80</cell><cell>14</cell></row><row><cell>1c3s5z</cell><cell>1 Colossi, 3 Stalkers, 5 Zealots</cell><cell>1 Colossi, 3 Stalkers,5 Zealots</cell><cell>180</cell><cell>162</cell><cell>15</cell></row><row><cell cols="2">bane vs bane 4 Banelings,20 Zerglings</cell><cell>4 Banelings,20 Zerglings</cell><cell>200</cell><cell>336</cell><cell>30</cell></row><row><cell>Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3s5z</cell><cell>3 Stalkers, 5 Zealots</cell><cell>3 Stalkers, 5 Zealots</cell><cell>150</cell><cell>128</cell><cell>14</cell></row><row><cell>3s vs 5z</cell><cell>3 Stalkers</cell><cell>5 Zealots</cell><cell>250</cell><cell>48</cell><cell>11</cell></row><row><cell>2c vs 64zg</cell><cell>2 Colossi</cell><cell>64 Zerglings</cell><cell>400</cell><cell>332</cell><cell>70</cell></row><row><cell cols="2">10m vs 11m 10 Marines</cell><cell>11 Marines</cell><cell>150</cell><cell>105</cell><cell>17</cell></row><row><cell>SuperHard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">27m vs 30m 27 Marines</cell><cell>30 Marines</cell><cell>180</cell><cell>285</cell><cell>36</cell></row><row><cell cols="2">3s5z vs 3s6z 3 Stalkers,5 Zealots</cell><cell>3 Stalkers,6 Zealots</cell><cell>170</cell><cell>136</cell><cell>15</cell></row><row><cell>MMM2</cell><cell cols="3">1 Medivac,2 Marauders,7 Marines 1 Medivac,3 Marauders,8 Marines 180</cell><cell>176</cell><cell>18</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/starry-sky6688/StarCraft 2 https://github.com/puyuan1996/MARL 3 We use StarCraft 2 Version SC2.4.10 in our experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An overview of recent progress in the study of distributed multi-agent coordination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanrong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="427" to="438" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grandmaster level in StarCraft II using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stabilising experience replay for deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1146" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning as a rehearsal for decentralized planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kraemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page" from="82" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 17th International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schr?der De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4292" to="4301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farquhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The representational capacity of action-value networks for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1862" to="1864" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-agent actorcritic for mixed cooperative-competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6379" to="6390" />
		</imprint>
	</monogr>
	<note>OpenAI Pieter Abbeel, and Igor Mordatch</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A concise introduction to decentralized POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The StarCraft Multi-Agent Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Man</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04043</idno>
		<idno>arXiv: 1902.04043</idno>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DOP: Off-policy Multi-Agent Decomposed Policy Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Addressing Function Approximation Error in Actor-Critic Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards Understanding Linear Value Decomposition In Cooperative Multi-Agent Qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00587</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrenboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03939v2</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
