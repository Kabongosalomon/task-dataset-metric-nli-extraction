<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Segmentation of LiDAR Sequences: Dataset and Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Loiseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LASTIG</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">IGN</orgName>
								<address>
									<region>ENSG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Landrieu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LASTIG</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">IGN</orgName>
								<address>
									<region>ENSG</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Segmentation of LiDAR Sequences: Dataset and Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LiDAR</term>
					<term>Transformer</term>
					<term>Autonomous Driving</term>
					<term>Real-Time</term>
					<term>On- line Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles. However, most semantic datasets and algorithms used for LiDAR sequence segmentation operate on 360 ? frames, causing an acquisition latency incompatible with real-time applications. To address this issue, we first introduce HelixNet, a 10 billion point dataset with fine-grained labels, timestamps, and sensor rotation information necessary to accurately assess the real-time readiness of segmentation algorithms. Second, we propose Helix4D, a compact and efficient spatiotemporal transformer architecture specifically designed for rotating Li-DAR sequences. Helix4D operates on acquisition slices corresponding to a fraction of a full sensor rotation, significantly reducing the total latency. Helix4D reaches accuracy on par with the best segmentation algorithms on HelixNet and SemanticKITTI with a reduction of over 5? in terms of latency and 50? in model size. The code and data are available at: https://romainloiseau.fr/helixnet Our dataset HelixNet, has several key advantages compared to standard datasets such as SemanticKITTI [3], see <ref type="table">Table 1</ref>. By organizing points with respect to sensor rotation and reporting their precise release times, we can accurately benchmark the real-time readiness of leading state-of-the-art LiDAR sequence segmentation algorithms. Furthermore, the pointwise sensor orientation allows us to split the data into slices of acquisition corresponding to a fraction of the sensor's rotation. These slices can be processed sequentially by our proposed network Helix4D, resulting in a lower acquisition latency and a more realistic scenario for autonomous driving. Based on a spatio-temporal transformer designed explicitly for LiDAR sequences, Helix4D is more than 50 times smaller than the current best semantic segmentation architectures and reaches state-of-the-art performance with significantly reduced latency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to their low acquisition latency and high precision, rotating LiDAR sensors are among the most prevalent sensors for autonomous vehicles <ref type="bibr">[39]</ref>. The acquired sequences of 3D points exhibit a complex structure in which the temporal and spatial dimensions are entangled through the rotation of the sensor around a reference point in motion; see <ref type="figure" target="#fig_0">Figure 1</ref>. However, this structure is often not reflected in the formatting of open-access LiDAR datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">25,</ref><ref type="bibr">29]</ref>, which are discrete sequences of range images, or frames, each corresponding to a 360 ? degree arc around the sensor. Consequently, most LiDAR semantic segmentation methods operate on one or several such frames at the same time, in the image <ref type="bibr" target="#b11">[12]</ref> or point cloud <ref type="bibr">[51,</ref><ref type="bibr">48,</ref><ref type="bibr">43]</ref> format. However, waiting for an entire frame to be acquired introduces an unavoidable latency of more than 100ms on top of the processing time, excluding applications for high-speed or urban driving. In this paper, we address this issue by introducing (i) HelixNet, the largest available LiDAR dataset, and whose fine-grained point information allows for the realistic real-time evaluation of segmentation methods, and (ii) Helix4D, a spatio-temporal transformer designed for the efficient segmentation of LiDAR sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We present an overview of the existing LiDAR datasets related to autonomous driving, and a summary of the recent developments in 3D semantic segmentation.</p><p>Autonomous Driving 3D Datasets. As autonomous driving becomes an increasingly realistic prospect, multiple datasets have been proposed to evaluate the performance of perception algorithms <ref type="bibr">[34,</ref><ref type="bibr" target="#b10">11]</ref>. In addition to cameras, rotating LiDARs have become one of the most prevalent sensors mounted on autonomous vehicles due to their high accuracy, low latency, and steadily decreasing prices <ref type="bibr">[39]</ref>. ApolloScape <ref type="bibr">[23]</ref>, DublinCity <ref type="bibr">[52]</ref>, and TerraMobilita/iQmulus <ref type="bibr">[44]</ref> have been  <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref> 5.2B 43k <ref type="bibr" target="#b18">19</ref> 1 city frame Rellis3D <ref type="bibr">[25]</ref> 1.5B 13k <ref type="bibr" target="#b15">16</ref> 1 city frame KITTI-360 <ref type="bibr">[29]</ref> 1.0B 81k 37 1 city frame A2D2 <ref type="bibr" target="#b16">[17]</ref> 387M 41k 38 3 cities frames Paris-Lille-3D <ref type="bibr">[38]</ref> 143M N/A 50 2 cities multi-frame Toronto3D <ref type="bibr">[42]</ref> 78M N/A 8 1 city multi-frame acquired with LiDAR setups that offer scans of urban environments with high precision and density. However, the vertical orientation of the emitters is not compatible with real-time road perception. Several prominent datasets such as NuScene <ref type="bibr" target="#b5">[6]</ref> or the very large ONCE dataset <ref type="bibr">[32]</ref> provide only object-level annotations (i.e. boxes). This paper focuses on semantic segmentation algorithms for roof-mounted rotating LiDAR sensors. In <ref type="table" target="#tab_0">Table 1</ref>, we report several key characteristics of such datasets [25, <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">38,</ref><ref type="bibr">42]</ref>. Our proposed dataset HelixNet is 70% larger than SemanticKITTI <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref>, and spans 6 cities and various environments. In contrast to previously released datasets, the 3D points of HelixNet are given with respect to the sensor rotation and in the order in which they are made available. This last point proves crucial for evaluating the precision and latency of segmentation algorithms in a setting that is compatible with real-time inference.</p><p>Deep Semantic Segmentation of 3D Point Clouds. The development of specific deep architectures for the semantic segmentation of 3D point clouds has led to a tremendous increase in performance <ref type="bibr">[21]</ref>. The first set of methods that operate on rotating LiDAR sequences processes data in range image format <ref type="bibr">[28,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b11">12]</ref>. Taking advantage of advances in the implementation of sparse convolutions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>, a second set of methods uses fine grids in polar [48], Cartesian <ref type="bibr">[43,</ref><ref type="bibr" target="#b7">8]</ref> or cylindrical [51,22] coordinates. A third kind of approach proposes exploiting the temporal dimension of LiDAR acquisitions by stacking contiguous frames <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref>. Observing that cylindrical partitions better capture the geometry of rotating LiDAR acquisition, our proposed Helix4D builds on the idea of Cylinder3D [51] and adds a temporal component to the architecture.</p><p>Due to their remarkable performance and scalability, transformers [45] have quickly been adapted from text processing to images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">31,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b6">7]</ref>, videos <ref type="bibr" target="#b0">[1]</ref>, or meshes <ref type="bibr">[30]</ref>. Transformers are also well suited to handle unordered sets, such as 3D point clouds <ref type="bibr">[20,</ref><ref type="bibr">49]</ref>. In particular, their scalability can be leveraged to achieve large receptive fields <ref type="bibr">[33,</ref><ref type="bibr">35]</ref> and more discriminative features <ref type="bibr" target="#b4">[5]</ref> Training Set Validation Set than purely convolutional approaches. Transformers can also efficiently process complex temporal <ref type="bibr">[26,</ref><ref type="bibr">46]</ref> and spatio-temporal <ref type="bibr" target="#b14">[15]</ref> sequences. In the wake of hybrid convolution-transformer models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>, our proposed Helix4D combines efficient cylindrical convolutions with a simplified spatio-temporal transformer architecture operating at low resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HelixNet: A Dataset for Online LiDAR Segmentation</head><p>We introduce HelixNet, a new large-scale and open-access LiDAR dataset intended for the evaluation of real-time semantic segmentation algorithms. In contrast to other large-scale datasets, HelixNet includes fine-grained data on sensor rotation and position, as well as point release time.</p><p>General Characteristics. As seen in <ref type="figure">Figure 2</ref>, HelixNet contains 20 sequences of 3D points, each corresponding to 6 to 7 minutes of continuous acquisition, for a total of 129 minutes. Scanning was performed by an HDL-64E Velodyne rotating LiDAR [24] mounted on a mobile platform <ref type="bibr">[36]</ref>. As shown in <ref type="figure">Figure 3</ref>, HelixNet covers multiple cities and a wide variety of environments such as a university campus, dense historical centers, and a highway interchange. With a total of 10 billion points across 78 800 frames and 8.85 billion individual labels, HelixNet is the largest densely annotated open-access rotating LiDAR dataset by a factor of 1.7 as shown in <ref type="table" target="#tab_0">Table 1</ref>. HelixNet follows the file format of SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>, allowing researchers to evaluate existing code with minimal effort. We use a 9-classes nomenclature: road (16.4% of all points), other surface (22.0%), building (31.3%), vegetation (8.5%), traffic signs (1.6%), static vehicle (4.9%), moving vehicle (2.1%), pedestrian (0.9%), and acquisition artifact (0.05%). Points without labels correspond to either un-annotated (6.2%) parts of the clouds due to their ambiguity, or point without echos (6.1%). Compared to fine-grained classes such as the ones used by SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> or Paris-Lille3D's [38], our focused nomenclature limits class imbalance and makes macro-averaged metrics more stable.</p><p>Each point is associated with the 9 following values: (1-3) Cartesian coordinates in a fixed frame of a reference, (4-6) cylindrical coordinate relative to the sensor at the time of acquisition, <ref type="bibr" target="#b6">(7)</ref> intensity, (8) fiber index, and (9) packet output time. As detailed in the next paragraph, the last two features are not typically available in large-scale datasets and cannot be inferred.</p><p>Sensor-Based Timing and Grouping. A rotating LiDAR consists of a set of lasers-or fibers-arranged on a rotating sensor head. The lasers send periodic pulses of light whose return times give the position of the impact points relative to the sensor. In the context of autonomous driving, these sensors are typically deployed on a moving platform and capture 3D points with centimetric accuracy. The sensor releases the data stream as a discrete temporal sequence of packets of 3D points. For an HDL-64E LiDAR, each packet contains 6 ? 64 points, corresponding to around 1 ? rotation of the sensor. To represent the real-time operational setting of autonomous driving, we associate with each point the timestamp of its packet output event, i.e. the instant the packet is available and not the acquisition time of the point. The latency between the acquisition of the first point and the complete transfer of its packet is 278?s. Although small compared to acquisition and inference times, this more rigorous timing constitutes a step towards a more realistic evaluation setting of segmentation algorithms of LiDAR sequences.</p><p>On top of its absolute position, we associate with each individual point its cylindrical coordinates relative to the position of the sensor at the exact time of its acquisition. This differs from other datasets such as SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>, which gives the relative position of all points but the absolute position of the sensor only once per frame. While sensor movement can be interpolated, the vehicle trajectory might not be linear and the sensor head rotates. For comparison, at 50km/h, the sensor moves more than 1.4m during each rotation.</p><p>LiDAR sequences are typically split into frames containing points that cover a 360 ? degree arc around the sensor. However, the acquisition geometry makes this grouping artificial. Indeed, the fibers (i.e. the individual lasers) do not all face the same direction: they are arranged around the sensor's heads at different angles, with a range of more than 17.3 ? . This means that the points within a packet are not vertically aligned but present a jagged profile as seen in <ref type="figure" target="#fig_1">Figure 4</ref>. In order to obtain frames with straight edges such as those of SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>, we would have to consider an acquisition over a sensor rotation of 377 ? , adding a further 5ms of latency. Contrary to other datasets, HelixNet contains the index of the emitter of each point and organizes the points with respect to the angle of the sensor itself This allows us to easily build frames or frame portions that are directly consistent with the rotation of the sensor head itself. This is important for measuring the real latency of segmentation methods and, as described in the next section, contributes to the efficiency of our proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Helix4D: Fast LiDAR Segmentation with Transformers</head><p>We consider a sequence of 3D points acquired by a rotating LiDAR on a mobile platform, which we split into chronologically ordered slices of acquisition. As represented in <ref type="figure" target="#fig_2">Figure 5</ref>, we process each slice with a U-Net architecture [37] with cylindrical convolutions <ref type="bibr">[51]</ref>. At the lowest resolution, a spatio-temporal transformer network connects neighboring voxels in space and time, resulting in a large receptive field. We first describe the construction of slices, then our cylindrical U-Net, and finally the transformer module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Temporal Slicing</head><p>Instead of processing the data frame-by-frame, we propose to split the sequence into slices covering a fixed portion of the sensor rotation, resulting in a shorter acquisition time and a lower latency. Each point i of the sequence is characterized by the angular position ? i of the sensor head at its exact time of acquisition. The points are sorted in chronological acquisition order i.e. ? i ? ? j if i &lt; j. We partition the sequence into groups of contiguous points called slices, acquired during a portion ?? ?]0, 2?] of a full rotation of the sensor itself. Choosing ?? = 2? corresponds to the classic frame-by-frame setting and implies an acquisition latency of 104ms in HelixNet or SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>. A slice size of ?? = 2?/5 leads to an acquisition latency of 21ms, which is more conducive to real-time processing of driving data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cylindrical U-Net</head><p>Inspired by the Cylinder3D model [51], we first discretize each slice along a fine cylindrical partition grid (1). Each point i is associated with a descriptor x point i based on its intensity, relative position with respect to the sensor in Cartesian and cylindrical coordinates, and its offset with respect to the center of its voxels in grid (1). We compute the point feature f point i by applying a shared Multi-Layer Perceptron (MLP) E point to x point i for all points i in the slice. The resulting f point i are then maxpooled with respect to the voxels of grid (1) to serve as input to a convolutional encoder E grid . The network E grid is composed of sparse cylindrical convolutions <ref type="bibr" target="#b17">[18]</ref> and strided convolutions for downsampling. E grid produces a set of L sparse feature maps f grid(1) , ? ? ? , f grid(L) with decreasing resolutions:</p><formula xml:id="formula_0">f point i = E point x point i (1) f grid(1) , ? ? ? ,f grid(L) = E grid maxpool f point ,<label>(2)</label></formula><p>where maxpool is performed with respect to grid (1). At the lowest resolution grid (L), we apply the transformer-based module T presented in the next subsection to the feature map f grid(L) to obtain the coarse cylindrical map g grid(L) : The decoder D grid combines cylindrical convolutions and strided transposed convolutions to map g grid(L) to a feature map g grid(1) at the highest resolution, and uses the maps f grid(L?1) , ? ? ? , f grid(1) through residual skip connections. We concatenate for each point i the descriptor g grid(1) (i) of its voxel in grid (1) and its point feature f point i . Finally, the point decoder D point associates a vector of class scores c point i with each point i:</p><formula xml:id="formula_1">g grid(L) = T f grid(L) .<label>(3)</label></formula><formula xml:id="formula_2">g grid(1) = D grid g grid(L) , f grid(L?1) , ? ? ? , f grid(1) (4) c point i = D point g grid(1) (i), f point i ,<label>(5)</label></formula><p>where [ ? ] is the channelwise concatenation operator. The network is supervised by the cross-entropy and Lov?sz-softmax <ref type="bibr" target="#b3">[4]</ref> losses directly on the point prediction, without class weights. Our approach differs from Cylinder3D [51] by relying on simple 3?3?3 sparse cylindrical convolutions instead of asymmetrical convolutions and dimensionbased context modeling. Furthermore, we do not use voxel-wise supervision.</p><p>Our simplified architecture results in a lighter computational and memory load, but can still learn rich spatio-temporal features thanks to the addition of the transformer module described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Spatio-Temporal Transformer</head><p>We denote by V the set of non-empty voxels at the lowest resolution grid (L) for all slices of the considered sequence. We associate with each voxel v of V a feature f voxel v defined as the value of f grid(L) at v. We remark that f voxel can be ordered as a non-strictly ordered time sequence, and propose to successively apply W independent transformer blocks T 1 , ? ? ? , T W whose architecture is described below. We denote by g voxel the resulting spatio-temporal voxel representation:</p><formula xml:id="formula_3">g voxel = T W ? ? ? ? ? T 1 (f voxel ) .<label>(6)</label></formula><p>We associate each voxel v of V with the absolute position (X v , Y v , Z v ) of its center, the release time T v of its first point, and the index I v of the sensor rotation of its corresponding slice. In order to use a sparse attention scheme, we define for each voxel v a spatio-temporal mask M (v) characterized by a radius R and a set of rotation offsets P ? N:</p><formula xml:id="formula_4">M (v) = {u | ?(X v , Y v , Z v ) ? (X u , Y u , Z u )? &lt; R , I v ? I u ? P } .<label>(7)</label></formula><p>In the context of autonomous driving, we choose R = 6m and P = {0, 5, 10}. With a standard rotation speed of 10Hz, this corresponds to considering slices 0.5 and 1 seconds in the past along with the current one. See <ref type="figure">Figure 6</ref> for an illustration of the receptive field and attention maps.</p><p>Simplified Transformer Block. We now define a single transformer block T w with H heads operating on a sequence of voxel features f voxel of dimension D. For each head h and each voxel v, we apply the following operations: These operations can be summarized as follows:</p><formula xml:id="formula_5">k h v , val h v = L h f voxel v (8) y h u,v = k h v ? k h u + PE h (u, v) for u ? M (v) (9) a h u,v u?M (v) = softmax y h u,v u?M (v) / ? K (10) f h v = u?M (v) a h u,v val h u (11) T w (f voxel ) v = f voxel v + [f 1 v , ? ? ? ,f H v ] .<label>(12)</label></formula><p>Our design is similar to the classical transformer architecture but uses keys as queries to save memory and computation. We also do not use feed-forward</p><formula xml:id="formula_6">?1 sec. ?0.5 sec. 0 sec. ?1 sec. ?0.5 sec. 0 sec. spatio-temporal mask M (v)</formula><p>voxel v sensor position cross-voxel attention <ref type="figure">Fig. 6</ref>: Spatio-Temporal Attention. We represent the spatio-temporal mask and attention score of one head of the transformer for two different voxels. The network gathers information from different frame offsets P as the sensor moves.</p><p>networks after averaging the values: the only learnable part of a block T w is its linear layers L h and its relative positional encoding PE h .</p><p>Since g voxel only requires information about the voxels of the current and past slices, it can be computed sequentially for all slices in the order in which the sensor releases them. For a given slice, the voxel map g grid(L) for non-empty voxels is given by the values of g voxel , and set to zero otherwise. To save computation at inference time, we store in memory the keys, values, and absolute positions of the voxels in past slices with a fixed buffer of max(P ) rotations. This allows us to allocate a large spatio-temporal receptive field to each voxel without supplementary computations.</p><p>Relative Positional Encoding. We propose to learn relative positional vectors</p><formula xml:id="formula_7">PE h (u, v) that encode the spatio-temporal offset (X u , Y u , Z u , T u )?(X v , Y v , Z v , T v )</formula><p>between voxels u and v for each transformer block w independently. Inspired by the work of Wu et al .</p><p>[47], we first discretize the offsets along each dimension d ? {X, Y, Z, T } with B d irregular bins. For each dimension d and head h, we learn B d weight vectors of size K. We define the functions PE h d : R ? R K that map the d-dimension of an offset to the vector associated with its corresponding bin. The positional encoding between two voxels u and v is the sum of the vectors corresponding to their discretized offsets in each dimension:</p><formula xml:id="formula_8">PE h (u, v) = PE h X (X u ? X v ) + PE h Y (Y u ? Y v ) + PE h Z (Z u ? Z v ) + PE h T (T u ? T v ) .<label>(13)</label></formula><p>Relative positional encoding vectors are used directly in the calculation of the compatibility score, as given in <ref type="bibr" target="#b8">(9)</ref>. Additional details on positional encoding are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluating Online Semantic Segmentation</head><p>We evaluate the performance and inference time of our approach and other state-of-the-art methods in both online and frame-by-frame settings. We use our proposed dataset HelixNet and the standard SemanticKITTI dataset.</p><p>Online Evaluation Setting We aim at evaluating the real-time readiness of rotating LiDAR semantic segmentation algorithms in the context of autonomous driving. The total latency of a model is determined by its inference speed and also the time it takes to acquire its input. Operating on full frames requires at least 104ms of acquisition, which is incompatible with realistic autonomous driving scenarios. Instead, we propose an online evaluation setting using the slices defined in Sec. 4.1. By default, we use a slice size of a fifth turn of the sensor head: ?? = 2?/5, corresponding to 21ms of acquisition. Slices are processed sequentially. We define the inference latency of a segmentation method as the average time between the release of the last point of a slice and its segmentation. To meet the real-time requirement, inference must be faster than the acquisition of a slice. Slower processing would cause the classification to continuously fall behind. Although thinner slices directly reduce acquisition latency, they also make the real-time requirement more strict: as a full turn must be processed in less than 104ms, a fifth turn must be in at most 21ms.</p><p>Adapting SemanticKITTI. SemanticKITTI <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> contains 43 552 frames along 22 sequences of LiDAR scans densely annotated with 19 classes. In contrast to HelixNet, SemanticKITTI is not formatted with respect to the sensor rotation and only gives the acquisition time and sensor position once per frame. To measure the latency, we make the following approximation: (i) the fibers are assumed to be vertically aligned, meaning that the angle of the points is the same as the sensor's; (ii) we interpolate the acquisition time of points between frames from their angular positions; (iii) we use the acquisition time as release time. To obtain the absolute positions of the voxels, we assume that the sensor jumps between the positions given by the camera poses for each frame. In our open-source implementation, we provide an adapted dataloader allowing methods already running on SemanticKITTI to be evaluated in the online setting with minimal adaptation.</p><p>Adapting Competing Methods. To evaluate the semantic segmentation performance and latency of other segmentation algorithms in the online setting, we process the point clouds corresponding to each slice independently and sequentially. This approach restricts the spatial receptive field to the extent of the slices. However, as the sensor moves, it is not straightforward to add past slices whose relative positions may no longer be valid. By explicitly modeling the spatiotemporal offset between voxels, Helix4D does not suffer from this limitation.</p><p>We selected five segmentation algorithms with open-source implementations and trained models for SemanticKITTI. SalsaNeXt <ref type="bibr" target="#b11">[12]</ref> uses range images, Polar-Net [48] and panoptic PolarNet [50] a bird's eye view polar grid, SPVNAS [43] a regular grid, and Cylinder3D [51] a cylindrical grid. We do not consider methods that stack frames as their structure and resulting latency is incompatible with the online setting. When using SemanticKITTI, we evaluate the provided pretrained models on the validation set. On HelixNet, we retrain the models from scratch <ref type="table">Table 2</ref>: Semantic Segmentation Results. Performance of Helix4D and competing approaches on HelixNet and on the validation set of SemanticKITTI ? , in the frame-byframe and online setting. We report the mean Intersection-over-Union (mIoU) and the inference time in ms. Methods meeting the real-time requirement are indicated with ?and those who do not with ?. ? SemanticKITTI is denoted as SK. Measuring the latency on this dataset requires making non-realistic approximations about the fiber position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Size Full frame 104ms  <ref type="table">Table 3</ref>: HelixNet Semantic Segmentation Scores. We report the IoU for each class of HelixNet evaluated in the online setting with slices of 72 ? . using the procedure of their official repository. We removed all test-time augmentations that resulted in prohibitive inference time. All methods are evaluated on the same workstation using a NVIDIA TESLA V100 32Go GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method R o a d O t h e r s u r f a c e B u i l d i n g V e g e t a t i o n T r a ffi c s i g n s S t a t i c v e h i c l e M o v i n g v e h i c l e P e d e s t r i a n A r t i f a c t Avg</head><p>Analysis. In <ref type="table">Table 2</ref>, we report performance in frame-by-frame and online setting with slices of 72 ? , for Helix4D and competing methods, for HelixNet and SemanticKITTI. We observe that Helix4D yields state-of-the-art accuracy, with mIoU scores only matched by Cylinder3D [51]. However, Cylinder3D is 50 times larger in terms of parameters and twice slower, not meeting the real-time requirement even in the full frame setting. As reported in <ref type="table">Table 3</ref>, distinguishing moving vehicles in HelixNet is particularly difficult. Our approach even largely outperforms Panoptic PolarNet despite this method using instance annotation as supervision, preventing us from evaluating on HelixNet. Helix4D yields significantly improved scores thanks to its larger spatio-temporal receptive fields: 14m and 1000ms vs. 8m and 21ms for Cylinder3D for a fifth rotation.  <ref type="figure">Fig. 7</ref>: Influence of Slice Size. We plot the processing time (left, in ms) and precision (right, in mIoU) of different methods with respect to the considered size of slices, estimated on the validation set of SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>. Methods whose inference time is slower than the acquisition time of the slice (red shaded area) do not meet the real time requirement. In the online setting, only two approaches meet the real-time requirement: SalsaNeXt <ref type="bibr" target="#b11">[12]</ref> and Helix4D. Our approach outperforms SalsaNeXt by over 10 mIoU points in both the full frame and the on-line settings. In short, Helix4D is as accurate as the largest and slowest models with an inference speed comparable to that of the fastest and less accurate models. The total latency (acquisition plus inference time) of our model evaluated online is 40ms (21 + 19ms), and reaches the same performance as Cylinder3D evaluated on full frame with a latency of 212ms (104 + 108ms), an acceleration of more than 5 folds.</p><p>In <ref type="figure">Figure 7</ref>, we report the inference time and mIoU for different slice sizes. Due to various overheads, the inference time appears in an affine relationship with the size of slices, making the real-time requirement stricter for smaller slices. Due to its very design, the performance of Helix4D is not affected by the slice size. In contrast, competing methods perform worse with smaller slices.</p><p>Ablation Study We assess on SemanticKITTI the impact of different design choices by evaluating several alterations of our method, reported in <ref type="table" target="#tab_5">Table 4</ref>. (a) Asymmetric Convolutions: we replace the 3 ? 3 ? 3 convolutions in our U-Net with the convolution design proposed by Cylinder3D [51]. We did not observe a significant change in performance and an increase in run-time of 50%, failing the real-time requirement for slices of 72 ? . (b) Cylindrical U-Net: we replace the transformer by a 1 ? 1 ? 1 convolution on the voxels of the lowest resolution. We observe a slight decrease in run-time and a significant drop of over 6 mIoU points. This result shows that the transformer is able to learn meaningful spatio-temporal features at low resolution. (c) Slice-by-Slice: we restrict the mask M (v) of each voxel to its current slice. This reduction in the temporal receptive field results in a drop of 4 mIoU points, without any appreciable acceleration. (d) w. Queries: we modify our simplified transformer to associate a query for each voxel along with keys and values, and use key-queries compatibilities. This does not affect the run-time and slightly decreases the performance. (e) w/o. Positional Encoding: we remove the relative positional encoding PE in the calculation of compatibilities in equation <ref type="bibr" target="#b8">(9)</ref>. This leads to a slightly decreased run time, but decreases performance by more than 2.5 points. This illustrates the advantage of explicitly modeling the spatio-temporal voxel offsets. (f ) Helix4D Tiny: we replace the learned pooling in our U-Net with maxpools and use narrower feature maps for a total of 306k parameters. This method only performs two points under Helix4D with a third of its parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced a novel online inference setting for the semantic segmentation of sequences of rotating liDAR 3D point clouds. Our proposed large-scale dataset HelixNet contains specific sensor information that allows a rigorous evaluation of the performance and latency of segmentation methods in our online setting. We also introduced Helix4D, a transformer-based network specifically designed for online segmentation, achieving state-of-the-art results with a fraction of the latency and parameters of competing methods. We hope that our open-source dataset and implementation will encourage the evaluation of future semantic liDAR segmentation methods in more realistic settings and help to bridge the gap between academic work on 3D perception and the operational constraints of autonomous driving. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Supplementary Material</head><p>In this supplementary material, we provide the exact implementation of He-lix4D (Sec. 7.1), details about HelixNet (Sec. 7.2), and a discussion about the evaluation of online semantic segmentation (Sec. 7.3). Code, data and interactive visualizations are available at: https://romainloiseau.fr/helixnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Helix4D Implementation Details</head><p>Architecture. We report here the detailed architecture of the point and voxels encoders and decoders E point , E grid , D grid , D point in <ref type="figure">Figure 8</ref>. We use L = 3 levels of cylindrical grids with decreasing resolution (? l r, ? l ?, ? l z). Our goal is to sufficiently decrease the resolution such that the number of nonempty voxels in grid (L) remains manageable by the spatio-temporal transformer. We chose the following resolutions: grid (1) with ? 1 r = 20cm, ? 1 ? = 1.33 ? , ? 1 z = 16.6cm, grid (2) is of ? 1 r = 60cm, ? 1 ? = 4 ? , ? 1 z = 33.3cm, and grid (3) is of ? 1 r = 180cm, ? 1 ? = 8 ? , ? 1 z = 66.6cm. This configuration corresponds to an average of around 2500 non empty voxels per rotations at the lowest resolution grid (3).</p><p>E point and D point are simple MLPs operating directly on point features, and applied in parallel to all points of a slice. As shown in <ref type="figure">Figure 8</ref>, E grid has two levels operating at different resolution in a U-Net fashion. The first level operates at the resolution of grid (1) and is composed of two 3 ? 3 ? 3 convolutions and a strided convolution with kernel size <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref> and stride <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>, yielding a map of resolution grid (2). The second level operates at the level of grid (2) and is composed of two 3 ? 3 ? 3 convolutions and a strided convolution with kernel size <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2]</ref> and stride <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2]</ref>, yielding a map of resolution grid (3).</p><p>The grid decoder has a symmetric architecture and uses strided transposed convolution to increase the resolution of its feature maps. The intermediate maps of E grid are simply added to the upsampled maps of D grid at the corresponding resolution to form residual skip-connections. </p><formula xml:id="formula_9">(? ? ?) if |x| &gt; ?? d ,<label>(14)</label></formula><p>where ? ? ? denotes the rounding operation, ? = 2, ? = 3, and ? = 1.25, and ? X = ? Y = 1.5m, ? Z = 0.5m, and ? T = 5 ? 104ms. See <ref type="figure">Figure 9</ref> for a visual representation of bins. We represent in <ref type="figure" target="#fig_0">Figure 10</ref> the compatibility? h,d (b) between keys and relative positional encodings within each bin b, head h and dimension d ? {X, Y, Z, T } averaged over 3 rotations:</p><formula xml:id="formula_10">y h,d (b) = 1 Card (V d (b)) (v,u)?V d (b) k h v ? PE h (u, v) ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_11">V d (b) = {(v, u) ? V | bin(d u ?d v ) = b}.</formula><p>We observe differentiated behaviors between the heads: Head 1 focuses on near voxels, Head 2 and Head 3 focus on the area above/below the voxel, respectively, and Head 4 focuses on the current frame.</p><p>Training. We use the ADAM [27] optimizer with a weight decay of 0.01, except for the positional encoding parameters which are learned without weight decay. We use a learning rate of 0.001 and decrease it to 0.0001 when the validation mIoU does not increase for consecutive 3 epochs. The compatibility is the cross product between voxel keys and the positional encoding, averaged over 3 rotations of HelixNet (see <ref type="formula" target="#formula_2">(15)</ref>).</p><p>Data Augmentation. During training, we use augmentation to improve the generalisation capabilities of the network: flipping of the xy 2D plane, rotation around the z axis, anisotropic scaling generated by a uniform distribution U[0.95, 1.05], and translation generated by a random normal N (0, 0.2m). Since our goal is to evaluate real-time semantic segmentation, we do not use test-time augmentation schemes, even though it could possibly improve the overall performance.</p><p>Real-Time Requirements. To better represent the limitation of embarked hardware, we ran Helix4D on a NVIDIA-2080Ti and observed only a 1ms increase in run time. In particular, Helix4D can still run in real time for slices of 72 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Details on HelixNet</head><p>We report the localization of the sequences of HelixNet in detail in <ref type="figure" target="#fig_0">Figure 11</ref>. The different classes used in HelixNet are defined as followed:</p><p>-Road: refers to the main driving lanes, not just any impervious surfaces. For example, parking spots and pedestrian alleys are excluded. -Other Surface: surface other than the road. Includes parks, grass, construction sites, parking spots, pedestrian plazas, and so on. -Buildings: comprise housings and monuments, but also urban hardscape such as bus stops, posts and poles, street fences, benches, street lights, and so on.  -Moving Vehicles: refers to any vehicle (car, truck, bike, motorbike, construction engine) with detectable movement, assessed with a 15s sliding time window. -Pedestrians: person travelling on foot.</p><p>-Artifact: acquisition artifacts, does not correspond to a physical impact point. This can be due to reflective surfaces or errors in the vehicle pose estimation among other reasons. -Unannotated: the human annotators could not confidently associate a label to some points due to low density or an ambiguous configuration. This is not a predicted class, and is not taken into account in the metrics. -No-Echo: some pulses never receive an echo, for example, when the impact points are too far away from the sensor. These points are affected r = 0, and are not taken into account in the computation of metrics. Keeping these points allows us to easily convert the point cloud sequence into a sequence of range images by simply reshaping the corresponding tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Evaluating Online Semantic Segmentation</head><p>Competing methods.  <ref type="figure" target="#fig_0">Figure 12</ref> for a representation of each method's partition.</p><p>Detailed results. In the main paper, we showed that Helix4D yields state-ofthe-art performance on HelixNet. <ref type="table">Table 5</ref> shows the corresponding confusion matrix, which highlights three main sources of error in HelixNet. First, roads and other-surfaces are not necessarily easy to distinguish. Second, the model tends to misclassify traffic-signs, pedestrians and vegetation as buildings. As urban hardscape such as post and poles, street fences, and public benchs being annotated as buildings, traffic-signs can be hard to identify. Pedestrians can be inside of buildings or structures, making them hard to distinguish, see <ref type="figure" target="#fig_0">Figure 13</ref>. Third, the distinction between static and mobile cars can be difficult for vehicle stopped in the driving lanes. This requires both understanding the road configuration and the dynamic of the scene. See the attached HTML file for an interactive representation of some of these errors.</p><p>In <ref type="table" target="#tab_8">Table 6</ref>, we report the quantitative values used in <ref type="figure">Figure 7</ref> of the main paper, and include a comparison with the official performance of the trained models as reported by the papers they were introduced in.  <ref type="table">Table 5</ref>: Helix4D Semantic Segmentation of HelixNet. We report the confusion matrix of the predictions of Helix4D on the test set of HelixNet, evaluated in the online setting with slices of 72 ? . <ref type="figure" target="#fig_0">Fig. 13</ref>: First frame from sequence 15 of HelixNet. In this frame from the test set, we observe difficult situations with pedestrians under a urban infrastructure surrounded by vegetation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Online LiDAR Segmentation. The 3D point sequences of rotating LiDAR data of our proposed dataset HelixNet follow a complex helix-like structure in space and time, represented in a by using the vertical axis for both time and elevation. We propose an efficient spatio-temporal transformer to process angular slices of data centered on the sensor's position. The slices are partitioned into voxels, each attending other voxels from past slices to build a large spatio-temporal receptive field b. Our proposed model can segment the LiDAR point stream c with state-of-the-art accuracy and in real-time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Rotation of the sensor head (b) Slices covering 120 ? Sensor Acquisition Geometry. We represent in a the acquisition of a rotating sensor, which is split into 1 ?3 turn slices in b. As the Laser emitters position forms an angle of over 17.3 ? around the sensor head, taking slices with respect to the sensor rotation ? results in a jagged profile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Helix4D Architecture. A point sequence is split into angular slices, whose points are encoded by E point and pooled along a fine-grained cylindrical partition. A convolutional encoder E grid yields feature maps at lower resolutions. We apply W consecutive spatio-temporal transformer blocks T w on the coarse voxels, with attention spanning across current and past slices. The resulting features are up-sampled to full resolution with a convolutional decoder D grid using the encoder's maps at intermediate resolutions through skip connections. Finally, the grid features are allocated to the points, which are classified by D point .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(i) A single linear layer L h generates both a key k h v of dimension K and a value val h v of dimension D/H. (ii) For all voxels u in the mask M (v), we define the compatibility score y h u,v as the cross-product between keys and with a learned relative positional encoding PE h (u, v). (iii) The cross-voxel attention a h u,v is obtained with a scaled softmax. (iv) The values val h u of voxels in M (v) are averaged into a vectorf h v using their respective cross-voxel attention as weights. (v) The vectorsf h v are concatenated channelwise across heads and added to the input of the block to define its output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>time &lt; acquisition time inference time &gt; acquisition time SPVNAS [43] (10.8M) SalsaNext [12] (6.7M) Cylinder3D [51] (55.9M) PolarNet [48] (13.6M) Panoptic-Polarnet [50] (13.7M) Helix4D (Ours) (1.0M)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>20. Guo, M.H., Cai, J.X., Liu, Z.N., Mu, T.J., Martin, R.R., Hu, S.M.: PCT: Point cloud transformer. Computational Visual Media (2021) 21. Guo, Y., Wang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M.: Deep learning for 3D point clouds: A survey. Transactions on pattern analysis and machine intelligence (2020) 22. Hong, F., Zhou, H., Zhu, X., Li, H., Liu, Z.: LiDAR-based panoptic segmentation via dynamic shifting network. CVPR (2021) 23. Huang, X., Cheng, X., Geng, Q., Cao, B., Zhou, D., Wang, P., Lin, Y., Yang, R.:The Apolloscape dataset for autonomous driving.CVPR Workshop (2018) 24. Inc., V.L.: HDL-64E User's Manual. Velodyne LiDAR Inc. 345 Digital Drive, Morgan Hill, CA 95037 (2008) 25. Jiang, P., Osteen, P., Wigness, M., Saripalli, S.: Rellis-3D dataset: Data, benchmarks and analysis. ICRA (2021) 26. Katharopoulos, A., Vyas, A., Pappas, N., Fleuret, F.: Transformers are RNNs: Fast autoregressive transformers with linear attention. ICML (2020) 27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 28. Liang, Z., Zhang, Z., Zhang, M., Zhao, X., Pu, S.: Rangeioudet: Range image based real-time 3D object detector optimized by intersection over union. CVPR (2021) 29. Liao, Y., Xie, J., Geiger, A.: KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2D and 3D. arXiv preprint arXiv:2109.13410 (2021) 30. Lin, K., Wang, L., Liu, Z.: End-to-end human pose and mesh reconstruction with transformers. CVPR (2021) 31. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. ICCV (2021) 32. Mao, J., Niu, M., Jiang, C., Liang, X., Li, Y., Ye, C., Zhang, W., Li, Z., Yu, J., Xu, C., et al.: One million scenes for autonomous driving: Once dataset. arXiv preprint arXiv:2106.11037 (2021) 33. Mao, J., Xue, Y., Niu, M., Bai, H., Feng, J., Liang, X., Xu, H., Xu, C.: Voxel transformer for 3D object detection. ICCV (2021) 34. Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P.: The mapillary vistas dataset for semantic understanding of street scenes. CVPR (2017) 35. Pan, X., Xia, Z., Song, S., Li, L.E., Huang, G.: 3D object detection with pointformer. CVPR (2021) 36. Paparoditis, N., Papelard, J.P., Cannelle, B., Devaux, A., Soheilian, B., David, N., Houzay, E.: Stereopolis ii: A multi-purpose and multi-sensor 3d mobile mapping system for street visualisation and 3d metrology. Revue fran?aise de photogramm?trie et de t?l?d?tection (2012) 37. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. MICCAI (2015) 38. Roynard, X., Deschaud, J.E., Goulette, F.: Paris-Lille-3D: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification. The International Journal of Robotics Research (2018) 39. Royo, S., Ballesta-Garcia, M.: An overview of LiDAR imaging systems for autonomous vehicles. Applied Sciences (2019) 40. Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for semantic segmentation. ICCV (2021) 41. Sun, P., Wang, W., Chai, Y., Elsayed, G., Bewley, A., Zhang, X., Sminchisescu, C., Anguelov, D.: Rsn: Range sparse net for efficient, accurate LiDAR 3D object detection. CVPR (2021) 42. Tan, W., Qin, N., Ma, L., Li, Y., Du, J., Cai, G., Yang, K., Li, J.: Toronto-3D: A large-scale mobile LiDAR dataset for semantic segmentation of urban roadways. CVPR Workshop (2020) 43. Tang, H., Liu, Z., Zhao, S., Lin, Y., Lin, J., Wang, H., Han, S.: Searching efficient 3D architectures with sparse point-voxel convolution. ECCV (2020) 44. Vallet, B., Br?dif, M., Serna, A., Marcotegui, B., Paparoditis, N.: Terramobilita/iqmulus urban point cloud analysis benchmark. Computers &amp; Graphics (2015) 45. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. NeurIPS (2017) 46. Vyas, A., Katharopoulos, A., Fleuret, F.: Fast transformers with clustered attention. NeurIPS (2020) 47. Wu, K., Peng, H., Chen, M., Fu, J., Chao, H.: Rethinking and improving relative position encoding for vision transformer. ICCV (2021) 48. Zhang, Y., Zhou, Z., David, P., Yue, X., Xi, Z., Gong, B., Foroosh, H.: Polarnet: An improved grid representation for online LiDAR point clouds semantic segmentation. CVPR (2020) 49. Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. CVPR (2021) 50. Zhou, Z., Zhang, Y., Foroosh, H.: Panoptic-polarnet: Proposal-free LiDAR point cloud panoptic segmentation. In: CVPR (2021) 51. Zhu, X., Zhou, H., Wang, T., Hong, F., Ma, Y., Li, W., Li, H., Lin, D.: Cylindrical and asymmetrical 3D convolution networks for LiDAR segmentation. CVPR (2021) 52. Zolanvari, S., Ruano, S., Rana, A., Cummins, A., da Silva, R.E., Rahbar, M., Smolic, A.: DublinCity: Annotated LiDAR point cloud and its applications. BMVC (2019)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Helix4D Detailled Architecture. Abstract version of the architecture showing all layers in E point , E grid , D grid and D point given for reproductibility. Relative Positional Encoding Bins. Each of the four dimension are split into irregular bins inspired by Wu et al . [47]. In our spatio-temporal transformer T , we use W = 2 successive transformer blocks, H = 4 heads, an input size of D = 128, and a key size of K = 8. The masks are defined by the rotation offset P = {0, 5, 10} and a radius of R = 6m. Relative Positional Encoding. We use B X = B Y = B Z = 7 and B T = Card(P ) = 3 irregular bins for the relative positional encoding. For an offset x along dimension d ? {X, Y, Z, T }, the corresponding bin for the pair of voxels u, v is given by the following piecewise function [47, Eq 18]: bin d (x) = ?x/? d ? if |x| ? ?? d sign(x) ? min ?, ? + ln(|x|/(?? d )) ln(?/?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Positional Encoding. We plot the average compatibility with respect to the offset learned for the second encoder block of Helix4D within different dimensional bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>-Vegetation: trees, shrubs, and bushes. -Road Signs: refers exclusively to hardscapes that present driving-related information: traffic signs, traffic lights, etc. -Static Vehicles: these vehicles can be parked, or simply stopped in traffic or at an intersection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Localization of the Sequences. HelixNet's data were acquired in 6 different cities with 4 of them in the Paris agglomeration, spanning a large variety of landscapes and urban configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>e r s u r f a c e B u i l d i n g V e g e t a t i o n T r a ffi c s i g n s S t a t i c v e h i c l e M o v i n g v e h i c l e P e d e s t r i a n A r t i f a c t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Embarked LiDAR Datasets with Semantic Point Annotations. With over 8.8B annotated 3D points, HelixNet is 70% larger than SemanticKITTI, and includes more diverse scenes spanning 6 different French cities. Contrary to other datasets, HelixNet arranges points with respect to the sensor rotation and contains fine-grained information about their release time.</figDesc><table><row><cell>Dataset</cell><cell>labels</cell><cell>frames</cell><cell>classes</cell><cell>span</cell><cell>format</cell></row><row><cell>HelixNet (Ours)</cell><cell>8.85B</cell><cell>78k</cell><cell>9</cell><cell>6 cities</cell><cell>sensor rotation</cell></row><row><cell>SemanticKITTI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study. We report the speed and accuracy of several modification of our Helix4D on the validation set of SemanticKITTI.</figDesc><table><row><cell>Method</cell><cell cols="2">Size Full Frame ?10 3 mIoU</cell><cell cols="2">104ms 1 ?5 Frame Inf. (ms) mIoU</cell><cell>21ms Inf. (ms)</cell></row><row><cell>Helix4D</cell><cell>985</cell><cell>66.7</cell><cell>45 ?</cell><cell>66.8</cell><cell>19 ?</cell></row><row><cell cols="2">(a) Asymmetric Convolutions 1171</cell><cell>66.6</cell><cell>56 ?</cell><cell>66.6</cell><cell>31 ?</cell></row><row><cell>(b) Cylindrical U-Net</cell><cell>985</cell><cell>58.6</cell><cell>22 ?</cell><cell>60.2</cell><cell>16 ?</cell></row><row><cell>(c) Slice-by-Slice</cell><cell>985</cell><cell>62.9</cell><cell>29 ?</cell><cell>62.6</cell><cell>19 ?</cell></row><row><cell>(d) w. Queries</cell><cell>993</cell><cell>65.2</cell><cell>45 ?</cell><cell>64.8</cell><cell>20 ?</cell></row><row><cell cols="2">(e) w/o. Positional Encoding 983</cell><cell>64.3</cell><cell>41 ?</cell><cell>64.1</cell><cell>18 ?</cell></row><row><cell>(f ) Helix4D Tiny</cell><cell>306</cell><cell>65.3</cell><cell>45 ?</cell><cell>64.9</cell><cell>17 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>We chose four segmentation algorithms with open-source implementations and trained models for SemanticKITTI. SalsaNeXt [12] uses range images, PolarNet [48] a bird's-eye view polar partition, SPVNAS [43] a regular grid, and Cylinder3D [51] cylindrical partition. See</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Data Partitioning Schemes. We represent commonly used partitioning: a SalsaNeXt [12] uses range images, b PolarNet [48] uses a bird-eye-view polar partition, c SPVNAS [43] uses a classic regular 3D grid, d Cylinder3D [51] uses a cylinder grid. Grids not to scale.</figDesc><table><row><cell></cell><cell>(a) Range image [12]</cell><cell></cell></row><row><cell>(b) 2D polar grid [48]</cell><cell>(c) 3D Euclidean grid [43]</cell><cell>(d) 3D cylinder grid [51]</cell></row><row><cell>Fig. 12:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Semantic KITTI Validation Set Results. "Ours" stands for results as computed on our workstation using a NVIDIA TESLA V100 32Go GPU.</figDesc><table><row><cell cols="7">Source for performance &amp; model weights time (ms.) min mean max std Implementation % scan Inference time (ms.)</cell><cell>mIoU</cell></row><row><cell>Cylinder3D [51]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">55.9 ? 10 6 parameters</cell></row><row><cell>[51]</cell><cell>-</cell><cell>100 (104)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.9</cell></row><row><cell>Ours</cell><cell cols="2">Official, pretrained 100 (104)</cell><cell>81</cell><cell>108</cell><cell>133</cell><cell>12</cell><cell>66.9</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>50 (52)</cell><cell>56</cell><cell>74</cell><cell>94</cell><cell>7</cell><cell>66.2</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>33 (35)</cell><cell>47</cell><cell>63</cell><cell>78</cell><cell>6</cell><cell>65.8</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>25 (26)</cell><cell>46</cell><cell>57</cell><cell>71</cell><cell>5</cell><cell>65.7</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>20 (21)</cell><cell>45</cell><cell>54</cell><cell>67</cell><cell>4</cell><cell>65.3</cell></row><row><cell>PolarNet [48]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">13.6 ? 10 6 parameters</cell></row><row><cell>[48]</cell><cell>-</cell><cell>100 (104)</cell><cell>-</cell><cell>94</cell><cell>-</cell><cell>-</cell><cell>53.6</cell></row><row><cell>Ours</cell><cell cols="2">Official, pretrained 100 (104)</cell><cell>37</cell><cell>49</cell><cell>51</cell><cell>4</cell><cell>58.2</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>50 (52)</cell><cell>32</cell><cell>40</cell><cell>47</cell><cell>4</cell><cell>57.8</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>33 (35)</cell><cell>31</cell><cell>41</cell><cell>47</cell><cell>3</cell><cell>57.1</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>25 (26)</cell><cell>29</cell><cell>38</cell><cell>45</cell><cell>4</cell><cell>57.5</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>20 (21)</cell><cell>29</cell><cell>36</cell><cell>49</cell><cell>5</cell><cell>56.9</cell></row><row><cell>SPVNAS [43]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">10.8 ? 10 6 parameters</cell></row><row><cell>[43]</cell><cell>-</cell><cell>100 (104)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.7</cell></row><row><cell>Ours</cell><cell cols="2">Official, pretrained 100 (104)</cell><cell>52</cell><cell>73</cell><cell>140</cell><cell>11</cell><cell>64.7</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>50 (52)</cell><cell>44</cell><cell>52</cell><cell>100</cell><cell>4</cell><cell>62.5</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>33 (35)</cell><cell>43</cell><cell>47</cell><cell>86</cell><cell>3</cell><cell>60.8</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>25 (26)</cell><cell>41</cell><cell>45</cell><cell>70</cell><cell>3</cell><cell>59.0</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>20 (21)</cell><cell>40</cell><cell>44</cell><cell>68</cell><cell></cell><cell>57.8</cell></row><row><cell>SalsaNeXt [12]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">6.7 ? 10 6 parameters</cell></row><row><cell>Ours</cell><cell cols="2">Official, pretrained 100 (104)</cell><cell>22</cell><cell>23</cell><cell>23</cell><cell>0.1</cell><cell>55.8</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>50 (52)</cell><cell>12</cell><cell>13</cell><cell>16</cell><cell>0.2</cell><cell>55.8</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>33 (35)</cell><cell>10</cell><cell>12</cell><cell>15</cell><cell>0.2</cell><cell>55.7</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>25 (26)</cell><cell>9</cell><cell>10</cell><cell>13</cell><cell>0.2</cell><cell>55.6</cell></row><row><cell>Ours</cell><cell>Official, pretrained</cell><cell>20 (21)</cell><cell>9</cell><cell>9.5</cell><cell>12</cell><cell>0.2</cell><cell>55.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by ANR project READY3D ANR-19-CE23-0007 and was granted access to the HPC resources of IDRIS under the allocation 2022-AD011012096R1 made by GENCI. The point cloud sequences of HelixNet were acquired during the Stereopolis II project [36]. HelixNet was annotated by FUTURMAP. We thank Zenodo for hosting the dataset. We thank Fran?ois Darmon, Tom Monnier, Mathis Petrovich and Damien Robert for inspiring discussions and valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">ViViT: A video vision transformer. ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aygun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maximov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4d panoptic LiDAR segmentation. CVPR (2021</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<title level="m">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SA-Det3D: Self-attention based context-aware 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving. CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Af2-s3net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">4d spatio-temporal convnets: Minkowski convolutional neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Combining EfficientNet and vision transformers for video deepfake detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coccomini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02612</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SalsaNext: Fast, uncertainty-aware semantic segmentation of LiDAR point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases. ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Point 4d transformer networks for spatiotemporal modeling in point cloud videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kassahun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ricou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06320</idno>
		<title level="m">A2D2: Audi autonomous driving dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263</idno>
		<title level="m">CMT: Convolutional neural networks meet vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
