<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A General Framework for Information Extraction using Dynamic Span Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
							<email>luanyi@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
							<email>dwadden@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
							<email>luheng@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
							<email>amyshah@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
							<email>ostendor@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence ? Google AI Language</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A General Framework for Information Extraction using Dynamic Span Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a general framework for several information extraction tasks that share span representations using dynamically constructed span graphs. The graphs are constructed by selecting the most confident entity spans and linking these nodes with confidenceweighted relation types and coreferences. The dynamic span graph allows coreference and relation type confidences to propagate through the graph to iteratively refine the span representations. This is unlike previous multitask frameworks for information extraction in which the only interaction between tasks is in the shared first-layer LSTM. Our framework significantly outperforms the state-of-the-art on multiple information extraction tasks across multiple datasets reflecting different domains. We further observe that the span enumeration approach is good at detecting nested span entities, with significant F1 score improvement on the ACE dataset. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most Information Extraction (IE) tasks require identifying and categorizing phrase spans, some of which might be nested. For example, entity recognition involves assigning an entity label to a phrase span. Relation Extraction (RE) involves assigning a relation type between pairs of spans. Coreference resolution groups spans referring to the same entity into one cluster. Thus, we might expect that knowledge learned from one task might benefit another.</p><p>Most previous work in IE (e.g., <ref type="bibr" target="#b24">(Nadeau and Sekine, 2007;</ref><ref type="bibr" target="#b2">Chan and Roth, 2011)</ref>) employs a pipeline approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations <ref type="bibr" target="#b23">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b37">Zhang et al., 2017)</ref>, entities and coreferences <ref type="bibr" target="#b8">(Hajishirzi et al., 2013;</ref><ref type="bibr" target="#b6">Durrett and Klein, 2014)</ref>, joint inference <ref type="bibr" target="#b31">(Singh et al., 2013)</ref> or multi-task (entity/relation/coreference) learning <ref type="bibr" target="#b19">(Luan et al., 2018a)</ref>. These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains.</p><p>In this paper, we introduce a general framework Dynamic Graph IE (DYGIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating a benefit from incorporating broader context learned from relation and coreference annotations. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example illustrating the potential benefits of entity, relation, and coreference contexts. It is impossible to predict the entity labels for This thing and it from within-sentence context alone. However, the antecedent car strongly suggests that these two entities have a VEH type. Similarly, the fact that Tom is located at Starbucks and Mike has a relation to Tom provides support for the fact that Mike is located at Starbucks. DYGIE uses multi-task learning to identify entities, relations, and coreferences through shared span representations using dynamically constructed span graphs. The nodes in the graph are dynamically selected from a beam of highly-confident mentions, and the edges are weighted according to the confidence scores of relation types or coreferences. Unlike the multi-task method that only shares span representations from the local context <ref type="bibr" target="#b19">(Luan et al., 2018a)</ref>, our framework leverages rich contextual span representations by propagating information through coreference and relation links. Unlike previous BIO-based entity recognition systems <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b13">Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016)</ref> that assign a text span to at most one entity, our framework enumerates and represents all possible spans to recognize arbitrarily overlapping entities.</p><p>We evaluate DYGIE on several datasets spanning many domains (including news, scientific articles, and wet lab experimental protocols), achieving state-of-the-art performance across all tasks and domains and demonstrating the value of coupling related tasks to learn richer span representations. For example, DYGIE achieves relative improvements of 5.7% and 9.9% over state of the art on the ACE05 entity and relation extraction tasks, and an 11.3% relative improvement on the ACE05 overlapping entity extraction task.</p><p>The contributions of this paper are threefold. 1) We introduce the dynamic span graph framework as a method to propagate global contextual information, making the code publicly available. 2) We demonstrate that our framework significantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Protocol Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous studies have explored joint modeling <ref type="bibr" target="#b23">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b37">Zhang et al., 2017;</ref><ref type="bibr" target="#b31">Singh et al., 2013;</ref><ref type="bibr" target="#b35">Yang and Mitchell, 2016)</ref>) and multi-task learning <ref type="bibr" target="#b25">(Peng and Dredze, 2015;</ref><ref type="bibr" target="#b26">Peng et al., 2017;</ref><ref type="bibr" target="#b19">Luan et al., 2018a</ref><ref type="bibr" target="#b18">Luan et al., , 2017a</ref> as methods to share representational strength across related in-formation extraction tasks. The most similar to ours is the work in <ref type="bibr" target="#b19">Luan et al. (2018a)</ref> that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DYGIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations.</p><p>Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b13">Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b20">Luan et al., 2017b</ref><ref type="bibr" target="#b21">Luan et al., , 2018b</ref>. However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently <ref type="bibr" target="#b10">Katiyar and Cardie (2018)</ref> and <ref type="bibr" target="#b33">Wang and Lu (2018)</ref> have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities.</p><p>Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion <ref type="bibr" target="#b36">(Yang et al., 2015;</ref><ref type="bibr" target="#b1">Bordes et al., 2013)</ref> is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations <ref type="bibr" target="#b26">(Peng et al., 2017;</ref><ref type="bibr" target="#b32">Song et al., 2018;</ref>. <ref type="bibr" target="#b3">Christopoulou et al. (2018)</ref> models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution <ref type="bibr" target="#b14">(Lee et al., 2017</ref> and semantic role labeling , the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes.</p><p>Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence <ref type="bibr" target="#b16">(Li and Ji, 2014;</ref><ref type="bibr" target="#b34">Xu et al., 2015;</ref><ref type="bibr" target="#b23">Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b37">Zhang et al., 2017)</ref>. These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings <ref type="bibr" target="#b28">(Peters et al., 2018)</ref> without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation. While the performance of DyGIE benefits from ELMo, it advances over some systems <ref type="bibr" target="#b19">(Luan et al., 2018a;</ref><ref type="bibr" target="#b30">Sanh et al., 2019)</ref> that also incorporate ELMo. The analyses presented here give insights into the benefits of joint modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Problem Definition The input is a document represented as a sequence of words D, from which we derive S = {s 1 , . . . , s T }, the set of all possible within-sentence word sequence spans (up to length L) in the document. The output contains three structures: the entity types E for all spans S, the relations R for all span pairs S ? S within the same sentence, and the coreference links C for all spans in S across sentences. We consider two primary tasks. First, Entity Recognition is the task of predicting the best entity type labels e i for each span s i . Second, Relation Extraction involves predicting the best relation type r ij for all span pairs (s i , s j ). We provide additional supervision by also training our model to perform a third, auxiliary task: Coreference resolution. For this task we predict the best antecedent c i for each span s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Model</head><p>We develop a general information extraction framework (DYGIE) to identify and classify entities, relations, and coreference in a multi-task setup. DYGIE first enumerates all text spans in each sentence, and computes a locallycontextualized vector space representation of each span. The model then employs a dynamic span graph to incorporate global information into its span representations, as follows. At each training step, the model identifies the text spans that are most likely to represent entities, and treats these spans as nodes in a graph structure. It constructs confidence-weighted arcs for each node according to its predicted coreference and relation links with the other nodes in the graph. Then, the span repre-sentations are refined using broader context from gated updates propagated from neighboring relation types and co-referred entities. These refined span representations are used in a multi-task framework to predict entity types, relation types, and coreference links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>In this section, we give an overview of the main components and layers of the DYGIE framework, as illustrated in <ref type="figure">Figure 2</ref>. Details of the graph construction and refinement process will be presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token Representation Layer</head><p>We apply a bidirectional LSTM over the input tokens. The input for each token is a concatenation of the character reprensetation, GLoVe <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref> word embeddings, and ELMo embeddings <ref type="bibr" target="#b28">(Peters et al., 2018)</ref>. The output token representations are obtained by stacking the forward and backward LSTM hidden states.</p><p>Span Representation Layer For each span s i , its initial vector representation g 0 i is obtained by concatenating BiLSTM outputs at the left and right end points of s i , an attention-based soft "headword," and an embedded span width feature, following <ref type="bibr" target="#b14">Lee et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreference Propagation Layer</head><p>The propagation process starts from the span representations g 0 i . At each iteration t, we first compute an update vector u t C for each span s i . Then we use u t C to update the current representation g t i , producing the next span representation g t+1 i . By repeating this process N times, the final span representations g N i share contextual information across spans that are likely to be antecedents in the coreference graph, similar to the process in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Propagation Layer</head><p>The outputs g N i from the coreference propagation layer are passed as inputs to the relation propagation layer. Similar to the coreference propagation process, at each iteration t, we first compute the update vectors u t R for each span s i , then use it to compute g t+1 i . Information can be integrated from multiple relation paths by repeating this process M times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Prediction Layer</head><p>We use the outputs of the relation graph layer g N +M i to predict the entity labels E and relation labels R. For entities, we pass g N +M i to a feed-forward network (FFNN) to produce per-class scores P E (i) for span s i . For relations, we pass the concatenation of g N +M i and g N +M j to a FFNN to produce per-class relation scores P R (i, j) between spans s i and s j . Entity and relation scores are normalized across the label space, similar to <ref type="bibr" target="#b19">Luan et al. (2018a)</ref>. For coreference, the scores between span pairs (s i , s j ) are computed from the coreference graph layer outputs (g N i , g N j ), and then normalized across all possible antecedents, similar to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Graph Construction and Span Refinement</head><p>The dynamic span graph facilitates propagating broader contexts through soft coreference and relation links to refine span representations. The nodes in the graph are spans s i with vector representations g t i ? R d for the t-th iteration. The edges are weighted by the coreference and relation scores, which are trained according to the neural architecture explained in Section 3.1. In this section, we explain how coreference and relation links can update span representations.</p><p>Coreference Propagation Similar to <ref type="bibr" target="#b19">(Luan et al., 2018a)</ref>, we define a beam B C consisting of b c spans that are most likely to be in a coreference chain. We consider P t C to be a matrix of real values that indicate coreference confidence scores between these spans at the t-th iteration. P t C is of size b c ? K, where K is the maximum number of antecedents considered. For the coreference graph, an edge in the graph is single directional, connecting the current span s i with all its potential antecedents s j in the coreference beam, where j &lt; i. The edge between s i and s j is weighted by coreference confidence score at the current iteration P t C (i, j). The span update vector u t C (i) ? R d is computed by aggregating the neighboring span representations g t j , weighted by their coreference scores P t C (i, j):</p><formula xml:id="formula_0">u t C (i) = j?B C (i) P t C (i, j)g t j<label>(1)</label></formula><p>where B C (i) is the set of K spans that are antecedents of s i ,</p><formula xml:id="formula_1">P t C (i, j) = exp(V t C (i, j)) j ?B C (i) exp(V t C (i, j)) (2) V t C (i, j) is a scalar score computed by concate- nating the span representations [g t i , g t j , g t i g t j ]</formula><p>, where is element-wise multiplication. The concatenated vector is then fed as input to a FFNN, similar to .</p><p>Relation Propagation For each sentence, we define a beam B R consisting of b r entity spans that are mostly likely to be involved in a relation. Unlike the coreference graph, the weights of relation edges capture different relation types. Therefore, for the t-th iteration, we use a tensor V t R ? R b R ?b R ?L R to capture scores of each of the L R relation types. In other words, each edge in the relation graph connects two entity spans s i and s j in the relation beam B R . V t R (i, j) is a L R -length vector of relation scores, computed with a FFNN with [g t i , g t j ] as the input. The relation update vector u t R (i) ? R d is computed by aggregating neighboring span representations on the relation graph:</p><formula xml:id="formula_2">u t R (i) = j?B R f (V t R (i, j))A R g t j ,<label>(3)</label></formula><p>where A R ? R L R ?d is a trainable linear projection matrix, f is a non-linear function to select the most important relations. Because only a small number of entities in the relation beam are actually linked to the target span, propagation among all possible span pairs would introduce too much noise to the new representation. Therefore, we choose f to be the ReLU function to remove the effect of unlikely relations by setting the all negative relation scores to 0. Unlike coreference connections, two spans linked via a relation are not expected to have similar representations, so the matrix A R helps to transform the embedding g t j according to each relation type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Updating Span Representations with Gating</head><p>To compute the span representations for the next iteration t ? {1, . . . , N + M }, we define a gating vector f t x (i) ? R d , where x ? {C, R}, to determine whether to keep the previous span representation g t i or to integrate new information from the coreference or relation update vectors u t x (i). Formally,</p><formula xml:id="formula_3">f t x (i) = g(W f x [g t i , u t x (i)]) (4) g t+1 i = f t x (i) g t i + (1 ? f t x (i)) u t x (i),</formula><p>where W f x ? R d?2d are trainable parameters, and g is an element-wise sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>The loss function is defined as a weighted sum of the log-likelihood of all three tasks:</p><formula xml:id="formula_4">(D,R * ,E * ,C * )?D ? E log P (E * | C, R, D) (5) + ? R log P (R * | C, D) + ? C log P (C * | D)</formula><p>where E * , R * and C * are gold structures of the entity types, relations and coreference, respectively. D is the collection of all training documents D. The task weights ? E , ? R , and ? C are hyperparameters to control the importance of each task.  We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the feed-forward functions have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout <ref type="bibr" target="#b7">(Gal and Ghahramani, 2016)</ref> for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings. The hidden layer dimensions and dropout rates are chosen based on the development set performance in multiple domains. The task weights, learning rate, maximum span length, number of propagation iterations and beam size are tuned specifically for each dataset using development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>DYGIE is a general IE framework that can be applied to multiple tasks. We evaluate the performance of DYGIE against models from two lines of work: combined entity and relation extraction, and overlapping entity extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity and relation extraction</head><p>For the entity and relation extraction task, we test the performance of DYGIE on four different datasets: ACE2004, ACE2005, SciERC and the Wet Lab Protocol Corpus. We include the relation graph propagation layer in our models for all datasets. We include the coreference graph propagation layer on the data sets that have coreference annotations available.</p><p>Data All four data sets are annotated with entity and relation labels. Only a small fraction of entities (&lt; 3% of total) in these data sets have a text span that overlaps the span of another entity. Statistics on all four data sets are displayed in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>The ACE2004 and ACE2005 corpora provide entity and relation labels for a collection of documents from a variety of domains, such as newswire and online forums. We use the same entity and relation types, data splits, and preprocessing as <ref type="bibr" target="#b23">Miwa and Bansal (2016)</ref> and <ref type="bibr" target="#b16">Li and Ji (2014)</ref>. Following the convention established in this line of work, an entity prediction is considered correct  if its type label and head region match those of a gold entity. We will refer to this version of the ACE2004 and ACE2005 data as ACE04 and ACE05. Since the domain and mention span annotations in the ACE datasets are very similar to those of OntoNotes <ref type="bibr" target="#b29">(Pradhan et al., 2012)</ref>, and OntoNotes contains significantly more documents with coreference annotations, we use OntoNotes to train the parameters for the auxiliary coreference task. The OntoNotes corpus contains 3493 documents, averaging roughly 450 words in length. The SciERC corpus <ref type="bibr" target="#b19">(Luan et al., 2018a)</ref> provides entity, coreference and relation annotations for a collection of documents from 500 AI paper abstracts. The dataset defines scientific term types and relation types specially designed for AI domain knowledge graph construction. An entity prediction is considered correct if its label and span match with a gold entity.</p><p>The Wet Lab Protocol Corpus (WLPC) provides entity, relation, and event annotations for 622 wet lab protocols <ref type="bibr" target="#b12">(Kulkarni et al., 2018)</ref>. A wet lab protocol is a series of instructions specifying how to perform a biological experiment. Following the procedure in <ref type="bibr" target="#b12">Kulkarni et al. (2018)</ref>, we perform entity recognition on the union of entity tags and event trigger tags, and relation extraction on the union of entity-entity relations and entity-trigger event roles. Coreference annotations are not available for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare DYGIE with current state of the art methods in different datasets. <ref type="bibr" target="#b23">Miwa and Bansal (2016)</ref> provide the current state of the art on ACE04. They construct a Tree LSTM using dependency parse information, and use the repre-sentations learned by the tree structure as features for relation classification. <ref type="bibr" target="#b0">Bekoulis et al. (2018)</ref> use adversarial training as regularization for a neural model. <ref type="bibr" target="#b37">Zhang et al. (2017)</ref> cast joint entity and relation extraction as a table filling problem and build a globally optimized neural model incorporating syntactic representations from a dependency parser. Similar to DYGIE, <ref type="bibr" target="#b30">Sanh et al. (2019)</ref> and <ref type="bibr" target="#b19">Luan et al. (2018a)</ref> use a multi-task learning framework for extracting entity, relation and coreference labels. <ref type="bibr" target="#b30">Sanh et al. (2019)</ref> improved the state of the art on ACE05 using multi-task, hierarchical supervised training with a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. <ref type="bibr" target="#b19">Luan et al. (2018a)</ref> previously achieved the state of the art on SciERC and use a span-based neural model like our DYGIE. <ref type="bibr" target="#b12">Kulkarni et al. (2018)</ref> provide a baseline for the WLPC data set. They employ an LSTM-CRF for entity recognition, following <ref type="bibr" target="#b13">Lample et al. (2016)</ref>. For relation extraction, they assume the presence of gold entities and train a maximum-entropy classifier using features from the labeled entities.</p><p>Results <ref type="table" target="#tab_4">Table 2</ref> shows test set F1 on the joint entity and relation extraction task. We observe that DYGIE achieves substantial improvements on both entity recognition and relation extraction across the four data sets and three domains, all in the realistic setting where no "gold" entity labels are supplied at test time. DYGIE achieves 7.1% and 7.0% relative improvements over the state of the art on NER for ACE04 and ACE05, respectively. For the relation extraction task, DYGIE attains 25.8% relative improvement over SOTA on ACE04 and 13.7% relative improvement on ACE05. For ACE05, the best entity extraction performance is obtained by switching the order between CorefProp and RelProp (RelProp first then CorefProp).</p><p>On SciERC, DYGIE advances the state of the art by 5.9% and 1.9% for relation extraction and NER, respectively. The improvement of DYGIE over the previous SciERC model underscores the ability of coreference and relation propagation to construct rich contextualized representations.</p><p>The results from <ref type="bibr" target="#b12">Kulkarni et al. (2018)</ref> establish a baseline for IE on the WLPC. In that work, relation extraction is performed using gold entity boundaries as input. Without using any gold entity information, DYGIE improves on the baselines by 16.8% for relation extraction and 2.2% for NER.  On the OntoNotes data set used for the auxiliary coreference task with ACE05, our model achieves coreference test set performance of 70.4 F1, which is competitive with the state-of-the-art performance reported in <ref type="bibr" target="#b14">Lee et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overlapping Entity Extraction</head><p>There are many applications where the correct identification of overlapping entities is crucial for correct document understanding. For instance, in the biomedical domain, a BRCA1 mutation carrier could refer to a patient taking part in a clinical trial, while BRCA1 is the name of a gene.</p><p>We evaluate the performance of DYGIE on overlapping entity extraction in three datasets: ACE2004, ACE2005 and GENIA. Since relation annotations are not available for these datasets, we include the coreference propagation layer in our models but not the relation layer. 2 Data Statistics on our three datasets are listed in <ref type="table" target="#tab_6">Table 3</ref>. All three have a substantial number (&gt; 20% of total) of overlapping entities, making them appropriate for this task.</p><p>As in the joint case, we evaluate our model on ACE2004 and ACE2005, but here we follow the same data preprocessing and evaluation scheme as <ref type="bibr" target="#b33">Wang and Lu (2018)</ref>. We refer to these data sets as ACE04-O and ACE05-O. Unlike the joint entity and relation task in Sec. 4.1, where only the entity head span need be predicted, an entity prediction is considered correct in these experiments if both its entity label and its full text span match a gold prediction. This is a more stringent evaluation criterion than the one used in Section 4.1. As before, we use the OntoNotes annotations to train the parameters of the coreference layer.</p><p>The GENIA corpus <ref type="bibr" target="#b11">(Kim et al., 2003)</ref> provides entity tags and coreferences for 1999 abstracts from the biomedical research literature. We only use the IDENT label to extract coreference clusters.   We use the same data set split and preprocessing procedure as <ref type="bibr" target="#b33">Wang and Lu (2018)</ref> for overlapping entity recognition.</p><p>Baselines The current state-of-the-art approach on all three data sets is <ref type="bibr" target="#b33">Wang and Lu (2018)</ref>, which uses a segmental hypergraph coupled with neural networks for feature learning. <ref type="bibr" target="#b10">Katiyar and Cardie (2018)</ref> also propose a hypergraph approach using a recurrent neural network as a feature extractor.</p><p>Results <ref type="table" target="#tab_8">Table 4</ref> presents the results of our overlapping entity extraction experiments on the different datsets. DYGIE improves 11.6% on the state of the art for ACE04-O and 11.3% for ACE05-O. DY-GIE also advances the state of the art on GENIA, albeit by a more modest 1.5%. Together these results suggest that DYGIE can be utilized fruitfully for information extraction across different domains with overlapped entities, such as bio-medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of Graph Propagation</head><p>We use the dev sets of ACE2005 and SciERC to analyze the effect of different model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Coreference and Relation Graph Layers</head><p>Tables 5 and 6 show the effects of graph propagation on entity and relation prediction accuracy,    where ?CorefProp and ?RelProp denote ablating the propagation process by setting N = 0 or M = 0, respectively. Base is the base model without any propagation. For ACE05, we observe that coreference propagation is mainly helpful for entities; it appears to hurt relation extraction. On SciIE, coreference propagation gives a small benefit on both tasks. Relation propagation significantly benefits both entity and relation extraction in both domains. In particular, there are a large portion of sentences with multiple relation instances across different entities in both ACE05 and Sci-ERC, which is the scenario in which we expect relation propagation to help.</p><p>Since coreference propagation has more effect on entity extraction and relation propagation has more effect on relation extraction, we mainly focus on ablating the effect of coreference propagation on entity extraction and relation propagation on relation extraction in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Coreference Propagation and Entities</head><p>A major challenge of ACE05 is to disambiguate the entity class for pronominal mentions, which requires reasoning with cross-sentence contexts. For example, in a sentence from ACE05 dataset, "One of [them] PER , from a very close friend of [ours] ORG ." It is impossible to identity whether them and ours is a person (PER) or organization (ORG) unless we have read previous sentences. We  hypothesize that this is a context where coreference propagation can help. <ref type="table" target="#tab_13">Table 7</ref> shows the effect of the coreference layer for entity categorization of pronouns. 3 DYGIE has 6.6% improvement on pronoun performance, confirming our hypothesis.</p><p>Looking further, <ref type="table" target="#tab_14">Table 8</ref> shows the impact on all entity categories, giving the difference between the confusion matrix entries with and without CorefProp. The frequent confusions associated with pronouns (GPE/PER and PER/ORG, where GPE is a geopolitical entity) greatly improve, but the benefit of CorefProp extends to most categories.</p><p>Of course, there are a few instances where CorefProp causes errors in entity extraction. For example, in the sentence "[They] ORG PER might have been using Northshore...", DYGIE predicted They to be of ORG type because the most confident antecedent is those companies in the previous sentence: "The money was invested in those companies." However, They is actually referring to these fund managers earlier in the document, which belongs to PER category.</p><p>In the SciERC dataset, the pronouns are uniformly assigned with a Generic label, which explains why CorefProp does not have much effect on entity extraction performance.</p><p>The <ref type="figure" target="#fig_2">Figure 3a</ref> shows the effect of number of iterations for coreference propagation in the entity extraction task. The figure shows that coreference layer obtains the best performance on the second iteration (N = 2).  <ref type="table" target="#tab_2">LOC  5  0  -2  -1  2  -1  0  WEA  0  3  0  0  1  -3  -1  GPE  -3  0  31  -26  3  -7  0  PER  0  -2  -3  18  -1  -26  4  FAC  4  -1  2  -3  2  -5  1  ORG  0  0  0  -8  -1  6  0  VEH  0  -2  -1  2  5</ref> -1 1  could have more impact. <ref type="figure" target="#fig_2">Figure 3b</ref> shows the effect of number of iterations for relation propagation in the relation extraction task. Our model achieves the best performance on the second iteration (M = 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Relation Propagation Impact</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced DYGIE as a general information extraction framework, and have demonstrated that our system achieves state-of-the art results on entity recognition and relation extraction tasks across a diverse range of domains. The key contribution of our model is the dynamic span graph approach, which enhance interaction across tasks that allows the model to learn useful information from broader context. Unlike many IE frameworks, our model does not require any preprocessing using syntactic tools, and has significant improvement across different IE tasks including entity, relation extraction and overlapping entity extraction. The addition of co-reference and relation propagation across sentences adds only a small computation cost to inference; the memory cost is controlled by beam search. These added costs are small relative to those of the baseline span-based model. We welcome the community to test our model on different information extraction tasks. Future directions include extending the framework to encompass more structural IE tasks such as event extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>COREFFigure 1 :</head><label>1</label><figDesc>Tom's car broke down as he arrived at Starbucks to meet Mike."This thing's useless!" Tom exclaimed as it gave off smoke. A text passage illustrating interactions between entities, relations and coreference links. Some relation and coreference links are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Relation F1 with different number of RelProp iterations M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>F1 score of each layer on ACE development set for different number of iterations. N = 0 or M = 0 indicates no propagation is made for the layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>shows relation scores as a function of number of entities in sentence for DYGIE and DYGIE without relation propagation on ACE05. The figure indicates that relation propagation achieves significant improvement in sentences with more entities, where one might expect that using broader context LOC WEA GPE PER FAC ORG VEH</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Relation F1 broken down by number of entities in each sentence. The performance of relation extraction degrades on sentences containing more entities. Adding relation propagation alleviates this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets for joint entity and relation extraction and their statistics. Ent: Number of entity categories.</figDesc><table /><note>Rel: Number of relation categories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: F1 scores on the joint entity and relation ex-</cell></row><row><cell>traction task on each test set, compared against the pre-</cell></row><row><cell>vious best systems. * indicates relation extraction sys-</cell></row><row><cell>tem that takes gold entity boundary as input.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Datasets for overlapping entity extraction and their statistics. Ent: Number of entity categories. Overlap: Percentage of sentences that contain overlapping entities.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Performance on the overlapping entity extraction task, compared to previous best systems. We report F1 of extracted entities on the test sets.</figDesc><table><row><cell></cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>DYGIE</cell><cell cols="6">87.4 86.7 87.1 56.2 60.9 58.4</cell></row><row><cell cols="7">?CorefProp 86.2 85.2 85.7 64.3 56.7 60.2</cell></row><row><cell>?RelProp</cell><cell cols="6">87.0 86.7 86.9 60.4 55.8 58.0</cell></row><row><cell>Base</cell><cell cols="6">86.1 85.7 85.9 59.5 55.7 57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablations on the ACE05 development set with different graph propagation setups. ?CorefProp ablates the coreference propagation layers, while ?RelProp ablates the relation propagation layers. Base is the system without any propagation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Ablations on the SciERC development set on different graph progation setups. CorefProp has a much smaller effect on entity F1 compared to ACE05.</figDesc><table><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Entity F1</cell><cell>84 86 88</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell cols="4">Num. iterations N</cell></row><row><cell cols="6">(a) Entity F1 with different</cell></row><row><cell cols="6">number of CorefProp it-</cell></row><row><cell cols="3">erations N .</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Entity extraction performance on pronouns in ACE05. CorefProp significantly increases entity extraction F1 on hard-to-disambiguate pronouns by allowing the model to leverage cross-sentence contexts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Difference in the confusion matrix counts for ACE05 entity extraction associated with adding CorefProp.</figDesc><table><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell>DYGIE</cell></row><row><cell>Relation F1</cell><cell>60</cell><cell></cell><cell></cell><cell cols="2">DYGIE?RelProp</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>3</cell><cell>4-5</cell><cell>6-11</cell><cell>12-max</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Num. entities in sentence</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the pre-processed ACE dataset from previous work and relation annotation is not available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Pronouns included: anyone, everyone, it, itself, one, our, ours, their, theirs, them, themselves, they, us, we, who</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Office of Naval Research under the MURI grant N00014-18-1-2670, NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We also thank the anonymous reviewers and the UW-NLP group for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A walk-based model on entity graphs for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint coreference resolution and named-entity linking with multi-pass sieves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jointly predicting predicates and arguments in neural semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Assoc. for Computational Linguistics (NAACL)</title>
		<meeting>Conf. North American Assoc. for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Genia corpus -a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An annotated corpus for machine reading of instructions in wet lab protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Machiraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Assoc. for Computational Linguistics (NAACL)</title>
		<meeting>Conf. North American Assoc. for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task learning for speaker-role adaptation in neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scientific information extraction with semisupervised neural tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The uwnlp system at semeval-2018 task 7: Neural relation extraction model with selectively incorporated concept embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop on Semantic Evaluation (SemEval)</title>
		<meeting>Int. Workshop on Semantic Evaluation (SemEval)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="788" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lingvisticae Investigationes</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A hierarchical multi-task approach for learning embeddings from semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint inference of entities, relations, and coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2013 workshop on Automated knowledge base construction</title>
		<meeting>of the 2013 workshop on Automated knowledge base construction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graphstate lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural segmental hypergraphs for overlapping mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint extraction of events and entities within a document context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR</title>
		<meeting>Int. Conf. Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
