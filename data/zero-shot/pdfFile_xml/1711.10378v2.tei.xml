<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saquib Sarfraz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IOSB</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eberle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification is a challenging retrieval task that requires matching a person's acquired image across non-overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discriminative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets. Code is available at https://github.com/pse-ecn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (re-id) in non-overlapping camera views poses a difficult matching problem. Most previous solutions try to learn the global appearance of a persons using Convolutional Neural Networks (CNNs) by either applying a straightforward classification loss or using a metric learning loss. To better learn local statistics, the same has been applied to image regions, e.g. by using horizontal stripes or grids <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. Because of the inherent challenge of matching between different views and poses of a person, <ref type="bibr">Figure 1</ref>. Camera view and body pose can significantly alter the visual appearance of a person. A different view might show different aspects of the clothing (e.g. a backpack) while an altered pose may lead to body parts (e.g. arms or legs) being located at different positions of the image. Pose information can also help guide the attention of a re-id approach in case of mis-aligned detections.</p><p>there is no implicit correspondence between the local regions of the images (see <ref type="figure">Figure 1</ref>). This correspondence can be established by explicitly using full body pose information for alignment <ref type="bibr" target="#b31">[32]</ref> or locally through matching corresponding detected body parts <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Using this local or global person description by incorporating the body pose or body parts information can strongly benefit person re-id.</p><p>In this work we show that incorporating a simple cue of the person's coarse pose (i.e. the captured view with respect to the camera) or the fine body pose (i.e. joint locations) suffice to learn a very discriminative representation with a simple classification loss. We present an appealing design choice to incorporate these cues and show its benefit in the performance gain over state-of-the-art methods on large and challenging surveillance benchmarks. We demonstrate that learning and combining view specific feature maps on a standard underlying CNN architecture results in a significantly better re-id embedding. Similarly an incorporation of body joint locations as additional input channels helps to increase the re-id accuracy.</p><p>For improving person retrieval, after computing the initial distances, a re-ranking step can often improve ranking quality by a good margin. Re-ranking has seen a renewed interest in recent years <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref>. The re-ranking problem is formulated as re-estimating the distances between probe and gallery images such that more correct results are ranked at the top of the returned lists. In recent proposals this was generally achieved by exploiting the similarity of the lists of top k nearest neighbors of both the probe and gallery image in question. Among the state-of-the-art re-ranking methods these neighborhood lists are often recomputed for each image pair, based on the common or reciprocal neighbors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b47">48]</ref>. This makes it more computationally demanding to recompute the distances between these varying length lists.</p><p>A second contribution of this work is a new re-ranking method that introduces the concept of expanded cross neighborhood distance. The method aggregates the distances of close neighbors of the probe and the gallery image, where the distance can simply be the direct euclidean distance or the distance based on the rank lists. We show that within this more general framework of re-ranking simple rank list comparison based on the directly obtained rank lists achieves state-of-the-art re-ranking performance without the requirement to recompute new rank lists.</p><p>In summary, our contributions are threefold: 1) We propose a new CNN embedding which incorporates coarse and fine-grained person pose information. 2) We propose a new unsupervised and automatic re-ranking method that achieves larger re-ranking improvements than previous methods. 3) Our pose-sensitive person re-id model and our re-ranking method set a new state-of-the-art on four challenging datasets. We also demonstrate the scalability of our approach with very large gallery sizes and its performance for person search in full camera images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years many state-of-the-art re-id results have been achieved by approaches relying on feature embeddings learned through CNNs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20]</ref>. We focus our discussion of related approaches to those which include a degree of pose information, as well as re-ranking methods.</p><p>Re-Id using Pose A person's body pose is an important cue for successful re-identification. The popular SDALF feature by Farenza et al. <ref type="bibr" target="#b7">[8]</ref> uses two axes dependent on the body's pose to derive a feature description with pose invariance. Cho et al. <ref type="bibr" target="#b5">[6]</ref> define four view angles (front, left, right, back) and learn corresponding matching weights to emphasize matching of same-view person images. A more fine-grained pose representation based on Pictorial Structures was first used in <ref type="bibr" target="#b4">[5]</ref> to focus on matching between individual body parts. More recently, the success of deep learning architectures in the context of re-id has lead to several works that include pose information into a CNN-based matching. In <ref type="bibr" target="#b42">[43]</ref> Zheng et al. propose to use a CNN-based external pose estimator to normalize person images based on their pose. The original and normalized images are then used to train a single deep re-id embedding. A similar approach is described by Su et al. in <ref type="bibr" target="#b31">[32]</ref>. Here, a sub-network first estimates a pose map which is then used to crop the localized body parts. A local and a global person representation are then learned and fused. Pose variation has also been addressed by explicitly detecting body parts through detection frameworks <ref type="bibr" target="#b40">[41]</ref>, by relying on visual attention maps <ref type="bibr" target="#b24">[25]</ref>, or body part specific attention modeling <ref type="bibr" target="#b41">[42]</ref>.</p><p>In contrast to our proposed method, these works mostly rely only on fine-grained pose information. Furthermore, these approaches either include pose information by explicitly normalizing their input images or by explicitly modeling part localization and matching these in their architecture. In contrast to this, our approach relies on confidence maps generated by a pose estimator which are added as additional channels to the input image. This allows a maximum degree of flexibility in the learning process of our CNN and leaves it to the network to learn which body parts are relevant and reliable for re-id. Apart from this finegrained pose information we show that a more coarse pose cue turns out to be even more important and can be effectively used to improve the re-id performance. Re-Ranking In the recent years, re-ranking techniques are drawing more and more attention in the area of person reid. Shen et al. <ref type="bibr" target="#b29">[30]</ref> use k-nearest neighbors (k-NN) to produce new rank lists and recompute distances based on these. Garcia et al. <ref type="bibr" target="#b8">[9]</ref> propose to jointly learn the context and content information in the ranking list to remove candidates in the top neighbors and improve performance of person re-id. <ref type="bibr" target="#b14">[15]</ref> extends this to revise the initial ranking list with a new similarity obtained from fusion of content and contextual similarity.</p><p>Li et al. <ref type="bibr" target="#b17">[18]</ref> first proposed to use the relative information of common nearest neighbors of each image to improve reranking. Ye et al. <ref type="bibr" target="#b35">[36]</ref> combined the common nearest neighbors of global and local features as new queries and revise the initial ranking list by aggregating these into new ranking lists. Using the similarity and dissimilarity cues from the neighbors of different baseline methods <ref type="bibr" target="#b36">[37]</ref> proposes a ranking aggregation algorithm to improve person re-id. In contrast to common neighbors, Jegou et al. <ref type="bibr" target="#b13">[14]</ref> use reciprocal neighbors (i.e. common neighbors that reciprocate in a k-neighborhood sense) and propose to compute a contextual dissimilarity measure (CDM). <ref type="bibr" target="#b23">[24]</ref> formally uses the kreciprocal neighbors to compute ranking lists. Most recent state-of-the-art re-ranking methods are based on computing these rank list comparisons using a generalized Jaccard distance. To overcome the associated complexity of computing intersection and unions of underlying variable length lists, Sparse Contextual Activation (SCA) <ref type="bibr" target="#b0">[1]</ref> encodes the neighborhood set into a sparse vector and then computes the dis- tance. To reduce the false positives and noise in the original ranked lists, more context is included by forming new rank lists based on reciprocal neighbors <ref type="bibr" target="#b13">[14]</ref>[24] <ref type="bibr" target="#b47">[48]</ref>. Zhong et al <ref type="bibr" target="#b47">[48]</ref> use the k-reciprocal lists and compute the Jaccard distance by using SCA encoding. They then propose to fuse this distance with the original distance to obtain the final re ranking. Note, while reciprocal list based comparisons provides the current best re-ranking scores, it requires an additional complexity of recomputing the reciprocal rank lists for each image pair.</p><p>In contrast to common or reciprocal neighbors and producing new rank lists based on these, we propose the concept of expanded neighbors and aggregating their cross distances among the images in a pair. We show that this results in a more effective re-ranking framework while not requiring to re-compute new rank-lists for each image pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pose-Sensitive Embedding</head><p>A person's pose and orientation to the camera can greatly affect the visual appearance in the image. Explicitly including this information into the learning process of a re-id model can thus often increase the resulting accuracy. Previous works have relied on either fine-grained pose information (e.g. joint keypoints) or coarse information (e.g. orientation to the camera). In this section we describe two new methods for including both levels of granularity into a pose-sensitive embedding. Both methods can be simultaneously incorporated into the same baseline CNN architecture and our experiments show that a combination of the two achieves a higher accuracy than either one alone. An overview of our CNN architecture with both types of pose information is depicted in <ref type="figure" target="#fig_0">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">View Information</head><p>We use the quantization ['front', 'back', 'side'] of a person's orientation to the camera as coarse pose information.</p><p>Since this information is dependent on the camera, as well as the person, we call it view information in the remainder of this work.</p><p>Our inclusion of view information into the re-id embedding is based on our prior work <ref type="bibr" target="#b26">[27]</ref> on semantic attribute recognition. A ternary view classifier is added as a sidebranch of our main re-id CNN. The tail part of the main CNN is then split into three equivalent units by replicating existing layers. The view classifier's three view prediction scores are used to weight the output of each of these units. This modulates the gradient flowing through the units, e.g. for a training sample with a strong 'front' prediction, mainly the unit weighted by the front-weight will contribute to the final embedding and thus only this unit will receive a strong gradient update for the current training sample. This procedure allows each unit to learn a feature map specialized for one of the three views. Importantly, and in contrast to <ref type="bibr" target="#b26">[27]</ref>, we do not weight and fuse final embeddings or prediction vectors but apply the weights to full feature maps which are then combined into the final embedding. This achieves a more robust representation.</p><p>We cannot generally assume to have view annotations available on the re-id dataset we want to train our embedding on. Thus, we pretrain a corresponding view classifier on the separate RAP <ref type="bibr" target="#b16">[17]</ref> pedestrian dataset which provides such annotations. We then directly transfer the classifier to our re-id model. Low-level features (i.e. early layers) can be shared between the view predictor and the re-id network in order to reduce model complexity.</p><p>In our default ResNet-50 architecture the view predictor branch is split off from the main network after the third dimensionality reduction step (i.e. at feature map dimensions 28 ? 28 ? 256). We then apply three consecutive convolutions with step sizes 2, 2, and 5 to reduce the dimension further (to 1 ? 1 ? 1024). The resulting feature vector is used to predict the view using a three-way softmax. As view units we use three replications of the ResNet Block-4. The 7 ? 7 ? 2048 dimensional fused output of the units is pooled and fed to a fully connected layer which yields our 1536 dimensional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Full Body Pose</head><p>As fine-grained representation of a person's pose we use the locations of 14 main body joint keypoints. To obtain this information we use the off-the-shelf DeeperCut <ref type="bibr" target="#b11">[12]</ref> model. In contrast to prior use of pose information for re-id, we do not use this information to explicitly normalize the input images. Instead, we include the information into the training process by adding an additional input channel for each of the 14 keypoints. These channels serve to guide the CNN's attention and help learn how to best apply the body joint information into the resulting embedding. To further increase this flexibility, we do not rely on the final keypoint decisions of the DeeperCut approach, but instead provide our re-id CNN with the full confidence map for each keypoint. This prevents any erroneous input based on hard keypoint decisions and leaves our model the chance to compensate for, or at least recognize, unreliable pose information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Details</head><p>We initialize all our CNNs with weights pretrained for ImageNet classification. In order to train a model with view information (Section 3.1) we start by fine-tuning only the view-predictor branch on the RAP dataset <ref type="bibr" target="#b16">[17]</ref>. Next we train only the view units and the final person identity classification layer on the target re-id dataset. The weights of the view predictor and all layers before the view units are fixed for this stage. This allows the randomly initialized view units and final layers to adapt to the existing weights of earlier layers.</p><p>When training an embedding including full body pose information (Section 3.2) the ImageNet weights do not match the size of our input, due to the additional 14 keypoint channels. To adapt the network for 17 channel inputs we start our training by fine-tuning only the first layer (Layer 0 in <ref type="figure" target="#fig_0">Figure 2</ref>), and the final person identity classification layer which are both initialized randomly. The remainder of the network remains fixed. Once these two layers are adapted to the rest of the network (i.e. convergence is observed), we proceed by fine-tuning the entire network.</p><p>For our final pose sensitive embedding (PSE) we combine both types of pose information into one network as depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. We initialize our training with the full body pose model described in the previous section and add the view predictor onto it. The view predictor is fine-tuned on the RAP dataset with pose maps and can benefit from the additional full body pose information. Further fine-tuning of the re-id elements of the network is then performed on the target re-id dataset as described above.</p><p>For all our CNN embeddings we employ the same train-ing protocol. Input images are normalized to channel-wise zero-mean and a standard variation of 1. Data augmentation is performed by resizing images to 105% width and 110% height and randomly cropping the training sample, as well as random horizontal flip (this is the main reason why we do not differentiate between left and right side views).</p><p>Training is performed using the Adam optimizer at recommended parameters with an initial learning rate of 0.0001 and a decay of 0.96 every epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expanded Cross Neighborhood Distance based Re-Ranking</head><p>In this section we introduce the concept of Expanded Cross Neighborhood (ECN) distance which can provide a very high boost in performance while not strictly requiring rank list comparisons. We show that, for an image pair, accumulating the distances of only the immediate twolevel neighbors of each image with the other image results in a promising re-ranking. Within this cross neighborhood based distance framework, the underlying accumulated distances can be just the original euclidean distances or the re-calculated rank-list based distances. We also show that within this framework using a simple list comparison measure on the initially obtained rank lists achieves the state of the art re-ranking performance. Our proposal is fully automatic, unsupervised and can work well without requiring to compute new rank lists.</p><p>Formally, given a probe image p and a gallery set G with B images G = {g i | i = 1, 2, ? ? ? , B}, the euclidean distance between p and each of the gallery g i is p ? g i 2 2 . Computing pairwise distance between all images in the gallery and probe sets, the initial ranking L(p, G) = {g o 1 , ? ? ? , g o B } for each image is then obtained by sorting this distance in an increasing order.</p><p>Given such initial rank lists L of all the images in the gallery and probe sets, we define the expanded neighbors of the probe p as the multiset N (p, M ) such that:</p><formula xml:id="formula_0">N (p, M ) ? {N (p, t), N (t, q)}<label>(1)</label></formula><p>where N (p, t) are the top t immediate neighbors of probe p and N (t, q) contains the top q neighbors of each of the elements in set N (p, t):</p><formula xml:id="formula_1">N (p, t) = {g o i | i = 1, 2, ? ? ? , t} N (t, q) = {N (g o i , q), ? ? ? , N (g o t , q)}<label>(2)</label></formula><p>A similar expanded neighbors multiset can be obtained for each of the gallery images N (g i , M ) in terms of its immediate neighbors and their neighbors. The total number of neighbors M in the set N (p, M ) or N (g i , M ) is M = t + t ? q. Finally the Expanded Cross Neighborhood (ECN) distance of an image pair (p, g i ) is defined as</p><formula xml:id="formula_2">ECN (p, g i ) = 1 2M M j=1 d(pN j , g i ) + d(g i N j , p) (3)</formula><p>where pN j is the jth neighbor in the probe expanded neighbor set N (p, M ) and g i N j is the jth neighbor in the ith gallery image expanded neighbor set N (g i , M ). The term d(?) is the distance between that pair. One can see that the ECN distance, above, just aggregates the distances of the expanded neighbors of each of the image in pair with the other. While we show in our evaluation that using the direct euclidean distance in Equation 3 results in a similar improvement in the rank accuracies, one can also use a more robust rank-list based distance to further enhance the performance, especially in terms of the mean average precision (mAP). These distances can be computed directly from the initial paired distance matrix or the resulting initial rank lists. Recent re-ranking proposals use the Jaccard distance for the list comparison which is computationally expensive, here we propose to use a rather simple list comparison similarity measure proposed by Jarvis and Patrick <ref type="bibr" target="#b12">[13]</ref>, and also successfully employed in a face verification task in <ref type="bibr" target="#b27">[28]</ref>. The list similarity is measured in terms of the position of top K neighbors of the two lists. For a rank list with B images, let pos i (b) denote the position of image b in the ordered rank list L i . In terms of considering only the first K neighbors in the list, the Rank-list similarity R is given by:</p><formula xml:id="formula_3">R(L i , L j ) = B b=1 [K + 1 ? pos i (b)] + ?[K + 1 ? pos j (b)] + (4)</formula><p>Here, [?] + = max(?, 0). This measure ensures to base similarity in terms of top K neighbors while taking into account their position in the list. From an implementation point, this rank list similarity can effectively be computed from the initially obtained rank lists by single matrix multiplication and addition operations. To use this in Equation 3, we convert it into the distance d = 1?R * where R * denotes the minmax scaling of values in R. Finally the parameters t, q and K (in case of using the rank-list distances) for computing the final ECN distance are set to t = 3, q = 8 and K = 25. while we show that these parameters choices are very stable in terms of performance on a number of different sized datasets, one can intuitively also see that using the strongest top neighbors in the first level (t) and expanding these to few more at the second level (q) makes sense. Note since our neighbors' of neighbor expansion only looks for the first and second level of neighbors, we do not need to compute an expensive KD-tree or neighborhood graphs to get these expanded sets in Equation 1, we can readily obtain these from the initially computed ordered rank list matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We report results using the standard cross camera evaluation in the single-query setting. Accuracy is measured by rank scores, obtained from cumulated matching characteristics (CMC), and mean average precision (mAP). Datasets: We evaluate our approach on four datasets, Market-1501 <ref type="bibr" target="#b43">[44]</ref> (Market), Duke-MTMC-reID <ref type="bibr" target="#b25">[26]</ref> (Duke), MARS <ref type="bibr" target="#b30">[31]</ref> and PRW <ref type="bibr" target="#b44">[45]</ref>.</p><p>The Market-1501 (Market) dataset consists of 32,668 bounding boxes of 1,501 distinct persons generated by a person detector on videos from six cameras. 751 persons are used for training and 750 for testing. The training set contains 12,936 images, the gallery set 19,732 images, and the query set has 3,368 images.</p><p>The Duke-MTMC-reID (Duke) dataset is created from data of eight cameras. Of 1,812 people in the data 1,404 occur in more than one camera. Training and test sets both consist of 702 persons. The training set includes 16,522 images, the gallery 17,661 images, and the query set 2,228 image. Person bounding boxes in the Duke dataset are manually annotated.</p><p>The MARS dataset consists of 20,478 tracklets of 1,261 re-occurring persons. Including 3,248 distractor tracklets this brings the total number of person images in the dataset to 1,191,993 with a train/test split of 509,914/681,089 images of 625 and 636 persons, respectively. This dataset is well suited to evaluate the performance of a re-id approach for person track retrieval.</p><p>The PRW dataset consists of 11,816 frames of video data. The images are annotated with 43,110 person bounding boxes of which 34,304 are assigned one of 932 person IDs. For training 5,134 frames including 482 different persons are available. At test time 2,057 cropped query images of persons must be found in a gallery of 6,112 full images. The PRW dataset allows for an evaluation of the robustness of a re-id method to false positive or mis-aligned person detections.</p><p>In order to compare to related approaches we split our evaluation into three parts. In Sections 5.1 and 5.2 we investigate key components of our pose-sensitive embedding and re-ranking, respectively. In Section 5.3 we compare our proposed embedding and re-ranking with state-of-the-art approaches. We also demonstrate the robust performance of our approach against detector errors and its scalability for very large galleries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Study of Pose Information</head><p>We investigate the usefulness of including different granularities of pose information into the CNN by performing separate experiments with only view information, only pose information, and a combination of both. Experiments are performed on Market and Duke. To show that our proposal is not strictly dependent on the underlying CNN ar-  <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Compared to a baseline without any explicitly modeled pose information, inclusion of either views or pose significantly increases the accuracy of the resulting feature embedding. This observation holds across both datasets, as well as both network architectures. For the ResNet model, the view information results in a larger absolute improvement of about 6-7% in mAP on both datasets while the pose information leads only to an improvement of about 2-3% in mAP. Results for the Inception-v4 model are less consistent. Both types of information still achieve large improvements but on the Market dataset the absolute improvement for both types lies around 10% in mAP while on Duke the 11% mAP improvement through pose information clearly outperforms the 4% gained by including view information.</p><p>Finally, a combination of the two types of information leads to a further consistent increase in mAP compared to the best result of either individual pose information. For instance, on the base ResNet-50 model, the combination achieves a further improvement in mAP of 2.1% and 5.3% on Market and Duke, respectively. Similarly, on the base Inception-v4 model the combination further improves the mAP by 3% on Market and 2.2% on Duke. This clearly indicates that our methods of including different degrees of pose information complement each other. View-predictor performance: The performance of the trained ResNet-50 view predictor on the annotated test set of RAP dataset is 82.2%, 86.9% and 81.9% on front, back, and side views, respectively. In order to illustrate its performance on our target re-id dataset we display mean images in <ref type="figure" target="#fig_2">Figure 3</ref>. These are obtained by averaging all images, on the test set of the target dataset, which are classified as front, left, or side. This visualization gives an impression of the view prediction accuracy on the target re-id data in the absence of annotated view labels. In the frontal mean image a skin-colored face region is clearly discernible, indicating that the majority of images were in fact frontal ones. Similarly, the back mean image correctly shows the backside of a person. The side view is more ambiguous, aside from the possible view predictor errors, mainly because we group left and right side into one combined class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Study of Re-Ranking</head><p>In <ref type="table">Table 2</ref> we compare several configurations of our proposed ECN re-ranking with other popular re-ranking methods across the Market, CUHK03 (detected) <ref type="bibr" target="#b18">[19]</ref> and MARS datasets. Note that the CUHK03 includes both the labeled and detected (using a person detector) person bounding boxes. We chose the CUHK03 (detected) as it is more challenging. We evaluate CHUK03 under the new fixed train/test protocol as used in <ref type="bibr" target="#b47">[48]</ref>  <ref type="bibr" target="#b38">[39]</ref>. To compare with the published results of several re-ranking methods on these datasets, we use the same baseline features, 2,048dim ID-discriminative embedding provided by <ref type="bibr" target="#b47">[48]</ref>. We compare with the previous re-ranking techniques for object retrieval and person re-id including contextual dissimilarity measure (CDM) <ref type="bibr" target="#b13">[14]</ref>, spatially constrained (k-NN) reranking <ref type="bibr" target="#b29">[30]</ref>, Average query expansion (AQE) <ref type="bibr" target="#b6">[7]</ref> and the current state-of-the-art Sparse Contextual Activation (SCA) <ref type="bibr" target="#b0">[1]</ref> , k-reciprocal encoding (k-reciprocal) <ref type="bibr" target="#b47">[48]</ref> and its direct multiplicative application Divide and Fuse (DaF) <ref type="bibr" target="#b38">[39]</ref>. As shown our ECN re-ranking achieves a consistent improvement in performance across all three datasets on both mAP and rank-1 metrics.</p><p>We provide the performance of the different components  <ref type="table">Table 2</ref>. Comparison of the proposed ECN re-ranking method with state-of-the-art on three datasets, Market-1501, CHUK03 (detected) and MARS. Baseline features: 2,048-dim IDdiscriminative Embedding fine tuned on Resnet (IDE-R) and Caf-feNet (IDE-C) <ref type="bibr" target="#b47">[48]</ref>. of our ECN framework. As shown in <ref type="table">Table 2</ref>, only using the rank-list distance of Equation 4 (rank-dist) still provides meaningful performance gains. Within the ECN framework just using the direct euclidean distances in <ref type="bibr">Equation 3</ref> 'ECN (orig-dist)' results in similar high performance gains in the rank-1 scores, in fact better than the state-of-the-art k-reciprocal <ref type="bibr" target="#b47">[48]</ref> method that uses the reciprocal list comparisons with local query expansion and fusion of rank and euclidean distances. As this does not involve computing any rank list based comparison, this result is an additional very attractive outcome of our proposal. Finally our ECN re-ranking using the simple rank-list comparison of Equation 4 as distance in the ECN Equation 3 provides the best results and improves the mAP further. Parameters impact: In all of our evaluations presented in <ref type="table">Table 2</ref> as well as in <ref type="table">Table 3</ref>, the ECN parameters are set to t=3 and q=8. Given the very different number of images in query and test sets of the used datasets, the results show the stability of these parameters. We studied the impact of changing these on Market and Duke datasets and found that it is subtle in the range for t ? <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> and q ? <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, the performance drops between ?0.2-0.8% on different combinations within this range. Similarly the impact of parameter K in Equation 4 works well within K ? <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>, with better performance when K &gt; 20 on all three large datasets Market, MARS and Duke. The jitter in accuracies with changing K in this range stays within ??2%.</p><p>Since CUHK03 is a relatively small dataset, both DaF <ref type="bibr" target="#b38">[39]</ref> and k-reciprocal <ref type="bibr" target="#b47">[48]</ref> report results on CUHK03 by using different parameters values for their methods than used for the other datasets. While we used the same ECN parameters of t=3, and q=8 on CUHK03, we obtained higher performance with the parameter K=10 instead of K=25 (as used on all other datasets) for the rank-list distance in Equation 4. The reported results in table 2 on CHUK03 dataset are with K=10, however with K=25, we still get better performance than the most state-of-the-art methods with mAP of 28.4% and rank-1 of 26.0%. Complexity analysis: The computational complexity of ECN is O(N 2 logN ) (same as other re-ranking methods) but it executes fewer computation steps by avoiding recomputing the neighbors' lists for each image pair. In its variant with ECN (orig-dist) it offers close improvements without having to re-compute the rank lists based distance (hence even fewer steps). For example, on the large Duke dataset (re-ranking on 19,889 total images), computation times averaged over five runs are 124.6s for the related work k-reciprocal <ref type="bibr" target="#b47">[48]</ref> while 115.3s and 73.2s for our ECN (rankdist) and ECN (orig-dist) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">State-of-the-art</head><p>In <ref type="table">Table 3</ref> we compare the performance of our approach with the published state-of-the-art on the three popular datasets (Market, Duke, and MARS). In the top section of the table we compare approaches without any re-ranking to our pose-sensitive embedding. The embedding achieves top accuracy on both MARS and Duke datasets. On the Market dataset our embedding performs slightly worse than the DPFL <ref type="bibr" target="#b2">[3]</ref> approach which employs two or more multi-scale embeddings. Across all three datasets the increase in mAP achieved by including pose information on the base ResNet ranges from 7.4% to 11.7%. In the bottom section of <ref type="table">Table  3</ref> we include the best published methods with re-ranking. In combination with our proposed re-ranking scheme we set a new state-of-the-art on all three datasets by large margins. On Market we increase mAP by 11.4%, on Duke by 19.2%, and on MARS by 4.5%. Real World Considerations: In real-world applications reid methods needs to be scalable (large gallery sizes) and are used in combination with automatic person detectors which can generate errors, such as mis-aligned detections or false positives. To investigate the scalability of our proposed PSE model, we evaluate on the Market+500k dataset to judge its robustness in real world deployment with very large galleries. The Market+500k dataset extends the Market dataset by including up to 500,000 distractor persons images. The relative change in mAP and rank-1 accuracy of our PSE model in comparison to other state-of-the-art approaches is depicted in <ref type="table" target="#tab_3">Table 4</ref>. While our embedding outperforms the published state-of-the-art without any distractors, the drop in accuracy observed when adding distractors is also notably less steep than that of other approaches. At 500,000 distractors our PSE's mAP has dropped by 12.5% while related approaches dropped by more than 14%, similarly PSE drops in rank-1 accuracy by ?7% while the related approaches drop by ?10%. This shows the quality of our PSE model for this more realistic setting.</p><p>In order to test our PSE embedding under detector errors, we train and evaluate its performance on the PRW dataset <ref type="bibr" target="#b44">[45]</ref>  <ref type="table">Table 3</ref>. Comparison of our approach with the published state-of-the-art. The top section of the table compares our embedding with state-of-the-art approaches that do not use re-ranking. The lower section compares our combination of embedding and re-ranking to other state-of-the-art methods that use re-ranking.  we observe similar trends as on Market or Duke. Both types of pose information improve notably over the baseline and achieve a further increase in accuracy when combined in the PSE embedding. The performance is stable when considering more detections per image (hence increasing false pos-itives) as shown in <ref type="table" target="#tab_4">Table 5</ref>. The PSE embedding achieves state-of-the-art accuracy, outperforming related approaches by at least 6.3% in mAP (when an average of 3 detections per image are considered). The results confirm the intuition that pose information is a helpful cue in identifying and handling mis-aligned and false-positive person detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented two related but independent contributions for person re-id and retrieval applications. We showed that both the fine and coarse body pose cues are important for re-id and proposed a new pose-sensitive CNN embedding which incorporates these. The PSE model currently relies on an external pose predictor, it would be useful to fully integrate this into the model. The re-ranking method is unsupervised and can be used for general image and video retrieval applications. Both our person re-id model and re-ranking method set new state-of-the-art on a number of challenging datasets independently and in concert with each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our pose sensitive embedding (PSE) architecture. As baseline architecture we employ either ResNet-50 or Inception-v4. Pose information is included through detailed body joint maps in the input, as well as through a coarse view predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Mean images of Market-1501 (left) and Duke (right) test sets using predictions of the PSE model's view predictor. The images show front, back and side view from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>75.9 89.8 92.5 97.3 36.6 61.8 74.8 79.8 89.4 Views only 61.9 81.5 92.3 94.9 98.1 40.3 62.7 76.6 81.1 90.3 Pose only 60.9 81.7 91.8 94.4 97.9 48.2 70.5 81.9 86.1 92.7 Comparison of different types of pose information. While views and full body pose individually lead to notable improvements, a combination of both often results in further improvements.</figDesc><table><row><cell>CNN</cell><cell>Method</cell><cell cols="2">Market-1501</cell><cell>Duke</cell></row><row><cell></cell><cell></cell><cell>mAP R-1</cell><cell cols="2">R-5 R-10 R-50 mAP R-1 R-5 R-10 R-50</cell></row><row><cell>Inception-v4</cell><cell cols="4">Baseline 51.9 PSE 64.9 84.4 93.1 95.2 98.4 50.4 71.7 83.5 87.1 93.1</cell></row><row><cell>ResNet-50</cell><cell>Baseline</cell><cell cols="3">59.8 82.6 92.4 94.9 98.2 50.3 71.5 83.1 87.0 94.1</cell></row><row><cell></cell><cell cols="4">Views only 66.9 88.2 95.4 97.2 98.9 56.7 76.9 87.3 90.7 95.7</cell></row><row><cell></cell><cell>Pose only</cell><cell cols="3">61.6 82.8 93.1 95.5 98.3 53.1 73.4 84.5 88.1 94.3</cell></row><row><cell></cell><cell>PSE</cell><cell cols="3">69.0 87.7 94.5 96.8 99.0 62.0 79.8 89.7 92.2 96.3</cell></row><row><cell cols="4">chitecture, besides using our main ResNet-50 base CNN,</cell></row><row><cell cols="4">we also show results on the popular Inception-v4 CNN. For</cell></row><row><cell cols="4">Inception-v4, the view predictor is branched out at the ear-</cell></row><row><cell cols="4">lier Reduction-A block and view units are similarly added</cell></row><row><cell cols="4">by using three Inception-C blocks at the end. Results of our</cell></row><row><cell>experiments are given in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Using the DPM detections provided with the dataset 90.2 78.9 84.4 70.7 74.9 PSE+ rank-dist (Eq. 4) 80.5 89.6 74.5 82.8 67.7 74.9 PSE+ ECN (orig-dist) 80.5 90.4 75.7 84.5 68.6 75.5 PSE+ ECN (rank-dist) 84.0 90.3 79.8 85.2 71.8 76.7</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell cols="2">Market-1501</cell><cell></cell><cell>Duke</cell><cell>MARS</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">mAP R-1 mAP R-1 mAP R-1</cell></row><row><cell cols="2">P2S[49]</cell><cell>CVPR17</cell><cell cols="2">44.3 70.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Spindle[41]</cell><cell>CVPR17</cell><cell>-</cell><cell>76.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Consistent Aware[21]</cell><cell>CVPR17</cell><cell cols="2">55.6 80.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">GAN[47]</cell><cell>ICCV17</cell><cell cols="4">56.2 78.1 47.1 67.7</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Latent Parts [16]</cell><cell>CVPR17</cell><cell cols="2">57.5 80.3</cell><cell>-</cell><cell>-</cell><cell cols="2">56.1 71.8</cell></row><row><cell cols="2">ResNet+OIM [35]</cell><cell>CVPR17</cell><cell>-</cell><cell>82.1</cell><cell>-</cell><cell>68.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ACRN[29]</cell><cell cols="5">CVPR17-W 62.6 83.6 52.0 72.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SVD [33]</cell><cell>ICCV17</cell><cell cols="4">62.1 82.3 56.8 76.7</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Part Aligned [42]</cell><cell>ICCV17</cell><cell cols="2">63.4 81.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">PDC [32]</cell><cell>ICCV17</cell><cell cols="2">63.4 84.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">JLML [20]</cell><cell>IJCAI17</cell><cell cols="2">65.5 85.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DPFL [3]</cell><cell>ICCV17-W</cell><cell cols="4">72.6 88.6 60.6 79.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Forest [50]</cell><cell>CVPR17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">50.7 70.6</cell></row><row><cell cols="2">DGM+IDE [38]</cell><cell>ICCV17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">46.8 65.2</cell></row><row><cell>Our</cell><cell>ResNet-50 Baseline PSE</cell><cell></cell><cell cols="6">59.8 82.6 50.3 71.5 49.5 64.5 69.0 87.7 62.0 79.8 56.9 72.1</cell></row><row><cell cols="2">Smoothed Manif. [2]</cell><cell>CVPR17</cell><cell cols="2">68.8 82.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">IDE (R)+XQDA+k-reciprocal [48]</cell><cell>CVPR17</cell><cell cols="2">61.9 75.1</cell><cell>-</cell><cell>-</cell><cell cols="2">68.5 73.9</cell></row><row><cell cols="3">IDE (R)+KISSME+k-reciprocal [48] CVPR17</cell><cell cols="2">63.6 77.1</cell><cell>-</cell><cell>-</cell><cell cols="2">67.3 72.3</cell></row><row><cell cols="2">DaF [39]</cell><cell>BMVC17</cell><cell cols="2">72.4 82.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PSE+ k-reciprocal [48]</cell><cell></cell><cell>83.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>52.3 49.1 45.2 79.5 73.8 71.5 68.3 APR ? * [22] 62.8 56.5 53.6 49.8 84.0 79.9 78.2 75.4 TriNet ? ? [11] 69.1 61.9 58.7 53.6 84.9 79.7 77.9 74.7 Our ResNet-50 Baseline 59.8 54.6 51.8 47.5 82.6 77.7 75.7 73.2 Views Only 66.9 61.5 58.9 54.8 88.2 84.4 83.2 81.2 Pose Only 63.0 57.7 54.9 50.6 83.6 80.0 77.9 75.1 PSE 69.0 63.4 60.8 56.5 87.7 84.1 82.6 80.8 Results of the PSE embedding on the Market-1501+500k distractors dataset ( ? = unpublished works, * = additional attribute ground truth, ? = x10 test-time augmentation). 45.9 77.9 18.8 45.9 77.4 19.2 45.7 76.0 DPM-Alex IDEdet [45] 20.2 48.2 78.1 20.3 47.4 77.1 19.9 47.2 76.4 DPM-Alex IDEdet+CWS [45] 20.0 48.2 78.8 20.5 48.3 78.8 20.5 48.3 78.59.1 84.6 28.4 58.6 84.4 29.1 58.1 83.4 DPM PSE 29.3 65.1 88.3 31.7 65.0 88.2 32.4 64.5 87.5</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">mAP by #Distractors</cell><cell cols="4">R-1 by #Distractors</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell cols="3">100k 200k 500k</cell><cell>0</cell><cell></cell><cell cols="2">100k 200k 500k</cell></row><row><cell cols="3">I+V  ? [46] 59.9 Detector Method</cell><cell>#detections=3</cell><cell></cell><cell cols="2">#detections=5</cell><cell></cell><cell cols="2">#detections=10</cell></row><row><cell></cell><cell></cell><cell cols="8">mAP R-1 R-20 mAP R-1 R-20 mAP R-1 R-20</cell></row><row><cell>DPM</cell><cell>IDEdet [45]</cell><cell cols="8">17.2 8</cell></row><row><cell cols="2">IAN (ResNet-101) [34]</cell><cell cols="2">23.0 61.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DPM</cell><cell>Baseline</cell><cell cols="8">25.4 59.0 83.9 27.5 59.1 83.9 28.3 58.1 83.3</cell></row><row><cell>DPM</cell><cell>View only</cell><cell cols="8">28.5 63.4 87.3 30.8 63.1 86.8 31.4 62.0 86.1</cell></row><row><cell>DPM</cell><cell>Pose only</cell><cell cols="2">26.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results of PSE on PRW dataset (robustness against false detections): Considering 3, 5 and 10 detections per image.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: The research described in this work is funded in part by the BMBF grant No. 13N14029.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse contextual activation for efficient visual re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1056" to="1069" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop on cross domain human identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving person re-identification via pose-aware multi-shot matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification ranking optimisation by discriminant context information analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1305" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clustering using a similarity measure based on shared near neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Patrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1025" to="1034" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A contextual dissimilarity measure for accurate and efficient image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition,CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Person re-identification with content and context re-ranking. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="6989" to="7014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A richly annotated dataset for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07054</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Common-nearneighbor analysis for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mukunoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2012 19th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1621" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Consistent-aware deep learning for person re-identification in a camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Query based adaptive re-ranking for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="397" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Person re-identification using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taalimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07336</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep view-sensitive pedestrian attribute inference in an endto-end model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06089</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pose, illumination and expression invariant pairwise facesimilarity measure via doppelg?nger list comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2494" to="2501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning attribute-complementary information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1435" to="1443" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3013" to="3020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MARS: A Video Benchmark for Large-Scale Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision ICCV</title>
		<meeting>the IEEE Conference on Computer Vision ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">IAN: the individual aggregation network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno>abs/1705.05552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coupled-view based ranking optimization for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person reidentification via ranking aggregation of similarity pulling and dissimilarity pushing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2553" to="2566" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Divide and fuse: A re-ranking approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00384</idno>
		<title level="m">Deep mutual learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deeply-learned partaligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<title level="m">Person re-identification in the wild</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A discriminatively learned cnn embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Point to set similarity based deep feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
