<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING WHAT AND WHERE -UNSUPERVISED DIS- ENTANGLING LOCATION AND IDENTITY TRACKING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Traub</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Neuro-Cognitive Modeling University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Otte</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Neuro-Cognitive Modeling</orgName>
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Menge</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Neuro-Cognitive Modeling</orgName>
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Karlbauer</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Neuro-Cognitive Modeling</orgName>
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Th?mmel</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">ML in Climate Science</orgName>
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">V</forename><surname>Butz</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Neuro-Cognitive Modeling</orgName>
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING WHAT AND WHERE -UNSUPERVISED DIS- ENTANGLING LOCATION AND IDENTITY TRACKING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our brain can almost effortlessly decompose visual data streams into background and salient objects. Moreover, it can anticipate object motion and interactions, which are crucial abilities for conceptual planning and reasoning. Recent object reasoning datasets, such as CATER, have revealed fundamental shortcomings of current vision-based AI systems, particularly when targeting explicit object encodings, object permanence, and object reasoning. Here we introduce a selfsupervised LOCation and Identity tracking system (Loci), which excels on the CATER tracking challenge. Inspired by the dorsal-ventral pathways in the brain, Loci tackles the binding problem by processing separate, slot-wise encodings of 'what' and 'where'. Loci's predictive coding-like processing encourages active error minimization, such that individual slots tend to encode individual objects. Interactions between objects and object dynamics are processed in the disentangled latent space. Truncated backpropagation through time combined with forward eligibility accumulation significantly speeds up learning and improves memory efficiency. Besides exhibiting superior performance in current benchmarks, Loci effectively extracts objects from video streams and separates them into location and Gestalt components. We believe that this separation offers an encoding that will facilitate effective planning and reasoning on conceptual levels. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human reasoning about visual scenes is characterized by segmenting a scene into individual entities and their interactions <ref type="bibr">[4; 13; 23]</ref>. This ability presents a non-trivial challenge for computational models of cognition <ref type="bibr">[33; 76]</ref>: the binding problem <ref type="bibr" target="#b70">[71]</ref>. Originally proposed and discussed mainly in cognitive neuroscience, the binding problem describes the fundamental challenge of how to bind features into objects, thereby segregating them from the background, and encoding them by means of compressed stable neural attractors <ref type="bibr">[5; 40; 65]</ref>.</p><p>Recent years have seen revolutionary progress in the ability of connectionist models to operate on complex natural images and videos <ref type="bibr">[29; 44; 59</ref>]. Yet, neural network models are not capable of addressing the binding problem in its full generality <ref type="bibr" target="#b32">[33]</ref>. Indeed, recent work on synthetic object reasoning datasets like CLEVR, CLEVRER, or CATER <ref type="bibr">[30; 39; 85]</ref> suggests that state-of-the-art video reasoning systems <ref type="bibr">[16; 81; 84]</ref>, still struggle to model fundamental physical object properties, such as hollowness, blockage, or object permanence-concepts that children develop during the first few months of their lives <ref type="bibr">[2; 57]</ref>.</p><p>In a comprehensive review on the binding problem in the context of neural networks, Greff et al. <ref type="bibr" target="#b32">[33]</ref> define three main challenges for solving the problem: representation, segregation, and composition. Representation refers to the challenge to build encodings of the essential properties of an object, including its appearance and potential dynamics. We will refer to these properties as the 'Gestalt' of an object <ref type="bibr">[42; 78; 79]</ref>. Object representations should be separable to avoid interference. Moreover, their shape and location should be disentangled to enable compositional generalizations. Meanwhile, they should share a common format to enable general purpose reasoning. Segregation describes the challenge to extract particular object representations from a perceived scene. This extraction should be done context-and task-dependently in order to identify currently relevant entities. The essential feature of a good segregation is that it allows effective dynamic predictions of the whole, rather than the parts. Composition characterizes the challenge to develop object representations that enable meaningful re-combinations of object properties. Moreover, these properties should enable the prediction of object interaction consequences, depending on the respective object properties. As a result, compositional representations allow conceptual reasoning about both object properties as well as relations and interactions between objects.</p><p>Our goal is to develop an architecture that solves the binding problem-respective challenges. We thus propose a novel Location and Identity tracking system in the context of tracking and reconstructing the behavior of objects from video data. A key motivation for our model is that the representation of an object's properties-'what it is'-should be explicitly disentangled from its location-'where it is'-mimicking ventral and dorsal processing in our brain, respectively <ref type="bibr">[61; 72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Previous work by <ref type="bibr" target="#b52">[53]</ref> has emphasized that, in general, unsupervised object representation learning is impossible because infinitely many variable models are consistent with the data. Inductive biases are thus necessary to ensure the effective learning of a system that segregates a visual stream of information into effective, compositional representations <ref type="bibr" target="#b32">[33]</ref>. Accordingly, we review related work in the light of the binding problem and their relation to the proposed Loci system.</p><p>Representation A powerful choice of representation format is the formulation of 'slots', which share a representational space but keep the codes of individual objects separate from one another. To ensure a common format between slot representations, typically, slot-respective encoder modules share their weights <ref type="bibr">[8; 54; 75]</ref>. To assign individual objects to individual slots, though, the system needs to break slot symmetry. Recurrent neural networks have been used to disentangle representations or assignments <ref type="bibr">[11; 26; 27; 31; 54]</ref>. Other mechanisms explicitly separate spatial slot locations <ref type="bibr">[20; 38; 48]</ref>, which we also do in Loci. However, instead of treating every spatial location as a potential object, each slot has a spotlight, which is designed to approximate the object's center. Another important challenge towards a compositional object representation is the disentanglement of salient features. Loci enforces the disentanglement of object slot locations (where it is), Gestalt codes (object properties), size (visual extent), and priority (current visibility with respect to other objects).</p><p>Segregation Segregating object instances from raw input images is traditionally solved via bounding box detection <ref type="bibr">[52; 66]</ref>, where more advanced techniques extract additional masks for instance segmentation <ref type="bibr">[15; 21; 36]</ref>. Through slot-attention mechanisms, recent unsupervised approaches partition image regions into separate slots to represent distinct objects <ref type="bibr">[32; 55]</ref>. To segregate objects from images, Burgess et al. <ref type="bibr" target="#b9">[10]</ref> and von K?gelgen et al. <ref type="bibr" target="#b76">[77]</ref> combine a soft attention (mask) approach with encoder, recurrent attention, and decoder modules, where slots compete for encoding individual objects. Loci pursues a similar approach but segregates objects even further to encourage object-respective 'what' and 'where' encodings, where the latter additionally disentangle location, size, and approximate depth. Slot-respective masks compete via softmax attention to actively minimize the prediction error of the visual content. Moreover, a pre-trained background model separates potentially interesting from uninteresting regions. While we keep the background modeling rather simple in this work, more advanced background modeling techniques may certainly be applied in the near future <ref type="bibr">[24; 62; 63; 74]</ref>.</p><p>Composition Compositional reasoning in our model builds on two modules, which process objectto-object interactions and object dynamics. Object-to-object interactions are modelled using Multi-Head Self-Attention (MHSA) <ref type="bibr" target="#b74">[75]</ref>, in close relation with <ref type="bibr">[25; 41]</ref>. Alternative approaches employ message passing neural networks to simulate object interactions <ref type="bibr">[6; 18; 37]</ref>. Object dynamics are modelled using a recurrent GateL0RD module <ref type="bibr" target="#b34">[35]</ref>. GateL0RD is specifically designed to apply latent state updates sparsely, which encourages the maintenance of stable object representations over long periods of time. Previous approaches have also employed recurrent structures to propagate slot representations forward in time <ref type="bibr">[38; 43]</ref>. Most recent frameworks, though, tend to employ fully auto-regressive designs based on transformers without explicit internal state maintenance <ref type="bibr">[25; 41; 58]</ref>.</p><p>Tracking models While our primary goal is to separate object location and Gestalt representations, a successful extraction of object positions will likely facilitate object tracking; a wide area of research on its own <ref type="bibr" target="#b82">[83]</ref>. Modern state-of-the-art methods for object tracking rely on features extracted via attention modules that are typically applied autoregressively on individual frames <ref type="bibr">[19; 58]</ref>.</p><p>Again, these approaches do not explicitly foster a separation of location and Gestalt, which potentially limits their applicability and accuracy in distractive environments, where humans still maintain high tracking skills as recently shown in <ref type="bibr" target="#b48">[49]</ref>. Loci maintains separate object representations and thus copes with the presence of distractors more readily. The tracking model proposed in <ref type="bibr" target="#b48">[49]</ref> is similar in spirit to Loci, extracting object representations and propagating them with a competition mechanism and a recurrent module. However, to model complex objects their model relies on additional supervision.</p><p>Disentangling movement and appearance is a common principle in video models with a notion of optical flow <ref type="bibr" target="#b50">[51]</ref>. But optical flow based methods notoriously struggle with occlusions as they can only represent parts of the scene that are currently visible. In contrast, Loci is able to maintain a representation of an object even under occlusion.</p><p>Previous work on CATER We focus Loci's evaluation on the CATER challenge <ref type="bibr" target="#b29">[30]</ref>. Previous SOTA methods on this challenge, like Multi-Hopper or OpNet <ref type="bibr">[68; 88]</ref>, have focused on reusing well-established neural network building blocks. Others have attempted to build their system on top of a supervised pre-trained bounding-box-based object detector <ref type="bibr">[15; 89]</ref>. In contrast, Loci effectively combines various representational formats, slot-oriented operations, and several kinds of interactive neural processing modules. It is trained in a fully self-supervised manner, but can be further fine-tuned with supervision to minimize location-specific prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>The core idea of Loci is to split the encoding of a moving entity into its 'where' and 'what' components. We use the Siamese network idea implemented in slot-attention <ref type="bibr" target="#b53">[54]</ref>, sharing weights across slots. In fact, Loci employs the same encoding and same decoding network for every slot starting from the raw image. Each encoder slot k generates output Gestalt vectors G k and position codes P k . Entity dynamics and interactions are then processed based on these pairs of latent Gestalt and location encodings. We predict the future states of latent encodings in a transition module and allow cross-slot interactions via multi-head attention. <ref type="figure">Figure 1</ref> shows the main components; information processing in one iteration from left to right. A detailed algorithmic description is provided in Appendix B. Here, we provide an overview and then detail the loss computation and training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LOCI OVERVIEW</head><p>The slot-wise encoder is based on a conventional ResNet architecture. The input consists of eight main feature maps, which encode the image state I t at current iteration t, the prediction error map E t , a common background mask M bg as well as-slot k specifically-a mask M t k , a position map P t k , and maps of both the other masks M t,s k and the other positions P t,s k (cf. <ref type="figure" target="#fig_2">Figure 2</ref>). The computation path of Gestalt and position for a slot is tree-shaped, starting with a shared ResNet-encoded trunk after which the path is split into four pathways, which compute Gestalt, location, size, and priority. This encoder design encourages that separately moving entities are encoded in individual ResNet activity dynamics as well as that each slot encodes 'what' (Gestalt) and 'where' (location, size, and priority) of the slot-encoded moving entity separately but conjointly. Thereby, the error map E t encourages active error minimization, following the principle of predictive coding fostering attractor dynamics <ref type="bibr">[40; 65]</ref>. Appendix B specifies further details.</p><p>The transition module then predicts location P t+1 k and Gestalt dynamics G t+1 k as well as interactions between the slot-encoded entities. This module is loosely based on the architecture of a transformerlike encoder <ref type="bibr" target="#b74">[75]</ref>, where several multi-head attention layers are stacked together with residual feed forward layers in between. We replace the residual feed forward layers with residual GateL0RD <ref type="bibr" target="#b34">[35]</ref> layers. GateL0RD is a recent gated recurrent neural network architecture, which is very well-suited to learn and distinctively encode sparse interaction events (events can be considered as "structured, describable, memorable units of experience." <ref type="bibr" target="#b2">[3]</ref>, cf. <ref type="bibr" target="#b85">[86]</ref>). In accordance with our entity-focused processing approach, we apply a Siamese GateL0RD version, operating on the individual slot level while still receiving information form other slots via the attention layers.</p><p>At the end of the transition module, the Gestalt code is pushed through a binarization layer, which is inspired by the principle of vector quantization <ref type="bibr" target="#b72">[73]</ref>. This layer enforces an information bottleneck and thus contributes to the development of disentangled entity codes.</p><p>The slot-wise decoder recombines the 'what' and 'where' output from the transition module. To do so, first the potential influence of each Gestalt code G k is computed over the output range by means of a 2d isotropic Gaussian, yielding density maps Q k . Next, a priority-?-based attention is applied to account for the fact that only one slot-object can be visible at any location (transparent objects are left for future work, see Algorithm-2 in Appendix B). As a result, when two slots cover the same location, the one with the lower priority will have its feature maps set to zero. The rest of the decoder is based on a conventional ResNet, which increases the resolution back to the video size. Like the encoder, the decoder shares weights across all slots. Eventually, the decoder outputs an RGB entity reconstructionR k and a maskM k for each slot k. The masks from all slots and from the background are then combined to construct the final output image predictionR.</p><p>To fully reconstruct the image, Loci employs a background encoder-decoder architecture, which generates background image estimates R bg and a corresponding background mask M bg . In case of a static background, a Gaussian mixture model over the whole training set is used. For more dynamic backgrounds, we employ a simple auto encoder. A 2d-bias value is used for the background mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TRAINING</head><p>Loci is trained using a binary cross-entropy loss (L BCE ) pixel-wise on the frame prediction applying rectified Adam optimization [RAdam, cf. 50]. Several regularizing losses are added to foster object permanence. Additionally, to speed-up learning, we use truncated backpropagation trough time and enhance the gradients with an e-prop-like accumulation of previous neural activities <ref type="bibr" target="#b6">[7]</ref>.</p><p>Empirical evaluations showed that backpropagating the gradients through the networks' inputs creates instabilities in the training process. Thus, we detach the gradients for the latent states. As a result, the only part of the network that needs to backpropagate gradients trough time are the GateL0RD layers. Here, we found that using the described combination of e-prop and backpropagation is not only comparable in terms of network accuracy, it also greatly decreases training wall-clock time, as it  <ref type="figure">Figure 1</ref>: Loci's main processing loop has three components: The tree-structured slots of the slot-wise encoder process individual entities k to generate the disentangled latent Gestalt code G k and position code P k . The transition module models slot-to-slot interactions and slot-dynamics. It is implemented by multiple alternating layers of multi-head attention and GateL0RD-a recurrent neural network that fosters the stable maintenance of hidden latent states <ref type="bibr" target="#b34">[35]</ref>. The resulting Gestalt and position codes are finally used in the slot-wise decoder to predict slot-specific RGB entity-respective reconstructions R k and masks M k .  allows the use of truncated backpropagation with length 1, effectively updating the weights after each input frame (see supplementary material for details).</p><p>Another important aspect for successful training is the use of a warmup phase, where we mask the loss of the network with a foreground mask computed with a threshold ? :</p><formula xml:id="formula_0">M = ? &lt; Mean I t ? R bg 2 , axis = 'rgb' ,<label>(1)</label></formula><p>where R bg is the background model, detailed above. The foreground mask enforces the network to initially focus on reconstructing the foreground objects only, that is, to focus on the visual signals that differ at least ? -strongly from the background module predictions. This supports learning to use the position-constrained encoding and decoding mechanism. After around 30 000 updates-when the network has sufficiently learned to use the position encodings-we gradually blend the loss from the masked foreground reconstruction to the full reconstruction.</p><p>Object Permanence Loss To encourage object permanence, an additional loss is computed based on Equation 2, which favors a slot that keeps encoding the same object, even if the object is temporarily occluded and thus invisible:</p><formula xml:id="formula_1">L o = k D k (P t k ,? t k ) ? D k (P t k , G t k ) 2 ,<label>(2)</label></formula><formula xml:id="formula_2">G t k =? t?1 k (1 ? max(M t k )) + G t k max(M t k ),<label>(3)</label></formula><p>where P k and G k denote location and Gestalt encoding in slot k, D k refers to the RGB representation part of the decoder network, while? t k denotes the Gestalt code averaged around the last time step in which the entity was visible, and M t k denotes the mask of slot k at time step t. As a result, L o is only applied when the object becomes invisible, which is the case when M t k is approaching zero.</p><p>Time Persistence Loss A second mechanism to enforce object permanence and to also regularize the network towards temporal consistent slot encodings, is a time persistent regularization loss:</p><formula xml:id="formula_3">L t = k D k (p 0 , G t?1 k ) ? D k (p 0 , G t k ) 2 ,<label>(4)</label></formula><p>where again D k refers to the RGB representation part of the decoder network and p 0 is the center position in the image. L t essentially penalizes large visual changes in the decoded object between two consecutive time steps.</p><p>Position Change Loss In order to encourage the network to predict small slot-position changes over time, a simple L 2 regularization loss is based on the position change between two time steps:</p><formula xml:id="formula_4">L p = 0.01 n (P t?1 k ? P t k ) 2<label>(5)</label></formula><p>Supervision Loss Finally, for the experiments that are using a supervised target object location signal for fine-tuning, detailed in Equation 6, we added a gating network ? that operates on the latent Gestalt codes G t k before binarization and predicts a single softmax probability, which is used to decide which entity corresponds to the Snitch in the CATER dataset. The location of the selected entity is then used in an L 2 -loss to foster regression to the target location provided in the dataset.</p><formula xml:id="formula_5">L s = ? s k P t k ?(? t k ) ? p t snitch 2<label>(6)</label></formula><p>The final loss for the network results from adding the individual loss components, denoting the unsupervised and supervised losses, respectively:</p><formula xml:id="formula_6">L unsup = L BCE + L o + L t + L p ,<label>(7)</label></formula><formula xml:id="formula_7">L sup = L BCE + L o + L t + L p + L s .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS &amp; RESULTS</head><p>We performed unsupervised training in all experiments. Only for the CATER challenge we additionally evaluated a version where we fine-tuned the network via supervision using Equation 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CATER-TRACKING CHALLENGE</head><p>In the CATER tracking challenge (cf. <ref type="figure">Figure 3</ref>), the task is to locate a unique yellow object, called Snitch, at the end of a video sequence. During the video, different geometric objects rotate, move, and hop in a scripted random way. By doing so, cone objects can engulf other objects and move them to another location before releasing them again, which can lead to situations where the Snitch remains hidden in the last frame of the video. Therefore, the challenge is not only to recognize containment events, but also to track the specific cone that contains the Snitch. For classification purposes, the 3D Snitch position is partitioned into a 6 ? 6 grid, resulting in a total of 36 classes. In order to account for the fact that a small location error could lead to miss-classification, a common reported metric is the L 1 grid distance. We additionally report the continuous L 2 distance with grid length 1, such that the distance is comparable to, but more informative than the L 1 grid distance.</p><p>As described in Girdhar &amp; Ramanan <ref type="bibr" target="#b29">[30]</ref>, we split the dataset with a ratio of 70:30 into a training and test set and further put aside 20% of the training set as a validation set, leaving 56% of the original data for training. To blend-in the supervision loss, we first set the supervision factor ? s = 0.01 for the first 4 epochs, fostering mostly unsupervised training, and then set ? s = 0.1 for the rest of the training process, to give the Snitch location a weak pull towards the target location.</p><p>In order to produce labels from the purely unsupervisedly trained network, we train a separate small classifier with around 17k parameters, which only operates on the latent states of the trained Loci network with a correction and a gating network. The correction network computes a residual for the location of each entity. The gating network computes softmax probabilities, which are used to select the location belonging to the Snitch object similar to Equation 6. To prevent supervised gradient flow, the classifier network module is trained once the unsupervised training has finished. Eventually, the <ref type="figure">Figure 3</ref>: The CATER Snitch localization challenge: The task is to locate the yellow spherical object called Snitch within the last frame. The challenge is that the Snitch might be contained and moved by a cone. So its location has to be inferred by recognizing and remembering a containment event and then tracking the position of the container. Image adapted from <ref type="bibr" target="#b29">[30]</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, Loci not only surpasses all previous methods by a large margin-achieving a top 1 accuracy of 90.7% and a top 5 accuracy of 98.5% with an L 1 distance of 0.14-it also surpasses most of the existing work when the main architecture is trained purely unsupervised. Loci learns object permanence merely by means of inductive bias and regularization. This is shown in <ref type="figure">Figure 4</ref>, where Loci keeps the representation of the contained Snitch in memory. While the shape details blur  over time, importantly, the locating is correctly tracked to reconstruct the Snitch once it is revealed. <ref type="figure" target="#fig_3">Figure 5</ref> also demonstrates the tracking performance in case of several co-occurring occlusions and double contained situations, where two cones are covering the Snitch. Here, the location output of the supervised gating network is marked in the video frames. A final analysis of Loci's object permanence abilities is shown in <ref type="figure" target="#fig_4">Figure 6</ref>, where the tracking accuracy is plotted for progressively extended periods of Snitch containment. For both the supervised and unsupervised tracking, the location error remains constant after an initial increase, even over extended time spans.</p><p>In additional lesion studies (detailed in Appendix A.3.5), we furthermore demonstrate the effective disentanglement of position, Gestalt, size, and priority codes, which underlines Loci's ability to separate position and Gestalt of objects from a scene without supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MOVING MNIST</head><p>The moving MNIST (MMNIST) challenge is a dataset for video prediction <ref type="bibr" target="#b68">[69]</ref>. The task is to predict the motion of two moving digits, with a maximum size of 28 ? 28 pixels, moving independently inside a 64 ? 64 pixel window and bouncing off the walls in a predictable manner. While the dataset is usually implemented by simply bouncing the entire 28 ? 28 MNIST sub-image within the 64 ? 64 window, we first crop each digit to its actual appearance in order to obtain a realistic bounding box of the digit and thus to generate more realistic bouncing effects (once the actual number instead of its bounding box touches the border). This removes the undesired bias in the dataset to remember each digit's bounding box individually, which is then the only way to correctly predict a bounce off a wall. Instead, now the network can predict the bounce based on the pixel information alone.</p><p>While the task was originally formulated to predict the next 10 frames after receiving the first 10 frames, we also evaluate the ability to generate temporal closed-loop predictions for up to 100 frames after being only trained to predict 10 frames. We compare Loci to the state-of-the-art approach PhyDNet, also designed to disentangle 'what' from 'where'; more specifically: Physical dynamics from unknown factors such as the digit's Gestalt code. While using the original code provided by the authors, we did not reach the same reported performance of PhyDNet using our unbiased  MMNIST dataloader. Nevertheless, PhyDNet reaches a high accuracy for the 10 frame prediction, which is only slightly topped by Loci. For extended temporal predictions, though, PhyDNet quickly dissolves the digits, while Loci preserves the Gestalt of each digit over the 100 predicted frames, as shown in <ref type="figure" target="#fig_5">Figure 7</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows a qualitative comparison between Loci and PhyDNet: While Loci consistently outperforms PhyDNet for the structural similarity index measure (SSIM, <ref type="bibr" target="#b81">[82]</ref>), PhyDNet has a lower MSE than Loci after 30 frames. This might be due to the blurring of PhyDNet, which accounts for the uncertainty in the position estimate at the cost of the digits shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OTHER DATASETS &amp; SUPPLEMENTARY MATERIAL</head><p>Apart from a video footage for the main experiments and further algorithmic details on Loci, we provide additional insights and tests in the supplementary material: We evaluate the real world tracking performance of Loci on a ten hour aquarium footage found on YouTube 2 . Additionally, we examine Gestalt preservation and indicators of intuitive physics in closed loop predictions on the CLEVRER dataset <ref type="bibr" target="#b84">[85]</ref>. Furthermore, several ablation studies are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We presented Loci-a novel location and identity disentangling artificial neural network, which learns to process object-distinct position and Gestalt encodings. Loci significantly exceeds current state-of-the-art architectures on the CATER challenge, while requiring fewer network parameters and less supervision. Loci thus offers a critical step towards solving the binding problem <ref type="bibr">[33; 71; 76]</ref>. We particularly hope that the mechanisms proposed in this work bear potential to enrich the field of object representation learning and highlight the importance of well-chosen inductive biases.</p><p>Currently, Loci operates on static background estimates and cameras, which we denote as its main limitation. We intend to extend the background model to incorporate rich and varying backgrounds more flexibly in future work. Potential avenues for this extension may include an explicit, separate encoding of ego-motion and depth <ref type="bibr" target="#b45">[46]</ref>. Furthermore, image reconstructions may be fused with subsequent image inputs in an information-driven manner. We also expect to create even more compact object-interaction encodings following theories of event-predictive cognition and related conceptions and models in computational neuroscience <ref type="bibr">[12; 14; 28; 70]</ref>. Moreover, we are excited to explore the potential of Loci to address problems of intuitive physics and causality <ref type="bibr">[60; 64; 67]</ref>, seeing that Loci offers a suitable compositional structure <ref type="bibr">[33; 45]</ref> to enable symbolic reasoning. Finally, we hope that Loci will also be well-combinable with reinforcement learning and model-predictive planning in system control setups to pursue active, goal-directed environmental interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>Funding is acknowledge from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC number 2064/1 -Project number 390727645 as well as from the Cyber Valley in T?bingen, CyVy-RF-2020-15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>As additional content, we first provide details on further evaluations we conducted with Loci. In order to evaluate the tracking performance of Loci in a real world example, we trained it on a 10 hour aquarium footage found on YouTube. The task poses the additional challenge to cope with backgrounds that are not fully stationary. The results are shown in <ref type="figure">Figure 8</ref> and demonstrate that Loci is able to track 15+ objects in a complex real world environment.</p><p>The Gestalt preserving performance of Loci for closed loop predictions are also demonstrated exemplary on the CLEVRER dataset in <ref type="figure">Figure 9</ref>. Here, the effects of collisions of different geometric objects are predicted into the future. While the location deviates visibly over time, Loci is able to preserve the Gestalt code also for more complex objects in a closed loop setup, which is considered specifically challenging for RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 CATER EVALUATION DETAILS</head><p>In order to produce the results from <ref type="table" target="#tab_0">Table 1</ref> we trained five networks with different initial seeds and and then evaluated each network five times with different initial seeds. In <ref type="table" target="#tab_0">Table 1</ref> we reported the mean results over the five evaluation runs from our best performing network (Network 3 from <ref type="table" target="#tab_2">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 BACKGOUND MODEL</head><p>In order to compute an background model for datasets with a simple static background like CATER, CLEVR or CLEVRER we use a simple Gaussian Mixture Model. The specific function used is createBackgroundSubtractorMOG2 from OpenCV <ref type="bibr" target="#b8">[9]</ref> which we use with a learningrate of 0.00001 to compute an background image based on the training set.</p><p>For more complex backgrounds like in the aquarium example where the camera is still static, we use an autoencoder ResNet that encodes the input image into a latent vector with the same size as the gestalt code. This latent background code is then run through an residual GateL0rd in order to capture temporal dynamics, like the chancing water level, and is then run through an ResNet decoder. The background autoencoder is pretrained using a L1 reconstruction loss in order to focus on the most dominant features of the background and to not care so much about foreground objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ABLATIONS</head><p>Using the binary cross-entropy with non binary targets, while producing valid gradients, gives little insights into the network's actual performance. In order to better compare different designs and to also take the objects into focus, we use a modified L 2 -loss for our ablation studies that is computed based on Equation <ref type="bibr" target="#b8">9</ref>:</p><formula xml:id="formula_8">L2 object = (O t b,c,h,w ? I t+1 b,c,h,w ) c=1?3 1 3 I t+1 c ? B t+1 c 2 2 (9)</formula><p>Here the MSE is masked with the error between the input and the background. As a result, we get a much higher error when the network produces a tracking error compared to a background reconstruction error. The L2 object loss takes into account instances, where, for example, the network overlooks an object or, makes a mistake in the prediction of the movement of an object. It thus offers itself as a good metric for comparing the performance of different architectures during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 GATEL0RD VS LSTM</head><p>As shown in <ref type="figure" target="#fig_7">Figure 10</ref>, using GateL0RD within the predictor network significantly increases the L2 object -Loss. Thus, it appears that GateL0RD's piece-wise constant latent state regularization mechanism indeed suitably biases the network towards assuming object permanence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 INPUT CHANNELS</head><p>In <ref type="figure" target="#fig_2">Figure 12</ref>, we show the networks performance in terms of L2 object when we zero out different input channels.  In <ref type="figure" target="#fig_12">Figure 13</ref>, we show the networks performance in terms of L2 object without certain regularization losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 E-PROP VS BACK-PROPAGATION TROUGH TIME</head><p>In <ref type="figure" target="#fig_8">Figure 11</ref> we compared the L2 object loss for truncated back-propagation through time with different sequence lengths against a version where we used back-propagation together with e-prop.</p><p>In all experiments the networks are trained on the full Cater sequence with 300 frames, which are fed into the network sequentially. We then updated and afterwards detached the gradients using truncated back-propagation through time with different time intervals. Using e-prop informed gradients not only drastically sped up training time, but it was also more sample-efficient while achieving the same performance as truncated back-propagation through time with an interval of three time steps. Especially interesting is the experiment where we tested truncated back-propagation with a sequence length of one, without using e-prop's informed gradients. In this case, the performance is significantly worse than when using e-prop's informed gradients.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5 LESION STUDIES</head><p>In order to quantify the effect of the gestalt and position codes, we conducted an ablation by adding normally distributed noise with standard deviations ? ? [0, 0.1, ..., 1.0] to the gestalt and position codes provided by the encoder before they were forwarded to the transition module and calculated the resulting accuracies and errors, which are reported in <ref type="figure">Figure 14</ref>. Furthermore, exemplary images of according position, priority, size, and gestalt code manipulations are presented in <ref type="figure" target="#fig_3">Figure 15</ref>. Effectively, manipulating the according codes result in changes in position, priority, size, or gestalt code, demonstrating the disentangled encoding of object features in the respective latent codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 E-PROP FOR GATEL0RD</head><p>GateL0RD is defined in Equation 10 till Equation <ref type="bibr" target="#b14">15</ref> following Gumbsch et al. <ref type="bibr" target="#b34">[35]</ref> with the gating network g, the candidate network r, and the cell state c. Next, we detail how we applied e-prop in GateL0RD.      core derivative and eligiblity determination: </p><formula xml:id="formula_9">r t j = ? i ? rec,r j,i c t?1 i + i ? in,r j,i x t i + b r j (candidate network state) (11) c t j = g t j r t j + (1 ? g t j )c t?1 j + h t 0 (consequent cell state) (12) p t j = ? i ? rec,p j,i c t i + i ? in,p j,i x t i + b p j (output factor p) (13) o t j = ? i ? rec,o j,i c t i + i ? in,o j,i x t i + b o j (output factor o) (14)</formula><formula xml:id="formula_10">? = ??(x) ?x = 0 if x ? 0 (1 ? ?(x) 2 ) otherwise (pseudo-derivative of gate)<label>(16)</label></formula><p>gating network forward eligibility propagation:</p><formula xml:id="formula_12">?E ?? j,i = ?E ?c t j t j,i + ?H(g t j )</formula><p>?g t j ?? j,i (actual update signal) <ref type="bibr" target="#b17">(18)</ref> rec,g,t j,i</p><formula xml:id="formula_13">= (1 ? g t j ) t?1 j,i + ? c t?1 i (r t j ? c t?1 j )</formula><p>(recurrent weights-specific elig.) <ref type="bibr" target="#b18">(19)</ref> in,g,t j,i</p><formula xml:id="formula_14">= (1 ? g t j ) t?1 j,i + ? x t i (r t j ? c t?1 j ) (input weights-specific elig.) (20) b,g,t j,i = (1 ? g t j ) t?1 j,i + ? (r t j ? c t?1 j ) (bias weight-specific elig.)<label>(21)</label></formula><p>candidate network forward eligibility propagation:</p><formula xml:id="formula_15">?E ?? j,i = ?E ?r t j t j,i (actual update signal)<label>(22)</label></formula><p>rec,r,t j,i</p><formula xml:id="formula_16">= (1 ? g t j ) t?1 j,i + (1 ? r t j 2 )c t?1 i g t j</formula><p>(recurrent weights-specific elig.) <ref type="bibr" target="#b22">(23)</ref> in,r,t j,i</p><formula xml:id="formula_17">= (1 ? g t j ) t?1 j,i + (1 ? r t j 2 )x t i g t j (input weights-specific elig.) (24) b,r,t j,i = (1 ? g t j ) t?1 j,i + (1 ? r t j 2 )g t j (bias weight-specific elig.)<label>(25)</label></formula><p>Note that the partial derivatives ?E ??j,i in Equation <ref type="bibr" target="#b17">18</ref> and Equation 22 address the current time step t only. Also note that ?E ?c t j and ?E ?r t j carry true gradient information of the current time step, backprogated through the feedforward connections of the architecture. In contrast, original e-prop uses local approximations of the learning signal only, which does not allow to stack multiple layers without losing the exact local gradient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B LOCI ALGORITHM</head><p>Loci processes a sequence of RGB video-encoding images I ? R T ?H?W ?3 . Processing is mostly done slot-wise, whereby the system is initialized with a variable number of K processing slots. Its main components consist of a slot-wise encoder, a transition module, and a slot-wise decoder. Moreover, a background processing module is implemented. The slot-wise encoder is implemented by a tree-structured ResNet-based processing encoder (see <ref type="figure">Figure B</ref>.6). The transition module processes slot-wise temporal dynamics and between-slot interaction dynamics. The slot-wise decoder is again implemented by a ResNet (see <ref type="figure" target="#fig_5">Figure B.7)</ref>. For simple backgrounds, we use a Gaussian Mixture Model to obtain a default background estimateR bg , which is used for the whole training set. For more complex backgrounds we use an additional Auto-Encoder Module.</p><p>In the remainder, we denote scalar values by lower-case letters, tensors by upper-case letters, and vectors by bold letters. Slot-specific activities with a subscript k ? 1, .., K, while we denote time by the superscript t. We drop t for temporary values.</p><p>We now first define data and neural encodings sizes and types used throughout Loci's processing pipeline. We then specify neural activity initialization. Finally, we detail the unfolding overall processing loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 SLOT-WISE ENCODER</head><p>Inputs The encoder inputs at each time step t consist of: Outputs Based on these inputs the slot-wise encoder network generates latent codes, which are forwarded to the transition module:</p><formula xml:id="formula_18">? RGB input image I t ? R H?W ?3 , ? MSE map E t ? R H?W ?1 , ? Slot-specific RGB image reconstructions R t k ? R H?W ?3 , ? Slot-specific mask predictions M t k ? R H?W ?1 , ? Slot-</formula><p>? Slot-specific position codes P t k ? R 4 encode an isotropic Gaussian (? t k , ? t k ) and a slotpriority code z t k ? Slot-specific Gestalt codes G t k ? R Dg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 TRANSITION MODULE</head><p>A transition module is used to refine these slot-codes and create a prediction for the next state, which is fed into the decoder. The input to the transition module equals P t , G t , which is processed across slots and per slot in the respective layers: Multi-Head Attention processes slot interactions (over slots), while GateL0RD processes slot-specific dynamics (per slot). In our main CATER implementation we use five attention layers with ten heads each with GateL0RD layers in between.</p><p>Outputs The output of the transition module has the same coding size as the input fed into it. Additionally, recurrent, slot-respective hidden states? t k are maintained in the time-recurrent GateL0RD layers:</p><p>? Slot-specific position codesP t ? R 4 ,</p><p>? Slot-specific Gestalt codes? t ? R Dg ,</p><p>? Slot-specific GateL0RD-layer-respective hidden states? t ? R D h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 SLOT-WISE DECODER</head><p>Inputs The outputs from the transition moduleP t and? t then act as the input to the decoder.</p><p>Outputs The output of the decoder includes the slot-respective masks and RGB reconstructions:</p><formula xml:id="formula_19">? Slot-specific mask outputsM t k ? R H?W , ? Slot-specific RGB image reconstructionsR t k ? R H?W ?3 ,</formula><p>which are then used as part of the input at the next iteration as specified above, that is,</p><formula xml:id="formula_20">M t k ? M t+1 k andR t k ? R t+1 k .</formula><p>We generate the combined reconstructed image R t by summing all slot estimatesR t?1 k and the background estimateR bg weighted with their corresponding masksM t?1 k andM t?1 bg , as specified further in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 SEQUENCE INITIALIZATION</head><p>At time step t = 0 we determine the network's inputs based on randomly generated, fictive position and Gestalt estimates for each slot k at time t = ?1, which are then fed through the Loci decoder module. Initial position and Gestalt codesP ?1 k and? ?1 k are sampled from an isotropic Gaussian distribution N P osition (? p , ? p ) and a factorized Gaussian distribution N Gestalt (? g , ? g ) with learnable parameters ? p ? R 3 , ? p ? R and ? g ? R Dg , ? g ? R Dg , respectively. The third position code value ofP ?1 k , that is, the Gaussian standard deviation? ?1 k , is initialized to zero, but lower bounded by 1/width via the algorithm cf. Line 17 in Algorithm1, that is, the distance between two neighboring pixels. The fourth position code value, that is, the priority value? ?1 k , is set to its index value? ?1 k ? k, inducing an ordered priority, which biases initial random slot assignments and thus bootstraps initial slot-assignment progress.</p><p>Based on the initial codes, we generate estimates of the output maskM ?1 k , reconstructionR ?1 k and Gaussian positions Q ?1 k by calling the Loci slot-wise decoder (see Algorithm 1 line 19 and following):</p><formula xml:id="formula_21">M ?1 k ,R ?1 k , Q ?1 k ? SlotW iseDecoder(P ?1 k ,? ?1 k ).</formula><p>We finally determine the first RGB image reconstructionR ?1 .</p><p>The hidden states of the recurrent neural network GateL0RD are initialized to zero, that is,? 0 ? 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 MAIN PROCESSING LOOP</head><p>Loci first runs the main processing loop for ten time steps with the first video image given a particular image sequence. It thereby bootstraps the objects into individual slots, somewhat similar to previous slot-attention approaches <ref type="bibr" target="#b53">[54]</ref>. After the ten initial time steps, Loci keeps re-initializing positions P t k of a slot k to random values (as specified above) given that max(M t k ) has been smaller than 0.5 for all time points until t. This induces an initial active search process. For longer video sequences, such as the Aquarium footage, this search process was also used for invisible slots, which increased the number of tracked objects, but negatively influenced the object permanence.</p><p>Note that the prediction error is calculated across the three RGB channels. It determines the MSE between the input I t and the static background R bg , multiplied per Hadamard-product with the forth square root MSE between the input I t and the predicted input R t?1 . The fourth square root emphasizes small differences, encouraging accurate encodings of individual entities.</p><p>In the transition module we apply a single trainable parameteric bias neuron alpha, as proposed in Bachlechner et al. <ref type="bibr" target="#b0">[1]</ref>, instead of layer-normalization. Alpha is initialized to zero. Its current value is multiplied with the output vector before the residual parts of the transition module. These alpha-residuals enforce the predictor to initially compute the identity function. The mechanism bootstraps Loci's learning progress by initially focusing it on developing decoder-suitable Gestalt encodings. </p><formula xml:id="formula_22">E t ? Mean (I t ? R bg ) 2 , axis = 'rgb' ? 4 Mean I t ?R t?1 2 , axis = 'rgb' 8: R t k , M t k , Q t k ?R t?1 k ,M t?1 k , Q t?1 k 9:</formula><p># Slot-wise encoder: <ref type="bibr">10:</ref> S t k ? Encoder T runk (I t , E t , R t k , M t k , M t,s k , Q t k , Q t,s k , M t bg )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>G t k ? Encoder Gestalt (S t k )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>P t k ? Encoder P osition (S t k ) 13:</p><formula xml:id="formula_23">P t k ? Cat [? t k , ? t k , z t k ] ? Cat [Encoder ? (P t k )</formula><p>, Encoder ? (P t k ), Encoder z (P t k )]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p># Transition module: 15:? t ,P t , H t = T ransitionM odule(G t , P t , H t?1 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p># Gestalt binarization: 17:? t ? Sigmoid(? t ) 18:? t ?? t +? t (1 ?? t )N (0, 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p># Slot-wise decoder: <ref type="bibr">20:</ref> # p encodes all pixel positions normalized to [?1, 1]; <ref type="bibr">21:</ref> # width denotes number of pixels in a row, inducing a lower bound of one pixel distance <ref type="bibr">22:</ref> Q k ? exp Here we take potential negative social impacts of Loci into consideration. Orientated on the NeurIPS Ethics Guidelines we identified two topics where Loci could potentially be used in an harmful way:</p><p>Could Loci directly facilitate injury to living beings? At its current stage it is unlikely that Loci could directly be used in any weapons systems. However, when developing Loci further, its unsupervised nature allows training on huge amounts of unlabeled data and then fine tune a weapons systems to identify and track a specific target.</p><p>Could Loci help develop or extend harmful forms of surveillance? Potentially yes; Loci already works well with static cameras and backgrounds. While we did not evaluate Loci's performance on pedestrians, it is likely that Loci could be trained to track pedestrians in a surveillance setting. Supervised fine-tuning could then potentially be used to identify a specific target group of people.</p><p>While these potential misuses of Loci are concerning, they are an unavoidable byproduct of advancing the field of (unsupervised) object tracking as a whole. Seeing that current object tracking systems are mostly black boxes, particularly when it comes to deciphering how and why they track certain entities, Loci may enable better control over what is tracked and where exactly tracking may be applied. Hopefully, Loci can thus be used to mitigate unwanted tracking biases, or, at least, to facilitate the identification of such tracking biases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Eight sources of information are fed into each slot-wise encoder (left side): top row: the current RGB image I t , the reconstructed RGB slot image R t k , the Gaussian slot position map Q t k , as well as the summations over other Gaussian position maps Q t,s k ; bottom row: the MSE E t , slot mask M t k , as well as the summations over other masks M t,s k , and the background mask M bg . The background module (right side) is either implemented as a Gaussian mixture model or a simple autoencoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Two challenging CATER tracking examples with several co-occurring containments / occlusions: Video frames overlaid with the predicted Snitch location (network trained with supervision).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison between unsupervised (left) and supervised (right) tracking, while the Snitch is contained progressively longer. On the x-axis are the number of frames the Snitch has been contained, while the left y-axis shows the L 2 distance to the target position. The right y-axis shows the number of times a Snitch was contained that long within the test-set (visualized in green). The thick line represents the median L 2 distance, while the thin line represents the mean L 2 distance. The weaker colored area shows the 90-10-quantile, while the stronger one corresponds to the 75-25-quantile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Comparison between ground truth top row, PhyDNet center row and Loci bottom row for a prediction of up to 90 frames. Both PhyDNet and Loci were trained on 10 frame prediction. While in PhyDNet, the appearance of the digits dissolves after a few frames beyond the initial training distribution, Loci manages to keep the Gestalt code and the location accurate until the fourth collision at around frame 42. The Gestalt codes remain stable until the end of the considered 100 time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Fully unsupervised real world tracking example trained on 10 hour aquarium footage. Predicting CLEVRER[85] 70 time-steps in closed loop after 50 times-steps of teacher forcing. Top row is ground truth wile bottom row shows Loci's closed loop predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Comparison between GateL0RD and LSTM modules within the predictor part of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Comparison between different sequence lengths for truncated back-propagation trough time vs back-propagation and e-prop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Ablation the importance of different input channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Ablation the importance of different regularization losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Top: Exhaustive evaluation of effects on the metrics resulting from different noise intensities added to the gestalt and position codes. Bottom: Explicit visualization of the bottom-most row (manipulating position only, blue), left-most column (manipulating gestalt only, orange), and diagonal from bottom left to top right in the plots at the top (manipulating both position and gestalt, To study the disentanglement of the different parts of our learned codes, we perform a lesion study. The principle of the study is as follows: For each component code (position, priority, size, and gestalt) we add gaussian noise to the code before applying the decoder while keeping all other codes as predicted by the model. x t i + b g j + N (0, ?) (gating network state) (10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>i = (1 ? g t j ) t?1 j,i + ?c t j ?? j,i (cell-specific eligibility value)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>specific sums of the other slots' mask predictions M t,s k = k ?{1,..,K}\k M t k ? Slot-specific isotropic Gaussian position maps Q t k ? R H?W , ? Slot-specific sums of the other slots' Gaussian position maps Q t,s k = k ?{1,..,K}\k Q t k ? Background mask M t bg ? R H?W , which is determined via 1 ? k?{1,..,K} M t k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>decode k ? Priority-Based-Attention(? t k , Q k ,?) 24:R t k ,M t k ? Decoder T runk (decode k ) t ? Softmax(Cat[M t 0 . . .M t k ,M bg ], axis = 'K') 27:R t ? Sum(Cat R t 1 , ..,R t K ,R bg ?M t ,axis = 'K') 28: end for 29: return [R 0 . . . R T ] x, y = sum(map0 * grid) Sigmoid(feature_map1 -10) sigma = sum(map1) * alpha TanH(feature_map2) z = sum(map2) Encoder architecture diagram BDecoder architecture diagram C POTENTIAL NEGATIVE SOCIETAL IMPACTS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of the CATER Snitch localization challenge. Referenced results are from<ref type="bibr" target="#b87">[88]</ref> and from the other system-respective papers. Object permanence shown on a CATER tracking example. While the network (after completely unsupervised training) struggles to keep the shape of the Snitch when contained for a longer time span, color and, importantly, the position of the Snitch are preserved during containment. Top row: input frames. Middle row: tracked objects visualized trough colored masks. Bottom row: RGB representation of the Snitch. location and Gestalt codes are computed for the whole dataset and extracted into a separate data file, from which the classifier module is trained, again using a 70:30 train/test split.</figDesc><table><row><cell>Method</cell><cell>Parameters (M)</cell><cell>Top 1</cell><cell>Top 5</cell><cell>L1 (grid)</cell><cell>L2</cell></row><row><cell>Random</cell><cell>-</cell><cell>2.8</cell><cell>13.8</cell><cell>3.9</cell><cell></cell></row><row><cell>Transformer [75]</cell><cell>15.01</cell><cell>13.7</cell><cell>39.9</cell><cell>3.53</cell><cell></cell></row><row><cell>SINet [56]</cell><cell>138.69</cell><cell>21.1</cell><cell>47.1</cell><cell>3.14</cell><cell></cell></row><row><cell>TSN (RGB) + LSTM [80]</cell><cell></cell><cell>25.6</cell><cell>67.2</cell><cell>2.6</cell><cell></cell></row><row><cell>DaSiamRPN [89]</cell><cell></cell><cell>33.9</cell><cell>40.8</cell><cell>2.4</cell><cell></cell></row><row><cell>I3D-50 + LSTM [16]</cell><cell></cell><cell>60.2</cell><cell>81.8</cell><cell>1.2</cell><cell></cell></row><row><cell>Hopper-transformer [88]</cell><cell>15.01</cell><cell>61.1</cell><cell>86.6</cell><cell>1.42</cell><cell></cell></row><row><cell>TSM-50 [47]</cell><cell></cell><cell>64.0</cell><cell>85.7</cell><cell>0.93</cell><cell></cell></row><row><cell>TPN-101 [84]</cell><cell></cell><cell>65.3</cell><cell>83.0</cell><cell>1.09</cell><cell></cell></row><row><cell>Hopper-sinet [88]</cell><cell>139.22</cell><cell>69.1</cell><cell>91.8</cell><cell>1.02</cell><cell></cell></row><row><cell>Inferno [17]</cell><cell>-</cell><cell>71.7</cell><cell>88.9</cell><cell>-</cell><cell></cell></row><row><cell>Hopper-multihop [88]</cell><cell>6.39</cell><cell>73.2</cell><cell>93.8</cell><cell>0.85</cell><cell></cell></row><row><cell>Aloe [22]</cell><cell></cell><cell>74.0</cell><cell>94.0</cell><cell>0.44</cell><cell></cell></row><row><cell>OPNet [68]</cell><cell>-</cell><cell>74.8</cell><cell>-</cell><cell>0.54</cell><cell></cell></row><row><cell>TFC V3D Depthwise [87]</cell><cell>24.64</cell><cell>79.7</cell><cell>95.5</cell><cell>0.47</cell><cell></cell></row><row><cell>Loci supervised (ours)</cell><cell>2.96</cell><cell>90.7</cell><cell>98.5</cell><cell>0.14</cell><cell>0.14</cell></row><row><cell>Loci unsupervised (ours)</cell><cell>4.14</cell><cell>78.4</cell><cell>92.0</cell><cell>0.45</cell><cell>0.44</cell></row><row><cell>frame 191</cell><cell>frame 194</cell><cell>frame 287</cell><cell></cell><cell>frame 291</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Moving MNIST prediction accuracy PhyDNet vs Loci. Both networks where trained using the same dataloader to predict the next 90 frames given an input sequence of 10 frames.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">T = 10</cell><cell></cell><cell></cell><cell cols="2">T = 20</cell><cell></cell><cell cols="2">T = 30</cell><cell></cell><cell cols="2">T = 50</cell><cell></cell><cell cols="2">T = 100</cell></row><row><cell cols="2">Method</cell><cell></cell><cell>MSE</cell><cell cols="2">SSIM</cell><cell cols="2">MSE</cell><cell>SSIM</cell><cell cols="2">MSE</cell><cell cols="2">SSIM</cell><cell>MSE</cell><cell cols="2">SSIM</cell><cell>MSE</cell><cell>SSIM</cell></row><row><cell cols="3">PhyDnet [34]</cell><cell>35.6</cell><cell cols="2">0.913</cell><cell cols="2">58.6</cell><cell>0.830</cell><cell></cell><cell>80.7</cell><cell cols="7">0.739 113.9 0.594 155.1 0.484</cell></row><row><cell>Loci</cell><cell></cell><cell></cell><cell>30.7</cell><cell cols="2">0.923</cell><cell cols="2">52.5</cell><cell>0.881</cell><cell></cell><cell>98.3</cell><cell cols="7">0.814 165.1 0.720 221.2 0.639</cell></row><row><cell>t=10</cell><cell>t=11</cell><cell>t=12</cell><cell>t=14</cell><cell>t=16</cell><cell>t=18</cell><cell>t=20</cell><cell>t=23</cell><cell>t=26</cell><cell>t=29</cell><cell>t=33</cell><cell>t=37</cell><cell>t=42</cell><cell>t=48</cell><cell>t=58</cell><cell>t=68</cell><cell>t=78</cell><cell>t=88 t=100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluations on the CATER Snitch challenge for supervised training.Table showsfive networks trained with different initial seeds, each evaluated five times with different initial seeds.</figDesc><table><row><cell>Network</cell><cell>Top 1</cell><cell>Top 5</cell><cell>L1 (grid)</cell><cell>L2</cell></row><row><cell>1</cell><cell>91.7</cell><cell>99.0</cell><cell>0.118</cell><cell>0.122</cell></row><row><cell></cell><cell>90.8</cell><cell>98.8</cell><cell>0.129</cell><cell>0.126</cell></row><row><cell></cell><cell>91.1</cell><cell>98.8</cell><cell>0.119</cell><cell>0.121</cell></row><row><cell></cell><cell>91.4</cell><cell>98.9</cell><cell>0.122</cell><cell>0.123</cell></row><row><cell></cell><cell>91.3</cell><cell>98.8</cell><cell>0.123</cell><cell>0.126</cell></row><row><cell>2</cell><cell>91.1</cell><cell>98.6</cell><cell>0.129</cell><cell>0.133</cell></row><row><cell></cell><cell>90.4</cell><cell>98.3</cell><cell>0.144</cell><cell>0.141</cell></row><row><cell></cell><cell>89.8</cell><cell>98.3</cell><cell>0.151</cell><cell>0.144</cell></row><row><cell></cell><cell>90.0</cell><cell>98.5</cell><cell>0.149</cell><cell>0.140</cell></row><row><cell></cell><cell>89.5</cell><cell>98.2</cell><cell>0.164</cell><cell>0.151</cell></row><row><cell>3</cell><cell>89.6</cell><cell>97.8</cell><cell>0.173</cell><cell>0.170</cell></row><row><cell></cell><cell>90.1</cell><cell>98.0</cell><cell>0.159</cell><cell>0.157</cell></row><row><cell></cell><cell>89.5</cell><cell>97.9</cell><cell>0.174</cell><cell>0.164</cell></row><row><cell></cell><cell>88.9</cell><cell>97.5</cell><cell>0.174</cell><cell>0.167</cell></row><row><cell></cell><cell>89.8</cell><cell>98.1</cell><cell>0.162</cell><cell>0.157</cell></row><row><cell>4</cell><cell>91.5</cell><cell>99.1</cell><cell>0.112</cell><cell>0.118</cell></row><row><cell></cell><cell>90.4</cell><cell>98.9</cell><cell>0.133</cell><cell>0.128</cell></row><row><cell></cell><cell>91.0</cell><cell>98.7</cell><cell>0.125</cell><cell>0.129</cell></row><row><cell></cell><cell>90.5</cell><cell>99.1</cell><cell>0.125</cell><cell>0.124</cell></row><row><cell></cell><cell>90.8</cell><cell>98.8</cell><cell>0.127</cell><cell>0.126</cell></row><row><cell>5</cell><cell>91.7</cell><cell>98.2</cell><cell>0.130</cell><cell>0.139</cell></row><row><cell></cell><cell>91.4</cell><cell>98.4</cell><cell>0.135</cell><cell>0.139</cell></row><row><cell></cell><cell>91.2</cell><cell>98.0</cell><cell>0.146</cell><cell>0.150</cell></row><row><cell></cell><cell>91.8</cell><cell>98.3</cell><cell>0.130</cell><cell>0.140</cell></row><row><cell></cell><cell>91.3</cell><cell>98.1</cell><cell>0.150</cell><cell>0.151</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluations on the CATER Snitch challenge for supervised training, statistics of the 25 evaluations presented inTable 3</figDesc><table><row><cell>Metric</cell><cell>min</cell><cell>mean</cell><cell>std</cell><cell>max</cell></row><row><cell>Top1</cell><cell>88.9</cell><cell>90.7</cell><cell>0.8</cell><cell>91.8</cell></row><row><cell>Top5</cell><cell>97.5</cell><cell>98.5</cell><cell>0.4</cell><cell>99.1</cell></row><row><cell>L1 (grid)</cell><cell>0.112</cell><cell>0.140</cell><cell>0.019</cell><cell>0.174</cell></row><row><cell>L2</cell><cell>0.118</cell><cell>0.139</cell><cell>0.015</cell><cell>0.170</cell></row><row><cell cols="2">A.3.3 REGULARIZATION LOSSES</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Evaluations of Loci's foreground segmentation masks. Trained networks from the Cater challenge are evaluated on CLEVR with mask by running the single CLEVR images 30 iterations through the network and then comparing the masks using the Intersecion over Union (IoU) metric. We compare a mean per mask accuracy (mask avg) and size weighted average that represents how many pixels where segmented correctly (pixel avg).</figDesc><table><row><cell>Network</cell><cell>mask avg (%)</cell><cell>pixel avg (%)</cell></row><row><cell>1</cell><cell>83.3</cell><cell>96.8</cell></row><row><cell>2</cell><cell>84.6</cell><cell>97.0</cell></row><row><cell>3</cell><cell>84.3</cell><cell>97.1</cell></row><row><cell>4</cell><cell>84.1</cell><cell>96.7</cell></row><row><cell>5</cell><cell>85.6</cell><cell>97.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Algorithm 1 LOCI-Algorithm (main processing loop) 1: Inputs: Input video I ? R T ?H?W ?3 , static backgroundR bg ? R H?W ?3 2: Network parameters: ? encoder , ? transition , ? decoder 3: Additional parameters: initialization parameters ? init ; background threshold ? bg , which is encoded as a uniform offset maskM bg ? ? bg</figDesc><table><row><cell cols="2">4: Initialize H ?1 k ,R ?1 k ,R ?1 ,M ?1 k , Q ?1 k # see Section B.4 for details</cell></row><row><cell cols="2">5: for t = 0 . . . T ? 1 do</cell></row><row><cell>6:</cell><cell># Pre-processing:</cell></row><row><cell>7:</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source Code: https://github.com/CognitiveModeling/Loci</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Aquarium footage on YouTube: https://www.youtube.com/watch?v=9Ej-0VRWmI8&amp;t= 32347s</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Manuel Traub and Matthias Karlbauer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rezero is all you need: Fast convergence at large depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1352" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object permanence in fivemonth-old infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?e</forename><surname>Baillargeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">S</forename><surname>Spelke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Wasserman</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0277(85)90008-3</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="191" to="208" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How does the mind render streaming experience as events?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Dare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">E</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kosie</surname></persName>
		</author>
		<idno type="DOI">10.1111/tops.12502</idno>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="79" to="105" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perceptual symbol systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="577" to="600" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstraction in perceptual symbol systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos Trans R Soc Lond B Biol Sci</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="1177" to="1187" />
			<date type="published" when="1435-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Scherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Hajek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darjan</forename><surname>Salaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09049</idno>
		<title level="m">Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Unsupervised scene decomposition and representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Monet: Unsupervised scene decomposition and representation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1901.11390" />
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards a unified sub-symbolic computational theory of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">V</forename><surname>Butz</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2016.00925</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">925</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">How the Mind Comes Into Being: Introducing Cognitive Science from a Functional and Computational Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><forename type="middle">F</forename><surname>Butz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Event-predictive cognition: A root for conceptual human thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">V</forename><surname>Butz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asya</forename><surname>Achimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bilkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Knott</surname></persName>
		</author>
		<idno type="DOI">10.1111/tops.12522</idno>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="10" to="24" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inferno: Inferring object-centric 3d scene representations without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00341</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Transformer tracking. CoRR, abs/2103.15436, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.15436" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatially invariant unsupervised object detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33013412</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33013412" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI&apos;19/IAAI&apos;19/EAAI&apos;19</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI&apos;19/IAAI&apos;19/EAAI&apos;19</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Object-based attention for spatiotemporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective attention and the organization of visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-3445.113.4.501</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="501" to="517" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relate: Physically plausible multi-object scene synthesis using structured latent spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11202" to="11213" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Savi++: Towards end-to-end object-centric learning from real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gamaleldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kipf</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.07764" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">GENESIS: generative scene inference and sampling with object-centric latent representations. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oiwi</forename><forename type="middle">Parker</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.13052" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1603.08575</idno>
		<ptr target="http://arxiv.org/abs/1603.08575" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structured event memory: A neuro-symbolic model of event cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charan</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000177</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="33" to="295" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Print</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">CATER: A diagnostic dataset for compositional actions and temporal reasoning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.04744" />
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-object representation learning with iterative variational inference. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.00450" />
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the binding problem in artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05208</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vincent Le Guen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparsely changing latent states for prediction and planning in partially observable domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Gumbsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Butz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reasoning about physical interactions with object-oriented prediction and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10972</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generative neurosymbolic machines. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<idno>abs/2010.12152</idno>
		<ptr target="https://arxiv.org/abs/2010.12152" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attractor dynamics and parallelism in a connectionist sequential machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Eighth Annual Meeting of the Cognitive Science Society<address><addrLine>Erlbaum, Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="531" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Conditional object-centric learning from video. CoRR, abs/2111.12594</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gamaleldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greff</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.12594" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Principles of Gestalt psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koffka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1935</biblScope>
			<pubPlace>Routledge, Abingdon, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sequential attend, infer, repeat: Generative modelling of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>abs/1806.01794</idno>
		<ptr target="http://arxiv.org/abs/1806.01794" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<meeting><address><addrLine>Red Hook; NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
		<idno>doi: 10.1017/ S0140525X16001837</idno>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised joint learning of depth, optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2105.14520</idno>
		<ptr target="https://arxiv.org/abs/2105.14520" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skand</forename><surname>Vishwanath Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2001.02407" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tracking without re-recognition in humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girik</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ennio</forename><surname>Lakshmi Narasimhan Govindarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mingolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/a2557a7b2e94197ff767970b67041697-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19473" to="19486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The emergence of objectness: Learning zero-shot segmentation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13137" to="13152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<idno>abs/1811.12359</idno>
		<ptr target="http://arxiv.org/abs/1811.12359" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/8511df98c02ab60aea1b2356c013bc0f-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11525" to="11538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11525" to="11538" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attend and interact: Higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Thought before language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">M</forename><surname>Mandler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2004.09.004</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="508" to="513" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Trackformer: Multi-object tracking with transformers. CoRR, abs/2101.02702, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2101.02702" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Trackformer: Multi-object tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="8844" to="8854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The Perception of Causality. Louvain, Ed. de l&apos;Institut Sup?rieur de Philosophie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michotte</surname></persName>
		</author>
		<editor>C. A. Mace</editor>
		<imprint>
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Object vision and spatial vision: Two cortical pathways</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mortimer</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">A</forename><surname>Macko</surname></persName>
		</author>
		<idno type="DOI">10.1016/0166-2236(83)90190-X</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="414" to="417" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Blockgan: Learning 3d object-aware scene representations from unlabelled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thu H Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6767" to="6778" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Giraffe: Representing scenes as compositional generative neural feature fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11453" to="11464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">The Book of Why -The new science of cause and effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Basic Books</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><forename type="middle">H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
		<idno type="DOI">10.1038/4580</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1999-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Towards causal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/2102.11107</idno>
		<ptr target="https://arxiv.org/abs/2102.11107" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning object permanence from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofri</forename><surname>Kleinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Event representations and predictive processing: The role of the midline default network core</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stawarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Bezdek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
		<idno type="DOI">10.1111/tops.12450</idno>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="164" to="186" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The binding problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Treisman</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0959-4388(96)80070-5</idno>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="178" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Haxby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="65" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>what&quot; and &quot;where&quot; in the human brain</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Investigating object compositionality in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="309" to="325" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Binding in models of perception and brain function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr Opin Neurobiol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="520" to="526" />
			<date type="published" when="1995-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Towards causal generative scene models via competition of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Julius Von K?gelgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12906</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A century of gestalt psychology in visual perception: I. perceptual grouping and figure-ground organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Wagemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kubovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?diger Von Der</forename><surname>Heydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1172" to="217" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A century of gestalt psychology in visual perception: Ii. conceptual and theoretical foundations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Wagemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Gepshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Kimchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Pomerantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Van Der Helm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1218" to="52" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Benchmarking unsupervised object representations for video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander S</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="183" to="184" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01442</idno>
		<title level="m">Clevrer: Collision events for video representation and reasoning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Event perception: A mind-brain perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><forename type="middle">K</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khena</forename><forename type="middle">M</forename><surname>Swallow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">S</forename><surname>Braver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">R</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="293" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05928</idno>
		<title level="m">Tfcnet: Temporal fully connected networks for static unbiased temporal reasoning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hopper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10574</idno>
		<title level="m">Multi-hop transformer for spatiotemporal reasoning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">1: Inputs: Gestalt: G k ? R 1,Dg , Gaussian 2d position: Q k ? R H ?W ?1 , priority: z ? R K 2: Additional parameters: values of the learnable ? w ? R K are initially set 25, while ? b ? R K = {0, 1, . . . , (K ? 1)} induces a default slot-order bias. 3: z ? (z?K+N (0, 0.1)+? b )?? w # Scale priorities and add Noise 4: # Subtract Gaussian attention from other slots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
	<note>Distractor-aware siamese networks for visual object tracking. scaled by priority (? denotes the sigmoid) 5: Q k ? max(0, Q k ? k ?{1,...,K}\k ?</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
