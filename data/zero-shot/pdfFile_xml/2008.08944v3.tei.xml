<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localizing Anomalies from Weakly-Labeled Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanwei</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Localizing Anomalies from Weakly-Labeled Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly Detection</term>
					<term>Anomaly Localization</term>
					<term>Weak Supervision</term>
					<term>Traffic Anomaly Dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video anomaly detection under video-level labels is currently a challenging task. Previous works have made progresses on discriminating whether a video sequence contains anomalies. However, most of them fail to accurately localize the anomalous events within videos in the temporal domain. In this paper, we propose a Weakly Supervised Anomaly Localization (WSAL) method focusing on temporally localizing anomalous segments within anomalous videos. Inspired by the appearance difference in anomalous videos, the evolution of adjacent temporal segments is evaluated for the localization of anomalous segments. To this end, a high-order context encoding model is proposed to not only extract semantic representations but also measure the dynamic variations so that the temporal context could be effectively utilized. In addition, in order to fully utilize the spatial context information, the immediate semantics are directly derived from the segment representations. The dynamic variations as well as the immediate semantics, are efficiently aggregated to obtain the final anomaly scores. An enhancement strategy is further proposed to deal with noise interference and the absence of localization guidance in anomaly detection. Moreover, to facilitate the diversity requirement for anomaly detection benchmarks, we also collect a new traffic anomaly (TAD) dataset which specifies in the traffic conditions, differing greatly from the current popular anomaly detection evaluation benchmarks. 1 Extensive experiments are conducted to verify the effectiveness of different components, and our proposed method achieves new state-of-the-art performance on the UCF-Crime and TAD datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Anomaly detection, which aims to recognize those behaviors or appearance patterns that do not conform to usual patterns <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, is of great importance for the alarm of potential risks or dangers. With the large-scale deployment of surveillance, an urgent requirement of intelligent systems is to automatically filter out possibly abnormal events.</p><p>Anomaly detection is typically tackled under constrained supervision that only normal data or limited annotations are provided in the training phase. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. As anomalous events rarely happen in real-life situations, which brings in the scarcity of annotations, several methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> have been proposed to model the shared pattern among normal videos in the training phase and detect the outliers as anomalies during testing. However, these methods often fail in identifying anomalies when facing complicated or unseen <ref type="figure">Fig. 1</ref>. Anomaly localization comparisons. Left: A comparison of Burglary case on UCF-Crime (x-axis corresponds to frames and y-axis corresponds to the anomaly score.). Groundtruth is shown in the top-left, following by three methods: Sultani et al. <ref type="bibr" target="#b3">[4]</ref>, Zhong et al. <ref type="bibr" target="#b4">[5]</ref> and ours. Right: ROC curves of frame-level anomaly localization on all anomaly videos scenes. Recently, researchers <ref type="bibr" target="#b3">[4]</ref> select to leverage the videolevel labels for developing robust anomaly detectors. The release of UCF-Crime dataset <ref type="bibr" target="#b3">[4]</ref> activates this direction which encourages the detectors to take the best of the weak signals of video-level. Although a large gain has been observed in this domain <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, it still lacks an efficient way to temporally localize anomalous frames.</p><p>In previous methods, the performances on the overall test set are calculated and reported as the evaluation results. However, in this case the temporal anomaly localization capability of detectors is somewhat unrevealed. Since the whole test set contains both the normal and anomaly videos, the superior performance on normal videos conceals the poor accuracy of anomaly localization within anomalous videos. To reveal the problem therein, we conduct statistic analysis on the anomaly data of UCF-Crime test set. ROC curves of two stateof-the-art (SOTA) methods, as well as ours, are plotted in <ref type="figure">Figure 1</ref>. The details of corresponding metrics can be found in Section IV-B. A test sample (video name: Burglary079) is also shown in the left part of the figure. We can find that the localization accuracy of the two methods on anomalous videos are 54.25% and 59.02% respectively, in term of AUC. It is worth mentioning that an AUC of 50% can be obtained by random binary prediction of anomalies. To sum up, there exists a large space for improving the temporal localization of anomalies.</p><p>To facilitate the localization property of anomaly detection, we propose a Weak-Supervised Anomaly Localization (WSAL) method to detect anomalies with video-level labels. In our WSAL model, we investigate into two aspects of the anomaly, which are the semantic and context. The anomalies are defined as the uncommon activities that differ from the usual pattern. Thus, the extracted semantics can act as a direct cue to infer anomalies. Based on this point, existing methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> treat each video as frame-by-frame images or direct optical flows and extract fine-grained semantic representations for further anomaly detection. While in this manner, the temporal evolution across consecutive frames is not adequately exploited. For example, in the long temporal domain, a sudden change of the dynamic variation uncovers the anomaly itself.</p><p>On the other hand, owing to rough supervisory signals of video level, anomaly detectors are prone to false alarms or missed detection. For instances, the drastic environment changes as well as noise interruptions caused by hardware failure may lead to unwanted high probabilities from anomaly detectors. These influences in long and untrimmed videos ought to be suppressed or excluded from the anomalies. Toward this end, we put forward a noise stimulation strategy to tackle inevitable interference lying in untrimmed videos, whose quality can not be guaranteed. Moreover, we introduce hand-crafted anomalies, similar to actual anomalies, to provide pseudo location signals as guidance for the model learning process. Above two strategies make up for our enhancement strategy to boost the weakly-supervised learning and strengthen the robustness of anomaly detection. Thoroughly, we equip raw video data with the augments of video noises and hand-crafted anomalies. As a consequence, the weak labels are expanded with pseudo location signals as auxiliary.</p><p>So far, there are few datasets available for anomaly detection, most of them are with small-scale or constrained scenarios, like UCSD Peds <ref type="bibr" target="#b9">[10]</ref>, Avenue <ref type="bibr" target="#b10">[11]</ref>, ShanghaiTech <ref type="bibr" target="#b7">[8]</ref>, and Street Scene <ref type="bibr" target="#b11">[12]</ref>. Also, these datasets are initially used for semi-supervised anomaly detection with normal training samples. For the problem under video-level scenario, only UCF-Crime <ref type="bibr" target="#b3">[4]</ref> dataset is now available publicly to our knowledge. Thus, we build a new large-scale traffic anomaly detection (TAD) dataset with long surveillance videos under traffic scene. The proposed dataset consists of realistic anomalies on roads with various appearance and motion pattern, which facilitates the diversity requirement for anomaly detection benchmarks. In addition, we implement and compare different SOTA anomaly detection approaches on the UCF-Crime and our TAD dataset. We hope the newly collected benchmark will boost the development of anomaly detection in research domain and real-life application. The main contributions of this paper are as follows: 1) Deeply delving into anomaly detection, we propose a weak-labeled anomaly localization method, in which we employ a high-order context encoding model to encode temporal variations as well as high-level semantic information for weak-supervised anomaly detection; 2) We introduce a weak-supervision enhancement strategy by stimulating video noises and building virtual indicative locations to suppress or exclude those interruption of false-anomaly signals; 3) We build a new weak-labeled traffic anomaly detection dataset with extensive benchmark tests, and report the new SOTA results on the proposed TADdataset as well as the UCF-Crime dataset.</p><p>The rest parts of the paper are organized as follows: In Section II, we review the literature of anomaly detection in surveillance videos. In Section III we introduce the proposed WSAL method in details. In Section IV, we conduct experiments to compare our proposed method with other SOTA methods as well as elaborated ablation studies to fully analyze different components. Finally, we conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The techniques of anomaly detection in surveillance videos have long been developed as a tool for mining unusual patterns in videos <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>. The family can be divided into two categories, based on how and how much supervision is accessible. The details are discussed in the following.</p><p>Video anomaly detectors are originally designed in an unsupervised manner <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> that only normal samples are available in the training phase without any labels. They first involve modeling normal behavior and then detecting samples that deviate from it. Motion trajectory, as one of common basic factors, has been utilized to detect anomalies in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Although such methods can be easily implemented and have a fast execution speed, tracking is prone to failure in crowded or cluttered scenes. An alternative approach is to tackle the original task as a problem of novelty detection, e.g., sparse coding <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, distance-based methods <ref type="bibr" target="#b23">[24]</ref>, the mixture of dynamic models on texture <ref type="bibr" target="#b24">[25]</ref> and the mixture of probabilistic PCA <ref type="bibr" target="#b25">[26]</ref>. These models are generally built on the low-level features (e.g., a histogram of oriented gradients (HOG) and the histogram of oriented flows (HOF)) extracted from densely sampled image patches. There are also works that improve the tradition approach into VAD, such as in <ref type="bibr" target="#b26">[27]</ref>, the authors proposes a spatial localization constrained sparse coding approach for anomaly detection in traffic scenes, which fuses these two aspects of motion orientation and magnitude to obtain a robust detection result. Several recent approaches have investigated the learning-based features using autoencoders <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which minimize reconstruction errors on the normal patterns in the training process. Shi et al. <ref type="bibr" target="#b29">[30]</ref> have proposed to modify original LSTM with ConvLSTM and used it for precipitation forecasting. Liu et al. <ref type="bibr" target="#b6">[7]</ref> have designed a future prediction network to infer the coming frames and detect anomalies according to the quality of predicted frames. Despite the advances in developing unsupervised anomaly detection approaches, these detectors are easily to fall down when dealing with complicated or unseen environments.</p><p>Recently, great advance has been witnessed in weak supervision, for example in <ref type="bibr" target="#b30">[31]</ref>, the authors introduce weak supervision into adversarial domain adaptation for improving the segmentation performance from synthetic data to real scenes. Inspired by them, various methods based on weak supervision situation have been introduced, they employ both normal and abnormal data along with video-level annotations for building robust anomaly detection model <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Among them, Multiple Instance Learning (MIL) is introduced for pattern modeling under weak supervision <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Sultani et al. <ref type="bibr" target="#b3">[4]</ref> consider anomaly detection as a MIL problem with a novel ranking loss function. Later, by extending it, Zhu et al. <ref type="bibr" target="#b5">[6]</ref> introduce the attention mechanism for better localizing anomalies. Due to the absence of anomaly positions in training phase, these two methods cannot predict anomaly frames well. For this, Zhong et al. <ref type="bibr" target="#b4">[5]</ref> attempt to construct supervised signals of anomaly positions through iteratively refining them. However these methods focus on predicting segment labels while neglecting modeling hidden temporal context information. Temporal or context aggregation technology has been widely adopted as in <ref type="bibr" target="#b33">[34]</ref> the authors adopt a volumetric structure to effectively synthesize spatiotemporal information of the same target from the current time and history frames to enhance detection. In <ref type="bibr" target="#b34">[35]</ref>, the authors propose two nuclear-and L2,1-norm regularized neighborhood preserving projection methods for extracting representative 2D image features. While in our work, we not only propose a high-order context encoding structure for temporal context aggregation but also modeling variations through the video sequences as a dynamic cue and incorporate it with semantic cues to better localize anomalies. Besides, we introduce a weak-supervision enhancement strategy to suppress false-anomaly signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head><p>In this section, we will introduce our Weak-Supervised Anomaly Localization (WSAL) method in details. We first give the basic formulation for the anomaly localization and core modules of our WSAL are elaborated thoroughly then.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formulation</head><p>The purpose of anomaly detection is to estimate the anomaly status of a video and localize the anomalies in the video sequence if exist. In the weakly supervised scenario, a video sequence X and its corresponding video-level annotation y ? {0, 1} are given, where the case 'y=1' means there exists anomaly in this sequence otherwise 'y=0' indicates that there is no anomaly in X . We start with dividing the entire video into several segments with equal lengths, denoted as X = (X 1 , X 2 , ? ? ? , X m ). The goal of video segmenting is to alleviate computation burden resulting from almost-repetitive video frames. For the i-th segment X i , we first use a classical convolutional network to extract features for each frame, and the segment feature x i is obtained by aggregating the features of all frames within the segment. As a consequence, the sequence X could now be represented by the m-tuple features (x 1 , x 2 , ? ? ? , x m ). We can now use this m-tuple to determine whether the current video contains any anomaly or not, in the manner that assigning each segment in the video with an anomaly score, indicating the probability of being anomalous.</p><p>To predict the state (normal or abnormal) of a video, we derive a novel function to describe the video by estimating the anomalous margin among a video, formally,</p><formula xml:id="formula_0">S(X ) = max i,j=1,...,m f (?(x i?k , . . . , x i , . . . , x i+k ), ?(x j?k , . . . , x j , . . . , x j+k )),<label>(1)</label></formula><p>where ? ? is a high-order function that encodes an anchored segment as well as its adjacent 2k segments in the temporal context. To mine the anomalies, we consider two aspects of information: spatial semantics and temporal variations. The function ? is modeled with a high-order dynamic regression to generate semantic features and predict variations within local window [?k, k]. Please see Section III-B for more details. ? f is a margin distance metric measuring the anomaly score margin between the segment position i and j. The more close the predicted anomaly scores are, the smaller the distance is. ? S(?) is the score of a video that computes the maximum relative distance of pairwise positions. The scores of normal videos are expected to be smaller than anomalous videos. Thus, the maximum-distance strategy constrains entire normal videos more smooth than anomaly videos, which complies with the conventional assumption. ? max function is chosen to capture the largest score margin, which can represent the extent of abnormalities in a video. Since anomaly scores are all close to zero in a normal video, leading to the score margin with a small value. While in an anomalous video, the anomalies, lying in normal background, will lead to a large score margin. Given a batch of training data {X <ref type="bibr" target="#b0">(1)</ref> , X <ref type="bibr" target="#b1">(2)</ref> , ? ? ? , X (n) } and the corresponding video labels {y <ref type="bibr" target="#b0">(1)</ref> , y <ref type="bibr" target="#b1">(2)</ref> , ? ? ? , y (n) }, we define a margin loss function as:</p><formula xml:id="formula_1">? 1 ({X (i) }| n i=1 ) = max {0, 1 ? 1 n 1 n i=1 [S(X (i) )|y (i) = 1] + 1 n 0 n j=1 [S(X (j) )|y (j) = 0] },<label>(2)</label></formula><p>here n 1 , n 0 are the total amounts of anomaly and normal samples. As the function only depends on video-level labels, the learning process belongs to the case of weak supervision. In addition, we augment training samples to generate two types of data: noise data {? (i) }|n i=1 and pseudo-location data {X (i) }|n i=1 , wheren andn are the amounts of pseudo samples. The former could help the detector reduce mis-judgement where some noised normal videos are predicted as anomaly labels, whilst the latter provides direct guidance to localize anomalous frames. Let</p><formula xml:id="formula_2">{X }| n i=1 = {? (i) }|n i=1 ? {X (i) }|n i=1</formula><p>denote all augmentation samples, where n =n +n. Finally, we derive the objective function to optimize as:</p><formula xml:id="formula_3">? = ? O ({X (i) }| n i=1 ) + ?? A ({X (i) }| n i=1 ),<label>(3)</label></formula><p>where ? is the balance factor between the original and the augmented data. Loss function ? O , defined on the original weak-labeled data, uses the margin loss ? 1 in Equation <ref type="formula" target="#formula_1">(2)</ref>, and the details will be listed in Section III-B. Loss ? A is imposed on noise data as well as pseudo-location data, which will be introduced in Section III-C. In testing process, given a video, we obtain the anomaly status of each segment by aggregating the consensus of spatial semantics and dynamic variations defined in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. High-order Context Encoding</head><p>Previous approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref> directly infer the anomaly scores from input visual features in an intuitive way, while neglecting the guidance of the temporal context for anomaly localization. Intuitively, the rarely occurred anomalies among the normal patterns will lead to significant changes in the time domain. Therefore, the dynamic variations in the time series are able to indicate the existence of anomalies. Inspired by this, we propose to leverage the temporal context information for the immediate spatial semantics and dynamic temporal variations, and aggregate both cues for accurately locating anomalies.</p><p>In the beginning, we design a High-order Context Encoding (HCE) model to extract high-level semantic features and encode the variations in time series. The input is the feature vectors (x 1 , ? ? ? , x m ) extracted from consecutive segments. The regression process is formulated as:</p><formula xml:id="formula_4">x t = j=?k,??? ,k, j =0 W j x t+j + W 0 x t + b,<label>(4)</label></formula><p>where W j is a projection function on the j-th segment, b is a bias term. The output encodes the context information of the anchored segment and adjacent segments, i.e.,</p><formula xml:id="formula_5">( x t?k , ? ? ? , x t?1 , x t , x t+1 , ? ? ? , x t+k ).</formula><p>The intuition is that tth high-order feature vector collects the fruitful information from its 2k neighbors, which can facilitate both the mining of immediate spatial semantics and local dynamic variations. Actually the regression can be stacked as a hierarchical structure by taking the output x as the input in a recursive manner. In practice, we find the simple one-layer regression can perform well. The neighbor size k controls the temporal context modeled in each local segment x t . Then to exploit the immediate semantic information of the anchored segment, we use a fully connected layer, activated by a Sigmoid function, to obtain an anomaly score. Formally:</p><formula xml:id="formula_6">? sem ( x t ) = ?(w sem x t + b sem ),<label>(5)</label></formula><p>where ? sem ( x t ) represents the semantics score, w sem and b sem are the weight and bias of the fully connected layer and ? stands for the sigmoid function.</p><p>To measure the variation between two adjacent segments, we take the cosine similarity measurement:</p><formula xml:id="formula_7">cos( x t?1 , x t ) = x t?1 x t /( x t?1 2 x t 2 )</formula><p>. The corresponding distance metric is 1?cos( x t?1 , x t ), which has a large value for dramatic variations. Then the second-order discrepancy of local variations is computed as an indicator of anomaly, which becomes:</p><formula xml:id="formula_8">? var ( x t ) = (2 ? cos( x t?1 , x t ) ? cos( x t , x t+1 )))/4, (6)</formula><p>where we make the score value divided by four to normalize the scalar into [0, 1].</p><p>Then, we obtain the singularity of a sequence from the dual context cues, with the margin measurement f as L1-distance: By plugging above singularity tuple into Equation <ref type="formula" target="#formula_1">(2)</ref>, the acquired margin losses of the dual context are denoted as ? sem 1 and ? var 1 . Since the scores of normal events are targeted to 0, and those of anomalous are sparse (scarce of anomalies), we place a sparsity constraint on the loss function. Added with the sparsity constraint of weight ?, the margin loss of dual context becomes:</p><formula xml:id="formula_9">S sem (X ) = max i,j=1,??? ,m |? sem ( x i ) ? ? sem ( x j )|,<label>(7)</label></formula><formula xml:id="formula_10">S var (X ) = max i,j=1,??? ,m |? var ( x i ) ? ? var ( x j )|.<label>(8)</label></formula><formula xml:id="formula_11">? O = ? sem 1 ({X (i) }| n i=1 ) + ? var 1 ({X (i) }| n i=1 ) + ? n n i=1 m t=1</formula><p>(|s sem t | + |s var t |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Enhanced Weak Supervision</head><p>Noise Simulation. As is mentioned in Section I, noises in videos lead to serious interference for anomaly detection, especially localization. Due to the unavoidable external factors, it tends to exist noisy artifacts such as lens jitter in the videos which is going to result in misjudgments. To mitigate this issue, we introduce a noise simulation strategy in which we fuse the raw videos with varying degrees of video noises, such as blur, picture interruption as well as lens jitter. Specifically, we augment the normal video sequences with three kinds of video noise simulations, which are motion blur (kernel size: 5, angle: [?45 ? , 45 ? ]), black/blue/purple blocks ([1/4, 1] of raw image size) and random scale (?20 to +20% on x-and y-axis independently)). We randomly choose m segments in a normal video sequence to augment and the augmented data are still treated normal.</p><p>Given the simulating noise data {? (i) }|n i=1 and the corresponding label set {? (i) }|n i=1 , we apply a supervised constraint on the predicted anomaly states {s</p><formula xml:id="formula_13">(i) t }|n i=1 : ? nse ({? (i) }|n i=1 ) = 1 nn i=1 m t=1 (s (i) t ?? (i) ) 2 ,<label>(10)</label></formula><p>s.t. ,s</p><formula xml:id="formula_14">(i) t = 1 2 ([s (i) t ] sem + [s (i) t ] var ).<label>(11)</label></formula><p>Hand-crafted Anomaly. The noise simulation strategy introduced above is able to alleviate the false alarm for normal videos. However, for anomaly videos, there still lacks enough data for model training. In particular, there exists no explicit location supervision in those anomalous videos which brings in great challenge for effective anomaly localization. To mitigate this issue, we then introduce hand-crafted anomaly to boost the anomaly localization performance via creating explicit location instructions for anomaly localization. We name hand-crafted anomalies as pseudo-location data {X (i) }|n i=1 . Specifically, we first randomly choose a pair of normal and abnormal videos. Then several random segments of the normal video are selected and fused with several segments from the abnormal video. Finally, the obtained segments are combined with the remaining normal video segments to form a pseudo anomalous sequence. Those fused segments are targeted to be abrupt in the obtained sequence to create a pseudo anomalous sample since the substitutes differ from the distribution of the original normal video due to different scenes.</p><p>To decrease the abrupt in the fused sequence, we fused the features of abnormal segments with the normal ones. The coefficient of the anomaly feature ranges in [0.2,0.5] and were randomly generated during the training process. Rather than simply assigning a fixed score (e.g., 1) for the simulated abnormal video will bring in a degenerate solution because the signal can encourage the remaining normal segments to have a high anomaly score along with pseudo-location data. To mitigate the issue above, we propose a simple yet effective skill by barely pushing the fused segments {X i }|n i=1 to have a higher score than the others. The supervision constraint is derived as:</p><formula xml:id="formula_15">? loc ({X i }|n i=1 ) = 1 nn i=1 t?I max(0,s (i) t ? max j / ?I {s (i) j }),<label>(12)</label></formula><p>wheres (i) t denotes the anomaly score estimated by HCE module and I is a collection of the indexes for those pseudo location segments of the hand-crafted anomalies. Integrating the above two augmentation techniques, the objective function of weak supervision enhancement strategy becomes:</p><formula xml:id="formula_16">? A = ? nse ({? (i) }|n i=1 ) + ? loc ({X i }|n i=1 ).<label>(13)</label></formula><p>Combining Eqn. 9 and Eqn. 13, we finally arrive at the overall objective function which is denoted by Eqn. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Traffic Anomaly Detection (TAD) Dataset</head><p>So far, most existing video anomaly datasets are prepared for unsupervised case, e.g., UCSD Pedestrian 1&amp;2 <ref type="bibr" target="#b9">[10]</ref>, Subway Entance &amp; Exit <ref type="bibr" target="#b1">[2]</ref>, Avenue <ref type="bibr" target="#b10">[11]</ref>, etc. These unsupervised datasets are either small in scale or under the constraint of limited scenes. For example, videos in Avenue are short and some of the anomalies are performed by actors (e.g., throwing paper), which are unrealistic. Different from them, UCF-Crime <ref type="bibr" target="#b3">[4]</ref> dataset is a newly released large-scale dataset proposed for weak supervision case. Long untrimmed surveillance videos, covering 13 real-world anomalies, are collected in the dataset. It has a total of 1, 900 surveillance videos, which consists of 1, 610 training videos and 290 test videos. Note that only video-level annotations are provided in the training set, and frame-level annotations are available for evaluation on the test set. The comparison of video anomaly detection datasets are shown in <ref type="table" target="#tab_1">Table I</ref>.</p><p>Although the datasets mentioned above have greatly promoted the development of anomaly detection methods, there still lacks benchmarks of enough diversity for evaluation. To further meet the benchmark diversity requirement, we here propose a new anomaly detection dataset which specifies in the traffic scenes, differing greatly from the datasets mentioned before. Traffic video monitoring plays an essential role in early warning and emergency assistance for car accidents. It is an urgent need to design effective anomaly detection systems for surveillance videos on roads. In traffic scenes, many factors, such as the vehicles moving at a high speed and various road conditions, add up to the hardness of anomaly detection. So far, there is not any specific dataset for traffic anomaly detection. Although UCF-Crime contains road accidents videos, most of anomalies in traffic scenarios are not covered in this dataset. Basically, a large-scale and complex dataset is of great importance for devising and evaluating various methods. It is out desire to push the study of anomaly detection towards the usage in real traffic application.</p><p>To date, there are also public-available anomaly datasets on traffic scenes, however they are with limited scenarios. For example, in <ref type="bibr" target="#b35">[36]</ref>, a video dataset of unsupervised traffic accident detection is released and authors in <ref type="bibr" target="#b36">[37]</ref> propose a fully supervised traffic anomaly detection benchmark. Both the above datasets are made up of samples of short clips, which can not represent the real situations that the anomalies rarely exist among a large quantity of normal data, and these datasets are not target for weak supervision. Moreover, these datasets only consist of first-person or dashboard videos, which lacks the videos captured by surveillance cameras on roads Hence, we are motivated to construct a new large-scale dataset under the traffic scenes for video anomaly detection under weak supervision. The collected TAD dataset consists of long untrimmed videos which cover 7 real-world anomalies on roads, including Vehicle Accidents, Illegal Turns, Illegal Occupations, Retrograde Motion, Pedestrian on Road, Road Spills and The Else (i.e., the remaining anomalies with fewer quantity are put together as one category). Some cases of the anomalies are shown in <ref type="figure" target="#fig_0">Figure 3</ref>. The proposed dataset is comprehensive that includes realistic videos from various scenarios, weather conditions and daytime periods.</p><p>Data collection. Traffic videos from various countries are collected and annotated under a detailed and unified plan. Raw videos are downloaded from YouTube or Google website. The collected videos are mostly recorded by CCTV cameras mounted on the roads. We remove videos which fall into any  of the following cases: manually edited, prank videos, and containing compilation. Videos with ambiguous anomalies are also excluded. Data partition and Annotation. Our TAD dataset contains a total of about 25 hours videos, average 1075 frames per clips. The anomalies randomly occur in each clip, about 80 frames average and there are one to two random anomalies in a video sequence. Finally, 500 traffic surveillance videos are saved and annotated for anomaly detection, with 250 abnormal and normal videos respectively. The whole dataset is randomly partitioned into two parts: training set with 400 videos, and test set with 100 videos. Both training and test sets contain normal and abnormal videos and all seven kinds of anomalies at various temporal locations in anomalous videos. Following the setting of weak supervision as <ref type="bibr" target="#b3">[4]</ref>, the training set is equipped with video-level annotations, and frame-level annotations are provided for the inference set.</p><p>Our proposed TAD dataset contains totally different abnormal scenarios than the current benchmarks. We believe that it could be used to better evaluate the effects of different anomaly detection algorithms from another perspective. We hope our TAD dataset could serve as a standard benchmark for better promoting the development of anomaly detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>As in <ref type="bibr" target="#b4">[5]</ref>, we adopt the Temporal Segment Network (TSN) <ref type="bibr" target="#b37">[38]</ref>, which is a powerful action feature extractor, as our backbone net. We use the BN-Inception version of TSN to extract features for our proposed WSAL method. We extract features from the global average pooling layer (1024-dim). For the UCF-Crime dataset, we use the model weights finetuned on UCF-Crime as in <ref type="bibr" target="#b4">[5]</ref> to extract features. While on our TAD dataset we only use the model weights pretrained on Kinetics-400 dataset. We first divide each video into 32 nonoverlapping segments empirically as in previous works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref> for a fair comparison. Hence, for each video, we have a 32 ? 1024 feature matrix. In our VAD model, the input features are first run through 2 fully connect layers with 512 and 128 dimension, respectively. Then, the dimensions of fully connect layers in HCE module and the immediate semantic score are 128 and 1, respectively. Dropout operations of 60% rate are implemented after each fully connect layer, except the fully connect layer for immediate semantic score. During the training phase, we randomly select 30 positive and 30 negative bags as a mini-batch. We employ Adagrad <ref type="bibr" target="#b38">[39]</ref> optimizer with the initial learning rate of 0.001. The parameter of sparsity constraint in the margin loss is set to ? = 0.00008 as in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> and the weight of strategy for weak supervision enhancement is set to ? = 1.0 for the best performance. We train the model for a total of 3K iterations, decrease the learning rate by half at 1.2K, 2.4K and stop at 3K. All hyper-parameters are the same for both UCF-Crime and TAD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>For anomaly detection <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b6">[7]</ref>, Receiver Operation Characteristic (ROC) is used as a standard evaluation metric. It is calculated by gradually changing the threshold of regular scores on the predicted anomaly scores. Then the Area Under Curve (AUC) is accumulated to a score for the performance evaluation. A higher value indicates a better anomaly detection performance. Following the previous works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, we apply ROC curves and frame-level AUC for anomaly detection performance comparison. Due to the lack of frame-level annotations on the training split and verification split for ablation studies, we use video-level AUC as the measurement for tuning the hyper-parameters. In addition, we also use the ROC and AUC on the anomaly subset to serve as the evaluation metric for anomaly localization ability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with SOTA Methods</head><p>On UCF-Crime dataset. For fair comparison, we reproduce the methods of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> by running their publicly released codes. Other statistical results are drawn from the work <ref type="bibr" target="#b3">[4]</ref>. We compare our our WSAL with several anomaly detection methods. Specifically, a binary SVM classifier is set as the baseline method. In this case, the anomalous and normal videos are treated as two separate class. Models from Lu et al. <ref type="bibr" target="#b10">[11]</ref> and Hasan et al. <ref type="bibr" target="#b16">[17]</ref> are two unsupervised methods, training with the normal videos in UCF-Crime training set. The remaining Sultani et al. <ref type="bibr" target="#b3">[4]</ref>, Zhu et al. <ref type="bibr" target="#b5">[6]</ref> and Zhong et al. <ref type="bibr" target="#b4">[5]</ref> are SOTA weakly-supervised methods. As shown in <ref type="table" target="#tab_1">Table II</ref>, on the whole test set which contains both the normal and abnormal videos, we boost the best performance of overall AUC from the 82.12% to 85.38% by a large margin. In <ref type="figure" target="#fig_2">Figure  5</ref>(a), we plot the ROC curves of SOTA methods on the whole UCF-Crime dataset and it vividly shows the superoirity of our proposed WSAL method over other SOTA methods. As for the Anomaly subset, our proposed method exceeds the SOTA detectors by 9% over <ref type="bibr" target="#b4">[5]</ref> and 13% over <ref type="bibr" target="#b3">[4]</ref>, achieving a significant progress on the anomaly localization perspective.</p><p>We draw the following conclusions upon above experimental results: 1) SVM classifier fails to distinguish the anomalous and normal videos, mainly because the normal patterns take the dominate position in both normal and anomalous videos, and make the classifier difficult to capture the rare anomalies; 2) By encoding the normal patterns and building the corresponding semantic boundary, unsupervised methods <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b10">[11]</ref> achieve better results than SVM classifier; 3) Owing to the benefits of weak labels, the performances of weakly-supervised methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b4">[5]</ref> are superior than above approaches. Nevertheless, previous weakly-supervised methods infer the anomaly status from high-level semantic features intuitively, while neglecting an important property of the anomaly, which is the dynamic evolution lying in time series. The considerable gain in anomaly localization promotes the improvement of overall anomaly detection accuracy. Some visual results of our method on test cases are shown in <ref type="figure" target="#fig_1">Figure  4</ref>. Compared with the other two SOTA methods, a large degree of distinction between the normal and anomalous can be achieved by our approach. As a result, a superior performance is achieved with the better anomaly-discriminating capability.</p><p>On the proposed TAD dataset. To compare the performance of different methods under other circumstances, we conduct comparison experiments on the TAD dataset. We compare our WSAL model with four SOTA anomaly detection methods, including two unsupervised methods (Luo et al. <ref type="bibr" target="#b7">[8]</ref> and Liu et al. <ref type="bibr" target="#b6">[7]</ref>) and two weakly-supervised methods (Sultani et al. <ref type="bibr" target="#b3">[4]</ref> and Zhu et al. <ref type="bibr" target="#b5">[6]</ref>) For unsupervised models, we follow their implementation and train the models on the training subset where only normal videos are provided. All models are re-trained with the same features extracted using TSN, except <ref type="bibr" target="#b6">[7]</ref> which takes the RGB frames as inputs.</p><p>The quantitative comparisons of AUC are revealed in Table III and the corresponding ROC curves are drawn in <ref type="figure" target="#fig_2">Figure 5</ref>(b). Similar as upon UCF-Crime, weakly-supervised methods are able to obtain much better performance than unsupervised ones. As both normal and abnormal training samples are provided, the weakly-supervised methods own much better understanding of the intrisic nature of anomaly. It demonstrates that 1) weak annotations with a little cost of time and work can greatly benefit the performance of VAD methods, and 2) the collected dataset with various scenes and different kinds of anomalies are comlex and extremely challenging for unsupervised methods, compared with current unsupervised benchmark, such as Ped 1/2, Avenue, etc., which could activate future research direction for unsupervised methods. In addition, our WSAL method also achieves better performance with a gain of 6% AUC over previous SOTA <ref type="bibr" target="#b5">[6]</ref>. The prominent advances on the two large-scale and comprehensive benchmarks prove the superiority of our method on detecting and localizing anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>To comprehensively study the impact of different components we proposed, we conduct various ablation studies in this part. All experiments are conducted on the UCF-Crime dataset, all hyper-parameters are kept the same as the WSAL method if not otherwise claimed.</p><p>Analysis of the dual context ensemble. To verify the effectiveness of the proposed ensemble mechanism of immediate semantics and dynamic variations, we construct three variants of the proposed WSAL method where only the immediate semantics or the dynamic variations or both of them are adopted. Detailed comparisons are presented in the first three lines of <ref type="table" target="#tab_1">Table IV</ref>. When only immediate semantics are exploited, the algorithm is able to achieve satisfactory accuracies of 81.44% and 61.13% w.r.t anomaly detection and anomaly localization. It means that the immediate semantics can provide useful information for the tasks, and if dynamic variations are adopted, the performances are boosted by 1.12% and 1.25% separately. One case for visualization is shown in <ref type="figure" target="#fig_3">Figure 6</ref>. There is only one clear and sharp peak in the prediction of dynamic variations cue, compared with the results of immediate semantics cue. It demonstrates that the dynamic variations are good at capturing the sudden occurrence of the anomaly even when the immediate semantics cue may bring in uncertainty. By aggregating the immediate semantics and dynamic variations cues, the detection performance is more robust under various circumstances, with a higher detection and localization accuracy. Thanks to the complementary characteristic of the immediate semantics and dynamic variations cue, which stand for different aspects of the anomaly. Analysis of HCE model. We study the influence of Highorder Context Encoding on the new training and verification splits of UCF-Crime. Since only video-level labels are available in verification split, video-level AUC is measured by aggregating the segment-level model predictions as in Formula 1 and then calculating the AUC results. As to the incorporation of temporal context, an appropriate temporal window size k is critical for the final performance. We slowly increase the window size k from 0 to 3 and the results are listed in <ref type="table" target="#tab_4">Table V</ref>. When the window size k grows, the accuracy of video-level predictions improves drastically from 0 to 1, with a performance gain of 1.6%. It means that appropriate aggregation of the temporal context possesses great potential for anomaly detection. The fruitful information in the temporal neighborhood facilitates the learning of anomaly semantics, as well as the encoding of the temporal evolution. Finally, we choose the size k = 2 for trading off between model size and performance, since the accuracy gain slows down when the window size further increases.</p><p>Further, We conduct ablation studies for analysing impacts  of high-order context encoding on various feature sources as in <ref type="table">Table.</ref> VI. For feature extraction on I3D <ref type="bibr" target="#b39">[40]</ref>, we implement official released model; On R(2+1)D <ref type="bibr" target="#b40">[41]</ref>, we use the official 18 layer model, both the models are pre-trained on kinetics-400. As is shown, when the high-order context encoding module is implemented with input feature of I3D and R(2+1)D, the anomaly location accuracy is increased by over 3% (from 51.23% to 55.04% with I3D inputs and from 47.96% to 51.25% with R(2+1)D inputs), together with about 2% on overall accuracy. Moreover, based on the TSN feature pre-trained on UCF-Crime (by tiling video-level label for each video segment as in <ref type="bibr" target="#b4">[5]</ref>), the proposed HCE module can still achieve remarkable progress with overall AUC from 82.95% to 84.44%. It demonstrates that although temporal aggregation has been done in the feature extraction model, our high-order context encoding is still beneficial for highlighting and capturing transient anomalous information in long-time sequences. It boosts the performance of video anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhanced Weak Supervision</head><p>We conduct studies to verify the effectiveness of the proposed video noise augmentation and hand-crafted anomaly separately. The detailed results are reported in <ref type="table" target="#tab_1">Table IV</ref>. We first augment the training process by adding video noise simulations. The anomaly detection AUC is boosted from 84.44% to 84.86% and the anomaly localization accuracy is further boosted by 1.33%. One case is plotted in <ref type="figure" target="#fig_4">Figure 7</ref>, our noise stimulation strategy contributes to alleviate the interference caused by lens jitters. Note that the anomaly localization performance improvement is non-trivial. It clearly demonstrates that the proposed noise augmentation strategy is able to aid the dynamic variation module for better capturing the real anomaly and achieving better understanding of the intrinsics of anomalies. When we manually synthetic some anomalies to aid the training process, our method achieves 0.51% and 1.60% performance gain over the training strategy where no noise augmentations are adopted. The performance promotions demonstrate that our synthetic anomaly data is able to provide extra useful supervision, indicating that larger abnormal detection dataset is needed for sufficient training of abnormal detection methods. If both augmentation strategies are combined, the proposed method is able to achieve much better performance than the two separate augmentation strategies. It indicates that the proposed two augmentation strategies are beneficial for the understanding of the anomaly concept by suppressing the interference coming from the environment as well as hardware failures, and generating pseudo signals that simulating the occurrence of anomalies.</p><p>Speed Analysis The whole model can run at 44 FPS on a single RTX 2080Ti GPU. Among them, the feature extraction model--TSN with BN-Inception backbone <ref type="bibr" target="#b37">[38]</ref> runs at 45 FPS, with the input frame resolution set as 224x224. Then, based on the extracted feature, it only consumes 1.81 ms (runs at 550 FPS) to predict the anomaly scores. In summary, our model is applicable in the on-line applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we focused on anomaly localization in surveillance videos and proposed a weakly supervised anomaly localization network that deeply exploring the temporal context in consecutive segments. Our model encoded temporal dynamic variations as well as high-level semantic information, and leveraged both of them for anomaly detection and localization. Furthermore, we devised a weak supervision enhancement strategy. The accuracy of anomaly localization was greatly improved under the introduced supervision of video noise augmentation and pseudo-location data. We also collected a new traffic anomaly detection dataset for evaluating methods under realistic scenarios on roads. SOTA methods were verified on UCF-Crime dataset and our TAD dataset. The experimental results showed that the proposed anomaly detector has performed significantly better than previous methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of different anomalies in the collected TAD dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of predictions on the UCF-Crime test cases. The x-axis denotes the video frame # and y-axis is corresponding to the anomaly score. In the figure, (a), (b), and (c) denotes the results of<ref type="bibr" target="#b3">[4]</ref>,<ref type="bibr" target="#b4">[5]</ref> and our model, respectively. The green curves are predictions of various approaches. The light orange regions are ground truth anomalies. Video names are labeled in the blank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>(a) ROC curves on UCF-Crime (b) ROC curves on our TAD ROC curves with various anomaly detection methods on the UCF-Crime dataset and TAD dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>An visualization case of the dual context cues on the UCF-Crime dataset. The light orange region denotes the groundtruth anomaly. From the top to the bottom, the curves represent the anomaly scores of immediate semantics cue, dynamic variations cue and consensus of above two cues, respectively. A more robust and smooth prediction is observed from the dual context model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>An visualization case of video noises on the UCF-Crime dataset. The light blue region in the video sequence contains video noises. The red curves in the top-right denotes the groundtruth anomalies. The curve in the middle-right represents the result of baseline method<ref type="bibr" target="#b4">[5]</ref>. The bottom-right curve belongs to the result of our method. In this case, the noise comes from the lens jitters. The drastic view change easily leads to the false detection of the basic model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2. Framework of WSAL. Video clips are organized in segment level and inputted into the backbone model. These extracted features are processed in HCE module to generate anomaly scores from the cues of immediate semantics and dynamic variations. Then the predicted scores are aggregated and supervised in a novel MIL Margin objective function using the video-level labels. In weak supervision, only video-level annotations are available, which lacks accurate temporal location guidance. Motivated by this, we introduce Enhanced Weak Supervision strategy for data augmentation and generating pseudo anomaly signals. Better viewed in color.</figDesc><table><row><cell>Video segments</cell><cell>?</cell><cell>HCE</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MIL Margin</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Function</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Enhanced Weak</cell></row><row><cell></cell><cell></cell><cell cols="2">Supervision Strategy</cell></row><row><cell>Video-level</cell><cell></cell><cell></cell><cell></cell></row><row><cell>label</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Enhanced Weak Supervision step</cell><cell>Score aggregation step</cell><cell cols="2">HCE High-order Context Encoding module</cell></row><row><cell cols="2">Dynamic Variation score</cell><cell>Immediate Semantic score</cell><cell>Final score</cell><cell>Extracted features</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I A</head><label>I</label><figDesc>COMPARISON OF ANOMALY DETECTION DATASETS</figDesc><table><row><cell>Dataset</cell><cell cols="3">Target Domain # Videos Total Frames</cell><cell>View</cell><cell>Supervision</cell></row><row><cell>UCSD Ped 1/2 [10]</cell><cell>Campus</cell><cell>98</cell><cell>18,560</cell><cell>3rd-person</cell><cell>Unsupervised</cell></row><row><cell>CUHK Avenue [11]</cell><cell>Campus</cell><cell>37</cell><cell>30,652</cell><cell>3rd-person</cell><cell>Unsupervised</cell></row><row><cell>Street scene [12]</cell><cell>Street</cell><cell>81</cell><cell>203,251</cell><cell>3rd-person</cell><cell>Unsupervised</cell></row><row><cell>Shanghai Tech [8]</cell><cell>Campus</cell><cell>81</cell><cell>317,398</cell><cell>3rd-person</cell><cell>Unsupervised</cell></row><row><cell>UCF-Crime</cell><cell>General</cell><cell>1900</cell><cell>13,769,300</cell><cell>3rd-person</cell><cell>Weakly-supervised</cell></row><row><cell>Aadv [37]</cell><cell>Traffic</cell><cell>1750</cell><cell>175,000</cell><cell>1st-person</cell><cell>Fully-supervised</cell></row><row><cell>A3D [36]</cell><cell>Traffic</cell><cell>1500</cell><cell>208,166</cell><cell>1st-person</cell><cell>Unsupervised</cell></row><row><cell>Ours</cell><cell>Traffic</cell><cell>500</cell><cell>540,272</cell><cell>1st&amp;3rd-person</cell><cell>Weakly-supervised</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>COMPARISON ON THE UCF-CRIME DATASET. * SYMBOL INDICATES THE METHOD IS TRAINED WITH NORMAL VIDEOS ONLY</figDesc><table><row><cell>Method</cell><cell cols="2">Overall AUC(%) Anomaly Subset AUC(%)</cell></row><row><cell>SVM</cell><cell>50</cell><cell>50</cell></row><row><cell>Hasan et al.* [17]</cell><cell>50.60</cell><cell>-</cell></row><row><cell>Lu et al.* [11]</cell><cell>65.51</cell><cell>-</cell></row><row><cell>Sultani et al. [4]</cell><cell>75.41</cell><cell>54.25</cell></row><row><cell>Zhu et al. [6]</cell><cell>79.10</cell><cell>62.18</cell></row><row><cell>Zhong et al. [5]</cell><cell>82.12</cell><cell>59.02</cell></row><row><cell>Ours</cell><cell>85.38</cell><cell>67.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON ON OUR TAD DATASET. * SYMBOL INDICATES THE METHOD IS TRAINED WITH NORMAL VIDEOS ONLY</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell cols="4">Overall AUC(%) Anomaly Subset AUC(%)</cell></row><row><cell></cell><cell cols="2">Luo et al.* [8]</cell><cell></cell><cell>57.89</cell><cell></cell><cell>55.84</cell></row><row><cell></cell><cell cols="2">Liu et al.* [7]</cell><cell></cell><cell>69.13</cell><cell></cell><cell>55.38</cell></row><row><cell></cell><cell cols="2">Sultani et al. [4]</cell><cell></cell><cell>81.42</cell><cell></cell><cell>55.97</cell></row><row><cell></cell><cell cols="2">Zhu et al. [6]</cell><cell></cell><cell>83.08</cell><cell></cell><cell>56.89</cell></row><row><cell></cell><cell cols="2">Ours</cell><cell></cell><cell>89.64</cell><cell></cell><cell>61.66</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell cols="7">ABLATION STUDIES OF OUR WSAL METHOD ON THE UCF-CRIME</cell></row><row><cell cols="7">DATASET. THE MEANINGS OF THE ABBREVIATIONS IN THE TABLE ARE AS</cell></row><row><cell cols="7">FOLLOWS: IS: IMMEDIATE SEMANTICS; DV: DYNAMIC VARIATIONS;</cell></row><row><cell cols="7">HCE: HIGH-ORDER CONTEXT ENCODING; NS: NOISE SUPPRESSION;</cell></row><row><cell></cell><cell></cell><cell cols="5">HA: HAND-CRAFTED ANOMALY</cell></row><row><cell>IS</cell><cell>DV</cell><cell>HCE</cell><cell>NS</cell><cell>HA</cell><cell>Overall AUC(%)</cell><cell>Anomaly Subset AUC(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.44</cell><cell>61.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.52</cell><cell>62.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.95</cell><cell>63.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84.44</cell><cell>64.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84.86</cell><cell>66.28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84.95</cell><cell>66.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85.38</cell><cell>67.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V ANALYSIS</head><label>V</label><figDesc>OF THE WINDOW SIZE IN HCE MODEL ON THE UCF-CRIME</figDesc><table><row><cell></cell><cell>DATASET</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Window Size</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>Video-level AUC(%)</cell><cell>93.39</cell><cell>95.01</cell><cell>95.65</cell><cell>95.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDIES OF VARIOUS FEATURE SOURCES ON UCF-CRIM E BENCHMARK. + SYMBOL INDICATES THE BASE MODEL IS EQUIPPED WITH HIGH-ORDER CONTEXT ENCODING MODULE.</figDesc><table><row><cell cols="3">Feature Source Overall AUC(%) Anomaly Subset AUC(%)</cell></row><row><cell>I3D [40]</cell><cell>72.43</cell><cell>51.23</cell></row><row><cell>I3D +</cell><cell>74.18</cell><cell>55.04</cell></row><row><cell>R(2+1)D [41]</cell><cell>72.47</cell><cell>47.96</cell></row><row><cell>R(2+1)D +</cell><cell>75.59</cell><cell>51.25</cell></row><row><cell>TSN [38]</cell><cell>82.95</cell><cell>63.65</cell></row><row><cell>TSN +</cell><cell>84.44</cell><cell>64.95</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM CSUR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abnormal events detection based on spatio-temporal co-occurences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion-aware feature for improved video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Street scene: A new dataset and evaluation protocol for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning regularity in skeleton trajectories for anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning memory-guided normality for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable prototypes for motion anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scherf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huisken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning object motion patterns for anomaly detection and improved object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a spacetime mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anomaly detection in traffic scenes via spatial-aware motion reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TITS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPs</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly supervised adversarial domain adaptation for semantic segmentation in urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tracking and identification of suspicious and abnormal behaviors using supervised machine learning technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Adhiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICACCC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An anomaly-introduced learning method for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatiotemporal tree filtering for enhancing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust neighborhood preserving projection by nuclear/l2, 1-norm regularization for image feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised traffic accident detection in first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Atkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anticipating accidents in dashcam videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
