<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Han</surname></persName>
							<email>hancheng@t3caic.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Driving Department</orgName>
								<address>
									<country>T3CAIC Technology</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichao</forename><surname>Zhao</surname></persName>
							<email>zhaoqichao@t3caic.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Driving Department</orgName>
								<address>
									<country>T3CAIC Technology</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyi</forename><surname>Zhang</surname></persName>
							<email>zhangshuyi@t3caic.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Driving Department</orgName>
								<address>
									<country>T3CAIC Technology</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinzi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Driving Department</orgName>
								<address>
									<country>T3CAIC Technology</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenlin</forename><surname>Zhang</surname></persName>
							<email>mitzhang@t3caic.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Driving Department</orgName>
								<address>
									<country>T3CAIC Technology</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Yuan</surname></persName>
							<email>yuanjinwei@t3caic.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Driving Department</orgName>
								<address>
									<country>T3CAIC Technology</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the last decade, multi-tasking learning approaches have achieved promising results in solving panoptic driving perception problems, providing both high-precision and high-efficiency performance. It has become a popular paradigm when designing networks for real-time practical autonomous driving system, where computation resources are limited. This paper proposed an effective and efficient multi-task learning network to simultaneously perform the task of traffic object detection, drivable road area segmentation and lane detection. Our model achieved the new stateof-the-art (SOTA) performance in terms of accuracy and speed on the challenging BDD100K dataset. Especially, the inference time is reduced by half compared to the previous SOTA model. Code will be released in the near future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although the great development of computer vision and deep learning, vision-based tasks (such as object detection, segmentation, lane detection, etc.) are still challenging in applications of low-cost autonomous driving. Recent efforts has been made for building a robust panoptic driving perception system, which is one of the key components for autonomous driving. The panoptic driving perception system helps the autonomous driving vehicle achieving a comprehensive understanding of its surrounding environment via common sensors such as cameras or Lidars. Camerabased object detection and segmentation tasks are usually preferred in the practical use of scene understanding for its low-cost. Object detection plays an important role in providing the position and size information of traffic obstacles, helping autonomous vehicle making accurate and timely decisions during the driving stage. In addition, drivable area segment and lane segment provide rich information for route planning and improving the driving safety as well.  Object detection and segmentation are two long-standing research topics in the computer vision area. There are a series of great work presented for object detection, such as CenterNet <ref type="bibr" target="#b2">[3]</ref>, Faster R-CNN <ref type="bibr" target="#b17">[18]</ref> and the YOLO family <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5]</ref>. Common segmentation networks are often applied for the drivable area segmentation problem, for example: UNET <ref type="bibr" target="#b9">[10]</ref>, segnet <ref type="bibr" target="#b0">[1]</ref> and pspNet <ref type="bibr" target="#b27">[28]</ref>. While for lane detection/segmentation, a more powerful network is needed in order to provide a better high-level and low-level feature fusion, so that the global structural context is considered for enhancing segmenting details <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>. However, it is often unpractical to run separate models for each individual task in a real-time autonomous driving system. Multi-task learning networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20]</ref> provide a potential solution for saving computational cost in this case, where the network is designed into an encoder-decoder pattern, and the encoder is effectively shared by different tasks.</p><p>In this paper, we presented an effective and efficient multi-task learning network after a thorough study on the previous approaches. We ran experiments on the challenging BDD100K dataset <ref type="bibr" target="#b25">[26]</ref>. Our model achieved the best performance in all the three tasks: 0.83 MAP for object detection task, 0.93 MIOU for the drivable area segmentation task, and 87.3 accuracy for lane detection. These numbers are all largely increased compared to the baseline. In addition, we increased the Frames Per Second (FPS) to be 91 running on NVIDIA TESLA V100, which is well above the value of 49FPS by YOLOP model in the same experiment settings. It further illustrates that our model can reduce the computational cost and guarantee real-time predictions while leaving space for improvement of other experimental research.</p><p>The main contributions of this work are summarized as follows:</p><p>? Better: we proposed a more effective model structure, and developed more sophisticated bag-of-freebies during, for example, Mosaic and Mixup were performed for data preprocessing and a novel of hybrid loss was applied.</p><p>? Faster: we implemented a more efficient network structure and memory allocation strategy for the model.</p><p>? Stronger: our model was trained under a powerful network architecture thus it is well generalized for adapting to various scenarios and simultaneously ensure the speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review the related work for all the tasks in panoptic driving perception topic. We also discussed effective model boosting techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Real-time traffic object detectors</head><p>Modern object detectors can be divided into one-stage detectors and two-stage detectors. Two-stage detectors consist of the region proposal component and the detection refinement component. These methods often perform with high precision and robust results in variant One-stage object detectors usually operates faster, thus often preferred in real-time practical use. The YOLO series keep in the active iteration with advanced one-stage object detection design, which provided inspirations for our experiments, including YOLO4, scaled yolov4, yolop and yolov7). In this paper, we use simple but powerful network structure and together with effective Bag of Freebies (BoF) methods to improve the object detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Drivable area and lane segmentation</head><p>Research in semantic segmentation has made remarkable progress by using fully convolutional neural network <ref type="bibr" target="#b1">[2]</ref> instead of traditional segmentation algorithms. With the extensive research in this area, higher-performance models have been designed, such as the classical encoder-decoder structure of Unet, and the pyramid pooling module used in PSPNet to extract different level of features, which helps effectively divide the drivable area.</p><p>For lane segmentation, due to the difficulties from its specific task characteristics, e.g., the slenderness of lane shape and the fragmented pixel distribution, lane segmentation require meticulous detection capabilities. SCNN <ref type="bibr" target="#b13">[14]</ref> proposed slice-by-slice convolution to transmit information among channels in each layer. Enet-SAD employs a selfattention rectification method that enables low-level feature to be learned from high-level feature, this can not only improve the performance but also keep a lightweight design for the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-task Approaches</head><p>The goal of multi-task learning is to design networks which can better learn shared representations from multitask supervisory signals. Mask RCNN inherits the idea of Faster RCNN, useing the ResNet with residual block <ref type="bibr" target="#b7">[8]</ref> architecture for feature extraction, and adds an additional mask prediction branch so that efficiently combines the task of instance segmentation and object detection. The authors of LSNet <ref type="bibr" target="#b3">[4]</ref> design a three-in-one network architecture and simultaneously performs object detection, instance segmentation and drivable area segmentation. They also design a cross-IOU loss in order to fit object in different scales and attributes. MultiNet uses one shared encoder and three separate decoders to achieve the task of scene classification, object detection and drivable area segmentation. YOLOP builds an encoder for feature extraction and three heads for processing specific tasks, achieving multi-task processing. Based on this, the work of HyBridNet adds Bifpn to further improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Bag of freebies</head><p>In order to improve the accuracy of object detection results without increasing the cost of inference, researchers usually take advantage of the fact that training stage and testing stage are separate. Data augmentation is often implemented to increase the diversity of input images, so that the designed object detection model is generalized enough across different domains. For example, regular image mirroring, adjustment on image brightness, contrast, hue, saturation, and noise are applied in YOLOP. These data augmentation methods are all pixel-level adjustments and preserve all the original pixel information in the adjusted region. In addition, authors of YOLO series proposed a method to simultaneously perform data augmentation for multiple images. For instance, Mosaic augmentation <ref type="bibr" target="#b5">[6]</ref> on four spliced images can improve the batch size and improve the variety of data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we introduce the proposed network architecture for multi-task learning in details. We discuss how an efficient feed-forward network is implemented to cooperatively accomplish the tasks of traffic object detection, drivable area segmentation and road lane detection. Moreover, optimization strategies for the model is presented as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall</head><p>We designed a more efficient network architecture based on a set of existing work, e.g., YOLOP, HybridNet, etc. Our model is inspired by the work of YOLOP and Hybrid-Net, we retain the core design concept but utilize a powerful backbone for feature extraction. In addition, different from the existing work, we utilize three branches of decoder head to perform the specific task instead of running the tasks of drivable area segmentation and lane detection in the same branch. The main reason for this change is that we find the task difficulty of traffic area segmentation and lane detection is entirely different, that means the two tasks have different requirements on the feature level thus better to have different network structures. Experiments in Section 4 illustrate the newly designed architecture can effectively improve the overall segmentation performance and introduce negligible overhead on computational speed. <ref type="figure" target="#fig_2">Figure  2</ref> shows the overall methodology flow chart of our design concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>The proposed network architecture is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. It consists of one shared encoder for feature extraction form the input image and three decoder heads for the corresponding task. This section demonstrates the network configuration of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Shared Encoder</head><p>Different to YOLOP which uses CSPdarknet as the backbone, we adopt the design of E-ELAN to utilize group convolution and to enable the weights of different layers to learn more diverse features. <ref type="figure" target="#fig_2">Figure 2</ref> shows the configuration of the group convolution.</p><p>In the neck part, feature generated from different stages are collected and fused by concatenation. Similar to YOLOP, we apply the Spatial Pyramid Pooling (SPP) module <ref type="bibr" target="#b6">[7]</ref> for fusing features in different scales and use Feature Pyramid Network (FPN) module <ref type="bibr" target="#b10">[11]</ref> for fusing features with different semantic levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Task Heads</head><p>As mentioned above, we designed three separate decoder heads for each individual task. Similar to YOLOv7, we adopt an anchor-based multi-scale detection scheme. First, we use Path Aggregation Network (PAN) <ref type="bibr" target="#b11">[12]</ref> which is bottom-up structure for better localization feature extraction. By combining feature from PAN and FPN, we are able to fuse the semantic information with these local features and then directly run detection on the multi-scale fused feature maps in PAN. Each grid in the multi-scale feature map will be assigned with three anchors of different aspect ratios, and the detection head will predict the offset of the position and the scaled height and width, as well as the probability and corresponding confidence for each class predict.</p><p>Drivable area segmentation and lane segmentation in the proposed method are performed in separate task heads with distinct network structure. Different to YOLOP where features for both tasks are from the last layer of neck, we employ different semantic level of features. We find that the feature extracted from deeper network layers is not necessary for drivable area segmentation comparing to the other two tasks. These deeper features are not able to improve the prediction performance but increase the difficulty of the model convergence during training. Thus, the branch of drivable area segmentation head is connected prior to the FPN module. Moreover, to compensate for the possible loss caused by this change, an additional upsampling layer is applied, i.e., there are total of four nearest interpolation upsampling applied in the decoder stage.</p><p>For lane segmentation, the task branch is connected to the end of FPN layer in order to extract features in the deeper level since road lines are often not slender and hard to detect in the input image. In addition, deconvolution is applied in the decoder stage of lane detection to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Design of BOF</head><p>Based on the design of YOLOP, we preserve the setting of the loss function in the detection part. L det is the loss for detection, which is a weighted sum loss of classification loss, object loss and bounding loss as shown in formula1 .</p><formula xml:id="formula_0">L det = ? 1 L class + ? 2 L obj + alpha 3 L box<label>(1)</label></formula><p>In addition, focal loss is used in L class and L obj to handle the sample imbalance problem. L class is utilized for penalizing classification and L obj is for the prediction confidence. L box reflects the distance of overlap rate, aspect ratio and scale similarity between predicted results and ground truth. Proper setting of the loss weights can effectively guarantee the result of multi-task detection. Cross-entropy loss was used for drivable area segmentation, which aims to minimize the classification error between the network output and the groundtruth. For lane segmentation, we use focal loss instead of cross-entropy loss. Because for hard classification tasks such as lane detection, using focal loss can effectively lead model to focus on the hard examples so that improves detection accuracy. In addition, we implemented a hybrid loss <ref type="bibr" target="#b28">[29]</ref> consisting of dice loss and focal loss in our experiment. Dice loss is able to learn the class distribution alleviating the imbalanced voxel problem. Focal loss has ability to force model to learn poor classified voxel. The final loss can be computed as Formula 2 as following.</p><formula xml:id="formula_1">L = L Dice + ?L F ocal = C ? c?1 c=0 T P p (c) T P p (c) + ?F N p + betaF P p (c) ? ? N c?1 c=0 N n=1 g n (c)(1 ? p n (c)) 2 log(p n (c)) (2) T P p (c) = N n=1 p n (c)g n (c) (3) F N p (c) = N n=1 (1 ? p n (c))g n (c) (4) F P p (c) = N n=1 p n (c)(1 ? g n (c))<label>(5)</label></formula><p>Where ? is trade-off between focal and dice loss, C is total number of category, thus, C is set as 2 since there are only two categories in drivable area segmentation and lane segmentation. T P p(c) , F N p(c) and F P p(c) means the true positive, false negative and false positive correspondingly.</p><p>It is worth mentioning that we introduce the augmentation strategy of Mosaic and Mixup <ref type="bibr" target="#b26">[27]</ref> in the multi-task learning approaches, which is the first time to our best of knowledge showing significant performance improvements for all the three tasks of object detection, drivable area and lane detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section introduces the dataset setting and parameter configuration of our experiments. All experiments in this paper were carried out based on the configuration environment of the graphics card TESLA V100 and torch 1.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use BDD100K as our benchmark dataset for our experimental studies, which is a challenging public dataset in the scene of driving. The dataset contains 100 thousands frames in driver perspective view, it is popularly used as an evaluation benchmark for computer vision research in autonomous driving. BDD100K dataset supports 10 vision tasks. Compared to other popular driving datasets such as Cityscapes and Camvid, BDD100K has more divert data and scenes considering weather condition, scene location and illumination, etc.. Similar to other studies, we split the dataset into a training set of 70 thousand images, a validation set of 10 thousand images, and a test set of 20 thousand images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training protocol</head><p>"Cosine Annealing" policy is used to adjust the learning rate the in training process where the initial learning rate is set as 0.01 and warm-restart <ref type="bibr" target="#b12">[13]</ref> is performed and set in the first 3 epochs. In addition, the momentum and weight decay are correspondingly set as 0.937 and 0.005. And total training epoch number is 300. We resize images in BDD100k dataset from 1280?720?3 to 640?640?3 in the traing stage and 1280?720?3 to 640?384?3 in the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>We compared the proposed model with a set of existing work qualitatively and quantitatively in this section. <ref type="table" target="#tab_0">Table 1</ref> shows a comparison among two SOTA multi-task models and our model. The results illustrate that our model possess much stronger network structure and more parameters, but performs much faster. This is benefit from the proposed effective network design and the sophisticated memory allocation strategy. We ran all the tests in the same experimental settings and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Model parameter and inference speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Traffic object detection results</head><p>Same as YOLOP, mAP50 and Recall are used as evaluation metrics here. Our model achieves higher mAP50 and competitive Recall, as shown in <ref type="table">Table 2</ref>   <ref type="table" target="#tab_1">Table 3</ref>. Results on drivable area segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Drivable area segment results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Lane detection</head><p>Lanes in the BDD100K dataset are annotated by two lines, so preprocessing is needed. First, we calculate the centerline based on the two annotated lines, then we draw a lane mask with a width of 8 pixels for training, while keeping the test set lane lines at 2 pixels wide. We use pixel accuracy and IoU of lanes as evaluation metrics. As shown in  <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 4</ref> show a visual comparison of YOLOP, Hybridnet and our YOLOPv2 on BDD100K dataset. <ref type="figure" target="#fig_3">Figure  3</ref> shows the results in daytime. The left column lists three scenarios for YOLOP, there are a few false drivable segments and missing segmentation for drivable area in the first scenario and there are redundant detection boxes for small objects and missing segmentation for drivable area in the second scenario. In the third scenario, missing detection for lane is found. The middle column shows three scenes for Hybridnet, there are discontinuous lane prediction in first scene, the problems of repeat detection for small vehicles and missing detection for lane exist in the second scene of Hybridnet and there are some false detection for vehicle and lane in third scene. The right columns show the results of our YOLOPv2, it illustrates our model provide better performance on various scene. <ref type="figure" target="#fig_4">Figure 4</ref> shows the results in night time. The left column provides the scenarios results for YOLOP, there are false detection and missing drivable area segmentation in the first scene, deviation of lane detection happens in the second scene and missing lane detection and drivable area segmentation in the third one. The middle column are the results for Hybridnet, there are missing detection for some vehicle and some false detection boxes in the first scenario. In the second scene, there exist some false detection and redundant detection boxes. There are missing drivable area segmentation and redundant vehicle detection box in the third scene. The images in right column are the results for our YOLOPv2, it demonstrates that our model successfully overcomes these problems and shows a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Discussion of visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">Ablation studies</head><p>We developed a variety of changes and improvements, and performed corresponding experiments. <ref type="table" target="#tab_3">Table 5</ref> shows a selected list of changes we did in the experiments and its corresponding improvement introduced to the entire network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposed an effective and efficient end-toend multi-task learning network that simultaneously per-  forms three driving perception tasks of object detection, drivable area segmentation, and lane detection. Our model achieves the new state-of-the-art performance on the challenging BDD100k dataset and largely exceeds the existing models in both speed and accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The network of YOLOPv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Methodology flow chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The day-time results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The night-time results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>. Comparison of model parameter and inference speed.</figDesc><table><row><cell>Network</cell><cell>Size</cell><cell>Params</cell><cell>Speed(fps)</cell></row><row><cell>YOLOP</cell><cell>640</cell><cell>7.9M</cell><cell>49</cell></row><row><cell>HybridNets</cell><cell>640</cell><cell>12.8M</cell><cell>28</cell></row><row><cell>YOLOPv2</cell><cell>640</cell><cell>38.9M</cell><cell>91</cell></row><row><cell cols="2">Network</cell><cell>mAP50</cell><cell>Recall</cell></row><row><cell cols="2">MultiNet</cell><cell>60.2</cell><cell>81.3</cell></row><row><cell cols="2">DLT-Net</cell><cell>68.4</cell><cell>89.4</cell></row><row><cell cols="2">Faster R-CNN</cell><cell>55.6</cell><cell>77.2</cell></row><row><cell cols="2">YOLOV5s</cell><cell>77.2</cell><cell>86.8</cell></row><row><cell>YOLOP</cell><cell></cell><cell>76.5</cell><cell>89.2</cell></row><row><cell cols="2">HybridNets</cell><cell>77.3</cell><cell>92.8</cell></row><row><cell cols="2">YOLOPv2</cell><cell>83.4</cell><cell>91.1</cell></row><row><cell cols="4">Table 2. Results on traffic object detection.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">illustrates the evaluating results for drivable area</cell></row><row><cell cols="2">segmentation and MIOU is used to evaluate the segmenta-</cell></row><row><cell cols="2">tion performance of different models. Our model get the</cell></row><row><cell cols="2">best performance with 0.93 mIOU.</cell></row><row><cell>Network</cell><cell>Drivable mIoU</cell></row><row><cell>MultiNet</cell><cell>71.6</cell></row><row><cell>DLT-Net</cell><cell>71.3</cell></row><row><cell>PSPNet</cell><cell>89.6</cell></row><row><cell>YOLOP</cell><cell>91.5</cell></row><row><cell>HybridNets</cell><cell>90.5</cell></row><row><cell>YOLOPv2</cell><cell>93.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 ,Table 4</head><label>44</label><figDesc>our model achieve the highest value in accuracy.</figDesc><table><row><cell>Network</cell><cell>Accuracy</cell><cell>Lane IoU</cell></row><row><cell>ENet</cell><cell>34.12</cell><cell>14.64</cell></row><row><cell>SCNN</cell><cell>35.79</cell><cell>15.84</cell></row><row><cell>ENet-SAD</cell><cell>36.56</cell><cell>16.02</cell></row><row><cell>YOLOP</cell><cell>70.50</cell><cell>26.20</cell></row><row><cell>HybridNets</cell><cell>85.40</cell><cell>31.60</cell></row><row><cell>YOLOPv2</cell><cell>87.31</cell><cell>27.25</cell></row></table><note>. Results on lane detection.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Evaluation of efficient experiments.</figDesc><table><row><cell>Training method</cell><cell cols="5">Speed(fps) mAP50 Recall mIoU Accuracy IoU</cell></row><row><cell>YOLOP (Baseline)</cell><cell>49</cell><cell>76.5</cell><cell>89.2 91.5</cell><cell>70.5</cell><cell>26.2</cell></row><row><cell>Fine-tuned + Backbone</cell><cell>93</cell><cell>81.1</cell><cell>89.4 91.2</cell><cell>65.4</cell><cell>23.1</cell></row><row><cell>Mosaic and Mixup</cell><cell>93</cell><cell>82.8</cell><cell>90.9 92.7</cell><cell>72.1</cell><cell>25.9</cell></row><row><cell>Convtranspose2d</cell><cell>91</cell><cell>82.9</cell><cell>90.8 93.1</cell><cell>75.2</cell><cell>26.1</cell></row><row><cell>Focal loss</cell><cell>91</cell><cell>84.8</cell><cell>91.8 93.4</cell><cell>82.2</cell><cell>27.9</cell></row><row><cell>Focal loss + Dice loss</cell><cell>91</cell><cell>83.4</cell><cell>91.1 93.2</cell><cell>87.3</cell><cell>27.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04899</idno>
		<title level="m">Qingming Huang, and Qi Tian. Location-sensitive visual recognition with cross-iou loss</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved mosaic: Algorithms for more complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Zhili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Physics: Conference Series</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1684</biblScope>
			<biblScope unit="page">12094</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unet 3+: A full-scale connected unet for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanfen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhua</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1055" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scnn: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brucek</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH computer architecture news</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dlt-net: Joint detection of drivable areas, lane lines, and traffic objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqiang</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4670" to="4679" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hybridnets: End-to-end perception network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Phan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09035</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaled-yolov4: Scaling cross stage partial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13029" to="13038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02696</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">You only learn one representation: Unified network for multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04206</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lanenet: Realtime lane detection networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Yolop: You only look once for panoptic driving perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manwen</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11250</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Anatomynet: deep learning for fast and fully automated whole-volume segmentation of head and neck anatomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="576" to="589" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
