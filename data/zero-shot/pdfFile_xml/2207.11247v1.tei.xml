<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Panoptic Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
							<email>jingkang001@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Zhe</forename><surname>Ang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zujin</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
							<email>kaiyang.zhou@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<email>wayne.zhang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Panoptic Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing research addresses scene graph generation (SGG)a critical technology for scene understanding in images-from a detection perspective, i.e., objects are detected using bounding boxes followed by prediction of their pairwise relationships. We argue that such a paradigm causes several problems that impede the progress of the field. For instance, bounding box-based labels in current datasets usually contain redundant classes like hairs, and leave out background information that is crucial to the understanding of context. In this work, we introduce panoptic scene graph generation (PSG), a new problem task that requires the model to generate a more comprehensive scene graph representation based on panoptic segmentations rather than rigid bounding boxes. A high-quality PSG dataset, which contains 49k wellannotated overlapping images from COCO and Visual Genome, is created for the community to keep track of its progress. For benchmarking, we build four two-stage baselines, which are modified from classic methods in SGG, and two one-stage baselines called PSGTR and PS-GFormer, which are based on the efficient Transformer-based detector, i.e., DETR. While PSGTR uses a set of queries to directly learn triplets, PSGFormer separately models the objects and relations in the form of queries from two Transformer decoders, followed by a prompting-like relation-object matching mechanism. In the end, we share insights on open challenges and future directions. We invite users to explore the PSG dataset on our project page https://psgdataset.org/, and try our codebase https://github.com/Jingkang50/OpenPSG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of scene graph generation (SGG) task is to generate a graph-structured representation from a given image to abstract out objects-grounded by bounding boxes-and their pairwise relationships <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">65]</ref>. Scene graphs aim to facilitate the understanding of complex scenes in images and has potential for a wide range of downstream applications, such as image retrieval <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">52,</ref><ref type="bibr">50]</ref>, visual reasoning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">53]</ref>, visual question answering (VQA) <ref type="bibr" target="#b20">[21]</ref>, image captioning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>, structured image generation and outpainting <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">66]</ref>, and robotics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Since the introduction of SGG <ref type="bibr" target="#b26">[27]</ref>, this problem has been addressed from a detection perspective, i.e., using bounding boxes to detect objects followed by  <ref type="figure">Fig. 1</ref>: Scene graph generation (a. SGG task) vs. panoptic scene graph generation (b. PSG task). The existing SGG task in (a) uses bounding box-based labels, which are often inaccurate-pixels covered by a bounding box do not necessarily belong to the annotated class-and cannot fully capture the background information.</p><p>In contrast, the proposed PSG task in (b) presents a more comprehensive and clean scene graph representation, with more accurate localization of objects and including relationships with the background (known as stuff), i.e., the trees and pavement.</p><p>the prediction of their pairwise relationships <ref type="bibr">[35,</ref><ref type="bibr">64,</ref><ref type="bibr">42]</ref>. We argue that such a bounding box-based paradigm is not ideal for solving the problem, and would instead cause a number of issues that impede the progress of the field. Firstly, bounding boxes-as labeled in current datasets [35]-only provide a coarse localization of objects and often contain noisy/ambiguous pixels belonging to different objects or categories (see the bounding boxes of the two persons in <ref type="figure">Fig. 1-a)</ref>. Secondly, bounding boxes typically cannot cover the full scene of an image. For instance, the pavement region in <ref type="figure">Fig. 1</ref>-a is crucial for understanding the context but is completely ignored. Thirdly, current SGG datasets often include redundant classes and information like woman-has-hair in <ref type="figure">Fig. 1</ref>-a, which is mostly deemed trivial <ref type="bibr">[42]</ref>. Furthermore, inconsistent and redundant labels are also observed in current datasets, e.g., the trees and benches in <ref type="figure">Fig. 1</ref>-a are labeled multiple times, and some extra annotations do not contribute to the graph (see isolated nodes). Using such labels for learning might confuse the model.</p><p>Ideally, the grounding of objects should be clear and precise, and a scene graph should not only focus on salient regions and relationships in an image but also be comprehensive enough for scene understanding. We argue that as compared to bounding boxes, panoptic segmentation <ref type="bibr" target="#b32">[33]</ref> labels would be a better choice for constructing scene graphs. To this end, we introduce a new problem, panoptic scene graph generation, or PSG, with a goal of generating scene graph representations based on panoptic segmentations rather than rigid bounding boxes.</p><p>To help the community keep track of the research progress, we create a new PSG dataset based on COCO <ref type="bibr">[43]</ref> and Visual Genome (VG) <ref type="bibr">[35]</ref>, which contains 49k well-annotated images in total. We follow COCO's object annotation schema of 133 classes; comprising 80 thing classes and 53 stuff (background) classes. To construct predicates, we conduct a thorough investigation into existing VGbased datasets, e.g., VG-150 [64], VrR-VG <ref type="bibr">[42]</ref> and GQA <ref type="bibr" target="#b23">[24]</ref>, and summarize 56 predicate classes with minimum overlap and sufficient coverage of semantics. See <ref type="figure">Fig. 1-b</ref> for an example of our dataset. From <ref type="figure">Fig. 1</ref>, it is clear that the panoptic scene graph representation-including both panoptic segmentations and the scene graph-is much more informative and coherent than the previous scene graph representation.</p><p>For benchmarking, we build four two-stage models by integrating four classic SGG methods [64, <ref type="bibr" target="#b38">71,</ref><ref type="bibr">58,</ref><ref type="bibr">54]</ref> into a classic panoptic segmentation framework <ref type="bibr" target="#b31">[32]</ref>. We also turn DETR <ref type="bibr" target="#b3">[4]</ref>, an efficient Transformer-based detector, into a one-stage PSG model dubbed as PSGTR, which has proved effective for the PSG task. We further provide another one-stage baseline called PSGFormer, which extends PSGTR with two improvements: 1) modeling objects and relations separately in the form of queries within two Transformer decoders, and 2) a prompting-like interaction mechanism. A comprehensive comparison of one-stage models and two-stage models is discussed in our experiments.</p><p>In summary, we make the following contributions to the SGG community:</p><p>-A New Problem and Dataset: We discuss several issues with current SGG research, especially those associated with existing datasets. To address them, we introduce a new problem that combines SGG with panoptic segmentation, and create a large PSG dataset with high-quality annotations. -A Comprehensive Benchmark: We build strong two-stage and one-stage PSG baselines, and evaluate them comprehensively on our new dataset, so that the PSG task is solidified in its inception. We find that one-stage models, despite having a simplified training paradigm, have great potential for PSG as it achieves competitive results on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Scene Graph Generation Existing scene graph generation (SGG) methods have been dominated by the two-stage pipeline that consists of object detection and pairwise predicate estimation. Given bounding boxes, early work predicts predicates using conditional random fields <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11]</ref> or casts predicate prediction into a classification problem <ref type="bibr" target="#b42">[75,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr">49]</ref>. Inspired by knowledge graph embeddings, VTransE <ref type="bibr" target="#b41">[74]</ref> and UVTransE <ref type="bibr" target="#b24">[25]</ref> are proposed for explicit predicate modeling. Follow-up works have investigated various variants based on, e.g., RNN and graph-based modeling [64, <ref type="bibr" target="#b38">71,</ref><ref type="bibr">58,</ref><ref type="bibr" target="#b34">67,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">38]</ref>, energy-based models [54], external knowledge [58, <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">69,</ref><ref type="bibr" target="#b37">70,</ref><ref type="bibr">46]</ref>, and more recently language supervision <ref type="bibr" target="#b44">[77,</ref><ref type="bibr" target="#b35">68]</ref>. Recent research has shifted the attention to problems associated with the SGG datasets, such as the long-tailed distribution of predicates [57,12], excessive visually-irrelevant predicates <ref type="bibr">[42]</ref>, and inaccurate localization of bounding boxes <ref type="bibr" target="#b28">[29]</ref>. In particular, a very recent study <ref type="bibr" target="#b28">[29]</ref> shows that training an SGG model to simultaneously generate scene graphs and predict semantic segmentation masks can bring about improvements, which inspires our research. In our work, we study panoptic segmentation-based scene graph generation in a more systematic way by formulating a new problem and building a new benchmark. We also notice that a closely-related topic human-object interaction (HOI) <ref type="bibr" target="#b18">[19]</ref> shares a similar goal with SGG, i.e., to detect prominent relations from the image. However, the HOI task restricts the model to only detect human-related relations while ignoring the valuable information between objects that is often critical to comprehensive scene understanding. Nevertheless, many HOI methods are applicable to SGG tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">60,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b45">78,</ref><ref type="bibr">61,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">45,</ref><ref type="bibr">55,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">72,</ref><ref type="bibr">59,</ref><ref type="bibr" target="#b40">73]</ref>, and some of them have inspired our PSG baselines <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Datasets</head><p>While early SGG works have constructed several smaller-size datasets such as RW-SGD <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr">VRD [46]</ref>, and UnRel-D [48], the large-scale Visual Genome (VG) <ref type="bibr">[35]</ref> quickly became the standard SGG dataset as soon as it was released in 2017, prompting subsequent work to research in a more realistic setting. However, several critical drawbacks of VG were raised by the community, and therefore, some VG variants were gradually introduced to address some problems. Notice that VG contains an impractically large number of 33,877 object classes and 40,480 predicate classes, leading VG-150 [64] to only keep the most frequent 150 object classes and 50 predicate classes for a more realistic setting. Later, VrR-VG <ref type="bibr">[42]</ref> argued that many predicates in VG-150 can be easily estimated by statistical priors, hence deciding to re-filter the original VG categories to only keep visually relevant predicate classes. However, by scrutinizing VrR-VG, we find many predicates are redundant (e.g., alongside, beside, besides) and ambiguous (e.g., beyond, down). Similar drawbacks appeared in another large-scale dataset with scene graph annotations called GQA <ref type="bibr" target="#b23">[24]</ref>. In summary, although relations play a pivotal role in SGG tasks, a systematic definition of relations is unfortunately overlooked across all the existing SGG datasets. Therefore, in our proposed PSG dataset, we consider both comprehensive coverage and non-overlap between words, and carefully define a predicate dictionary with 56 classes to better formulate the scene graph problem. <ref type="figure">Fig. 2</ref> shows the word cloud of the predicate classes, where font size indicates frequency.</p><p>Apart from the problem of predicate definition, another critical issue of SGG datasets is that they all adopt bounding box-based object grounding, which inevitably causes a number of issues such as coarse localization (bounding boxes cannot reach pixel-level accuracy), inability to ground comprehensively (bounding boxes cannot ground backgrounds), tendency to provide trivial information (current datasets usually capture objects like head to form the trivial relation of person-has-head), and duplicate groundings (the same object could be grounded by multiple separate bounding boxes). These issues together have caused the low-quality of current SGG datasets, which impede the progress of the field. Therefore, the proposed PSG dataset tries to address all the above problems by grounding the images using accurate and comprehensive panoptic segmentations with COCO's appropriate granularity of object categories. <ref type="table" target="#tab_0">Table 1</ref> compares the statistics of the PSG dataset with classic SGG datasets.</p><p>Panoptic Segmentation The panoptic segmentation task unifies semantic segmentation and instance segmentation <ref type="bibr" target="#b32">[33]</ref> for comprehensive scene understanding, and the first approach is a simple combination of a semantic segmentation model and an instance segmentation model to produce stuff masks and thing masks respectively <ref type="bibr" target="#b32">[33]</ref>. Follow-up work, such as Panoptic FPN <ref type="bibr" target="#b31">[32]</ref> and UPSNet [63], aim to unify the two tasks in a single model through multitask learning to achieve gains in compute efficiency and segmentation performance. Recent approaches (e.g., MaskFormer <ref type="bibr" target="#b9">[10]</ref>, Panoptic Segformer [41] and K-Net <ref type="bibr" target="#b43">[76]</ref>) have turned to more efficient architectures based on Transformers like DETR <ref type="bibr" target="#b3">[4]</ref>, which simplifies the detection pipeline by casting the detection task as a set prediction problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem and Dataset</head><p>Recap: Scene Graph Generation We first briefly review the goal of the classic scene graph generation (SGG) task, which aims to model the distribution:</p><formula xml:id="formula_0">Pr (G | I) = Pr (B, O, R | I) ,<label>(1)</label></formula><p>where I ? R H?W ?3 is the input image, and G is the desired scene graph which comprises the bounding boxes B = {b 1 , . . . , b n } and labels O = {o 1 , . . . , o n } that correspond to each of the n objects in the image, and their relations in the set R = {r 1 , . . . , r l }. More specifically, b i ? R 4 represents the box coordinates, o i ? C O and r i ? C R belong to the set of all object and relation classes.</p><p>Panoptic Scene Graph Generation Instead of localizing each object by its bounding box coordinates, the new task of panoptic scene graph generation (PSG task) grounds each object with the more fine-grained panoptic segmentation. For conciseness, we refer to both objects and background as objects. Formally, with panoptic segmentation, an image is segmented into a set of masks M = {m 1 , . . . , m n }, where m i ? {0, 1} H?W . Each mask is associated with an object with class label o i ? C O . A set of relations R between objects are also predicted. The masks do not overlap, i.e., n i=1 m i ? 1 H?W . Hence, PSG task models the following distribution:</p><formula xml:id="formula_1">Pr (G | I) = Pr (M, O, R | I) .<label>(2)</label></formula><p>PSG Dataset To address the PSG task, we build our PSG dataset following these three major steps. Readers can find more details in the Appendix.</p><p>Step 1: A Coarse COCO &amp; VG Fusion: To create a dataset with both panoptic segmentation and relation annotations, we use the 48,749 images in the intersection of the COCO and VG datasets with an automatic but coarse dataset fusion process. Specifically, we use an object category matching procedure to match COCO's segmentations with VG's bounding boxes, so that part of VG's predicates are applied to COCO's segmentation pairs. Due to the inherent mismatch between the label systems and localization annotations of VG and COCO, the auto-generated dataset is very noisy and requires further cleaning.</p><p>Step  <ref type="bibr" target="#b23">[24]</ref>. The selected 56 predicates are maximally independent (e.g., we only keep "over/on" and do not have "under") and cover most common cases in the dataset.</p><p>Step 3: A Rigorous Annotation Process: Building upon the noisy PSG dataset, we require the annotators to 1) filter out incorrect triplets, and 2) supplement more relations between not only object-object, but also objectbackground and background-background pairs, using the predefined 56 predicates. To prevent ambiguity between predicates, we ask the annotators strictly not to annotate using general relations like on, in when a more precise predicate like parked on is applicable. With this rule, the PSG dataset allows the model to understand the scene more precisely and saliently. Quality Control: The PSG dataset goes through a professional dataset construction process. The main authors first annotated 1000 images to construct a detailed documentation (available in project page), and then employed a professional annotation company (sponsored by SenseTime) to annotate the training set within a month (US$11K spent). Each image is annotated by two workers and examined by one head worker. All the test images are annotated by the authors. Summary: Several merits worth highlighting by virtue of the novel and effective procedure of PSG dataset creation: 1) Good grounding annotation from the pixel-wise panoptic segmentation from COCO dataset [43], 2) Clear category system with 133 objects (i.e., things plus stuff) and 56 predicates with appropriate granularity and minimal overlaps, and 3) Accurate and comprehensive relation annotations from a rigorous annotation process that pays special attention to salient relations between object-object, object-background and backgroundbackground. These merits address the notorious shortcomings [37] of classic scene graph datasets discussed in Sec. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and Metrics</head><p>This section introduces the evaluation protocol for the PSG task. Following the settings of the classic SGG task [65,5], our PSG task comprises two sub-tasks: predicate classification (when applicable) and scene graph generation (main task) to evaluate the PSG models. Predicate classification (PredCls) aims to generate a scene graph given the ground-truth object labels and localization. The goal is to study the relation prediction performance without the interference of the segmentation performance. Notice that this metric is only applicable to two-stage PSG models in Sec. 4.1, since onestage models cannot leverage the given segmentations to predict scene graph. Scene graph generation (SGDet) aims to generate scene graphs from scratch, which is the main result for the PSG task.</p><p>We also notice that classic SGG tasks contain another sub-task of scene graph classification (SGCls), which provide the ground-truth object groundings to simplify the scene graph generation process. We find SGCls is not applicable for PSG baselines. Unlike SGG tasks where object detectors such as Faster-RCNN [51] can utilize ground-truth object bounding boxes to replace predictions from the Region Proposal Network (RPN), panoptic segmentation models are unable to directly use the ground-truth segmentations for classification, so the SGCls task is inapplicable even for two-stage PSG methods.</p><p>The classic metrics of R@K and mR@K are used to evaluate the previous two sub-tasks, which calculates the triplet recall and mean recall for every predicate category, given the top K triplets from the PSG model. Notice that PSG grounds objects with segmentation, a successful recall requires both subject and object to have mask-based IOU larger than 0.5 compared to their ground-truth counterparts, with the correct classification on every position in the S-V-O triplet.</p><p>While the triplet recall rates that mentioned above are the main metric for PSG task, since objects are required to be grounded by segmentation masks, panoptic segmentation metrics <ref type="bibr" target="#b32">[33]</ref> such as PQ <ref type="bibr" target="#b31">[32]</ref> can be used for model diagnosis. However, it is not considered as the core evaluation metric of PSG task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PSG Baselines</head><p>To build a comprehensive PSG benchmark, we refer to frameworks employed in the classic SGG task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">65]</ref> and prepare two-stage and one-stage baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Two-Stage PSG Baselines</head><p>Most prior SGG approaches tackle the problem in two stages: first performing object detection using off-the-shelf detectors like Faster-RCNN [51], then pairwise relationship prediction between these predicted objects. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we follow a similar approach in establishing two-stage baselines for the PSG task: 1) using pretrained panoptic segmentation models of classic Panoptic FPN <ref type="bibr" target="#b31">[32]</ref> to extract initial object features, masks and class predictions, and then 2) processing them using a relation prediction module from classic scene graph generation methods like IMP [64], MOTIFS <ref type="bibr" target="#b38">[71]</ref>  to obtain the final scene graph predictions. In this way, classic SGG methods can be adapted to solve the PSG task with minimal modification. Formally, the two-stage PSGG baselines decompose formulation from Eq. 1 to Eq. 3.</p><formula xml:id="formula_2">Pr (G | I) = Pr (M | I) ? Pr (O | M, I) ? Pr (R | O, M, I) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A One-Stage PSG Baseline -PSGTR</head><p>Unlike classic dense prediction models (e.g., Faster-RCNN [51]) with sophisticated design, the transformer-based architectures support flexible input and output specifications. Based on the end-to-end DETR <ref type="bibr" target="#b3">[4]</ref> and its extension to the HOI task <ref type="bibr" target="#b46">[79]</ref>, we naturally design a one-stage PSG method named PSGTR to predict triples and localizations simultaneously, which can be directly modeled as Eq. 2 without decomposition.</p><p>Triplet Query Learning Block As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, PSGTR first extracts image features from a CNN backbone and then feeds the features along with queries and position encoding into a transformer encoder-decoder. Here we expect the queries to learn the representation of scene graph triplets, so that for each triplet query, the subject/predicate/object predictions can be extracted by three individual Feed Forward Networks (FFNs), and the segmentation task can be completed by two panoptic heads for subject and object, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSG Prediction Block</head><p>To train the model, we extend the DETR's Hungarian matcher [36] into a triplet Hungarian matcher. To match the triplet query T i ? Q T with ground truth triplets G, all contents in the triplet (i.e., all outputs that are predicted from T i ) are used, including the class of subjectT S i , relation T R i , and objectT O i , and localization of subjectsT S i and objectsT O i . Therefore, the triplet matching (tm) cost C tm is designed with the combination of class matching C cls and segments matching C seg :  where ? is the mapping function to correspond each triplet query T i ? Q T to the closest ground truth triplet. The triplet query set Q T collects all the |Q T | triplet queries. The optimization objective is thus:</p><formula xml:id="formula_3">C tm T i , G ?(i) = k?{S,O} C seg T k i ,G k ?(i) + k?{S,R,O} C cls T k i ,G k ?(i) ,<label>(4)</label></formula><formula xml:id="formula_4">? = arg max ? |Q T | i=1 C tm T i , G ?(i) .<label>(5)</label></formula><p>Once the matching is done, the total loss L total can be calculated by applying cross-entropy loss L cls for labels and DICE/F-1 loss [47] for segmentation L cls :</p><formula xml:id="formula_5">L total = |Q T | i=1 ? ? k?{S,O} L seg T k i ,G k ?(i) + k?{S,R,O} L cls T k i ,G k ?(i) ? ? .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Alternative One-Stage PSG Baseline -PSGFormer</head><p>Based on the PSGTR baseline that explained in Section 4.2, we extend another end-to-end HOI method <ref type="bibr" target="#b30">[31]</ref> and further propose the alternative one-stage PSG baseline named PSGFormer, featured by an explicit relation modeling with a prompting-like matching mechanism. The model diagram is illustrated in Figure 5 and will be elaborated as follows.</p><p>Object &amp; Relation Query Learning Block Compared to the classic objectoriented tasks such as object detection and segmentation, the most significant uniqueness of PSG task as well as SGG task is their extra requirements on the predictions of relations. Notice that the relation modeling in our two-stage baselines depends on features from object-pairs, while PSGTR implicitly models the objects and relations altogether within the triplets, the important relation modeling has not been given a serious treatment. Therefore, in the exploration of PSGFormer, we explicitly model the relation query R i ? Q R , as well as object query O i ? Q O separately, in hope that object queries to specially pay attentions to objects (e.g., person and phones), and relation queries to focus on the area where the relationship takes place in the picture (e.g., person looking-at phone). Similar to PSGTR in <ref type="figure" target="#fig_2">Figure 4</ref>, both object and relation queries with CNN features and position encoding are fed into a transformer encoder, but being decoded with their corresponding decoder, i.e., object or relation decoder, so that the queries can learn the corresponding representations.</p><p>Object &amp; Relation Query Matching Block In PSGFormer, each object query yields an object prediction with FFN and a mask prediction with a panoptic head, and each relation query yields a relation prediction. However, due to the parallel process of object queries and relation queries, the missing interdependence between different query types makes the triplet still not formed. To connect object and relation queries for compositing triplets, we are inspired by the design in HOTR <ref type="bibr" target="#b30">[31]</ref> and implement a prompting-like query matching block.</p><p>Query matching block models the triplet composition task as a fill-in-theblank question with prompts, i.e., by prompting a relation, we expect a pair of suitable objects provided by their corresponding object queries can be selected, so that a complete subject-predicate-object triplet, can be generated. Therefore, two selectors, i.e., subject selector and object selector, are required.</p><p>Given a relation query R i ? Q R as prompt, both subject selector and object selector should return the most suitable candidate to form a complete triplet. We use the most standard cosine similarity between object queries and the provided relation query and pick the highest similarity to determine the subject and object candidates. It should also be noticed that subject and object selectors should rely on the level of association between objects and relation queries rather than the semantic similarity. Besides, object queries are regarded as different roles (i.e., subject or object) in different selectors. Therefore, the object queries are expected to pass another two FFNs to extract some specific information for subject (with FFN denoted as f S ) and object (with FFN denoted as f O ), so that the distinguishable subject and object representations are obtained from the object queries. With the idea above, a set of subjects S are generated in Eq. 7, with the i-th subject corresponding to the i-th relation query R i . With a similar process, the object set O is also generated.</p><formula xml:id="formula_6">S = S i | S i = arg max O (f S (O j ) ? R i ) , O j ? Q O , R i ? Q R .<label>(7)</label></formula><p>Till now, the subject set S and the object set O are well-prepared by subject and object selectors, with the i-th subject query S i and the i-th object O i corresponding to the i-th relation query R i . Finally, it is straightforward to obtain all the matched triplet T, which is shown in Eq. 8.</p><formula xml:id="formula_7">T = {(S i , R i , O i ) | S i ? S, R i ? R, O i ? O} .<label>(8)</label></formula><p>Apart from interpreting the query matching as a prompt-like process, it can also be considered as a cross-attention mechanism. For a relation query (Q), the goal is to find the high-attention relations among all subject / object representations, which are considered as keys (K). The subject / object labels predicted by the corresponding representations are regarded as values (V), so that the QKV attention model outputs the labels of selected keys. <ref type="figure" target="#fig_3">Fig. 5</ref>-c is generally depicted following this interpretation.</p><p>PSG Prediction Block Similar to PSGTR, with the predicted triplets prepared, the prediction block can finally train the model using L total from Eq. 6. In addition, with object labels and masks predicted by object queries, a standard training loss introduced in panoptic segmentation DETR <ref type="bibr" target="#b3">[4]</ref> is used to enhance the object decoder and avoid duplicate object groundings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first report the results of all PSG methods introduced in the paper. Implementation details are available in the appendix, and all codes are integrated in the OpenPSG codebase, which is developed based on MMDetection <ref type="bibr" target="#b6">[7]</ref>. Most of the two-stage SGG implementations refer to MMScene-Graph [62] and Scene-Graph-Benchmark.pytorch [56].  <ref type="figure">Fig. 6</ref> reports the panoptic segmentation result using PQ and visualizes the segmentation results of two examples as well as the predicted scene graph in the form of triplet lists.  <ref type="bibr">'20)</ref> all originate from the SGG task and are adapted for the PSG task. Different backbones of ResNet-50 <ref type="bibr" target="#b19">[20]</ref> and ResNet-101 <ref type="bibr" target="#b19">[20]</ref> are used. Notice that predicate classification task is not applicable to one-stage PSG models, so the corresponding results are marked as '?'. Models are trained using 12 epochs by default. ? denotes that the model is trained using 60 epochs. Two-Stage Baselines Rely on First-Stage Performance For predicate classification task (PredCls) that is only applicable to two-stage models, the provided ground-truth segmentation can significantly improve the triplet prediction performance. For example, even the most classic method IMP can reach over 30% R@20, which already exceeds all the available R@20 under the scene graph generation (SGDet) task (cf . 28.4% by PSGTR). This phenomenon indicates that a good panoptic segmentation performance could naturally benefit the PSG task. Further evidence where the performance of IMP on the SGDet task is almost halved (from 32% to 17% on R@20) strengthens the above conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>Some SGG Techniques for VG are not Effective for PSG <ref type="table" target="#tab_4">Table 2</ref> shows that the results of some two-stage baselines (i.e., IMP, MOTIFS, and VCTree) are generally proportional to their performance on SGG tasks, indicating that the advantages of the two-stage models (i.e., MOTIFS and VCTree) are transferable to PSG task. However, we notice that another two-stage baseline, GPSNet, does not seem to exceed its SGG baselines of MOTIFS and VCTree in the PSG task. The key advantage of GPSNet over MOTIFS and VCTree is that it explicitly models the direction of predicates. While the design can be effective in the VG dataset where many predicates are trivial with obvious direction of predicates (e.g., of in hair-of-man, has in man-has-head), PSG dataset gets rid of these predicates, so the model may not be effective as expected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Segmentation</head><p>Original Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Triplets person carrying backpack person standing on skis person on snow person standing on skis skis on snow sky over snow person driving bus person carrying backpack person walking on pavement bus driving on road bus driving on road bus driving on road tree over bus building beside bus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth person walking on road sky over road bus in front of bus sky over bus sky over pavement bus on pavement tree beside building person walking on pavement person walking on pavement bus driving on road person walking on road sky over building bus beside bus bus driving on road person walking on road bus beside bus person carrying backpack person walking on pavement bus driving on road bus driving on road person beside person person walking on road bus parked on road person walking on pavement person walking on road person beside person person walking on pavement sky over bus sky over pavement person walking on pavement bus beside bus person carrying backpack sky over tree sky over road person walking on road bus beside bus person walking on pavement bus driving on road bus driving on road sky over building bus in front of bus person carrying backpack bus driving on road person carrying backpack bus in front of bus sky over pavement sky over bus person walking on pavement person carrying backpack bus parked on road bus beside bus bus on road bus driving on road bus parked on road sky over building bus driving on road bus on road sky over tree person walking on pavement sky over building person standing on pavement person standing on pavement person walking on pavement sky in front of bus sky over bus bus attached to road bus beside road person walking on pavement person carrying pavement person standing on pavement bus driving on road bus on pavement bus on road bus parked on road person driving bus bus driving on road person driving bus sky beside bus person walking on pavement sky over bus bus beside bus bus driving on road person walking on pavement person walking on pavement person carrying backpack person beside person bus beside bus bus driving on pavement person walking on pavement bus driving on pavement person walking on pavement sky over building bus beside bus person walking on road person walking on pavement bus parked on road bus parked on road bus driving on pavement person walking on pavement bus driving on pavement sky over skis snowboard on sky snowboard on snow sky over snowboard person on snowboard person beside person sky over person person beside person person standing on skis person in front of sky person standing on snow skis on snow person beside person person in front of person sky over person person standing on snow person on snow sky over person sky over snow person on snowboard person looking at person snow attached to snowboard person standing on snowboard person looking at person person in front of person person standing on skis snowboard on snow sky over skis skis on snow sky over snowboard person standing on snow person on snow person going down snow person beside person person beside person person on snowboard sky over person sky over person sky over person sky over snow person standing on snowboard person in front of person sky over skis person in front of person person standing on skis person looking at person sky over snowboard person looking at person person going down snow person on snow sky over person person on snow person beside person skis on snow sky over person person beside person sky over person person playing snowboard sky over snow snowboard on snow sky over skis person standing on skis person on skis person in front of person skis on snow person on skis person on skis sky over skis sky over person skis on snow sky over skis sky over person person standing on skis person playing skis skis on snow skis on snow sky over person skis on snow sky over person sky over snow person going down snow person going down snow skis on snow person on snow sky beside person person wearing skis person playing skis person wearing skis sky over person person standing on skis person going down snow person going down snow person standing on skis sky over snow person on snow sky over person person standing on skis person on snow person playing skis skis on snow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image</head><p>Ground Truth Segmentation <ref type="figure">Fig. 6</ref>: Visualization of segmentations and the top 20 predicted triplets of 5 PSG methods. The panoptic segmentation metric PQ is also reported. The colors of the subject and object in the triplet corresponds to the mask in the segmentation result. Reasonable triplets are marked by ticks. Triplets that match the ground-truth are marked by green ticks. One-stage models can provide more reasonable and diverse triplet predictions, but they are unable to achieve a good panoptic segmentation result.</p><p>PSGFormer is an Unbiased PSG Model When the training schedule is limited to 12 epochs, the end-to-end baseline PSGFormer outperforms the best two-stage model VCTree by significant 4.8% on mR@20 and 8.5% on mR@100. Although PSGFormer still cannot exceed two-stage methods on the metrics of R@20/50/100, its huge advantage in mean recall indicates that the model is unbiased in predicting relations. As <ref type="figure">Fig. 6</ref> shows, PSGFormer can predict unusual but accurate relations such as person-going down-snow in the upper example, and the imperceptible relation person-driving-bus in the lower example. Also, in the upper example, PSGFormer predicts an interesting and exclusive triplet person-wearing-skis. This unique prediction should come from the design of the separate object / relation decoders, so that relation queries can independently capture the meaning of the predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSGTR Obtains SOTA Results with Long Training Time</head><p>In PSGTR, every triplet is expected to be captured by a query, which needs to predict everything in the triplet simultaneously, so the model is required to better focus on the connections between objects. Besides, the cross-attention mechanism in the transformer encoder and triplet decoder enable each triplet query access to the information of the entire image. Therefore, PSGTR is considered as the most straightforward and simplest one-stage PSG baseline with minimal constraints or prior knowledge. As a result, although PSGTR only achieves one-digit recall scores in 12 epochs, it surprisingly achieves SOTA results with a prolonged training time of 60 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Challenges and Outlook</head><p>Challenges While some prior knowledge introduced by some two-stage SGG methods might not be effective in the PSG task, we expect that more creative knowledge-aided models can be developed for the PSG task in the era of multimodality, so that more interesting triplets can be extracted with extra priors. However, it should be noted that although priors can be useful to enhance performance, the PSG prediction should heavily rely on visual clues. For example, in <ref type="figure">Fig. 6</ref>, person-walking on-pavement should be identified if the model can perceive and understand the subtle visual differences between walking and standing. Also, PSG models are expected to predict more meaningful and diverse relations, such as rare relations like feeding and kissing, rather than only being content with statistically common or positional relations. <ref type="figure">Fig. 6</ref> visualizes the panoptic segmentation results of PSG methods, where PSGTR only obtains a miserable PQ result even with good PSG performance. The reason is that triplet queries in PSGTR produce object groundings independently, so that one object might be referred and segmented by several triplets, and the deduplication or the re-identification (Re-ID) process is non-trivial. Although the performance of Re-ID does not affect PSG metrics, it might still be critical to form an accurate and logical scene understanding for real-world applications. Outlook Apart from attracting more research on the learning of relations (either closed-set or open-set) and pushing the development of scene understanding, we also expect the PSG models to empower more exciting downstream tasks such as visual reasoning and segmentation-based scene graph-to-image generation.  <ref type="figure" target="#fig_6">Fig. A1</ref>. In particular, we would like to highlight some specific advances in the PSG dataset from sub-figure (a), which readers can confirm from other sub-figures. In (a), VG-150 does not contain key information of 'woman flying kite', and also has ambiguous relations like 'at'. Fortunately, PSG addresses the key information but with a more general predicate 'playing'. It is because in PSG, the predicate definition follows the rule of 'being representative with proper granularity, not too specific'. Refer to Sec. A.4 for more information. Also, PSG gathers far more comprehensive and accurate triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation between PSG and Panoptic Segmentation</head><p>For object groundings, it is noticeable that in VG, the grounding of 'beach' is inaccurate, which only covers half of the actual beach. It can be problematic since a successful recall of a triplet requires a correct matching (big IOU) between predicted groundings and ground truth, in addition to a correct classification of triplets. Therefore, an incorrect annotation on object grounding can cause an inaccurate evaluation on scene graph generation too. Apparently, object grounding of PSG is far more accurate than VG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PSG Dataset Statistics</head><p>The PSG dataset has a total of 48,749 annotated images with 56 predicate classes, and 80 thing and 53 stuff classes (same as the COCO dataset [43]).</p><p>Here is a list of average statistics for each image: , where they share 48,749 images. Namely, for a given image, it has panoptic segmentation annotations from COCO, as well as scene graph annotations from Visual Genome. However, we cannot merge the two datasets directly as not only do they have different object annotations, they also define different object categories. Therefore, we attempted to conduct the following dataset merging process. Merging COCO and Visual Genome Annotations Dataset merging requires solving two intermediate tasks, saying 1) Object Category Matching: to figure out a mapping between the object categories in COCO to the object categories in Visual Genome, and 2) Object Instance Matching: for each image, to find out which object annotations in COCO correspond to which object annotations in Visual Genome. The goal of dataset merging is that, once the matching is achieved, we can transfer over the relationship annotations directly. However, since the matching process will not be completely perfect, we planned to bring these preliminary annotations to experienced and trained annotators for a final round of cleaning and annotation.</p><formula xml:id="formula_8">-</formula><p>With a clear goal of dataset matching, we introduce the details of two intermediate tasks of object category matching and object instance matching. After the matching process, we can get a noisy preliminary PSG dataset (ref. <ref type="figure" target="#fig_3">Fig.A5(b)</ref>) that awaits the final cleaning.</p><p>Object Category Matching: For a given COCO object category, what object categories in Visual Genome does it correspond to? For example, "car" in COCO may be matched to both "car" and "vehicle" in Visual Genome.</p><p>We encode the text of each object category into a feature vector using fast-Text <ref type="bibr" target="#b2">[3]</ref> word embeddings, and compute a similarity score for each (COCO, Visual Genome) object category pair using their cosine similarity.</p><p>This similarity score will be useful for matching object instances in COCO to that in Visual Genome, as described in the next section.</p><p>Object Instance Matching: The relationship annotations for each image in Visual Genome are tied to Visual Genome object annotations (object categories + bounding boxes), which are different from its corresponding COCO object annotations (object categories + panoptic segmentations). In order to transfer over the relationships to COCO object annotations, we can attempt to match each object instance as annotated in COCO, to an object instance as annotated in Visual Genome. Intuitively, if an object as annotated in COCO has a high overlap with an object as annotated Visual Genome, and their object categories are similar, they are likely to be referring to the same object. A sketch of the algorithm used to perform the matching is as follows. For each image, we:</p><p>1. compute the bounding box IoU of each object in COCO to each object in Visual Genome (using the tightest bounding box of the segmentation). 2. we then perform a greedy approach by always considering the instance pair with the highest IoU: (a) If their categories match, i.e. if the similarity score between the word embeddings of their category names are above a certain threshold, we'll match the pair together and remove them from the candidate pool. After the matching, the relationship annotations in Visual Genome can be transferred over to the COCO object annotations. This process is repeated for all the variants VG-150 [64], GQA <ref type="bibr" target="#b23">[24]</ref> and VrR-VG <ref type="bibr">[42]</ref>. This helps to maximize the recall of potentially correct scene graph relationships and alleviates the difficulty of the final annotation task for the annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Process</head><p>Building upon the preliminary (noisy) PSG dataset (shown in <ref type="figure" target="#fig_3">Fig. A5</ref>), we patiently trained our annotators to 1) filter out incorrect triplets, and 2) supplement more relations between not only object-object, but also object-background and background-background pairs, using the predefined 56 predicates. The definition of 56 predicates will be explained in the next section. The noisy triplets (for later filtering) are shown to be a good practice for annotators, prompting them providing both salient and detailed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Predicate Dictionary</head><p>The design of predicate dictionary is inspired by COCO's practice on object categories selection. According to COCO, the selected categories must be representative, be relevant to practical applications, and be common with high occurrence. Also, a proper level of granularity should also be considered. With these principles in mind, we refer to all the predicates left in the preliminary PSG dataset, sorting them according to their occurrence, and carefully select the predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practice for the Principles</head><p>To meet the principles mentioned above, several processes are designed:</p><p>-For Representative: we deduplicate the predicates and try to make the remaining predicates orthogonal. For example, we shrink a list of similar predicates of 'parked along', 'parked alongside', 'parked at', 'parked behind', 'parked beside', 'parked by', 'parked in', 'parked in front of', 'parked near', 'parked next to', 'parked on' (existed in VG and GQA) to only keep one predicate as 'parked on'. Also, for the bidirected relation pairs such as 'in front of' and 'behind', we only keep one direction, i.e., 'in-front-of'. Similarly, only 'over' is included while 'beneath' is excluded. With this process, we make our vocabulary very concise and thus representative. -For Practicality: Since the goal of the PSG dataset is to facilitate the development of scene understanding tasks, inspired by VrR-VG [42], we get rid of many positional relations that fill the GQA dataset <ref type="bibr" target="#b23">[24]</ref>, such as 'on the left of' and 'on the right of', and especially focus on the visual-related predicates during our dictionary building. -For Coverage: After several iterations, we finally decided to include 56 predicates with property and can well cover almost all the existing critical relations in the PSG dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Predicate Definitions</head><p>We provided a detailed explanation on each predicate with image examples to ensure the consistent performance from annotators. We will provide the handbook in our PSG website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>All experiments are performed in a single unified codebase using the MMDetection framework to facilitate reproducibility.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Two-Stage PSG Baseline</head><p>Fine-tuning the Base Model We first fine-tune the Panoptic FPN base model on the panoptic segmentation annotations from our PSG dataset. The model is initialized from the best performing pretrained weights provided by MMDetection, and then trained using a batch size of 8. The SGD optimizer is used with a learning rate of 0.02, momentum of 0.9, weight decay of 0.0001, and gradient clipping with a max L2 norm of 35. Training runs for 12 epochs, with a learning rate schedule that linearly warms up from 0.02 / 3 to 0.02 over 500 iterations, and decays by a factor of 0.1 at the 8th and 11th epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training the Scene Graph Prediction Head</head><p>Using the fine-tuned Panoptic FPN above, we freeze its weights and only train the scene graph prediction head. This essentially treats the Panoptic FPN as a black-box feature extractor and panoptic segmentation predictor. For each predicted object, we extract a feature vector using RoIAlign (like in MaskRCNN), making use of the tightest bounding box around its segmentation mask. With grid features, and class predictions and bounding box localizations for each object at hand, we can feed these into any scene graph prediction head for training and prediction. We use a batch size of 16, and the SGD optimizer with a learning rate of 0.03, momentum of 0.9, and weight decay of 0.0001, and gradient clipping with a max L2 norm of 35. Training runs for 12 epochs, with a learning rate schedule that linearly warms up from 0.03 / 3 to 0.03 over 500 iterations, and decays by a factor of 0.1 at the 7th and 10th epochs. The hyperparameters for the MOTIFS, VCTree and GPSNet models all follow the same settings in their respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 PSGTR</head><p>As it is described in Section 4.2, our PSGTR model extends DETR to PSG task with new heads and a triplet Hungarian matcher. In detail, we implement each of those Feed Forward Networks (FFNs) by a 3-layer MLP, and each panoptic head, following DETR segmentation, consists of a multi-head attention layer and a 6-layer FPN-like CNN. Besides, the number of queries is set as 100 which indicates that 100 possible relations are predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training settings</head><p>In general, we follow most of the training strategies of DETR. We adopt the same AdamW optimizer with 10 ?4 learning rate and 10 ?4 weight decay for PSGTR except for the backbone which is trained with learning rate of 10 ?5 . For initialization, we directly use COCO pretrained DETR to initialize the weights of our backbone and transformer. Besides, we also generally follow DETR's data augmentation which does cropping and resizing operations with settings such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333. However, it should be noted that when cropping images, we also filter the ground truth of bounding boxes and relations pairs that might be cropped. We train our model for 60 epochs with a step scheduler at epoch 40, and it finally takes us around 2 days to train on eight V100 GPUs with batch size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 PSGFormer</head><p>PSGFormer is built on the baseline of PSGTR so that most of the training details are shared. In detail, PSGFormer also implements each FFN by a 3-layer MLP, and each panoptic head by a multi-head attention layer and a 6-layer FPN-like CNN as DETR does. Besides, the number of object queries and relation queries are set as 100. We follow the training hyperparameters of PSGTR including optimizer, learning rate, data augmentation, etc. Notice that PSGFormer also has an auxiliary task of pure panoptic segmentation with object decoder, the ratio between the main task on triplet supervision and the auxiliary panoptic segmentation supervision is 5 to 1. We train our model for 60 epochs, taking around 2.5 days to train on eight V100 GPUs with batch size 1. <ref type="figure">Fig. A6</ref> shows PSGTR's predict results in a triplet-by-triplet fashion, as a complementary to the lower example in <ref type="figure">Fig. 6</ref>. Notice that panoptic segmentation <ref type="figure">Fig. A6</ref>: Visualization of PSGTR Result Triplet-by-Triplet. PSGTR uses triplet queries to directly predict subject / object masks in the triplets, and the subject / object across triplets are not dependent, so the panoptic segmentation visualization is chaos in <ref type="figure">Fig. 6</ref>. However, if we visualize PSGTR result triplet-by-triplet, the result looks good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visualization of PSGTR Result Triplet-by-Triplet</head><p>visualization is chaos in <ref type="figure">Fig. 6</ref>. It is because PSGTR uses triplet queries to directly predict subject / object masks in the triplets, and the subject / object across triplets are not dependent, and the re-identification of each subject / object is no-trivial, and we use a simple post-processing method of pixel-wise argmax function to merge the segments, but it will still split one object into parts. However, it does not mean that PSGTR cannot segment objects well when predicting triplets. As we visualize the PSGTR result triplet-by-triplet in <ref type="figure">Fig. A6</ref>, the result looks good.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2207.11247v1 [cs.CV] 22 Jul 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Two-stage PSG baselines using Panoptic FPN. a) In stage one, for each thing/stuff object, Panoptic FPN<ref type="bibr" target="#b31">[32]</ref> produces a segmentation mask with its tightest bounding box to crop out the object feature. The union of relevant objects can produce relation features. b) In the second stage, the extracted object and relation features are fed into by any existing SGG relation model to predict the relation triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>PSGTR: One-stage PSG baseline. The one-stage model takes in a) features extracted by CNNs with positional encoding, and a set of queries aiming to represent triplets. b) Query learning block processes image features with Transformer encoderdecoder and use queries to represent triplet information. Then, c) the PSG prediction head concretes the triplet predictions by producing subject/object/predicate classes using simple FFNs, and uses panoptic heads for panoptic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>PSGFormer: The proposed one-stage PSG method. a) Two types of queries, i.e., object queries and relation queries, are fed into transformer block with CNN features and positional encoding. b) Query Learning Block processes image features with one encoder and output object or relation queries with the corresponding decoder. c) Object queries and relation queries interact with each other in the prompting-like query matching block, so that the triplets are formed and proceed to d) PSG prediction block to output results or compute loss as PSGTR behaves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>35. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision (IJCV) (2017) 36. Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics quarterly (1955) 37. Li, L., Chen, L., Huang, Y., Zhang, Z., Zhang, S., Xiao, J.: The devil is in the labels: Noisy label correction for robust scene graph generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 38. Li, Y., Ouyang, W., Zhou, B., Shi, J., Zhang, C., Wang, X.: Factorizable net: an efficient subgraph-based framework for scene graph generation. In: Proceedings of the European Conference on Computer Vision (ECCV) (2018) 39. Li, Y.L., Liu, X., Lu, H., Wang, S., Liu, J., Li, J., Lu, C.: Detailed 2d-3d joint representation for human-object interaction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 40. Li, Y.L., Zhou, S., Huang, X., Xu, L., Ma, Z., Fang, H.S., Wang, Y., Lu, C.:Transferable interactiveness knowledge for human-object interaction detection. In:Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 41. Li, Z., Wang, W., Xie, E., Yu, Z., Anandkumar, A., Alvarez, J.M., Lu, T., Luo, P.:Panoptic segformer. arXiv preprint arXiv:2109.03814 (2021) 42. Liang, Y., Bai, Y., Zhang, W., Qian, X., Zhu, L., Mei, T.: Vrr-vg: Refocusing visually-relevant relationships. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 43. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Proceedings of the European Conference on Computer Vision (ECCV) (2014) 44. Lin, X., Ding, C., Zeng, J., Tao, D.: Gps-net: Graph property sensing network for scene graph generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 45. Liu, Y., Chen, Q., Zisserman, A.: Amplifying key cues for human-object-interaction detection. In: Proceedings of the European Conference on Computer Vision (ECCV) (2020) 46. Lu, C., Krishna, R., Bernstein, M., Fei-Fei, L.: Visual relationship detection with language priors. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 852-869 (2016) 47. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: International Conference on 3D Vision (3DV) (2016) 48. Peyre, J., Sivic, J., Laptev, I., Schmid, C.: Weakly-supervised learning of visual relations. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2017) 49. Qi, M., Li, W., Yang, Z., Wang, Y., Luo, J.: Attentive relational networks for mapping images to scene graphs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 50. Qi, M., Wang, Y., Li, A.: Online cross-modal scene retrieval by binary representation and semantic graph. In: Proceedings of the ACM International Conference on Multimedia (ACM MM) (2017) 51. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems (2015) 52. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., Manning, C.D.: Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In: Proceedings of the fourth workshop on vision and language (2015) 53. Shi, J., Zhang, H., Li, J.: Explainable and explicit visual reasoning over scene graphs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 54. Suhail, M., Mittal, A., Siddiquie, B., Broaddus, C., Eledath, J., Medioni, G., Sigal, L.: Energy-based learning for scene graph generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 55. Tamura, M., Ohashi, H., Yoshinaga, T.: Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 56. Tang, K.: A scene graph generation codebase in pytorch (2020), https://github. com/KaihuaTang/Scene-Graph-Benchmark.pytorch 57. Tang, K., Niu, Y., Huang, J., Shi, J., Zhang, H.: Unbiased scene graph generation from biased training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 58. Tang, K., Zhang, H., Wu, B., Luo, W., Liu, W.: Learning to compose dynamic tree structures for visual contexts. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 59. Wang, S., Duan, Y., Ding, H., Tan, Y.P., Yap, K.H., Yuan, J.: Learning transferable human-object interaction detector with natural language supervision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 60. Wang, T., Anwer, R.M., Khan, M.H., Khan, F.S., Pang, Y., Shao, L., Laaksonen, J.: Deep contextual attention for human-object interaction detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 61. Wang, T., Yang, T., Danelljan, M., Khan, F.S., Zhang, X., Sun, J.: Learning human-object interaction detection using interaction points. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 62. Wang, W.: Mmscenegraph (2021), https://github.com/Kenneth-Wong/ MMSceneGraph 63. Xiong, Y., Liao, R., Zhao, H., Hu, R., Bai, M., Yumer, E., Urtasun, R.: Upsnet: A unified panoptic segmentation network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 64. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L.: Scene graph generation by iterative message passing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 65. Xu, P., Chang, X., Guo, L., Huang, P.Y., Chen, X., Hauptmann, A.G.: A survey of scene graph: Generation and application. IEEE Transactions on Neural Networks and Learning Systems (TNNLS) (2020) 66. Yang, C.A., Tan, C.Y., Fan, W.C., Yang, C.F., Wu, M.L., Wang, Y.C.F.: Scene graph expansion for semantics-guided image outpainting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) A PSG Dataset Details A.1 More comparisons between VG and PSG More comparisons between VG-150 and PSG examples are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. A1 :</head><label>A1</label><figDesc>More comparisons between VG-150 and PSG examples. Every subfigure has VG triplets and their groundings on the left and PSG on the right. Apart from precise pixel-wise grounding, PSG dataset gets rid of trivial relations (e.g., head-on-dog in (d), person-has-hair in (e)), and keep salient ones (e.g., person-holding-remote in (e)). Relations with background are also included (e.g., elephant-walking-on-grass).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. A2 :</head><label>A2</label><figDesc>PSG Dataset Constrution Process. The object categories and annotations in (a) VG-150 are different from that in COCO (image in (b) shows the COCO panoptic segmentation). After matching COCO objects to VG objects using the object matching process, any corresponding relationship annotations can also be transferred over from (a) to (b). These preliminary automatic but noisy relationship annotations are then sent to be processed by a final round of cleaning and annotation to produce the final PSG dataset in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(b) If the categories don't match, we don't match them and regard this pair as invalid. (c) Move on to the next object pair with the highest IoU (start from 2. again). Repeat until there are no object pair candidates left, or if the remaining pairs have an IoU of 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. A3 :</head><label>A3</label><figDesc>Proportion of Predicate Classes (Sum=100%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. A4 :</head><label>A4</label><figDesc>Distribution of the Number of Relations per image in the PSG Dataset. The bulk of the images have around 5 -10 relationship annotations, and ranges from 1 -43 annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. A5 :</head><label>A5</label><figDesc>Distribution of the Density of Relations per image in the PSG Dataset. The density of relations is defined as the number of annotated relations divided by the total number of possible relations in an image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparsion between classic SGG datasets and PSG dataset. #PPI counts predicates per image. DupFree checks whether duplicated object groundings are cleaned up. Spvn indicates whether the objects are grounded by bounding boxes or segmentations.</figDesc><table><row><cell>Dataset</cell><cell cols="5">#Image #ObjCls #RelCls #PPI DupFree Spvn</cell><cell>Source</cell></row><row><cell>VG [35]</cell><cell>108K</cell><cell>34K</cell><cell>40K</cell><cell>21.4</cell><cell cols="2">BBox COCO &amp; Flickr</cell></row><row><cell>VG-150 [64]</cell><cell>88K</cell><cell>150</cell><cell>50</cell><cell>5.7</cell><cell>BBox</cell><cell>Clean VG</cell></row><row><cell>VrR-VG [42] GQA [24] PSG</cell><cell>59K 85K 49K</cell><cell>1,600 1,703 133</cell><cell>117 310 56</cell><cell>3.4 50.6 5.7</cell><cell cols="2">BBox BBox Re-annotate VG Clean VG Seg Annotate COCO</cell><cell>Fig. 2: Word Cloud for PSG Predicates.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, VCTree [58], and GPSNet [44]</figDesc><table><row><cell>Panoptic FPN</cell><cell></cell></row><row><cell></cell><cell>Object Features</cell></row><row><cell>Feature Map</cell><cell>Relation Features</cell></row><row><cell>Image</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Triplet Queries Image Feature Map Backbone Transformer Encoder Positional Encoding + ? ? Triplet Decoder Subject Class Prediction FFN &amp; Panoptic Head Subject Mask Prediction Predicate Class Prediction Object Class Prediction Object Mask Prediction Subject Object (c) PSG Prediction Block (b) Query Learning Block (a) Input</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 reports</head><label>2</label><figDesc>the scene graph generation performance of all the methods mentioned in Sec. 4.1, Sec. 4.2, and Sec. 4.3 under the PSG dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison between all baselines and PSGFormer. Recall (R) and mean recall (mR) are reported. IMP [64] (CVPR'17), MOTIFS [71] (CVPR'18), VC-Tree [58] (CVPR'19), and GPSNet [44] (CVPR</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>/ 20.5 50.8 / 22.6 52.7 / 23.3 20.6 / 9.70 22.1 / 10.2 22.5 / 10.2</figDesc><table><row><cell cols="2">Backbone Method</cell><cell></cell><cell>Predicate Classification</cell><cell></cell><cell>Scene Graph Generation</cell></row><row><cell></cell><cell></cell><cell cols="4">R/mR@20 R/mR@50 R/mR@100 R/mR@20 R/mR@50 R/mR@100</cell></row><row><cell></cell><cell>IMP</cell><cell cols="4">31.9 / 9.55 36.8 / 10.9 38.9 / 11.6 16.5 / 6.52 18.2 / 7.05 18.6 / 7.23</cell></row><row><cell></cell><cell>MOTIFS</cell><cell cols="4">44.9 / 20.2 50.4 / 22.1 52.4 / 22.9 20.0 / 9.10 21.7 / 9.57 22.0 / 9.69</cell></row><row><cell></cell><cell cols="5">VCTree 45.3 GPSNet 31.5 / 13.2 39.9 / 16.4 44.7 / 18.3 17.8 / 7.03 19.6 / 7.49 20.1 / 7.67</cell></row><row><cell>ResNet-50</cell><cell>PSGTR</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.82 / 1.29 4.16 / 1.54 4.27 / 1.57</cell></row><row><cell></cell><cell>PSGFormer</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16.</cell></row></table><note>8 / 14.5 19.2 / 17.4 20.2 / 18.7 PSGTR? - - - 28.4 / 16.6 34.4 / 20.8 36.3 / 22.1 PSGFormer? - - - 18.0 / 14.8 19.6 / 17.0 20.1 / 17.6 ResNet-101 IMP 30.5 / 8.97 35.9 / 10.5 38.3 / 11.3 17.9 / 7.35 19.5 / 7.88 20.1 / 8.02 MOTIFS 45.1 / 19.9 50.5 / 21.5 52.5 / 22.2 20.9 / 9.60 22.5 / 10.1 23.1 / 10.3 VCTree 45.9 / 21.4 51.2 / 23.1 53.1 / 23.8 21.7 / 9.68 23.3 / 10.2 23.7 / 10.3 GPSNet 38.8 / 17.1 46.6 / 20.2 50.0 / 21.3 18.4 / 6.52 20.0 / 6.97 20.6 / 7.17 PSGTR - - - 3.47 / 1.18 3.88 / 1.56 4.00 / 1.64 PSGFormer - - - 18.0 / 14.2 20.1 / 18.3 21.0 / 19.8 PSGTR? - - - 28.2 / 15.4 32.1 / 20.3 35.3 / 21.5 PSGFormer? - - - 18.6 / 16.7 20.4 / 19.3 20.7 / 19.7</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>56 Predicates in the Dictionary -Positional Relations (6): over, in front of, beside, on, in, attached to. -Common Object-Object Relations (5): hanging from, on the back of, falling off, going down, painted on. -Common Actions (31): walking on, running on, crossing, standing on, lying on, sitting on, leaning on, flying over, jumping over, jumping from, wearing, holding, carrying, looking at, guiding, kissing, eating, drinking, feeding, biting, catching, picking (grabbing), playing with, chasing, climbing, cleaning (washing, brushing), playing, touching, pushing, pulling, opening. -Human Actions (4): cooking, talking to, throwing (tossing), slicing. -Actions in Traffic Scene (4): driving, riding, parked on, driving on. -Actions in Sports Scene (3): about to hit, kicking, swinging.</figDesc><table><row><cell>-Interaction between Background (3): entering, exiting, enclosing (sur-</cell></row><row><cell>rounding, warping in).</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image understanding using vision and reasoning through scene description graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferm?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reasoning with scene graphs for robot planning under partial observability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m">Enriching word vectors with subword information</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01111</idno>
		<title level="m">Scene graphs: A survey of generations and applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Say as you wish: Fine-grained control of image caption generation with abstract scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<idno>abs/2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning of visual relations: The devil is in the tails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic image manipulation using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous scene representations for embodied ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image captioning with scene-graph based semantic concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning and Computing</title>
		<meeting>the International Conference on Machine Learning and Computing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting and recognizing humanobject interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene graph reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML Workshop Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>GRL+</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual compositional learning for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affordance transfer learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextual translation embedding for visual relationship detection and scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segmentation-grounded scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hotr: End-to-end human-object interaction detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting visual relationships using box attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (CVPR-W)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops (CVPR-W)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linguistic structures as weak supervision for visual scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bridging knowledge graphs to generate scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning visual commonsense for robust scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mining the benefits of two-stage and one-stage hoi detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient two-stage detection of humanobject interactions with a novel unary-pairwise transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relationship proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">K-Net: Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to generate scene graph from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cascaded human-object interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
