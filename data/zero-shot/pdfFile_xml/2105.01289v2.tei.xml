<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Learning for Clustering via Building Consensus</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer</publisher>
				<availability status="unknown"><p>Copyright Springer</p>
				</availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><forename type="middle">Anand</forename><surname>Deshmukh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft, Mountain View</orgName>
								<address>
									<postCode>94043</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayanth</forename><surname>Reddy Regatti</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><surname>Manavoglu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urun</forename><surname>Dogan</surname></persName>
							<email>urundogan@gmail.com</email>
						</author>
						<title level="a" type="main">Representation Learning for Clustering via Building Consensus</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Nature</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Springer</publisher>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
					<note>? Work was done while author was at Microsoft</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Learning</term>
					<term>Clustering</term>
					<term>Representation Learning</term>
					<term>Consensus Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on unsupervised representation learning for clustering of images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be close in the representation space (exemplar consistency), and/or similar images must have similar cluster assignments (population consistency). We define an additional notion of consistency, consensus consistency, which ensures that representations are learned to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a single clustering algorithm. We define a clustering loss by executing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, consensus clustering using unsupervised representation learning (ConCURL), improves upon the clustering performance of state-of-the-art methods on four out of five image datasets. Furthermore, we extend the evaluation procedure for clustering to reflect the challenges encountered in real-world clustering tasks, such as maintaining clustering performance in cases with distribution shifts. We also perform a detailed ablation study for a deeper understanding of the proposed algorithm. The code and the trained models are available at https://github.com/JayanthRR/ConCURL NCE.</p><p>1. We introduce different notions of consistency (exemplar, population and consensus) that are used in unsupervised representation learning. 2. We propose a novel clustering algorithm that incorporates the above three consistency constraints and can be trained in an end-to-end way. An ensemble is generated in the consensus clustering objective by performing random transformations on the underlying embeddings. We combine several methods, which is not trivial, and this combination, along with our new consensus loss, is novel. 3. We show that the proposed algorithm ConCURL (consensus clustering with unsupervised representation learning) outperforms baselines on popularly used computer vision datasets when evaluated with clustering metrics. 4. We demonstrate the clustering abilities of trained models under a data shift and argue for the need for different evaluation metrics for deep clustering algorithms. 5. We study the impacts of various hyperparameters, data augmentation methods, and image resolutions on the clustering ability of the proposed algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of artificial intelligence (AI) advanced significantly in the previous decade due to developments in deep learning <ref type="bibr" target="#b30">(LeCun, Bengio, &amp; Hinton, 2015)</ref>. In the early years of this field, deep learning methods exhibited stellar supervised learning performance, where each data sample was coupled with a ground truth (labeled data), e.g., each image was associated with a category. Unfortunately, generating labeled datasets is time consuming and expensive, and there may not be enough experts to label the data at hand (e.g., medical images). The straightforward solution is to use clustering for large-scale problems.</p><p>In this work, we focus on representation learning for the unsupervised learning task of clustering images. Clustering is a ubiquitous task and has been actively used in many different scientific and practical pursuits <ref type="bibr" target="#b15">(Frey &amp; Dueck, 2007;</ref><ref type="bibr" target="#b25">Jain, Murty, &amp; Flynn, 1999;</ref><ref type="bibr" target="#b33">Masulli &amp; Schenone, 1999;</ref><ref type="bibr" target="#b52">Xu &amp; Wunsch, 2005)</ref>. Clustering algorithms do not learn representations and are hence limited to data for which we have a good representation available.</p><p>Advancements in deep learning techniques have enabled the end-to-end learning of rich image representations for supervised learning. For the purposes of clustering, however, such features learned via supervised learning cannot be obtained due to lack of available labels. Therefore, supervised learning approaches fall short of providing a solution. Self-supervised learning addresses the issue of learning representations without labeled data. Self-supervised learning is a subfield of unsupervised learning in which the main goal is to learn general-purpose representations by exploiting user-defined tasks (pretext tasks) <ref type="bibr" target="#b5">(Caron et al., 2020;</ref><ref type="bibr" target="#b9">Chen, Kornblith, Norouzi, &amp; Hinton, 2020;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr" target="#b20">He, Fan, Wu, Xie, &amp; Girshick, 2020;</ref><ref type="bibr" target="#b49">Z. Wu, Xiong, Yu, &amp; Lin, 2018;</ref><ref type="bibr" target="#b55">Zhuang, Zhai, &amp; Yamins, 2019;</ref><ref type="bibr" target="#b55">Zhuang et al., 2019)</ref>. Representation learning algorithms have been shown to achieve good results when evaluated using a linear evaluation protocol, semisupervised training on ImageNet, or transfer to downstream tasks. A straightforward solution to the clustering problem is to use the features obtained via self-supervised learning and apply an out-of-the-box clustering algorithm (such as k-means) to compute data clusters. However, the performance of these features for clustering (using an out-of-the-box clustering algorithm) is not known, and as seen in our results, these features may be improved for clustering purposes.</p><p>On the other hand, deep clustering involves simultaneously learning cluster assignments and features using deep neural networks. Simultaneously learning the feature spaces with a clustering objective in deep clustering may lead to degenerate solutions, which until recently limited end-to-end implementations of clustering with representation learning approaches <ref type="bibr" target="#b3">(Caron, Bojanowski, Joulin, &amp; Douze, 2018a)</ref>. Subsequently, several works have been developed <ref type="bibr" target="#b3">(Caron et al., 2018a;</ref><ref type="bibr" target="#b22">Huang, Gong, &amp; Zhu, 2020a;</ref><ref type="bibr" target="#b26">Ji, Henriques, &amp; Vedaldi, 2019a;</ref><ref type="bibr" target="#b36">Niu, Zhang, Wang, &amp; Liang, 2020a;</ref><ref type="bibr" target="#b40">Shah &amp; Koltun, 2018;</ref><ref type="bibr" target="#b43">Tao, Takagi, &amp; Nakata, 2021;</ref><ref type="bibr" target="#b47">J. Wu et al., 2019a;</ref><ref type="bibr" target="#b50">Xie, Girshick, &amp; Farhadi, 2016a)</ref>. We provide details regarding some of these works in Section 1.1. Our previous work shows some encouraging results but we extend the work substantially <ref type="bibr" target="#b38">(Regatti, Deshmukh, Manavoglu, &amp; Dogan, 2021)</ref>. We categorize the current clustering and representation learning works based on the consistency constraints that are used to define their objective functions. We define an additional notion of consistency, consensus consistency, which ensures that representations are learned to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a clustering algorithm. We use consensus consistency and propose an end-to-end learning approach that outperforms other end-to-end learning methods for image clustering. We summarize our contributions as follows:</p><p>1.1 Related Work Self-Supervised Learning: Self-supervised learning is used to learn representations in an unsupervised way by defining some pretext tasks. There are many different flavors of self-supervised learning, such as instance discrimination (ID) tasks (Z. <ref type="bibr" target="#b49">Wu et al., 2018;</ref><ref type="bibr" target="#b55">Zhuang et al., 2019)</ref> and contrastive techniques <ref type="bibr" target="#b9">(Chen et al., 2020;</ref><ref type="bibr" target="#b20">He et al., 2020)</ref>. In ID tasks, each image is considered its own category so that the learned embeddings are well separated (Z. <ref type="bibr" target="#b49">Wu et al., 2018)</ref>. Building on the ID task, <ref type="bibr" target="#b55">Zhuang et al. (2019)</ref> proposed a local aggregation (LA) method based on a robust clustering objective (using multiple runs of k-means) to move statistically similar data points closer in the representation space and dissimilar data points further away. In contrastive techniques such as simple contrastive learning of visual representations (SimCLR) <ref type="bibr" target="#b9">(Chen et al., 2020)</ref> and momentum contrast (MoCo) <ref type="bibr" target="#b19">(He, Fan, Wu, Xie, &amp; Girshick, 2019)</ref>, representations are learned by maximizing the agreement between different the augmented views of the same data example (known as positive pairs) and minimizing the agreement between the augmented views of different examples (known as negative pairs). Recent works, including Bootstrap Your Own Latent (BYOL) <ref type="bibr">(Grill et al., 2020)</ref> and swapping assignments between multiple views (SwAV) <ref type="bibr" target="#b5">(Caron et al., 2020)</ref>, have achieved state-of-the-art results without requiring negative pairs. Although self-supervised learning methods exhibit impressive performance on a variety of problems, it is not clear whether learned representations are good for clustering. Clustering with Representation Learning:</p><p>DEC <ref type="bibr" target="#b50">(Xie et al., 2016a)</ref> is one of the first algorithms to show that deep learning can be used to effectively cluster images in an unsupervised manner; this approach uses features learned from an autoencoder to fine-tune the cluster assignments. Deep-Cluster <ref type="bibr" target="#b3">(Caron et al., 2018a)</ref> shows that it is possible to train deep convolutional neural networks (DeCNNs) in an end-to-end manner with pseudolabels that are generated by a clustering algorithm. Subsequently, several works <ref type="bibr" target="#b22">(Huang et al., 2020a;</ref><ref type="bibr" target="#b26">Ji et al., 2019a;</ref><ref type="bibr" target="#b36">Niu et al., 2020a;</ref><ref type="bibr" target="#b40">Shah &amp; Koltun, 2018;</ref><ref type="bibr" target="#b47">J. Wu et al., 2019a)</ref> have introduced end-to-end clustering-based objectives and achieved state-of-the-art clustering results. For example, in the Gaussian attention network for image clustering (GATCluster) <ref type="bibr" target="#b36">(Niu et al., 2020a)</ref>, training is performed in two distinct steps (similar to <ref type="bibr" target="#b3">Caron et al. (2018a)</ref>), where the first step is to compute pseudotargets for a large batch of data and the second step is to train the model in a supervised way using these pseudotargets. Both DeepCluster and GATCluster use k-means to generate pseudolabels that may not scale well. J. <ref type="bibr" target="#b47">Wu et al. (2019a)</ref> proposed deep comprehensive correlation mining (DCCM), where discriminative features are learned by taking advantage of the correlations among the data using pseudolabel supervision and the triplet mutual information among the features. However, DCCM may be susceptible to trivial solutions <ref type="bibr" target="#b36">(Niu et al., 2020a)</ref>. Invariant information clustering (IIC) <ref type="bibr" target="#b26">(Ji et al., 2019a)</ref> maximizes the mutual information between the class assignments of two different views of the same image (paired samples) to learn representations that preserve the commonalities between the views while discarding instance-specific details. It has been argued that the presence of an entropy term in mutual information plays an important role in avoiding degenerate solutions. However, a large batch size is needed for the computation of mutual information in IIC; this process may not be scalable for larger image sizes, which are common in popular datasets <ref type="bibr" target="#b26">(Ji et al., 2019a;</ref><ref type="bibr" target="#b36">Niu et al., 2020a)</ref>. <ref type="bibr" target="#b22">Huang et al. (2020a)</ref> extended the celebrated maximal margin clustering idea to the deep learning paradigm by learning the most semantically plausible clusters through the minimization of a proposed partition uncertainty index. Their pixel intensity clustering (PICA) algorithm uses a stochastic version of this index, thereby facilitating minibatch training. PICA fails to assign a sample-correct cluster when that sample has either high foreground or background similarity to samples in other clusters. In a more recent approach, contrastive clustering <ref type="bibr" target="#b31">(Li et al., 2021)</ref>, a contrastive learning loss (as in SimCLR <ref type="bibr" target="#b9">(Chen et al., 2020)</ref>) was adopted along with an entropy term to avoid degenerate solutions. Similarly, <ref type="bibr" target="#b43">Tao et al. (2021)</ref> combined ID (Z. <ref type="bibr" target="#b49">Wu et al., 2018)</ref> with novel softmax-formulated decorrelation constraints for representation learning and clustering. Their approach outperforms state-of-the-art methods and improves upon the instance discrimination method. Our method also improves upon ID and outperforms the method of <ref type="bibr" target="#b43">Tao et al. (2021)</ref> on all datasets considered. There are other non-end-to-end approaches, such as <ref type="bibr">SCAN (Van Gansbeke, Vandenhende, Georgoulis, Proesmans, &amp; Van Gool, 2020)</ref>, which use the learned representations from a pretext task to find the images that are semantically closest to the given image using the nearest neighbors algorithm. Similarly, one more state of the art non-end-to-end approach, SPICE <ref type="bibr" target="#b35">(Niu, Shan, &amp; Wang, 2021)</ref> divides the clustering network in two parts -one to measure instance level similarity and one to identify cluster level discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Consensus Clustering</head><p>One of the distinguishing factors between supervised learning and unsupervised learning is the existence of ground truth labels that construct a global constraint based on examples. In most self-supervised learning methods, the ground truth is replaced with some consistency constraint <ref type="bibr" target="#b9">(Chen et al., 2020)</ref>. Without a doubt, the performance of any self-supervised method is a function of the power of the consistency constraint used. We define two types of consistency constraints: exemplar consistency and population consistency.</p><p>Definition 1. Exemplar consistency: Representation learning algorithms that learn closer representations (in terms of some distance metric) for different augmentations of the same data point are said to follow exemplar consistency.</p><p>Examples of the usage of exemplar consistency include contrastive learning methods such as MoCo <ref type="bibr" target="#b19">(He et al., 2019)</ref> and SimCLR <ref type="bibr" target="#b9">(Chen et al., 2020)</ref>. In these methods, a positive pair of images is defined as any two image augmentations of the same image, and a negative pair consists of any two different images.</p><p>Definition 2. Population consistency: Representation learning algorithms that ensure that learned representations satisfy the consistency constraint, where two similar data points or any augmentations of the same data points should belong to the same cluster (or population), are said to follow population consistency.</p><p>Deep Cluster <ref type="bibr" target="#b3">(Caron et al., 2018a</ref>) is a prominent self-supervised method that utilizes population consistency, i.e., Definition 2, by enforcing a clustering constraint on the input dataset. Please note that each cluster assignment contains data points that are similar to each other. Similarly, SwAV <ref type="bibr" target="#b5">(Caron et al., 2020)</ref> is an example of the population consistency method.</p><p>Definition 3. Consensus consistency: Representation learning algorithms that are able to learn representations that induce similar partitions for variations in the given representation space (subsets of features, random projections, etc. ), different clustering algorithms (k-means, Gaussian mixture models (GMMs), etc.) or different initializations of clustering algorithms are said to follow consensus consistency.</p><p>Earlier works on consensus consistency did not consider representation learning and used the knowledge reuse framework (see <ref type="bibr" target="#b42">Strehl and Ghosh (2002)</ref>, <ref type="bibr" target="#b16">Ghosh and Acharya (2011))</ref>, where the cluster partitions were available (the features were irrelevant) or the features of the data were fixed. For example, <ref type="bibr" target="#b12">Fern and Brodley (2003)</ref> successfully applied random projections to consensus clustering by performing k-means clustering on multiple random projections of the fixed features of input data. In contrast, the notion of consensus consistency here deals with learning representations that achieve a consensus regarding the cluster assignments of multiple clustering algorithms. One example of a method that enforces consensus consistency is LA <ref type="bibr" target="#b55">(Zhuang et al., 2019)</ref>. LA builds on the ID task (Z. <ref type="bibr" target="#b49">Wu et al., 2018)</ref> and was proposed as a method based on a robust clustering objective (using multiple runs of k-means) to move statistically similar data points closer in the representation space and dissimilar data points further away. However, <ref type="bibr" target="#b55">Zhuang et al. (2019)</ref> did not evaluate the method with clustering metrics and only focused on linear evaluation using the learned features. Subsequently, we conducted a study to evaluate the clustering performance of these features (see Appendix) and observed that LA performed poorly when evaluated for clustering accuracy. In Definition 3, we inherently assume that the clustering algorithms under consideration have been tuned properly. Unfortunately, the definition of consensus consistency is ill posed, and there can be arbitrarily many different partitions that can satisfy the given condition 1 . We show that when exemplar consistency is used as an inductive bias, the resulting objective function achieves impressive performance on challenging datasets. Combining the exemplar and population constraints with consensus consistency seamlessly and effectively for clustering is the basis of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Loss for Consensus and Population Consistency</head><p>We focus on learning generic representations that satisfy Definition 3 for clustering. By using different clustering algorithms or different representation variations (such as projections), one can easily generate multiple different partitions of the same data. In unsupervised learning, it is not known which partitioning is correct. To tackle this problem, some additional assumptions are needed.</p><p>We assume that there is an underlying latent space Z * (possibly not unique) such that all clusterings (based on latent space, algorithm or initialization variations) that take input data from this latent space produce similar data partitions. Furthermore, every clustering algorithm that also takes the true number of clusters as input produces the partition that is closest to the hypothetical ground truth. Moreover, we assume that there exists a function h : X ? Z * , where X represents the input space and Z * represents the underlying latent space. We call this assumption the principle of consensus. The open question is how one constructs an efficient loss that reflects the principle of consensus. We define one such way below.</p><p>Given an input batch of images X b ? X , the goal is to partition these images into K clusters. We obtain p views of these images (by different image augmentation approaches) and define a loss such that cluster assignment of any of the p views matches the target estimated from any other view. Without loss of generality, we define a loss for p = 2 views. The two views X 1 b , X 2 b are generated using two randomly chosen image augmentations.</p><p>We learn a representation space Z 0 at the end of every training iteration and obtain M variations of Z 0 as {Z 1 , Z 2 , ..., Z M } (e.g., random projections). The goal is to build an efficient loss according to the principle of consensus among Z 0 and its M variations {Z 1 , Z 2 , ..., Z M } such that we learn the latent space Z * at the end of training (i.e., the learned features lie in the latent space described above). For a given batch of images X b and a representation space Z m , ?m ? [1, ..., M ], we denote the cluster assignment probability of image i and cluster j for view 1 as p 1 i,j (Z m ) and that for view 2 as p 2 i,j (Z m ). We concisely usep <ref type="bibr">(1,m)</ref> ,p <ref type="bibr">(2,m)</ref> when we talk about all the images and all the clusters. Here, we define a loss that incorporates "population consistency" and "consensus consistency". We assume that the target cluster assignment probabilities for the representation Z 0 are given (as in DeepCluster <ref type="bibr" target="#b3">(Caron et al., 2018a)</ref>), and they are denoted as q 1 i,j for view 1 and q 2 i,j for view 2. We define the loss for any representation space Z and batch of images X b as</p><formula xml:id="formula_0">L 1 Zm = ? 1 2B B i=1 K j=1 q 2 ij log p 1 ij (Z m ) L 2 Zm = ? 1 2B B i=1 K j=1 q 1 ij log p 2 ij (Z m ), L Z = M m=1 L 1 Zm + L 2 Zm .</formula><p>(1)</p><p>Note that here, consensus among the clustering results is defined via the number of common targets q. An overview of the procedure is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The exact details regarding how to obtain variations of Z 0 and calculate the cluster assignment probabilities p and targets q are described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">End-to-End Stochastic Gradient Descent (SGD)-Based Trainable Consensus Loss</head><p>In this section, we propose an end-to-end trainable algorithm and define a way to compute p and q. When the cluster assignment probabilities p can take any values in the set [0, 1], we refer to the process as soft clustering, and when p is restricted to the set {0, 1}, we refer to the process as hard clustering. Without loss of generality, in this paper, we focus on soft clustering, which makes it easier to define a loss function using the probabilities and update the parameters using the gradients to enable end-to-end learning. We follow the soft clustering framework presented in SwAV <ref type="bibr" target="#b5">(Caron et al., 2020)</ref>, which is a centroidbased technique that aims to maintain consistency between the clusterings of the augmented views X 1 b and X 2 b . We store a set of randomly initialized prototypes C 0 = {c 1 0 , ? ? ? , c K 0 } ? R d?K , where K is the number of clusters and d is the dimensionality of the prototypes. These prototypes are used to represent clusters and define a "consensus consistency" loss. We compute M variations of C 0 as C 1 , ..., C M exactly as we compute the M variations of Z 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Cluster assignment probability p</head><p>We use a two-layer multilayer perceptron (MLP) g to project the features f 1 = f ? (X 1 b ) and f 2 = f ? (X 2 b ) to a lower-dimensional space Z 0 (of size d). The outputs of this MLP (referred to as cluster embeddings) are denoted as Z 1 0 = {z 1,1 0 , . . . , z 1,B 0 } and Z 2 0 = {z 2,1 0 , . . . , z 2,B 0 } for view 1 and view 2, respectively. Note that h : X ? Z defined in 2.1 is equivalent to the composite function of f : X ? ? and g : ? ? Z. For a latent space Z, we compute the probability of assigning a cluster j to image i using the normalized vectorsz 1,i = z 1,i z 1,i ,z 2,i = z 2,i z 2,i andc j = c j c j as</p><formula xml:id="formula_1">p 1 i,j (Z, C) = exp( 1 ? z 1 i ,c j ) j exp( 1 ? z 1 i ,c j ) , p 2 i,j (Z, C) = exp( 1 ? z 2 i ,c j ) j exp( 1 ? z 2 i ,c j )</formula><p>.</p><p>(2)</p><p>We concisely write p 1</p><formula xml:id="formula_2">i (Z) = {p 1 i,j (Z, C)} K j=1 and p 2 i = {p 2 i,j (Z, C)} K j=1 .</formula><p>Here, ? is a temperature parameter, and we set its value to 0.1, similar to <ref type="bibr" target="#b5">Caron et al. (2020)</ref>. Note that we use p i to denote the predicted cluster assignment probabilities for image i (when not referring to a particular view), and the shorthand notation p is used when i is clear from context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Targets q</head><p>The idea of predicting the assignments p and then comparing them with the highconfidence estimates q (referred to as codes henceforth) of the predictions was proposed by <ref type="bibr" target="#b50">Xie et al. (2016a)</ref>. While <ref type="bibr" target="#b50">Xie et al. (2016a)</ref> used pretrained features (from autoencoders) to compute the predicted assignments and the codes, the use of their approach in an end-to-end unsupervised manner might lead to degenerate solutions. <ref type="bibr" target="#b0">Asano, Rupprecht, and Vedaldi (2019)</ref> avoided such degenerate solutions by enforcing an equipartition constraint (the prototypes equally partitioned the data) during code computation using the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b10">(Cuturi, 2013)</ref>. <ref type="bibr" target="#b5">Caron et al. (2020)</ref> followed a similar formulation but computed the codes for the two views separately in an online manner for each minibatch. The assignment codes are computed by solving the following optimization problem:</p><formula xml:id="formula_3">Q 1 = arg max Q?Q Tr(Q T C T 0 Z 1 0 ) + H(Q) Q 2 = arg max Q?Q Tr(Q T C T 0 Z 2 0 ) + H(Q),<label>(3)</label></formula><formula xml:id="formula_4">where Q = {q 1 , . . . , q B } ? R K?B + , Q is the transportation polytope defined by Q = {Q ? R K?B + s.t Q1 B = 1 K 1 K , Q T 1 K = 1 B 1 B } 1 K is a vector of ones of dimension K and H(Q) = ? i,j Q i,j log Q i,j .</formula><p>The above optimization is computed using a fast version of the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b10">(Cuturi, 2013)</ref>, as described by <ref type="bibr" target="#b5">Caron et al. (2020)</ref>.</p><p>After computing the codes Q 1 and Q 2 , to maintain the consistency between the clustering results of the augmented views, the loss is computed using the probabilities p ij and the assigned codes q ij by comparing the probabilities of view 1 with the assigned codes of view 2 and vice versa, as in equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Defining variations of Z 0 and C 0</head><p>To compute {Z 1 , ..., Z M }, we project the d-dimensional space Z 0 to a Ddimensional space using a random projection matrix. We follow the same procedure to compute {C 1 , ..., C M } from C 0 . At the beginning of the algorithm, we randomly initialize M such transformations and fix them throughout training. Suppose that by using a particular random transformation (a randomly generated matrix A), we obtai? z = Az,c = Ac. We then compute the softmax probabilities using the normalized vectorsz/ z andc/ c . This step is repeated with the M transformation results in the M predicted cluster assignment probabilities for each view. When the network is untrained, the embeddings z are random, and applying the random transformations, followed by computing the predicted cluster assignments, leads to a diverse set of soft cluster assignments. The parameter weights are trained by using the stochastic gradients of the loss for updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Backbone loss</head><p>To better capture exemplar consistency, based on previous evidence of successful clustering with the ID approach <ref type="bibr" target="#b43">(Tao et al., 2021)</ref>, we use ID (Z. <ref type="bibr" target="#b49">Wu et al., 2018)</ref> as one of the losses, as in <ref type="bibr" target="#b43">Tao et al. (2021)</ref>. The exemplar objective of ID is to classify each image as its own class.</p><p>Given n images and a neural network f ? for calculating features, we first normal-</p><formula xml:id="formula_5">ize the featuresf ? (x) = f ? (x) f ? (x)</formula><p>. Then, ID defines the probability of an example x being recognized as the i-th example as</p><formula xml:id="formula_6">P (i|f ? (x)) = exp f ? (x i ),f ? (x) /? n j=1 exp f ? (x j ),f ? (x) /? .<label>(4)</label></formula><p>ID then uses the uniform distribution as a noise distribution P n = 1 n to compute the probability that data example x comes from a data distribution P d as opposed to the noise distribution P n as h(i, f ? (x)) := P (i|f ? (x)) P (i|f ? (x))+mPn(i) . Assuming that the noise samples are m times more frequent than actual data samples, the ID loss is defined as</p><formula xml:id="formula_7">L b = ?E P d [log h(i, x)] ? mE Pn [log(1 ? h(i, x ))] ,<label>(5)</label></formula><p>where x is the feature from a randomly drawn image other than image x in a given dataset. We exactly follow the framework developed in Z. <ref type="bibr" target="#b49">Wu et al. (2018)</ref> to implement the ID loss.</p><p>The final loss that we seek to minimize is the combination of the losses L Z (equation 1) and L b (equation 5),</p><formula xml:id="formula_8">L total = ?L Z + ?L b .<label>(6)</label></formula><p>where ?, ? are nonnegative constants. Details of the algorithm are given Algorithm 1 and we also provide a PyTorch-style psuedocode in Algorithm 2 in the Appendix.</p><p>Algorithm 1 Consensus Clustering algorithm (ConCURL)</p><formula xml:id="formula_9">1: Dataset X = {x i } N i=1 , number clusters K, batch size B, weights (?, ?)</formula><p>, number of random transformations M , projection dimension d 2: Randomly initialize network the networks f ? , g; K prototypes (c 1:K ), and M random projection matrices to dimension d (R 1:M ) and epoch e = 0 3: while e &lt; total epoch number do 4:</p><formula xml:id="formula_10">for b ? {1, 2, . . . , N B } do 5: Select B samples as X b from X 6:</formula><p>Make a forward pass on the two views:</p><formula xml:id="formula_11">f ? (X 1 b ) and f ? (X 2 b ) 7: z 1 1:B ?? g(f ? (X 1 b )), z 2 1:B ?? g(f ? (X 2 b )) 8:</formula><p>Compute loss L b using (4, 5) 9:</p><p>Compute p 1 i,j , p 2 i,j for both views using <ref type="formula">(2)</ref> 10:</p><p>Compute codes of the current batch q using <ref type="formula" target="#formula_3">(3)</ref> 11:</p><formula xml:id="formula_12">for m ? {1, 2, . . . , M } do 12:z ?? Rmz,c ?? Rmc 13: Compute (p (1,m) i,j ,p (2,m) i,j</formula><p>) for normalizedz,c using <ref type="formula">(2)</ref> 14:</p><p>end for 15:</p><p>Compute loss L Z using (1) <ref type="bibr">16:</ref> Compute total loss (6). Update the network f ? , g parameters, and prototypes using gradients 17: end for 18: e := e + 1 19: end while 20: Make forward pass on all the data and store the features 21: Compute k-means clustering of the features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Computing the cluster metrics</head><p>In this section, we describe the approach used to compute the cluster assignments and the metrics chosen to evaluate their quality. Note that we assume that the number of true clusters (K) in the data is known.</p><p>There are two ways to compute the cluster assignments. The first way is to use the embeddings generated by the backbone; here, the embeddings are the outputs of the ID block f ? (x). The embeddings of all the images are computed, and then we perform k-means clustering.</p><p>The second method is to use the soft clustering block to compute the cluster assignments. It is sufficient to use the computed probability assignments {p i } N i=1 or the computed codes {q i } N i=1 and assign the cluster index as c i = arg max k q ik for the i th data point. Once the model is trained, in this second approach, cluster assignment can be performed online without requiring the computation of the embeddings of all the input data.</p><p>We evaluate the quality of the clusterings using metrics such as the cluster accuracy, normalized mutual information (NMI), and adjusted Rand index (ARI). To compute the clustering accuracy, we are required to solve an assignment problem (computed using a Hungarian match <ref type="bibr" target="#b28">(Kuhn, 1955</ref><ref type="bibr" target="#b29">(Kuhn, , 1956</ref>) between the true class labels and the cluster assignments. In our analysis, we observe that using k-means with the embeddings produced by the ID block achieves better clustering accuracy, and we use this method throughout the paper while evaluating our proposed algorithm.  <ref type="bibr" target="#b14">Fred and Jain (2005)</ref> discussed different ways to generate cluster ensembles; these methods are tabulated in <ref type="table" target="#tab_0">Table 1</ref>. In our proposed algorithm, we focus on choosing of the appropriate data representation to generate cluster ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generating Multiple Clustering Results</head><p>By fixing a stable clustering algorithm, we can generate arbitrarily large ensembles by applying different transformations on the embeddings. Random projections were previously successfully used in consensus clustering <ref type="bibr" target="#b12">(Fern &amp; Brodley, 2003)</ref>. By generating ensembles using random projections, we have control over the amount of diversity we can induce into the framework by varying the dimensionality of the random projection. In addition to random projections, we also use diagonal transformations <ref type="bibr" target="#b21">(Hsu, Levine, &amp; Finn, 2018)</ref> where different components of the representation vector are scaled differently. <ref type="bibr" target="#b21">Hsu et al. (2018)</ref> illustrated that such scaling enables a diverse set of clusterings, which is helpful for the meta learning task. We study ablations over the number of transformations needed and the dimensions of these transformations in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Understanding the Consensus Objective</head><p>We investigate a potential hypothesis regarding "training driven by noisy cluster assignments" that can shed light on the success of ConCURL 2 . The hypothesis stems from the following intuition. Using different clustering algorithms, the generated cluster assignments are noisy versions of the hypothetical ground truth; as the training process progresses, the noise in the cluster assignments is reduced, and eventually all the different clustering algorithms considered generate similar cluster assignments.  We verify this hypothesis empirically with the help of the following experiments on the STL-10 dataset: (i) we observe the noisy clusterings generated by using random projections and (ii) verify that the noise in the cluster assignments is reduced as training progresses.</p><p>For the purpose of demonstrating noisy cluster assignments, we use synthetic data as follows. We generate three clusters in R 2 , as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), and compute the centroids of each cluster. Here, the centroids act as the prototypes. We then generate a Gaussian random projection matrix A with a dimensionality of R 2?2 . We first normalize the embeddings (2-dimensional features) and the centroids (see <ref type="figure" target="#fig_1">Figure 2</ref>(b)). Using the matrix A, we transform both the embeddings and prototypes for the new space and normalize the resultant vectors.</p><p>We follow the soft clustering framework discussed earlier and compute the soft cluster assignments for the original and transformed data. We observe that the cluster assignment probabilities in the new space are noisy versions of the cluster assignment probabilities in the original space (see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>To verify that the noise in the cluster assignment probabilities is reduced as training progresses, we perform the following experiment. We measure the similarity among the cluster assignments at every epoch to observe the effect of consensus as training progresses. For each random projection used, we use the cluster assignment probabilitiesp and compute cluster assignments by taking an arg max onp for each image. We obtain M such cluster assignments due to the M random projections. We then compute a pairwise NMI (similar to the analysis of <ref type="bibr" target="#b12">Fern and Brodley (2003)</ref>) between every two cluster assignments and compute the average and standard deviation of the pairwise NMI values across the M (M ?1) implies that the two clusters are uncorrelated. We observe from <ref type="figure" target="#fig_1">Figure 2</ref> that the pairwise NMI increases as training progresses and becomes closer to 1. At the beginning of training, the cluster assignments are very diverse (small NMI scores with a large standard deviation), and as training progresses, the diversity is reduced (large NMI scores with a smaller standard deviation). This observation leads us to conclude that for the applied clustering algorithms (defined using different random projections), we have learned an embedding space where the different cluster assignments concur. In other words, "consensus consistency" is achieved. Additionally, it is evident from our empirical results in Section 4 that we achieve an improved overall clustering accuracy.</p><p>If noisy cluster assignments are the reason behind the improved performance, one might wonder if it is sufficient to simply add noise to the original cluster assignments rather than computing multiple cluster assignments. However, this may not be fruitful because if noise is added externally, one must define a scheduler to reduce the noise as training progresses. However, in the case of ConCURL, the end-to-end learning algorithm determines the rate of consensus or agreement between p andp itself. In the next section, we provide empirical evidence of the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>Evaluating clustering algorithms is a notoriously hard problem. The reference text <ref type="bibr" target="#b24">Jain and Dubes (1988)</ref> states the following: The validation of clustering structures is the most difficult and frustrating part of cluster analysis. Without a strong effort in this direction, cluster analysis will remain a black art accessible only to those true believers who have experience and great courage.</p><p>In the literature on representation learning for clustering, e.g., <ref type="bibr" target="#b23">Huang, Gong, and Zhu (2020b)</ref>; <ref type="bibr" target="#b31">Li et al. (2021)</ref>; <ref type="bibr" target="#b43">Tao et al. (2021)</ref>, to evaluate the performance of different algorithms, the following methodology has been used: as a set of models with some hyperparameters are trained, these models are sorted by the observed clustering performance. Finally the best model's results are reported. This methodology is called max performance in the remainder of the paper. We assess the quality of the learned embeddings by using five challenging image datasets for clustering and report their performance with the max-performance strategy. Although the maxperformance procedure provides some insights into the performance of the method under consideration, we provide additional insights by significantly extending the evaluations. In practice, it is desirable that the learned models can be utilized for different datasets other than the training dataset. However, the max-performance method may not be suitable for this purpose. To address this, we design two additional experiments that focus on the performance of cross-model features under a distribution shift. Furthermore, we also assess the quality of the learned embeddings in image retrieval tasks. Finally, we present a detailed ablation study to assess the impact of the loss terms, data augmentation methods, hyperparameters and architecture choices utilized to obtain a more complete picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Clustering with the Max-Performance Strategy</head><p>We evaluate our algorithm and compare it with existing methods on some popular image datasets, namely, ImageNet-10, ImageNet-Dogs, STL-10, CIFAR-10, and CIFAR100-20.</p><p>For CIFAR100-20, we use the 20 meta classes as the class labels while evaluating the clustering results. For STL-10, similar to the earlier PICA <ref type="bibr" target="#b22">(Huang et al., 2020a)</ref> and GATCluster <ref type="bibr" target="#b36">(Niu et al., 2020a</ref>) approaches, we use both training and testing splits for training and evaluation. Note that PICA also uses an unlabeled data split with 100k points in STL-10, which we do not use. ImageNet-10 and ImageNet-Dogs are subsets of ImageNet, and we use only the training splits for these two datasets <ref type="bibr" target="#b11">(Deng et al., 2009)</ref>. We use the same classes as  for evaluation on the ImageNet-10 and ImageNet-Dogs datasets. The dataset summary is given in <ref type="table" target="#tab_3">Table 3</ref>. We evaluate the cluster accuracy, NMI, and ARI of each computed cluster assignment (see the Appendix for details). In our comparison, we consider some state-of-the-art methods that were developed for image clustering problems and targeted for end-to-end training scenarios with random initialization. We should note that we do not consider baselines that use prior   information, e.g., the nearest neighbors algorithm derived by using pretrained models. The implementation details of ConCURL are provided in the Appendix, and the results are presented in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>We observe that ConCURL outperforms the baseline algorithms considered in terms of all three metrics for all the datasets except STL-10. ConCURL improves the state-of-the art clustering accuracy by approximately 17.5% on ImageNet-Dogs, by 12.7% on CIFAR100-20 and by 3.8% on CIFAR-10. Although ConCURL improves upon the results of ID <ref type="bibr" target="#b43">(Tao et al., 2021)</ref>, please note that ID is the backbone used in this paper and is slightly worse than IDFD, as shown in <ref type="bibr" target="#b43">Tao et al. (2021)</ref>.</p><p>The proposed method achieves good clustering performance on popular computer vision datasets. Similar to all the algorithms considered, we assume that K, the number of clusters, is known. However, this may not hold true in practice in real-world applications. In such a case, we may assume an estimate for the upper bound on the number of clusters to use as the number of prototypes. Additionally, we also assume that the dataset is equally distributed among the K clusters. If this assumption (also common in the literature <ref type="bibr" target="#b22">(Huang et al., 2020a;</ref><ref type="bibr" target="#b36">Niu et al., 2020a)</ref>) does not hold, the fast Sinkhorn-Knopp algorithm used to solve Eq. 3 may not be optimal. In the previous section, we studied the clustering performance on the training split used to train the algorithm. Here, we shall evaluate the clustering performance on a held out test set. We shall use the standard test split for the datasets CIFAR-10, CIFAR100-20, ImageNet-10, and ImageNet-Dogs. We used the trained models to extract the features for each test dataset and compute the clustering as above. We observe from <ref type="table" target="#tab_6">Table 6</ref> that the performance doesn't get affected much on the test set. This shows that the algorithm is able to extract good feature representations for clustering on data not used for training which shows good generalization ability of the algorithm when the data is drawn from the same distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Performance on the Test split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Class-specific accuracy</head><p>We present the class-specific accuracies (percentage) and confusion matrices in <ref type="figure" target="#fig_4">Figure 4</ref>. In each row i, j th entry in the matrix represents the percentage of samples from category i belonging to the cluster of category j. For better visualization, we round each percentage to the nearest integer.Row sum may not be equal to 100 because we are rounding to the nearest integer. For perfect clustering, all elements along the diagonal should be equal to 100. Here, we note some interesting observations. For ImageNet-10, the airliner category shows the worst performance, with 7% of airliner samples being confused with the airship category. Additionally, 4% of the samples from the orange category are categorized as soccer balls. In ImageNet-Dogs, there are three types of spaniels-Blenheim spaniels, Brittany spaniels and Welsh springer spaniel, which look very similar to each other. Nineteen percent of the samples from the Blenheim spaniel category are categorized as Brittany spaniels, and 5% are categorized as Welsh springer spaniels. Similarly, 16% of the samples from    For STL-10, none of the samples from the cat category are categorized correctly. Even though STL-10 and CIFAR-10 have the same list of categories, STL-10 seems harder to cluster than CIFAR-10. Note that STL-10 has only 13000 images to learn representations, while in CIFAR-10, 60000 images are used. For CIFAR100-20, none of the samples from aquatic mammals are categorized correctly. Thirty-seven percent of the samples from aquatic mammals are categorized as fish, and 19% are categorized as large omnivores and herbivores. In the case of reptiles, only 15% of the examples are categorized correctly, but 23% are categorized as fish, 16% as large carnivores and 13% as insects. Surprisingly, 28% of the samples from trees are categorized as aquatic mammals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Out-of-Distribution Results</head><p>In this section, we evaluate ConCURL by performing clustering on dataset that is not used during training. We focus mainly on studying the clustering performance on datasets that maybe similar to the training dataset and datasets that may have a different number of clusters than the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Cross-Model Accuracy</head><p>Here, we calculate the clustering performance achieved when the model is trained on one dataset but evaluated on a different dataset that may have a different number of clusters. For example, the first row in <ref type="table" target="#tab_9">Table 7</ref> gives the performance of the model trained on ImageNet-10 and evaluated on both ImageNet-10 and ImageNet-Dogs. Similarly, the second row shows the performance of the model trained on ImageNet-Dogs. We find that performance on ImageNet-10 is decreased to 35.6% when the model trained on ImageNet-Dogs is used instead of the model trained on ImageNet-10. Similarly, the performance on ImageNet-Dogs is decreased to 17.7 % when the model trained on ImageNet-10 is used instead of the model trained on ImageNet-Dogs. <ref type="table" target="#tab_10">Table 8</ref> provides the same performance metrics for CIFAR-10 and CIFAR100-20.</p><p>For cross-model performance to be high, the embedding function must be generalizable to the out of distribution dataset. It is important to observe that for each pair of datasets considered, the distributions of the datasets are very different due to the classes being completely different in both cases (ImageNet-10 vs ImageNet-Dogs, and CIFAR-10 vs CIFAR100-20). However, since we are considering datasets with a small number of datapoints and small number of classes (see <ref type="table" target="#tab_3">Table 3</ref>), the representation power of the learnt embeddings is limited and this affects the cross-model accuracy. Moreover, the consensus loss L Z here assumes knowledge of the number of clusters in the dataset. Therefore, the embeddings learnt by optimizing the L total loss on one dataset may be sub-optimal for evaluating clustering on a dataset with different number of clusters.</p><p>It is clear that these performance drops are significant, and the generalization performance of the learned embeddings needs to be assessed by taking out-ofdistribution datasets into account. However, since there are only 2 groups of different datasets, it is difficult to reach a definitive conclusion. Hence, in the following section, we propose a new evaluation methodology that sheds light on the out-of-distribution performance of the learned embeddings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">ImageNet random-10 and random-15 accuracies</head><p>Here, we compare the baseline model trained with ID (Z. <ref type="bibr" target="#b49">Wu et al., 2018)</ref> with our proposed method ConCURL. We randomly sample 10 and 15 classes from the 1000class ImageNet data and evaluate the clustering accuracy obtained on the training split of the data using the model trained on the original ImageNet-10 and ImageNet-Dogs sets. We repeat the process 100 times for both the 10-class and 15-class datasets and call them the random-10 and random-15 datasets, respectively. Note that we do not retrain the model on the randomly sampled dataset; we only evaluate the model on this set. We show the histogram of the obtained accuracies for these 100 random datasets. In <ref type="figure">Figure 5</ref>, we compare the accuracy of the ConCURL model and the baseline ID model trained on ImageNet-10 on both random-10 and random-15. Along with the histogram, we show a Gaussian distribution (along the red dotted line) with first and second moments equal to the average and standard deviation of all accuracies, respectively. Similarly, in <ref type="figure">Figure 6</ref>, we show the accuracies obtained based on models trained on ImageNet-Dogs. Among the models trained on ImageNet-10, the baseline ID model performs slightly better than the proposed ConCURL model. The trend is reversed for the evaluation based on the model trained on ImageNet-Dogs,</p><p>where ConCURL performs better than the baseline model. Even though the proposed method performs best with the max-performance strategy, it performs slightly worse on random-10. This result strengthens our argument regarding the need to go beyond the traditional reporting of maximum performance based on the ACC, NMI and ARI metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cluster Visualizations</head><p>In this section, we provide two visualizations of the ImageNet-10 dataset. In <ref type="figure">Figure 7</ref>, each row presents randomly drawn images from each cluster. Images that have red-and-yellow borders are categorized incorrectly and should belong to different clusters. For example, the first image in the fourth row should be in the truck category, but it is categorized as an airliner. In the soccer ball category, there are two mistakes: the first image should be categorized as a truck, and the fourth image should be categorized as a dog. In the ninth row, the last image should be categorized as an airship, but it is categorized as a truck.</p><p>To check whether the learned representations that are closest to each other belong to the same category, we use a retrieval task. In <ref type="figure">Figure 8</ref>, we show the results. The <ref type="figure">Fig. 7</ref>: Images from the same cluster: In ImageNet-10, we randomly sample six images from all ten clusters and show them above. Each row presents one cluster and images that have red-and-yellow borders are categorized incorrectly and should belong to different clusters. <ref type="figure">Fig. 8</ref>: Image retrieval: The first image in each row was used as a query (random samples from the dataset ImageNet-10), and the five images nearest to the query image were retrieved using their representations. Images with red-and-yellow borders are retrieved incorrectly first image in each row was used as a query (random samples from the dataset), and the five images nearest to the query image were retrieved using their representations. Ideally, one would expect all retrieved images to belong to the same category as the query image. For the example from the soccer ball category, the first image retrieved does not belong to this category; however, both images have water as their main feature. In the last row, the second image retrieved is a penguin and ideally should not be one of the closest matches to an image in the orange category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>Although the proposed method is trained in an end-to-end manner, each component of the method may have a different impact on the results. We conduct various controlled experiments to quantify the impact of the losses (Section 5.1), the data augmentation methods (Section 5.2), the image resolution (Section 5.3), the number of transformations (Section 5.4), the dimensionality of each transformation (Section 5.4) and the architecture choice (Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects of the Loss Terms</head><p>We consider the following scenarios: train with only loss L Z (Consensus Loss), train with only loss L b (ID), train with both losses L b + L Z (ConCURL). We do this for CIFAR-10, CIFAR100-20 datasets. We then compare the clustering performance of all three scenarios and observe that training with both losses improves the performance. Additionally, we also compare the loss trajectories during training. We compare the L b loss for the case when we train only L b and for the case when we train only L Z . We repeat this for loss L Z . The results are summarized in <ref type="table" target="#tab_11">Table 9</ref>, where we observe that training with both the losses provides a much better clustering performance as compared to training with the losses individually. The exemplar consistency trains with the objective of classifying each data point into its own class. The population and consensus consistencies train without regarding for discrimination among the individual data points. An algorithm that is trained with only the consensus loss therefore is not effective in discriminating individual data points. From <ref type="figure">Figure 9</ref>(a), we can observe that when we train with only L Z , we do not observe any improvement in the loss L b ; when we train with L b + L Z , we On the other hand, it is possible that an algorithm that is trained to discriminate individual data points can show some improvement on the consensus loss L Z . From <ref type="figure">Figure 9</ref>(b), we can observe that L Z is lesser when trained with L b + L Z than when training only with L Z . We observe a small decrease in L Z value when trained only with L b . This shows that optimizing L b helps to some extent in achieving a better L Z .</p><p>From this discussion, we observe that both losses contribute differently to the training without much interference or conflicts. They indeed complement each other as we observe improved clustering performance for ConCURL from <ref type="table" target="#tab_11">Table 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Data Augmentation Methods</head><p>Augmenting the training data is a standard technique for training deep learning methods <ref type="bibr" target="#b41">(Shorten &amp; Khoshgoftaar, 2019)</ref>. The backbones used in this study rely on the different views that are generated by applying different augmentations to the input  <ref type="bibr" target="#b44">Tian et al. (2020)</ref> investigated the impact of data augmentation on contrastive learning methods and shed some light on this topic. In our setting, we would like to quantify the impacts of several data augmentations on the consensus loss.</p><p>In <ref type="table" target="#tab_0">Table 10</ref>, we show the maximum accuracy achieved when all data augmentation approaches are used and when we skip one data augmentation technique at a time. When all data augmentation methods are used, the maximum accuracies achieved are 0.8459 and 0.4798 on CIFAR-10 and CIFAR100-20, respectively. When random resized cropping data augmentation is dropped, we obtain the maximum accuracy drops for both datasets, followed by color jitter. Other data augmentation techniques are important for obtaining the best possible accuracy but do not have as much of an effect as color jitter and random resized cropping. In <ref type="figure" target="#fig_0">Figures 11 and 12</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Image Resolution</head><p>Image resolution is often considered a free parameter <ref type="bibr" target="#b36">(Niu et al., 2020a)</ref>, and however, its effect on clustering performance is not evaluated rigorously in most works. We try to quantify the effects of different resolutions to the greatest extent possible, given that some datasets are available only at specific resolutions. For STL-10, we use 32 ? 32, 64 ? 64 and 96 ? 96 resolutions. For ImageNet-10 and ImageNet-Dog-15, we use 96 ? 96, 160 ? 160 and 224 ? 224 resolutions. The results are given in <ref type="table" target="#tab_0">Table 11</ref>. The best performance for ImageNet-10 and ImageNet-Dogs is obtained at a resolution of 160, and for STL-10, the best performance is obtained at a resolution of 96. It is not clear why ImageNet-10 and ImageNet-Dogs do not yield the best performance at high resolutions, and further investigation is needed; we keep this as an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Distribution of Accuracies Across the Set of Hyperparameters</head><p>The proposed consensus loss has two parameters. The first is the number of transformations used, and the second is the dimensionality of the projection space. To    understand the proposed loss, we conduct a detailed experimental study on STL-10 and CIFAR100-20 4 . The hyperparameters used are given in <ref type="table" target="#tab_0">Table 12</ref>. Due to the sheer number of conducted experiments, we supply the summary statistics obtained on a random set. We report the empirical mean and standard deviation of the marginal distribution of the quantity under investigation. Let P ?,?,d,l be the joint distribution over the hyperparameters ? (temperature parameter), l (learning rate), ? (natural log of the number of transformations) and d (dimensionality of the projection space). We consider n h as the number of distinct values used in the experiment for each hyperparameter h ? {?, ?, d, l} based on <ref type="table" target="#tab_0">Table 12</ref>. We the denote accuracy from each experiment based on the hyperparameters used as a ?,?,d,l . Let P hi|hj be the conditional marginal distribution of hyperparameter h i given h j and the conditional empirical mean of P hi|hj be m(P hi|hj ). In this case, the conditional empirical mean m(P hi|hj ) when h i = d and h j = ? can be calculated using m(P d|? ) = 1 n??n l ? l a ?,?,d,l . The conditional empirical means and standard deviations of other hyperparameters are calculated in the same way. In <ref type="figure" target="#fig_0">Figure 13</ref>, we show each conditional empirical mean with a blue dot, and each red line around a dot represents a standard deviation. For both STL-10 and CIFAR100-20, we see a trend regarding the number of projections. For STL-10, the smaller the number of random projections, the better the results are, and for CIFAR100-20, increasing the number of random projections is helpful for improving the clustering accuracy up to some point. Note that when the number of random projections is equal to zero, our setting is equivalent to the baseline ID model, and our approach always performs better than ID. This means that the optimal number of random projections is greater than or equal to one. There is no such clear trend in the number of dimensions of the random projections.</p><p>The max-performance procedure provides some insights into the performance of the algorithms at hand, although it does not provide the whole picture because it does not consider the robustness of the performance differences. In <ref type="table" target="#tab_0">Table 13</ref>, we give the hyperparameters that yield the max performance. In other words, finding a hyperparameter set that yields better performance than the baseline is the core idea behind the max-performance procedure. We ask the following question: given a hyperparameter grid, how likely is our method to achieve better accuracy than the baseline? In <ref type="figure" target="#fig_0">Figure  14</ref>, we report the empirical accuracy distributions on STL-10 and CIFAR100-20 for all hyperparameters given in <ref type="table" target="#tab_0">Table 12</ref>. The red dotted lines show the corresponding baseline accuracy for each dataset. For STL-10, only approximately 12.5% of the hyperparameter sets yield better results than the baseline. On the other hand, for CIFAR100-20, approximately 90% of the hyperparameter sets yield better results than the baseline. In other words, it does not require a significant amount of computational power to find a better model than the state-of-the art models for CIFAR100-20; however, the situation is the opposite for STL-10. The results given in <ref type="figure" target="#fig_0">Figure 14</ref> suggest that when comparing models, multiple metrics need to be considered, not only the max-performance procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Architecture Choice</head><p>In this work, we use ResNet-18 and ResNet-50 as network architectures. For both ResNet-18 and ResNet-50, we sweep over the same set of hyperparameter choices, i.e., the temperature, number of projections and projection dimensionality, and report the results for ImageNet-10 dataset with image resolution of 160?160. <ref type="figure" target="#fig_0">Figure 15</ref> shows the distribution of ? acc , which is defined as the accuracy difference between ResNet-50 and ResNet-18. <ref type="figure" target="#fig_0">Figure 15</ref> indicates that ResNet-50 slightly outperforms ResNet-18, i.e., the mean difference is approximately 0.5%. <ref type="figure" target="#fig_0">Fig. 16</ref>: Comparison of the runtimes per epoch for ID and ConCURL on CIFAR-10 dataset. We vary the number of random transformations in the computation of the consensus loss and is mentioned in paranthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Runtime Comparison</head><p>To study the runtime of the proposed method, we compare the time taken per epoch for the baseline ID algorithm and the proposed algorithm. Due to the additional loss computation, the time taken to run the proposed algorithm is higher which can be observed from <ref type="figure" target="#fig_0">Figure 16</ref>. The additional time taken is mainly due to computing the consensus loss for the different number of transformations. The current implementation computes the forward pass for the different transformations sequentially thus increasing the runtime. However, a more time efficient implementation where the forward passes for all the different random transformations are computed in parallel can make the runtime more comparable to the baseline ID algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce different notions of the consistency constraints that are enforced in different unsupervised/self-supervised learning algorithms. We propose a novel clustering algorithm that seamlessly incorporates all three consistency constraints (exemplar, population and consensus) and achieves state-of-the-art clustering results for four out of five popular and challenging computer vision datasets. Our work on consensus clustering is significantly different from earlier consensus clustering works that do not learn representations. Moreover, we initiate a discussion on the adequacy of the currently used methods for evaluating clustering algorithms. We significantly extend the evaluation procedure for clustering algorithms, thereby reflecting the challenges of applying clustering to real-world tasks. We provide evaluation results for ConCURL and other state-of-the-art clustering algorithms based on max-performance criteria, according to which ConCURL outperforms other algorithms on most datasets. However, its average performance according to out-ofdistribution criteria highlights the need to use the proposed evaluation methods for deep clustering algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 ACC</head><p>The ACC is computed by first computing a cluster partition of the input data. Once the partitions are computed and cluster indices are assigned to each input data point, a linear assignment map is computed using the Kuhn-Munkres (Hungarian) algorithm, which reassigns the cluster indices to the true labels of the data. The ACC is then given by</p><formula xml:id="formula_13">ACC = N i=1 I{y true (x i ) = c(x i )} N ,</formula><p>where y true (x i ) is a true label of x i and c(x i ) is the cluster assignment produced by an algorithm (after Hungarian mapping).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 NMI</head><p>For two clusters U, V , where each contains U, V clusters and U i represents the number of samples in cluster U i of clustering result U (similar for V ), the MI is given by</p><formula xml:id="formula_14">M I(U, V ) = |U | i=1 |V | j=1 |U i ? V i | N log N |U i ? V j | |U i ||V j |</formula><p>where N is the number of data points under consideration. The NMI is defined as</p><formula xml:id="formula_15">N M I(U, V ) = M I(U, V ) M I(U, U )M I(V, V )</formula><p>B.3 ARI 5 Suppose that R is the ground truth clustering result and that S is a partition. The RI of S is given as follows. Let a be the number of pairs of elements that are in the same set in R and in S; let b be the number of pairs of elements that are in different sets in R and in S. Then,</p><formula xml:id="formula_16">RI = a + b n 2 ARI = RI ? E[RI] max(RI) ? E[RI]</formula><p>of 128 dimensions instead of the usual 1000 dimensions. The output of the final fully connected layer proceeds to compute the noise contrastive estimation (NCE) loss, and the feature representations (the layer before the fully connected layer) are fed to the clustering part.</p><p>For the clustering part, the MLP projection head g consists of a hidden layer of size 2048, followed by batch normalization and rectified linear unit (ReLU) layers, and an output layer of size 256. The prototypes are thus chosen to have 256 dimensions. Note that we fix the number of prototypes to be equal to the number of ground truth classes in the dataset. It has been shown, however, that overclustering leads to better representations <ref type="bibr" target="#b0">(Asano et al., 2019;</ref><ref type="bibr" target="#b5">Caron et al., 2020;</ref><ref type="bibr" target="#b26">Ji et al., 2019a)</ref>, and we can extend our model to include an overclustering block with a larger set of prototypes <ref type="bibr" target="#b26">(Ji et al., 2019a)</ref> and alternate the training procedure between the blocks.</p><p>We train the algorithm for 2000 epochs on all datasets. We use the SGD optimizer with a learning rate decay of 0.1 at prespecified epochs <ref type="bibr">(600,</ref><ref type="bibr">950,</ref><ref type="bibr">1300,</ref><ref type="bibr">1650,</ref><ref type="bibr">2000)</ref> to perform the updates for all datasets. We perform a coarse learning rate search and find that 0.03 is the best-performing setting. We use a batch size of 128 for all the datasets. To evaluate the cluster accuracy, we compute the cluster assignments using MiniBatchKMeans 6 with a batch size of 6000 and 20 random initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Image augmentations</head><p>The different views X 1 b , X 2 b are not the same as the views in multiview datasets <ref type="bibr" target="#b39">(Schops et al., 2017)</ref>. The views referred to in this paper correspond to different augmented views that are generated by image augmentation techniques, such as Ran-domHorizontalFlip and RandomCrop. We explain the generation process of multiple augmented views, which have been shown to be very effective in unsupervised learning <ref type="bibr" target="#b9">(Chen et al., 2020)</ref>. Indeed, it is possible to use more than two augmented views, but we limit to the number to two for the sake of simplicity. <ref type="bibr" target="#b5">Caron et al. (2020)</ref> proposed an augmentation technique (Multi-Crop) to use more than two views. In this work, we use the augmentation methods used in <ref type="bibr" target="#b9">Chen et al. (2020)</ref>; <ref type="bibr">Grill et al. (2020)</ref>. We first crop a random patch of the input image with a scale ranging from 0.08 to 1.0 and resize the cropped patch to 224?224 (96?96 for smaller-resolution datasets such as STL10). The resulting image is then flipped horizontally with a probability of 0.5. We then apply color transformations, starting by applying grayscale with a probability of 0.2 followed by randomly changing the brightness, contrast, saturation and hue with a probability of 0.8. Then, we apply a Gaussian blur with a kernel size of 23?23 and a sigma chosen uniformly and randomly between 0.1 and 2.0. The probabilities of applying Gaussian blur are 1.0 for view 1 and 0.5 for view 2. During the evaluation, we resize the image such that the smaller edge of the image is of size 256 (not required for STL-10, CIFAR-10, and CIFAR100-20), and a center crop operation is performed with the resolution mentioned in the main paper. We finally normalize the image channels with the mean and standard deviation computed on ImageNet. Additionally, during training, we experiment with applying a Sobel filter after all the image augmentation steps are performed but before the forward pass. Applying a Sobel filter reduces the number of channels in the input images to 2. We also experiment by augmenting the RGB images with the output of the Sobel transform, resulting in 5-channel input images. In both of these cases, the input channels in the first convolution layer are modified accordingly. All image augmentations are computed using PyTorch's torchvision module (available in version 1.7.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Random transformations</head><p>To compute the random transformations on the embeddings z, we follow two techniques. We use Gaussian random projections with an output dimensionality of d and transform the embeddings z to the new space with a dimensionality of d. In Gaussian random projections, the projection matrix is generated by picking rows from a Gaussian distribution such that they are orthogonal. We also use diagonal transformation <ref type="bibr" target="#b21">(Hsu et al., 2018)</ref>, where we multiply z with a randomly generated diagonal matrix with the same dimensions as z. We initialize M random transformations at the beginning, and they are kept fixed throughout the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Comparison with LA</head><p>LA <ref type="bibr" target="#b55">(Zhuang et al., 2019)</ref> builds on nonparametric ID (Z. <ref type="bibr" target="#b49">Wu et al., 2018)</ref> and uses a robust clustering objective (it uses a closest neighbors set generated using multiple runs of k-means) similar to consensus clustering to move statistically similar points closer in the representation space and dissimilar points farther away. By using the linear evaluation protocol on ImageNet, the authors demonstrate that the representations learned with LA are better than those obtained without LA.</p><p>However, the performance of these features with respect to clustering was not discussed. Since LA is similar to our work in spirit, we perform a study on the clustering performance of the features learned using LA 7 on the ImageNet-10 and ImageNet-Dogs datasets. The results are presented in <ref type="table" target="#tab_0">Table D1</ref>. We observe that the clustering performance of our proposed ConCURL algorithm is much better than the clustering performance of the LA method. Note that the clustering performance of the ID features is better than that of LA, and our algorithm further improves upon the clustering performance of ID. One major difference between our work and LA is the way in which we generate the ensemble. Our approach allows us to control and measure the diversity of the ensemble, which can be useful in making algorithm design choices. Although LA controls the ensemble by varying the number of clustering results and the number of clusters in each clustering result, which aptly suits the objective that LA is solving, the resultant ensembles are limited to utilizing k-means clustering (the authors showed that other clustering approaches were either not scalable or not optimal). In our case, by applying feature space transformations, we have much more freedom in generating the ensemble. We use random projections and diagonal transformations, but there could be other transformations on the feature space that we have not yet explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Implementation Details of LA</head><p>We train the model for 500 epochs. Since the original implementation of LA was designed for ImageNet, we perform a hyperparameter search as follows. Using the config file from the official PyTorch implementation, we create 36 configuration files by varying the learning rate and k-means-k. In the original config file provided by the authors, k-means-k = 30000 (for 1.28 million images). We scale the k-means-k for ImageNet-10 (13000 images) and ImageNet-Dogs (19500 images) accordingly and try six different values. In particular, we try learning rates = [0.003, 0.005, 0.01, 0.03, 0.05, 0.1], and k-means-k = <ref type="bibr">[10,</ref><ref type="bibr">15,</ref><ref type="bibr">100,</ref><ref type="bibr">310,</ref><ref type="bibr">452,</ref><ref type="bibr">500]</ref>.</p><p>The number of background neighbors is 4096, as used in the original paper. We run ResNet-18 experiments for the full hyperparameter search (36 experiments) and evaluate the cluster metrics. Additionally, we choose the top 5 choices of hyperparameters from above and run ResNet-34 experiments with those parameters for both datasets. In <ref type="table" target="#tab_0">Table D1</ref>, we present the best results obtained for each dataset.</p><p>The code repository uses a different version of ResNet (PreActResNet). Therefore, for evaluating the clustering performance, we take the output of the layer before the final dense layer. For ResNet-18, the output dimensions of this layer are (512,7,7), and we take the mean along the (1,2) dimensions and use the resulting 512-dimensional vector. We compute the k-means clustering results on these embeddings using faiss (https://github.com/facebookresearch/faiss) for the training split of the data and compute the cluster metrics as mentioned in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>? Funding -Work was done at Microsoft with Microsoft's support. ? Conflict of interests -Authors are from Microsoft and The Ohio State University.</p><p>There are no conflict of interests to disclose. ? Ethics approval -Not applicable ? Consent to participate -Yes ? Consent for publication -Yes ? Availability of data and materials -The datasets used are available at CIFAR-10 ,CIFAR-100, STL10 or can be requested at ImageNet-10,ImageNet-Dogs. All the trained models, their usage is available here. ? Code availability -Code is available here. ? Authors' contributions -All authors -Aniket Anand Deshmukh, Jayanth Reddy Regatti, Eren Manavoglu and Urun Dogan contributed to this work and were essential to complete this submission.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An illustration of the consensus loss part of ConCURL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>a: Three-cluster dataset = z, b: Normalized data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.182 0.164 0.300 0.175 0.138 0.138 0.054 0.028 0.272 0.192 0.138 0.137 0.103 0.033 DEC 0.359 0.276 0.186 0.381 0.282 0.203 0.195 0.122 0.079 0.301 0.257 0.161 0.185 0.136 0.050 DAC 0.470 0.366 0.257 0.527 0.394 0.302 0.275 0.219 0.111 0.522 0.396 0.306 0.238 0.185 0.088 Deep Cluster 0.334 N/A N/A N/A N/A N/A N/A N/A N/A 0.374 N/A N/A 0.189 N/A N/A DDC 0.489 0.371 0.267 0.577 0.433 0.345 N/A N/A N/A 0.524 0.424 0.329 N/A N/A N/A IIC 0.610 N/A N/A N/A N/A N/A N/A N/A N/A 0.617 N/A N/A 0.257 N/A N/A DCCM 0.482 0.376 0.262 0.710 0.608 0.555 0.383 0.321 0.182 0.623 0.496 0.408 0.327 0.285 0.173 GATCluster 0.583 0.446 0.363 0.762 0.609 0.572 0.333 0.322 0.200 0.610 0.475 0.402 0.281 0.215 0.116 PICA 0.713 0.611 0.531 0.870 0.802 0.761 0.352 0.352 0.201 0.696 0.591 0.512 0.337 0.310 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Confusion matrices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Histogram of clustering accuracies for models trained on ImageNet-10: (a) ConCURL model evaluated on random-10, (b) ConCURL model evaluated on random-15, (c) Baseline (ID) model evaluated on random-10, (d) Baseline (ID) model evaluated on random-15 Histogram of clustering accuracies for models trained on ImageNet-Dogs: (a) ConCURL model evaluated on random-10, (b) ConCURL model evaluated on random-15, (c) Baseline (ID) model evaluated on random-10, (d) Baseline (ID) model evaluated on random-15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>We perform an ablation over the losses used and compare the loss values for each of the cases for CIFAR100-20. We compare the clustering metrics for ablation over the losses used for CIFAR100-20 and observe that Train with L b + L Z (ConCURL) outperforms training with only L b or L Z . observe a similar trajectory as training only with L b . This shows that training with L Z does not conflict with L b loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>CIFAR100-20: dimensionality of the projection space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 :</head><label>13</label><figDesc>Components of the consensus loss; ablation of STL-10 and CIFAR100-20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 :</head><label>14</label><figDesc>The dotted red lines show the accuracy of the baseline, i.e.,ID Tao et al.  (2021), on the corresponding dataset, and DE is the density estimate of the empirical distribution: (a) Empirical accuracy distribution for STL-10, (b) Empirical accuracy distribution for CIFAR100-20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 :</head><label>15</label><figDesc>Empirical distribution of the performance difference between Residual Network (ResNet)-50 and ResNet-18. DE is the density estimate of the empirical distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Different ways to generate ensembles</figDesc><table><row><cell>Data Representation</cell><cell>Clustering algorithms</cell></row><row><cell cols="2">Different data preprocessing techniques Multiple clustering algorithms (k-means, GMMs, etc)</cell></row><row><cell>Subsets of features</cell><cell>Same algorithm with different parameters or initializations</cell></row><row><cell>Different transformations of the fea-</cell><cell>Combination of multiple clustering results and different</cell></row><row><cell>tures</cell><cell>parameters or initializations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Predicted cluster assignment probabilities and target probabilities obtained from the Sinkhorn algorithm for four data points</figDesc><table><row><cell></cell><cell cols="3">Cluster assignment probability p</cell><cell></cell><cell cols="3">Cluster assignment probabilityp</cell><cell></cell><cell cols="3">Targets q from Sinkhorn</cell></row><row><cell></cell><cell cols="3">Cluster 1 Cluster 2 Cluster 3</cell><cell></cell><cell cols="3">Cluster 1 Cluster 2 Cluster 3</cell><cell></cell><cell cols="3">cluster 1 cluster 2 cluster 3</cell></row><row><cell>1</cell><cell>0.6473</cell><cell>0.2587</cell><cell>0.0940</cell><cell>1</cell><cell>0.6764</cell><cell>0.2271</cell><cell>0.0965</cell><cell>1</cell><cell>1.0</cell><cell>8.9e-09</cell><cell>1.7e-17</cell></row><row><cell>2</cell><cell>0.7180</cell><cell>0.1812</cell><cell>0.1008</cell><cell>2</cell><cell>0.7305</cell><cell>0.1680</cell><cell>0.1015</cell><cell>2</cell><cell>1.0</cell><cell>9.1e-13</cell><cell>8.7e-18</cell></row><row><cell>3</cell><cell>0.0832</cell><cell>0.3160</cell><cell>0.6008</cell><cell>3</cell><cell>0.0802</cell><cell>0.3371</cell><cell>0.5827</cell><cell cols="2">3 6.8e-18</cell><cell>2.2e-6</cell><cell>1.0</cell></row><row><cell>4</cell><cell>0.1543</cell><cell>0.5917</cell><cell>0.2541</cell><cell>4</cell><cell>0.1357</cell><cell>0.5784</cell><cell>0.2860</cell><cell cols="2">4 2.5 e-12</cell><cell>1.0</cell><cell>5.4e-8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Pairwise NMI values as a way to measure the diversity in the ensemble; the results are obtained for the STL-10 dataset, and the pairwise NMI values for different random projection dimensions are shown (the original dimensionality of z is 256)</figDesc><table><row><cell>NMI mean</cell><cell>0.2 0.4 0.6 0.8</cell><cell>0 200 400 600 800 1000 epochs 64 128 512 256</cell><cell>NMI std</cell><cell>0.0 0.1 0.2 0.3</cell><cell>epochs 0 200 400 600 800 1000 64 128 256 512</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell cols="2">Fig. 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Dataset summary</figDesc><table><row><cell>Dataset</cell><cell>Classes</cell><cell>Split</cell><cell cols="2">Samples Resolution</cell></row><row><cell>ImageNet-10</cell><cell>10</cell><cell>train</cell><cell>13000</cell><cell>160 ? 160</cell></row><row><cell>ImageNet-Dogs</cell><cell>15</cell><cell>train</cell><cell>19500</cell><cell>160 ? 160</cell></row><row><cell>STL-10</cell><cell>10</cell><cell>train+test</cell><cell>13000</cell><cell>96? 96</cell></row><row><cell>CIFAR-10</cell><cell>10</cell><cell>train</cell><cell>50000</cell><cell>32? 32</cell></row><row><cell>CIFAR100-20</cell><cell>20</cell><cell>train</cell><cell>50000</cell><cell>32? 32</cell></row><row><cell cols="4">4.1.1 Comparison with state-of-the-art baselines</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Methods Compared</figDesc><table><row><cell>Method</cell><cell>Reference</cell></row><row><cell>k-means</cell><cell>Macqueen (1967)</cell></row><row><cell>SC</cell><cell>Ng, Jordan, and Weiss (2002)</cell></row><row><cell>AC</cell><cell>Franti, Virmajoki, and Hautamaki (2006)</cell></row><row><cell>NMF</cell><cell>Cai, He, Wang, Bao, and Han (2009)</cell></row><row><cell>AE</cell><cell>Bengio, Lamblin, Popovici, Larochelle, and Montreal (2007)</cell></row><row><cell>SDAE</cell><cell>Vincent, Larochelle, Lajoie, Bengio, and Manzagol (2010)</cell></row><row><cell>DeCNN</cell><cell>Zeiler, Krishnan, Taylor, and Fergus (2010)</cell></row><row><cell>JULE</cell><cell>Yang, Parikh, and Batra (2016)</cell></row><row><cell>DEC</cell><cell>Xie, Girshick, and Farhadi (2016b)</cell></row><row><cell>DAC</cell><cell>Chang, Wang, Meng, Xiang, and Pan (2017)</cell></row><row><cell>Deep Cluster</cell><cell>Caron, Bojanowski, Joulin, and Douze (2018b)</cell></row><row><cell>DDC</cell><cell>Chang et al. (2019)</cell></row><row><cell>IIC</cell><cell>Ji, Henriques, and Vedaldi (2019b)</cell></row><row><cell>DCCM</cell><cell>J. Wu et al. (2019b)</cell></row><row><cell>GATCluster</cell><cell>Niu, Zhang, Wang, and Liang (2020b)</cell></row><row><cell>PICA</cell><cell>Huang et al. (2020b)</cell></row><row><cell>CC</cell><cell>Li et al. (2021)</cell></row><row><cell>ADC</cell><cell>Haeusser, Plapp, Golkov, Aljalbout, and Cremers (2019)</cell></row><row><cell>ID</cell><cell>Tao et al. (2021)</cell></row><row><cell>IDFD</cell><cell>Tao et al. (2021)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Clustering with the max-performance strategy</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Clustering on the test dataset</figDesc><table><row><cell>Dataset</cell><cell>Train Dataset</cell><cell>Test Dataset</cell></row><row><cell></cell><cell cols="2">ACC NMI ARI ACC NMI ARI</cell></row><row><cell>CIFAR-10</cell><cell cols="2">0.846 0.762 0.715 0.838 0.744 0.700</cell></row><row><cell cols="3">CIFAR100-20 0.479 0.468 0.303 0.427 0.430 0.254</cell></row><row><cell cols="3">ImageNet-10 0.958 0.908 0.910 0.914 0.876 0.838</cell></row><row><cell cols="3">ImageNet-Dogs 0.695 0.630 0.532 0.660 0.675 0.515</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>the Brittany spaniel category are categorized as Blenheim spaniels, and 16% more are categorized as Welsh springer spaniels. Forty-four percent of the samples from the Welsh springer spaniel category are categorized as Brittany spaniels, and 15% are categorized as Blenheim spaniels. Kelpies and Dobermans are also confused with each other, where 39% of the kelpie samples are categorized as Dobermans, and 29% of the Doberman samples are categorized as kelpies. For CIFAR-10, 33% of the samples from the dog category are categorized as cats, and 16% of the samples from the cat category are categorized as dogs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>ImageNet-10 vs. ImageNet-Dogs: cross-model performance</figDesc><table><row><cell>Model Training Dataset</cell><cell>ImageNet-10</cell><cell>ImageNet-Dogs</cell></row><row><cell></cell><cell cols="2">ACC NMI ARI ACC NMI ARI</cell></row><row><cell>ImageNet-10</cell><cell cols="2">0.958 0.908 0.910 0.177 0.127 0.068</cell></row><row><cell>ImageNet-Dogs</cell><cell cols="2">0.356 0.298 0.184 0.695 0.630 0.532</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>CIFAR-10 vs. CIFAR100-20: cross-model performance</figDesc><table><row><cell>Model Training Dataset</cell><cell>CIFAR-10</cell><cell>CIFAR100-20</cell></row><row><cell></cell><cell cols="2">ACC NMI ARI ACC NMI ARI</cell></row><row><cell>CIFAR-10</cell><cell cols="2">0.846 0.762 0.715 0.178 0.158 0.061</cell></row><row><cell>CIFAR100-20</cell><cell cols="2">0.464 0.359 0.250 0.480 0.468 0.304</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Ablation on the losses during training</figDesc><table><row><cell>Loss used</cell><cell>CIFAR100-20</cell><cell>CIFAR-10</cell></row><row><cell></cell><cell cols="2">ACC NMI ARI ACC NMI ARI</cell></row><row><cell>L b only 3</cell><cell cols="2">0.437 0.429 0.259 0.826 0.732 0.684</cell></row><row><cell>L Z only</cell><cell cols="2">0.302 0.333 0.145 0.739 0.667 0.539</cell></row><row><cell cols="3">L b + L Z (ConCURL) 0.479 0.468 0.303 0.846 0.762 0.715</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Data augmentation details</figDesc><table><row><cell></cell><cell cols="3">Data Augmentation Type</cell><cell>Abbreviation CIFAR-10 CIFAR100-20</cell></row><row><cell></cell><cell cols="3">All data augmentations</cell><cell>All DA</cell><cell>0.8459</cell><cell>0.4798</cell></row><row><cell cols="4">No random horizontal flip</cell><cell>No RHF</cell><cell>0.7689</cell><cell>0.4551</cell></row><row><cell></cell><cell cols="3">No random gray scale</cell><cell>No RGS</cell><cell>0.7750</cell><cell>0.4357</cell></row><row><cell cols="4">No randomly applied blur</cell><cell>No RAB</cell><cell>0.7907</cell><cell>0.4295</cell></row><row><cell></cell><cell cols="3">No color jitter</cell><cell>No CJ</cell><cell>0.6463</cell><cell>0.2536</cell></row><row><cell cols="4">No random resized cropping</cell><cell>No RRC</cell><cell>0.2988</cell><cell>0.1322</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Effect of Data Augmentation</cell></row><row><cell>Accuracy</cell><cell>0.5 0.6 0.7 0.8</cell><cell></cell><cell>All DA No RHF No RGS No RAB No CJ No RRC</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>0</cell><cell cols="2">Epochs 250 500 750 1000 1250 1500 1750 2000</cell></row><row><cell></cell><cell></cell><cell cols="3">Fig. 11: Effect of data augmentation on CIFAR-10</cell></row><row><cell cols="2">image. Recently,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>, we show how the running mean of accuracy progresses during training for each of the experiments inTable 10.</figDesc><table><row><cell>Accuracy</cell><cell>0.25 0.30 0.35 0.40 0.45</cell><cell>All DA No RHF No RGS No RAB No CJ No RRC</cell><cell>Effect of Data Augmentation</cell></row><row><cell></cell><cell>0.20</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="2">250 500 750 1000 1250 1500 1750 2000 Epochs</cell></row><row><cell></cell><cell cols="3">Fig. 12: Effect of data augmentation on CIFAR100-20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Effects</figDesc><table><row><cell cols="6">of different resolutions for STL-10, ImageNet-10 and</cell></row><row><cell></cell><cell cols="3">ImageNet-Dogs</cell><cell></cell></row><row><cell></cell><cell>32</cell><cell>64</cell><cell>96</cell><cell>160</cell><cell>224</cell></row><row><cell>STL-10</cell><cell cols="3">0.531 0.724 0.749</cell><cell>NA</cell><cell>NA</cell></row><row><cell>ImageNet-10</cell><cell>NA</cell><cell>NA</cell><cell cols="3">0.887 0.958 0.946</cell></row><row><cell>ImageNet-Dogs</cell><cell>NA</cell><cell>NA</cell><cell cols="3">0.629 0.695 0.679</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters and the range values used for the experiments</figDesc><table><row><cell></cell><cell>min</cell><cell cols="2">max step size</cell></row><row><cell>? = temperature parameter</cell><cell>0.3</cell><cell>1</cell><cell>0.1</cell></row><row><cell>l = learning rate</cell><cell>0.015</cell><cell>0.09</cell><cell>0.015</cell></row><row><cell>? = natural log of the number of transformations</cell><cell>0</cell><cell>7</cell><cell>1</cell></row><row><cell>d = dimensionality of the projection space</cell><cell>80</cell><cell>168</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Hyperparameters for obtaining maximum performance</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">STL-10 ImageNet-10 ImageNet-Dogs CIFAR-10 CIFAR100-20</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell>1.0</cell><cell></cell><cell>0.5</cell><cell>0.85</cell><cell>0.35</cell></row><row><cell></cell><cell></cell><cell>l</cell><cell></cell><cell cols="2">0.03</cell><cell></cell><cell>0.03</cell><cell></cell><cell>0.06</cell><cell>0.015</cell><cell>0.06</cell></row><row><cell></cell><cell></cell><cell cols="2">exp(?)</cell><cell>4</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>64</cell><cell>64</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell>d</cell><cell></cell><cell cols="2">80</cell><cell></cell><cell>104</cell><cell></cell><cell>136</cell><cell>152</cell><cell>32</cell></row><row><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.68 0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.62</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table D1 :</head><label>D1</label><figDesc>Comparison with LA on ImageNet-10</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>ImageNet-10</cell><cell></cell><cell cols="3">ImageNet-Dogs</cell></row><row><cell>Method\Metrics</cell><cell>Acc</cell><cell>NMI</cell><cell>ARI</cell><cell>Acc</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>LA</cell><cell cols="5">0.393 0.346 0.213 0.201 0.133</cell><cell>0.06</cell></row><row><cell>ConCURL</cell><cell cols="4">0.958 0.907 0.909 0.695</cell><cell>0.63</cell><cell>0.531</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">a) Degenerate solution where all cluster assignments are the same. b) Random assignment can satisfy this condition given that all clustering process produces the same but random assignments</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A theoretically grounded explanation of ConCURL is considered future work due to the nonconvexity of deep learning methods and the nonconvexity of the proposed loss.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">pairs. An NMI score of 1.0 signifies that the two clusters perfectly correlate with each other, and a score of 0.0</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For CIFAR-10, CIFAR100-20 we provide results of rerun that is better than reported in<ref type="bibr" target="#b43">(Tao et al., 2021)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We conduct a similar but smaller study on the remaining dataset, and we observe similar trends.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-score</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">using the official PyTorch implementation available at https://github.com/neuroailab/LocalAggregation-PyTorch/tree/master/config</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Evaluation Metrics</head><p>We evaluate our algorithm by computing traditional clustering metrics (the ACC, NMI, and ARI), which we discuss below in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Implementation Details</head><p>In this section, we discuss the implementation of the proposed algorithm. We use PyTorch version 1.7.1 for the implementation. The ID block of the algorithm uses the code from the implementation of ID (Z. <ref type="bibr" target="#b49">Wu et al., 2018)</ref> available at https:// github.com/zhirongw/lemniscate.pytorch. The repository uses PyTorch version 0.3, and appropriate changes are made to use it with the latest version of PyTorch. We use ResNet-18 and ResNet-50 blocks during our experiments. In the ID block, the ResNet architecture is modified as follows. The final fully connected layer consists</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Montreal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="153" to="160" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://ijcai.org/Proceedings/09/Papers/171.pdf" />
		<title level="m">Locality preserving nonnegative matrix factorization. Ijcai</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1010" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (eccv)</title>
		<meeting>the european conference on computer vision (eccv)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eccv</title>
		<imprint>
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="139" to="156" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep discriminative clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ieee international conference on computer vision (iccv)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.626</idno>
	</analytic>
	<monogr>
		<title level="j">Iccv</title>
		<imprint>
			<biblScope unit="page" from="5880" to="5888" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ieee conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random projection for high dimensional data clustering: A cluster ensemble approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (icml-03)</title>
		<meeting>the 20th international conference on machine learning (icml-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast agglomerative clustering using a k-nearest neighbor graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Franti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Virmajoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hautamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1881" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings using evidence accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="835" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Clustering by passing messages between data points. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="972" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cluster ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>others (2020)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern recognition</title>
		<editor>T. Brox, A. Bruhn, &amp; M. Fritz</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep semantic clustering by partition confidence maximisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8849" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep semantic clustering by partition confidence maximisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
			<publisher>Cvpr</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Algorithms for clustering data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee international conference on computer vision</title>
		<meeting>the ieee international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>Iccv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variants of the hungarian method for assignment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="253" to="258" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<title level="m">Contrastive clustering. Aaai</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5-th berkeley symposium on mathematical statistics and probability</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A fuzzy clustering based segmentation system as support to diagnosis in medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Masulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schenone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="147" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf" />
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<editor>T.G. Dietterich, S. Becker, &amp; Z. Ghahramani</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09382</idno>
		<title level="m">Representation Learning for Clustering via Building Consensus</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Spice: Semantic pseudo-labeling for image clustering</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gatcluster: Self-supervised gaussianattention network for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="735" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gatcluster: Self-supervised gaussianattention network for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eccv</title>
		<imprint>
			<biblScope unit="page" from="735" to="751" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Consensus clustering with unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Regatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dogan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01245</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with high-resolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3260" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01449</idno>
		<title level="m">Deep continuous clustering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Clustering-friendly representation learning via instance discrimination and feature decorrelation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00131</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Scan: Learning to classify images without labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep comprehensive correlation mining for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee international conference on computer vision</title>
		<meeting>the ieee international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8150" to="8159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep comprehensive correlation mining for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>Iccv</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045390.3045442" />
	</analytic>
	<monogr>
		<title level="j">JMLR.org</title>
		<imprint>
			<biblScope unit="page" from="478" to="487" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Icml</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Survey of clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="678" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Cvpr</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deconvolutional networks. Computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee international conference on computer vision</title>
		<meeting>the ieee international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
