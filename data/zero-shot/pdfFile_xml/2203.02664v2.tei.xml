<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixiang</forename><surname>Ru</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
							<email>baosheng.yu@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
							<email>dubo@whu.edu.cnzhanyibing@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised semantic segmentation (WSSS) with image-level labels is an important and challenging task. Due to the high training efficiency, end-to-end solutions for WSSS have received increasing attention from the community. However, current methods are mainly based on convolutional neural networks and fail to explore the global information properly, thus usually resulting in incomplete object regions. In this paper, to address the aforementioned problem, we introduce Transformers, which naturally integrate global information, to generate more integral initial pseudo labels for end-to-end WSSS. Motivated by the inherent consistency between the self-attention in Transformers and the semantic affinity, we propose an Affinity from Attention (AFA) module to learn semantic affinity from the multihead self-attention (MHSA) in Transformers. The learned affinity is then leveraged to refine the initial pseudo labels for segmentation. In addition, to efficiently derive reliable affinity labels for supervising AFA and ensure the local consistency of pseudo labels, we devise a Pixel-Adaptive Refinement module that incorporates low-level image appearance information to refine the pseudo labels. We perform extensive experiments and our method achieves 66.0% and 38.9% mIoU on the PASCAL VOC 2012 and MS COCO 2014 datasets, respectively, significantly outperforming recent end-to-end methods and several multi-stage competitors. Code is available at https://github.com/ rulixiang/afa.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation, aiming at labeling each pixel in an image, is a fundamental task in vision. In the past decade, deep neural networks have achieved great success in semantic segmentation. However, due to the data-hungry nature of deep neural networks, fully-supervised semantic * Corresponding author. segmentation models usually require a large amount of data with labour intensive pixel-level annotations. To settle this problem, some recent methods seek to devise semantic segmentation models using weak/cheap labels, such as imagelevel labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>, points <ref type="bibr" target="#b2">[3]</ref>, scribbles <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b51">52]</ref>, and bounding boxes <ref type="bibr" target="#b23">[24]</ref>. Our method falls into the category of weakly-supervised semantic segmentation (WSSS) using only image-level labels, which is the most challenging one in all WSSS scenarios. Prevailing WSSS methods with image-level labels commonly adopt a multi-stage framework <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref>. Specifically, these methods firstly train a classification model and then generate Class Activation Maps (CAM) <ref type="bibr" target="#b58">[59]</ref> as the pseudo labels. After refinement, the pseudo labels are leveraged to train a standalone semantic segmentation network as the final model. This multi-stage framework needs to train multiple models for different purposes, thus obviously complicating the training streamline and slowing down the efficiency. To avoid this problem, several end-to-end solutions have been recently proposed for WSSS <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b2">3]</ref>. However, these methods are commonly based on convolution neural networks and fail to explore the global feature relations properly, which turns out to be crucial for activating integral object regions <ref type="bibr" target="#b12">[13]</ref>, thus significantly affecting the quality of generated pseudo labels.</p><p>Recently, Transformers <ref type="bibr" target="#b41">[42]</ref> have achieved significant breakthroughs in numerous visual applications <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b4">5]</ref>. We argue that the Transformer architecture naturally benefits the WSSS task. Firstly, the self-attention mechanism in Transformers could model the global feature relations and conquer the aforementioned drawback of convolutional neural networks, thus discovering more integral object regions. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we find that the multi-head selfattention (MHSA) in Transformers could capture semanticlevel affinity, and can thus be used to improve the coarse pseudo labels. However, the affinity captured in MHSA is still inaccurate ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), i.e., directly applying MHSA as affinity to revise the labels does not work well in practice, which is shown in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>.</p><p>Based on the above analysis, we propose a Transformerbased end-to-end framework for WSSS. Specifically, we leverage Transformers to generate CAM as the initial pseudo labels, to avoid the intrinsic drawback of convolutional neural networks. We further exploit the inherent affinity in Transformer blocks to improve the initial pseudo labels. Since the semantic affinity in MHSA is coarse, we propose an Affinity from Attention (AFA) module, which aims to derive reliable pseudo affinity labels to supervise the semantic affinity learned from the MHSA in Transformer. The learned affinity is then employed to revise the initial pseudo labels via random walk propagation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, which could diffuse object regions and dampen the falsely activated regions. To derive highly-confident pseudo affinity labels for AFA and ensure the local consistency of the propagated pseudo labels, we further propose a Pixel-Adaptive Refinement module (PAR). Based on the pixel-adaptive convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>, PAR efficiently integrates the RGB and position information of local pixels to refine the pseudo labels, enforcing better alignment with low-level image appearance. In addition, given the simplicity, our model can be trained in an end-to-end manner, thus avoiding a complex training pipeline. Experimental results on PASCAL VOC 2012 <ref type="bibr" target="#b11">[12]</ref> and MS COCO 2014 <ref type="bibr" target="#b28">[29]</ref> demonstrate that our method remarkably surpasses recent end-to-end methods and several multi-stage competitors.</p><p>In summary, our contributions are listed as follows. ? We propose an end-to-end Transformer-based framework for WSSS with image-level labels. To the best of our knowledge, this is the first work to explore Transformers for WSSS. ? We exploit the inherent virtue of Transformer and devise an Affinity from Attention (AFA) module. AFA learns reliable semantic affinity from MHSA and propagates the pseudo labels with the learned affinity. ? We propose an efficient Pixel-Adaptive Refinement (PAR) module, which incorporates the RGB and position information of local pixels for label refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly-Supervised Semantic Segmentation</head><p>Multi-stage Methods. Most WSSS methods with imagelevel labels are accomplished in a multi-stage process. Commonly, these approaches train a classification network to produce the initial pseudo pixel-level labels with CAM.</p><p>To address the drawback of incomplete object activation of CAM, <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b39">40]</ref> utilize Erasing strategy to erase the most discriminative regions and thus discover more complete object regions. Inspired by the observation that the classification network tends to focus on different object regions at different training stages, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b17">18]</ref> accumulate the activated regions in the training process. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref> propose to mine semantic regions from multiple input images, discovering similar semantic regions. A prevailing group of WSSS trains classification network with auxiliary tasks to ensure integral object discovery <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. Some recent researches interpret CAM generation from novel perspectives, such as causal inference <ref type="bibr" target="#b54">[55]</ref>, information bottleneck theory <ref type="bibr" target="#b21">[22]</ref>, and anti-adversarial attack <ref type="bibr" target="#b22">[23]</ref>. Affinity label AFA cls. prediction <ref type="figure">Figure 3</ref>. The proposed end-to-end framework for WSSS. We use a Transformer backbone as the encoder to extract feature maps. The initial pseudo labels are generated with CAM <ref type="bibr" target="#b58">[59]</ref> and then refined with the proposed PAR. In the AFA module, we derive the semantic affinity from MHSA in Transformer blocks. AFA is supervised with the pseudo affinity labels derived from the refined label. Next, we employ the learned affinity to revise the pseudo labels via random walk propagation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. The propagated labels are finally refined with PAR as the pseudo labels for the segmentation branch.</p><p>and stochastic low-level information transfer, 1Stage <ref type="bibr" target="#b3">[4]</ref> achieves comparable performance with multi-stage models. In <ref type="bibr" target="#b52">[53]</ref>, RRM takes CAM as the initial pseudo labels and employs CRF <ref type="bibr" target="#b19">[20]</ref> to produce the refined label as supervision for segmentation. RRM also introduces an auxiliary regularization loss <ref type="bibr" target="#b40">[41]</ref> to ensure the consistency between segmentation map and low-level image appearance. <ref type="bibr" target="#b56">[57]</ref> introduces the adaptive affinity field <ref type="bibr" target="#b16">[17]</ref> with weighted affinity kernels and feature-to-prototype alignment loss to ensure the semantic fidelity. The above methods commonly adopt CNN and raise the inherent drawback of convolution, i.e., failing to capture the global information, leading to the incomplete activation of objects <ref type="bibr" target="#b12">[13]</ref>. In this work, we explore Transformers for end-to-end WSSS to address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer in Vision</head><p>In <ref type="bibr" target="#b10">[11]</ref>, Dosovitskiy et al. proposes Vision Transformer (ViT), the first work to apply pure Transformer architecture for visual recognition tasks, achieving astonishing performance on visual classification benchmarks. Later variants show ViT also benefits downstream vision tasks, such as semantic segmentation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b57">58</ref>], depth estimation <ref type="bibr" target="#b32">[33]</ref>, and video understanding <ref type="bibr" target="#b4">[5]</ref>. In <ref type="bibr" target="#b12">[13]</ref>, Gao et al. proposes the first Transformers-based method (TS-CAM) for weaklysupervised object localization (WSOL). Close to WSSS, WSOL aims at localizing objects with only image-level supervision. TS-CAM trains a ViT model with image-level supervision, generates semantic-aware CAM, and couples the generated CAM with semantic-agnostic attention maps. The semantic-agnostic attention maps are derived from the attention of class token against other patch tokens. Nevertheless, TS-CAM didn't exploit the intrinsic seman-tic affinity in MHSA to promote the localization results. In this work, we propose to learn the reliable semantic affinity from MHSA and propagate CAM with the learned affinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first introduce the Transformer backbone and CAM to generate initial pseudo labels. We then present the Affinity from Attention (AFA) module to learn reliable semantic affinity and propagate the initial pseudo labels with the learned affinity. Afterward, we introduce a Pixel-Adaptive Refinement (PAR) module to ensure the local consistency of pseudo labels. The overall loss function for optimization is presented in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer Backbone</head><p>As shown in <ref type="figure">Fig. 3</ref>, our framework uses the Transformers as the backbone. An input image is firstly split into h ? w patches, where each patch is flattened and linearly projected to form h ? w tokens. In each Transformer block, the multi-head self-attention (MHSA) is used to capture global feature dependencies. Specifically, for the i th head, patch tokens are projected with Multi-Layer Perception (MLP) layers and construct queries Q i ? R hw?d k , keys K i ? R hw?d k , and values V i ? R hw?dv . d k is feature dimension of queries and keys, and d v denotes the feature dimension of values. Based on Q i , K i and V i , the selfattention matrix S i and outputs X i are</p><formula xml:id="formula_0">S i = Q i K i ? d k , X i = softmax(S i )V i .<label>(1)</label></formula><p>The final output X o of the Transformer block is constructed by feeding (X 1 X 2 ... X n ) into feed-forward layers (FFN), i.e.,</p><formula xml:id="formula_1">X o = FFN(X 1 X 2 ... X n ), where FFN(?)</formula><p>consists of Layer Normalization <ref type="bibr" target="#b5">[6]</ref> and MLP layers. (? ?) denotes the concatenation operation. By stacking multiple Transformer blocks, the backbone produces feature maps for the subsequent modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CAM Generation</head><p>Considering the simplicity and inference efficiency, we adopt class activation maps (CAM) <ref type="bibr" target="#b58">[59]</ref> as the initial pseudo labels. For the extracted feature maps F ? R hw?d and a given class c, the activation map M c is generated via weighting the feature maps in F with their contribution to class c, i.e., the weight matrix W in the classification layer,</p><formula xml:id="formula_2">M c = ReLu d i=1 W i,c F i ,<label>(2)</label></formula><p>where ReLu function is used to remove the negative activations. Min-Max normalization is applied to scale M c to [0, 1]. A background score ? (0 &lt; ? &lt; 1) is then used to differentiate foreground and background regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Affinity from Attention</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we notice the consistency between MHSA in Transformers and the semantic-level affinity, which motivates us to use MHSA to discover the object regions. However, since no explicit constraints are imposed on self-attention matrices during the training process, the learned affinity in MHSA is typically coarse and inaccurate, which means directly applying MHSA as affinity to refine the initial labels does not work well <ref type="figure" target="#fig_1">(Fig 2 (c)</ref>). Therefore, we propose the Affinity from Attention module (AFA) to counter this problem.</p><p>Assuming MHSA in a Transformer block is denoted as S ? R hw?hw?n , where hw is the flattened spatial size and n is the number of attention heads. In our AFA module, we directly produce the semantic affinity by linearly combining the multi-head attention, i.e., using an MLP layer. Essentially, the self-attention mechanism is a kind of directed graphical model <ref type="bibr" target="#b42">[43]</ref>, while the affinity matrix should be symmetric since nodes sharing the same semantics are supposed to be equal. To perform such transformation, we simply add S and its transpose. The predicted semantic affinity matrix A ? R hw?hw is thus denoted as</p><formula xml:id="formula_3">A = MLP(S + S ).<label>(3)</label></formula><p>Here, we use the matrix transpose operator to denote the transpose of each self-attention matrix in tensor S. Pseudo Affinity Label Generation. To learn the favorable semantic affinity A, a key step is to derive a reliable pseudo affinity label Y af f as supervision. As shown in <ref type="figure">Fig. 3</ref>, we</p><p>derive Y af f from the refined pseudo labels (the refinement module will be introduced later). We first use two background scores ? l and ? h , where 0 &lt; ? l &lt; ? h &lt; 1, to filter the refined pseudo labels to reliable foreground, background, and uncertain regions. Formally, given CAM M ? R h?w?C , the pseudo label Y p is constructed as</p><formula xml:id="formula_4">Y i,j p = ? ? ? ? ? argmax(M i,j,: ), if max(M i,j,: ) ? ? h , 0, if max(M i,j,: ) ? ? l , 255, otherwise,<label>(4)</label></formula><p>where 0 and 255 denote the index of the background class and the ignored regions, respectively. argmax(?) extracts the semantic class with the maximum activation value. The pseudo affinity label Y af f ? R hw?hw is then derived from Y p . Specifically, for Y p , if the pixel (i, j) and (k, l) share the same semantic, we set their affinity as positive; otherwise, their affinity is set as negative. Note that if pixels (i, j) or (k, l) are sampled from the ignored regions, their affinity will also be ignored. Besides, we only consider the situation that pixel (i, j) and (k, l) are in the same local window, and disregard the affinity of distant pixel pairs. Affinity Loss. The generated pseudo affinity label Y af f is then used to supervise the predicted affinity A. The affinity loss term L af f is constructed as</p><formula xml:id="formula_5">L af f = 1 N + (ij,kl)?R + (1 ? sigmoid(A ij,kl )) + 1 N ? (ij,kl)?R ? sigmoid(A ij,kl ),<label>(5)</label></formula><p>where R + and R ? denote the set of positive and negative samples in Y af f , respectively. N + and N ? count the number of R + and R ? . Intuitively, Eq. 5 enforces the network to learn highly confident semantic affinity relations from MHSA. On the other hand, since the affinity prediction A is the linear combination of MHSA, Eq. 5 also benefits the learning of self-attention and further helps to discover the integral object regions. Propagation with Affinity. The learned reliable semantic affinity could be used to revise the initial CAM. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, we fulfill this process via random walk <ref type="bibr" target="#b43">[44]</ref>. For the learned semantic affinity matrix A, the semantic transition matrix T is derived as</p><formula xml:id="formula_6">T = D ?1 A ? , with D ii = k A ik ? ,<label>(6)</label></formula><p>where ? &gt; 1 is a hyper-parameter to ignore trivial affinity values in A, and D is a diagonal matrix to normalize A rowwise. The random walk propagation for the initial CAM M ? R h?w?C is accomplished as</p><formula xml:id="formula_7">M af f = T * vec(M ),<label>(7)</label></formula><p>where vec(?) vectorizes M . This propagation process diffuses the semantic regions with high affinity and dampens the wrongly activated regions so that the activation maps align better with semantic boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pixel-Adaptive Refinement</head><p>As shown in <ref type="figure">Fig. 3</ref>, the pseudo affinity label Y af f is derived from the initial pseudo labels. However, the initial pseudo labels are typically coarse and locally inconsistent, i.e., neighbor pixels with similar low-level image appearance may not share the same semantic. To ensure the local consistency, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref> adopt dense CRF <ref type="bibr" target="#b19">[20]</ref> to refine the initial pseudo labels. However, CRF is not a favorable choice in end-to-end framework since it remarkably slows down the training efficiency. Inspired by <ref type="bibr" target="#b3">[4]</ref>, which utilizes the pixel-adaptive convolution <ref type="bibr" target="#b36">[37]</ref> to extract local RGB information for refinement, we incorporate the RGB and spatial information to define the low-level pairwise affinity and construct our Pixel-Adaptive Refinement module (PAR).</p><p>Given the input image I ? R h?w?3 , for the pixel at position (i, j) and (k, l), the RGB and spatial pairwise terms are defined as:</p><formula xml:id="formula_8">? ij,kl rgb = ? |Iij ? I kl | w1? ij rgb 2 , ? ij,kl pos = ? |Pij ? P kl | w2? ij pos 2 ,<label>(8)</label></formula><p>where I ij and P ij denote the RGB information and the spatial location of pixel (i, j), respectively. In practice, we use the XY coordinates as the spatial location. In Eq. 8, ? rgb and ? pos denote the standard deviation of RGB and position difference, respectively. w 1 and w 2 control the smoothness of ? rgb and ? pos , respectively. The affinity kernel for PAR is then constructed by normalizing ? rgb and ? pos with softmax and adding them together, i.e., , <ref type="bibr" target="#b8">(9)</ref> where (x, y) is sampled from the neighbor set of (i, j), i.e. N (i, j), and w 3 adjusts the importance of the position term. Based on the constructed affinity kernel, we refine both the initial CAM and the propagated CAM. The refinement is conducted for multiple iterations. For CAM M ? R h?w?C , in iteration t, we have</p><formula xml:id="formula_9">M i,j,c t = (k,l)?N (i,j) ? ij,kl M k,l,c t?1 .<label>(10)</label></formula><p>For the neighbor pixel sets N (?), we follow <ref type="bibr" target="#b3">[4]</ref> and define it as the 8-way neighbors with multiple dilation rates. Such design ensures the training efficiency, since the dilated neighbors of a given pixel can be easily extracted using 3?3 dilated convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Network Training</head><p>As shown in <ref type="figure">Fig. 3</ref>, our framework consists of three loss terms, i.e., a classification loss L cls , a segmentation loss L seg , and an affinity loss L af f .</p><p>For the classification loss, following the common practice, we feed the aggregated features into a classification layer to compute the class probability vector p cls , then employ the multi-label soft margin loss as the classification function.</p><formula xml:id="formula_10">L cls = 1 C C c=1 (y c log(p c cls ) + (1 ? y c ) log(1 ? p c cls )),<label>(11)</label></formula><p>where C is the total number of classes, and y is the ground truth image-level label.</p><p>For the segmentation loss L seg , we adopt the commonlyused cross-entropy loss. As shown in <ref type="figure">Fig. 3</ref>, the supervision for the segmentation branch is the revised label with affinity propagation. In order to obtain better alignment with low-level image appearance, we use the proposed PAR to further refine the propagated labels. The affinity loss L af f for affinity learning is previously described in Eq. 5.</p><p>The overall loss is the weighted sum of L cls , L af f , and L seg . In addition, to further promote the performance, we also employ the regularization loss L reg used in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b52">53]</ref>, which ensures the local consistency of the segmentation predictions. The overall loss is finally formulated as</p><formula xml:id="formula_11">L = L cls + ? 1 L seg + ? 2 L af f + ? 3 L reg ,<label>(12)</label></formula><p>where ? 1 , ? 2 , and ? 3 balance the contributions of different losses.  <ref type="bibr" target="#b48">[49]</ref>, which is a more friendly backbone for image segmentation tasks than the vanilla ViT <ref type="bibr" target="#b57">[58]</ref>. In brief, MiT uses overlapped patch embedding to keep local consistency, spatial-reductive self-attention to accelerate computation, and FFN with convolutions to safely replace position embedding. For the segmentation decoder, we use the MLP decoder head <ref type="bibr" target="#b48">[49]</ref>, which fuses multi-level feature maps for prediction with simple MLP layers. The backbone parameters are initialized with ImageNet-1k <ref type="bibr" target="#b9">[10]</ref> pre-trained weights, while other parameters are randomly initialized. Implementation Details. We use an AdamW optimizer <ref type="bibr" target="#b29">[30]</ref> to train our network. For the backbone parameters, the initial learning rate is set as 6 ? 10 ?5 and decays every iteration with a polynomial scheduler. The learning rate for other parameters is 10 times the learning rate of backbone parameters. The weight decay factor is set as 0.01. For data augmentation, random rescaling with a range of [0.5, 2.0], random horizontally flipping, and random cropping with a cropping size of 512?512 are adopted. The batch size is set as 8. For the experiments on the PASCAL VOC dataset, we train the network for 20,000 iterations. To ensure the initial pseudo labels are favorable, we warm-up the classification branch for 2,000 iterations and the affinity branch for the next 4,000 iterations. For experiments on the MS COCO dataset, the number of total iterations is 80,000. Accordingly, the number of warm-up iteration for the classification and affinity branch are 5,000 and 15,000, respectively. The default hyper-parameters are set as follows. For pseudo label generation, the background thresholds (? h , ? l ) are (0.55, 0.35). In PAR, same as <ref type="bibr" target="#b3">[4]</ref>, the dilation rates for extracting neighbor pixels are <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>. We set the weight factors (w 1 , w 2 , w 3 ) as (0.3, 0.3, 0.01). When computing the affinity loss, the radius of the local window to ignore distant affinity pairs is set as 8. In Eq. 6, we set the power factor ? as 2. The weight factors in Eq. 12 are 0.1, 0.1, and 0.01, respectively. The detailed investigation of the hyper-parameters is reported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Initial Pseudo Label Generation.</head><p>In this work, we use the popular CAM to generate the initial pseudo labels. Empirically, for a CNN-based classification network, the choice of pooling method notably affects the quality of CAM. Specifically, global max-pooling (gmp) tends to underestimate the object size, while global average-pooling (gap) typically overestimates the object regions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b58">59]</ref>. Here, we investigate the favorable pooling method for Transformer-based classification network. We first generalize gmp and gap with top-k pooling, i.e., av- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Analysis</head><p>The quantitative results of ablation analysis are reported in Tab. 2. Tab. 2 shows that our baseline model based on Transformers achieves 46.7% mIoU on the PASCAL VOC val set. The proposed PAR and AFA further significantly improve the mIoU to 56.2% and 62.6%, respectively. With the auxiliary regularization loss L reg , the proposed framework achieves 63.8% mIoU. The CRF postprocessing brings further 2.2% mIoU improvements, promoting the final performance to 66.0% mIoU. In short, the quantitative results in Tab. 2 demonstrate our proposed modules are remarkably effective. AFA. The motivation of AFA is to learn reliable semantic affinity from MHSA and revise the pseudo labels with the learned affinity. In <ref type="figure">Fig. 4</ref>, we present some example images of the self-attention maps (extracted from the last Transformer block) and the learned affinity maps. <ref type="figure">Fig. 4</ref> shows that our AFA could effectively learn reliable semantic affinity from the inaccurate MHSA. The affinity loss in the AFA module also encourages the MHSA to model the semantic relations well. In <ref type="figure">Fig. 4</ref>, we also present the pseudo labels generated from our model without AFA module (w/o AFA), with AFA module but no random walk propagation (AFA w/o prop.) and with full AFA module. For the generated pseudo labels, the AFA module brings notable visual</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFA w/o prop MHSA Images</head><p>Ground Truth with Affinity map w/o AFA <ref type="figure">Figure 4</ref>. Visualization of the MHSA maps, learned affinity maps, and generated pseudo labels for segmentation. " " denotes the query point to visualize the attention and affinity maps. improvements. The affinity propagation process further diffuses the regions with high semantic affinity and dampens the regions with low affinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refined label Ground truth Initial label</head><p>In Tab. 3, we report the quantitative results of the generated pseudo labels on PASCAL VOC train and val set. We also report the results of performing random walk propagation with the average vanilla MHSA as semantic affinity (AFA prop. with MHSA). The results show that the affinity learning loss in the AFA module remarkably improves the accuracy of the pseudo labels (from 54.4% mIoU to 66.3% mIoU on the train set). The propagation process could further promote the reliability of pseudo labels, which harvests the performance gains in Tab. 2. It is also noted that propagation with the naive MHSA significantly reduces the accuracy, demonstrating our motivation and the effectiveness of the AFA module. PAR. The proposed PAR aims at refining the initial pseudo labels with low-level image appearance and position information. In <ref type="figure" target="#fig_3">Fig. 5</ref>, we present the qualitative improvements of PAR. <ref type="figure" target="#fig_3">Fig. 5</ref> shows PAR effectively dampens the falsely activated regions, enforcing better alignment with the lowlevel boundaries. Quantitatively, as shown in Tab. 4, our PAR improves the CAM (generated with Transformer baseline) from 48.2% to 52.9%, which outperforms PAMR <ref type="bibr" target="#b3">[4]</ref>, which is also based on the dilated pixel-adaptive convolution to incorporate local image appearance information. Tab. 4 also demonstrates the position kernel ? pos in PAR is beneficial for refining CAM. More investigation details on PAR are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to State-of-the-art</head><p>PASCAL VOC 2012. We report the semantic segmentation performance on PASCAL VOC 2012 val and test set in Tab. 5. R101 and WR38 denote the method uses ResNet101 <ref type="bibr" target="#b14">[15]</ref> and WideResNet38 <ref type="bibr" target="#b47">[48]</ref> as backbone, respectively. Tab. 5 shows that the proposed model clearly surpasses previous state-of-the-art end-to-end methods. Our method achieves 83.8% of its fully-supervised counterpart, i.e., Segformer <ref type="bibr" target="#b48">[49]</ref>, while 1Stage <ref type="bibr" target="#b3">[4]</ref> and AA&amp;LR <ref type="bibr" target="#b56">[57]</ref> only achieve 77.6% and 79.1% of WideResNet38, respectively. Our method is also competitive with some recent multistage WSSS methods, such as OAA+ <ref type="bibr" target="#b15">[16]</ref>, SEAM <ref type="bibr" target="#b44">[45]</ref>, SC-CAM <ref type="bibr" target="#b6">[7]</ref>, and CDA <ref type="bibr" target="#b37">[38]</ref>. It's also noted that our method also outperforms RRM with MiT-B1 as backbone, which demonstrates the efficay of the proposed AFA and PAR. MS COCO 2014. We present the semantic segmentation performance on the challenging MS COCO 2014 dataset in Tab. 6. Our end-to-end method could achieve 38.9% mIoU on the val set, which remarkably outperforms most recent multi-stage methods (except RIB <ref type="bibr" target="#b21">[22]</ref>).</p><p>Qualitative Results. In <ref type="figure">Fig. 6</ref>, we present the qualitative results of our method on the PASCAL VOC and MS COCO  <ref type="figure">Figure 6</ref>. Segmentation results of 1Stage <ref type="bibr" target="#b3">[4]</ref> and our method on VOC and COCO val set.</p><p>val set. On the PASCAL VOC dataset, visually, our method could outperform 1Stage <ref type="bibr" target="#b3">[4]</ref> and produce segmentation results that align finely with object boundaries. The qualitative results on MS COCO dataset are also comparable with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we explore the intrinsic virtue of Transformer architecture for WSSSS tasks. Specifically, we use a Transformer-based backbone to generate CAM as the initial pseudo labels, avoiding the inherent flaw of CNN. Besides, we note the consistency between the MHSA and semantic affinity, and thus propose the AFA module. AFA derives reliable affinity labels from pseudo labels, imposes the affinity labels to supervise the MHSA, and produces reliable affinity predictions. The learned affinity is used to revise the initial pseudo labels via random walk propagation. On PASCAL VOC and MS COCO datasets, our method achieves new state-of-the-art performance for endto-end WSSS. In a broader view, the proposed method also shows a novel perspective for vision transformers, i.e. guiding the self-attention with semantic relation to ensure better feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">More Technical Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Details of Backbone</head><p>We use the MiT-B1 proposed in SegFormer <ref type="bibr" target="#b48">[49]</ref> as the backbone, which is a more friendly backbone for image segmentation tasks than the vanilla ViT <ref type="bibr" target="#b10">[11]</ref>. SegFormer uses Overlapped Patch Merging layers with different strides to produce multi-scale feature maps. As shown in <ref type="figure">Fig. 7</ref>, in SegFormer, the feature of Stage #4 is h 32 ? w 32 . To obtain the initial pseudo labels (CAM) with higher resolution, we change the stride of the last patch merging layer from 2 to 1, increasing resolution of the feature maps to the size of h 16 ? w 16 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SegFormer:</head><p>This work: <ref type="figure">Figure 7</ref>. The size of feature maps of different stags.</p><formula xml:id="formula_12">Stage #1 Stage #4 Stage #3 Stage #2</formula><p>In practice, to produce the semantic affinity prediction, we use the multi-head self-attention (MHSA) matrices extracted from the last stage, which could capture the highlevel semantic information. The MHSA matrices are concatenated to form S ? R hw 256 ? hw 256 ?nk and prediction the semantic affinity, where n and k are the number of Transformer blocks and heads in each block, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Mask for Affinity Loss</head><p>Inspired by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, when computing affinity loss, we only consider the situation that pixel pairs are in the same local window with the radius of r, and disregard their affinity if the distance is too far. Specifically, given a pixel (i, j), if pixel (k, l) is the same window with (i, j), their affinity is computed; otherwise, their affinity is ignored. Unlike <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, which extract pixel pairs when computing affinity loss, we efficiently implemented by applying a mask. The conceptual illustration of this strategy and an example mask is presented in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ignored regions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Valid regions</head><p>Pixel <ref type="figure">Figure 8</ref>. Left: Illustration of the valid pixel pairs. Right: Example mask for computing the affinity loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Hyper-parameters</head><p>Affinity from Attention. In Tab. 7, we present segmentation results on the PASCAL VOC val set with different radius r of the local window size when computing the affinity loss. Intuitively, a small r can not provide enough affinity pairs while a large r may not ensure the reliability of distant affinity pairs. As shown in Tab. 7, r = 8 is a proper choice. Pixel-Adaptive Refinement. In Tab. 8, we report the impact of different configurations of the proposed Pixel-Adaptive Refinement, including the dilation rates, position kernel, and the number of iteration. Tab. 8 shows that for the same dilation rates, our PAR remarkably outperforms PAMR <ref type="bibr" target="#b3">[4]</ref>, demonstrating the necessity of the position kernel. Tab. 9 presents the impact of the weights factors of PAR. For simplicity, we set w 1 = w 2 . Tab. 9 shows w 1 = 0.3, w 2 = 0.3, w 3 = 0.01 is a favorable choice. Weight Factors. We present the segmentation results on the PASCAL VOC val set with different weight factors of loss terms in Tab. 10. ? 1 = 0.1, ? 2 = 0.2, ? 2 = 0.01 is a preferred choice for our framework. Background Scores We investigate the impact of the background scores (? l , ? h ) to filter the pseudo labels to the reliable foreground, background, and uncertain regions. Intuitively, large ? h and small ? l could produce more reliable pseudo labels but reduce the number of valid labels. On the contrary, small ? h and large ? l will introduce noise to the pseudo labels. Note that the average value of ? h and ? l is always 0.45, which is the preferred background score for generated CAM in our preliminary experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">More Quantitative Results</head><p>We present the per-category segmentation results on PASCAL VOC val set in Tab 12. Our method achieves the best results for most categories. The results on test set are available at the official PASCAL VOC evaluation website 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">More Qualitative Results</head><p>We present more qualitative results as follows.   <ref type="figure" target="#fig_0">Figure 10</ref>. Improvements of the proposed pixel-adaptive refinement (PAR) module on the pseudo labels. The pseudo labels are generated with CAM and Transformer baseline. The proposed PAR could effectively dampen the falsely activated regions and ensure the alignment with low-level image appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c) Refined label (b) Ground truth (c) Initial label (a) Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFA w/o MHSA Images</head><p>Ground Truth with Affinity map w/o AFA <ref type="figure" target="#fig_0">Figure 11</ref>. Visualization of the MHSA maps, learned affinity maps, and generated pseudo labels for segmentation. " " denotes the query point to visualize the attention and affinity maps. The pseudo labels are generated with our model without AFA module (w/o AFA), with AFA module but no random walk propagation (AFA w/o prop.) and with full AFA module (with AFA). For the generated pseudo labels, the AFA module brings notable visual improvements. The affinity propagation process further diffuses the regions with high semantic affinity and dampens the regions with low affinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth 1Stage</head><p>Ours Ground Truth Ours PASCAL VOC MS COCO <ref type="figure" target="#fig_0">Figure 12</ref>. Semantic segmentation results on PASCAL VOC val (left) and MS COCO val set (right). Our method outperforms 1Stage <ref type="bibr" target="#b3">[4]</ref> and is comparable with ground truth labels.  <ref type="figure" target="#fig_0">Figure 13</ref>. Visualization of the MHSA maps extracted from model without and with our AFA. " " denotes the query point. Our AFA could help the MHSA to capture better semantic affinity.</p><p>,QGH[RI+HDG ZHLJKW <ref type="figure" target="#fig_0">Figure 14</ref>. The learned weights of each head of self-attention in the AFA module. Here we only present the 8 heads of the last Transformer block. The MHSA matrices do not contribute equally to semantic affinity. Some self-attention matrices (head #2, head #3, and head #5) contribute negatively to semantic affinity. The learned weights suggest applying MHSA directly as semantic affinity is not beneficial for the pseudo labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Image and the query points (denote with " ") to visualize the attention and affinity maps; (b) the self-attention maps in Transformer blocks only capture coarse semantic-level affinity relations; (c) the learned reliable semantic affinity from selfattention with our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>CAM generated with (b) Transformers activates more integral regions than (a) CNN. Refining CAM with (c) coarse MHSA doesn't work well, while (d) the learned affinity could remarkably improve the generated CAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>ij,kl = exp(? ij,kl rgb ) (x,y) exp(? ij,xy rgb ) + w3 exp(? ij,kl pos ) (x,y) exp(? ij,xy pos )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Examples of PAR's improvements on the pseudo labels. The pseudo labels are generated with CAM and Transformer baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>CAM generated with (a) Transformers activates more integral regions than (b) CNN. Refining CAM with (c) coarse MHSA doesn't work well, while (d) the learned affinity could remarkably improve the generated CAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>PAR Transformer PAR Feature maps Classification layer CAM MHSA Affinity prediction Refined label Seg. decoder Seg. prediction Pseudo label Image MLP layer Classification Segmentation Affinity learning Initial pseudo label Random walk</head><label></label><figDesc></figDesc><table><row><cell>End-to-End Methods. Due to the extremely limited su-</cell></row><row><cell>pervision, training an end-to-end model for WSSS with fa-</cell></row><row><cell>vorable performance is difficult. [31] proposes an adap-</cell></row><row><cell>tive Expectation-Maximization framework to infer pseudo</cell></row><row><cell>ground truth for segmentation. [32] tackles WSSS with</cell></row><row><cell>image-level labels as a multi-instance learning (MIL) prob-</cell></row><row><cell>lem and devises the Log-Sum-Exp aggregation function</cell></row><row><cell>to drive the network to assign correct pixel labels. Incor-</cell></row><row><cell>porating nGWP pooling, pixel-adaptive mask refinement,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Impact of top-k pooling with different top percentages on CAM. The results are evaluated on the PASCAL VOC train and val set and reported in mIoU(%).</figDesc><table><row><cell></cell><cell>gap</cell><cell>50%</cell><cell>25%</cell><cell>10%</cell><cell>gmp</cell></row><row><cell>train</cell><cell>30.7</cell><cell>34.5</cell><cell>39.6</cell><cell>43.5</cell><cell>48.2</cell></row><row><cell>val</cell><cell>31.1</cell><cell>34.8</cell><cell>39.7</cell><cell>43.6</cell><cell>48.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablation studies of our proposed method on PASCAL VOC val set. Evaluation of the pseudo labels for segmentation.</figDesc><table><row><cell>Method</cell><cell>PAR</cell><cell>AFA</cell><cell>Lreg</cell><cell>CRF</cell><cell>val</cell></row><row><cell>Our Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56.2</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.6 63.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>train</cell><cell>val</cell></row><row><cell>PSA [2]</cell><cell></cell><cell></cell><cell></cell><cell>59.7</cell><cell>-</cell></row><row><cell>IRN [1]</cell><cell></cell><cell></cell><cell></cell><cell>66.5</cell><cell>-</cell></row><row><cell>1Stage [4]</cell><cell></cell><cell></cell><cell></cell><cell>66.9</cell><cell>65.3</cell></row><row><cell></cell><cell>w/o AFA</cell><cell></cell><cell></cell><cell>54.4</cell><cell>54.2</cell></row><row><cell>Ours</cell><cell cols="3">AFA (w/o prop.) AFA (prop. with MHSA)</cell><cell>66.3 58.3</cell><cell>64.4 55.9</cell></row><row><cell></cell><cell>AFA</cell><cell></cell><cell></cell><cell>68.7</cell><cell>66.5</cell></row></table><note>eraging the top k% values in each feature map. In this situa- tion, gmp and gap are two special cases of top-k pooling, i.e., top-100% and top-1 pooling. We present the impact of top-k pooling with different k in Tab. 1. Tab. 1 shows that in our framework, for the Transformer-based classification network, using gmp for feature aggregation helps to gener- ate CAM with favorable performance, which is owing to the capacity of global modeling of self-attention.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of refinement methods for CAM.</figDesc><table><row><cell>? rgb</cell><cell>?pos</cell><cell>train</cell></row><row><cell>CAM</cell><cell></cell><cell>48.2</cell></row><row><cell>PAMR [4]</cell><cell></cell><cell>51.4</cell></row><row><cell>PAR</cell><cell></cell><cell>51.7 52.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Semantic segmentation results on PASCAL VOC 2012 dataset. Sup. denotes supervision type. F: full supervision; I: image-level labels; S: saliency maps. ? denotes our implementation. Semantic segmentation results on MS COCO dataset.</figDesc><table><row><cell>Method</cell><cell>Sup.</cell><cell>Backbone</cell><cell>val</cell><cell>test</cell></row><row><cell>Fully-supervised models.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepLab [8]</cell><cell></cell><cell>R101</cell><cell cols="2">77.6 79.7</cell></row><row><cell>WideResNet38 [48]</cell><cell>F</cell><cell>WR38</cell><cell cols="2">80.8 82.5</cell></row><row><cell>Segformer  ? [49]</cell><cell></cell><cell>MiT-B1</cell><cell>78.7</cell><cell>-</cell></row><row><cell cols="3">Multi-Stage weakly-supervised models.</cell><cell></cell><cell></cell></row><row><cell>OAA+ [16] ICCV'2019</cell><cell></cell><cell>R101</cell><cell cols="2">65.2 66.4</cell></row><row><cell>MCIS [39] ECCV'2020</cell><cell></cell><cell>R101</cell><cell cols="2">66.2 66.9</cell></row><row><cell>AuxSegNet [50] ICCV'2021</cell><cell>I + S</cell><cell>WR38</cell><cell cols="2">69.0 68.6</cell></row><row><cell>NSROM [51] CVPR'2021</cell><cell></cell><cell>R101</cell><cell cols="2">70.4 70.2</cell></row><row><cell>EPS [25] CVPR'2021</cell><cell></cell><cell>R101</cell><cell cols="2">70.9 70.8</cell></row><row><cell>SEAM [45] CVPR'2020</cell><cell></cell><cell>WR38</cell><cell cols="2">64.5 65.7</cell></row><row><cell>SC-CAM [7] CVPR'2020</cell><cell></cell><cell>R101</cell><cell cols="2">66.1 65.9</cell></row><row><cell>CDA [38] ICCV'2021 AdvCAM [23] CVPR'2021</cell><cell>I</cell><cell>WR38 R101</cell><cell cols="2">66.1 66.8 68.1 68.0</cell></row><row><cell>CPN [56] ICCV'2021</cell><cell></cell><cell>R101</cell><cell cols="2">67.8 68.5</cell></row><row><cell>RIB [22] NeurIPS'2021</cell><cell></cell><cell>R101</cell><cell cols="2">68.3 68.6</cell></row><row><cell cols="3">End-to-End weakly-supervised models.</cell><cell></cell><cell></cell></row><row><cell>EM [31] ICCV'2015</cell><cell></cell><cell>VGG16</cell><cell cols="2">38.2 39.6</cell></row><row><cell>MIL [32] CVPR'2015</cell><cell></cell><cell>-</cell><cell cols="2">42.0 40.6</cell></row><row><cell>CRF-RNN [34] CVPR'2017</cell><cell></cell><cell>VGG16</cell><cell cols="2">52.8 53.7</cell></row><row><cell>RRM [53] AAAI'2020</cell><cell>I</cell><cell>WR38</cell><cell cols="2">62.6 62.9</cell></row><row><cell>RRM  ? [53] AAAI'2020</cell><cell></cell><cell>MiT-B1</cell><cell>63.5</cell><cell>-</cell></row><row><cell>1Stage [4] CVPR'2020</cell><cell></cell><cell>WR38</cell><cell cols="2">62.7 64.3</cell></row><row><cell>AA&amp;LR [57] ACM MM'2021</cell><cell></cell><cell>WR38</cell><cell cols="2">63.9 64.8</cell></row><row><cell>Ours</cell><cell></cell><cell>MiT-B1</cell><cell cols="2">66.0 66.3</cell></row><row><cell>Method</cell><cell>Sup.</cell><cell cols="2">Backbone</cell><cell>val</cell></row><row><cell cols="3">Multi-Stage weakly-supervised models.</cell><cell></cell><cell></cell></row><row><cell>EPS [25] CVPR'2021 AuxSegNet [50] ICCV'2021</cell><cell>I + S</cell><cell>R101 WR38</cell><cell></cell><cell>35.7 33.9</cell></row><row><cell>SEAM [45] CVPR'2020</cell><cell></cell><cell>WR38</cell><cell></cell><cell>31.9</cell></row><row><cell>CONTA [55] NeurIPS'2020</cell><cell></cell><cell>WR38</cell><cell></cell><cell>32.8</cell></row><row><cell>CDA [38] ICCV'2021</cell><cell>I</cell><cell>WR38</cell><cell></cell><cell>31.7</cell></row><row><cell>CGNet [21] ICCV'2021</cell><cell></cell><cell>WR38</cell><cell></cell><cell>36.4</cell></row><row><cell>RIB [22] NeurIPS'2021</cell><cell></cell><cell>R101</cell><cell></cell><cell>43.8</cell></row><row><cell cols="3">End-to-End weakly-supervised models.</cell><cell></cell><cell></cell></row><row><cell>Ours Ours + CRF</cell><cell>I</cell><cell>MiT-B1 MiT-B1</cell><cell></cell><cell>38.0 38.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Impact of the radius r when computing the affinity loss. The results are evaluated on the val set of PASCAL VOC 2012.</figDesc><table><row><cell>radius r</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell></row><row><cell>val</cell><cell>62.4</cell><cell>62.7</cell><cell>63.8</cell><cell>61.5</cell><cell>59.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Ablation of the dilation rates, position kernel and number of iteration of the proposed PAR. The results are evaluated on the train set of PASCAL VOC 2012 in mIoU (%).</figDesc><table><row><cell>Dilations</cell><cell cols="2">?pos Iter train</cell></row><row><cell>1 2 4 8 12 24</cell><cell></cell><cell></cell></row><row><cell>CAM</cell><cell></cell><cell>48.2</cell></row><row><cell>PAMR[4]</cell><cell></cell><cell>51.4</cell></row><row><cell>CRF</cell><cell></cell><cell>54.5</cell></row><row><cell></cell><cell>15</cell><cell>48.8</cell></row><row><cell></cell><cell>15</cell><cell>49.9</cell></row><row><cell>PAR</cell><cell>15 15</cell><cell>51.3 51.5</cell></row><row><cell></cell><cell>15</cell><cell>52.9</cell></row><row><cell></cell><cell>20</cell><cell>52.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .Table 10 .Table 11 .</head><label>91011</label><figDesc>Ablation of weight factors of the proposed PAR. The results are evaluated on the train set of PASCAL VOC 2012. Impact of the weights of loss terms. The results are evaluated on the val set of PASCAL VOC 2012. Impact of the background scores ? h , ? l . The results are evaluated on the val set of PASCAL VOC 2012.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>w3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.005</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell></row><row><cell></cell><cell>0.1</cell><cell>51.9</cell><cell>51.7</cell><cell>50.1</cell><cell>-</cell></row><row><cell>w1 &amp; w2</cell><cell>0.3 0.5</cell><cell>52.8 51.9</cell><cell>52.9 52.5</cell><cell>51.4 51.3</cell><cell>48.4 48.3</cell></row><row><cell></cell><cell>0.7</cell><cell>-</cell><cell>51.6</cell><cell>50.9</cell><cell>48.0</cell></row><row><cell></cell><cell>?1</cell><cell>?2</cell><cell>?3</cell><cell></cell><cell>val</cell></row><row><cell>Default</cell><cell>0.1</cell><cell>0.1</cell><cell cols="2">0.01</cell><cell>63.8</cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell>62.8</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>61.6</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell>57.8</cell></row><row><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell>63.4</cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell>61.7</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell>58.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.005</cell><cell>62.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.02</cell><cell>62.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.05</cell><cell>61.5</cell></row><row><cell></cell><cell></cell><cell>? h</cell><cell>? l</cell><cell></cell><cell>val</cell></row><row><cell></cell><cell></cell><cell>0.65</cell><cell>0.25</cell><cell></cell><cell>60.7</cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell>0.3</cell><cell></cell><cell>62.5</cell></row><row><cell>Default</cell><cell></cell><cell>0.55</cell><cell>0.35</cell><cell></cell><cell>63.8</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>0.4</cell><cell></cell><cell>62.9</cell></row><row><cell></cell><cell></cell><cell>0.45</cell><cell>0.45</cell><cell></cell><cell>60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Evaluation and comparison of the semantic segmentation results in mIoU on the val set.</figDesc><table><row><cell></cell><cell cols="4">RRM[53] 1Stage [4] AA&amp;LR [57] Ours</cell></row><row><cell>bkg</cell><cell>87.9</cell><cell>88.7</cell><cell>88.4</cell><cell>89.9</cell></row><row><cell>aero</cell><cell>75.9</cell><cell>70.4</cell><cell>76.3</cell><cell>79.5</cell></row><row><cell>bicycle</cell><cell>31.7</cell><cell>35.1</cell><cell>33.8</cell><cell>31.2</cell></row><row><cell>bird</cell><cell>78.3</cell><cell>75.7</cell><cell>79.9</cell><cell>80.7</cell></row><row><cell>boat</cell><cell>54.6</cell><cell>51.9</cell><cell>34.2</cell><cell>67.2</cell></row><row><cell>bottle</cell><cell>62.2</cell><cell>65.8</cell><cell>68.2</cell><cell>61.9</cell></row><row><cell>bus</cell><cell>80.5</cell><cell>71.9</cell><cell>75.8</cell><cell>81.4</cell></row><row><cell>car</cell><cell>73.7</cell><cell>64.2</cell><cell>74.8</cell><cell>65.4</cell></row><row><cell>cat</cell><cell>71.2</cell><cell>81.1</cell><cell>82.0</cell><cell>82.3</cell></row><row><cell>chair</cell><cell>30.5</cell><cell>30.8</cell><cell>31.8</cell><cell>28.7</cell></row><row><cell>cow</cell><cell>67.4</cell><cell>73.3</cell><cell>68.7</cell><cell>83.4</cell></row><row><cell>table</cell><cell>40.9</cell><cell>28.1</cell><cell>47.4</cell><cell>41.6</cell></row><row><cell>dog</cell><cell>71.8</cell><cell>81.6</cell><cell>79.1</cell><cell>82.2</cell></row><row><cell>horse</cell><cell>66.2</cell><cell>69.1</cell><cell>68.5</cell><cell>75.9</cell></row><row><cell>motor</cell><cell>70.3</cell><cell>62.6</cell><cell>71.4</cell><cell>70.2</cell></row><row><cell>person</cell><cell>72.6</cell><cell>74.8</cell><cell>80.0</cell><cell>69.4</cell></row><row><cell>plant</cell><cell>49.0</cell><cell>48.6</cell><cell>50.3</cell><cell>53.0</cell></row><row><cell>sheep</cell><cell>70.7</cell><cell>71.0</cell><cell>76.5</cell><cell>85.9</cell></row><row><cell>sofa</cell><cell>38.4</cell><cell>40.1</cell><cell>43.0</cell><cell>44.1</cell></row><row><cell>train</cell><cell>62.7</cell><cell>68.5</cell><cell>55.5</cell><cell>64.2</cell></row><row><cell>tv</cell><cell>58.4</cell><cell>64.3</cell><cell>58.5</cell><cell>50.9</cell></row><row><cell>mIOU</cell><cell>62.6</cell><cell>62.7</cell><cell>63.9</cell><cell>66.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http : / / host . robots . ox . ac . uk : 8080 / anonymous / GHJIIH.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the National Natural Science Foundation of China under Grants 62141112, 41871243, and 62002090, the Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies) under Grant 2019AEA170, the Major Science and Technology Innovation 2030 "New Generation Artificial Intelligence" key project (No. 2021ZD0111700). Dr. Baosheng Yu is supported by ARC project FL-170100117.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards single stage weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peri</forename><surname>Akiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10309</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8991" to="9000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ts-cam: Token semantic coupled attention map for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative region suppression for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1754" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unlocking the potential of ordinary classifier: Class-specific adversarial erasing framework for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeokjun</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Hoon</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehee</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6994" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing information bottleneck for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Antiadversarially manipulated attributions for weakly and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4071" to="4080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bbam: Bounding box attribution map for weakly supervised semantic and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaehun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2643" to="2652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Railroad is not a train: Saliency as pseudo-pixel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5495" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Group-wise semantic mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pseudo-mask matters inweakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<idno>2021. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining bottom-up, top-down, and smoothness cues for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3529" to="3538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning visual words for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixiang</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="982" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation with visual words learning and hybrid pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixiang</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<title level="m">Erik Learned-Miller, and Jan Kautz. Pixel-adaptive convolutional neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Context decoupling augmentation for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7004" to="7014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="347" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ecs-net: Improving weakly supervised semantic segmentation by using connections between class activation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7283" to="7292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On regularized losses for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning randomwalk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12275" to="12284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Embedded discriminative attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16765" to="16774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6984" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Nonsalient region object mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Sen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2623" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Affinity attention graph neural network for weakly supervised semantic segmentation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-toend weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, number 07</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dynamic feature regularized loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01296</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Causal intervention for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Complementary patch for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7242" to="7251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adaptive affinity loss and erroneous pseudo-label refinement for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
