<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neighborhood-aware Geometric Encoding Network for Point Cloud Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifa</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haining</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwei</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renmin</forename><surname>Han</surname></persName>
						</author>
						<title level="a" type="main">Neighborhood-aware Geometric Encoding Network for Point Cloud Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The distinguishing geometric features determine the success of point cloud registration. However, most point clouds are partially overlapping, corrupted by noise, and comprised of indistinguishable surfaces, which makes it a challenge to extract discriminative features. Here, we propose the Neighborhood-aware Geometric Encoding Network (NgeNet) for accurate point cloud registration. NgeNet utilizes a geometric guided encoding module to take geometric characteristics into consideration, a multi-scale architecture to focus on the semantically rich regions in different scales, and a consistent voting strategy to select features with proper neighborhood size and reject the specious features. The awareness of adaptive neighborhood points is obtained through the multi-scale architecture accompanied by voting. Specifically, the proposed techniques in NgeNet are model-agnostic, which could be easily migrated to other networks. Comprehensive experiments on indoor, outdoor and object-centric synthetic datasets demonstrate that NgeNet surpasses all of the published state-ofthe-art methods. The code will be available at https://github.com/zhulf0804/NgeNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud registration serves as the key component in various downstream tasks such as 3D reconstruction <ref type="bibr" target="#b12">(Geiger et al., 2011)</ref> and SLAM <ref type="bibr" target="#b34">(Salas-Moreno et al., 2013;</ref><ref type="bibr" target="#b49">Zhang &amp; Singh, 2015)</ref>. Point cloud registration tries to find a transformation that best aligns two overlapping point clouds. However, due to the partial overlap, noise, and indistinguishable surfaces (planes, smooth surfaces, etc.), the robust and accurate registration of real-world point clouds remains a challenging issue.</p><p>Various methods have been proposed in previous decades. <ref type="bibr">1</ref>   <ref type="figure">Figure 1</ref>. The neighborhood size of a sampling point affects its information representation (proportion of overlapping points) in the overlapping point cloud. If an over-large neighborhood is selected, the correlation of the corresponding points on two overlapping point clouds decrease dramatically, leading to a mismatch.</p><p>ICP <ref type="bibr" target="#b3">(Besl &amp; McKay, 1992</ref>) is a classic method to register two point clouds by iteratively finding correspondences and estimating pose transformation. Subsequently, several improvements <ref type="bibr" target="#b31">(Rusinkiewicz &amp; Levoy, 2001;</ref><ref type="bibr" target="#b43">Yang et al., 2015)</ref> are proposed to promote performance by searching in a larger pose space and changing objective functions. Meanwhile, hand-crafted descriptors such as FPFH <ref type="bibr" target="#b32">(Rusu et al., 2008;</ref><ref type="bibr" target="#b33">2009</ref>) and SHOT <ref type="bibr" target="#b37">(Tombari et al., 2010)</ref> are devised to discover the local invariance in point clouds and find the pairwise correspondences to assist transformation estimation. However, to design an efficient and robust descriptor that fully utilizes the point information is non-trivial.</p><p>Recently, a number of deep learning methods have been devised for point cloud registration, which can be roughly divided into two categories: end-to-end learning methods and feature-learning based methods. The end-to-end learning methods complete both point feature learning and transformation estimation in one forward pass, some <ref type="bibr" target="#b39">(Wang &amp; Solomon, 2019a;</ref><ref type="bibr" target="#b45">Yew &amp; Lee, 2020;</ref><ref type="bibr" target="#b10">Fu et al., 2021;</ref><ref type="bibr" target="#b5">Choy et al., 2020;</ref><ref type="bibr" target="#b20">Lee et al., 2021)</ref> of which rely on correspondences establishment for subsequent Procrustes analysis, while the others pay more attention to global features between the point clouds <ref type="bibr" target="#b35">Sarode et al., 2019;</ref><ref type="bibr" target="#b18">Huang et al., 2020)</ref>. In contrast, feature-learning based methods <ref type="bibr" target="#b7">(Deng et al., 2018b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b14">Gojcic et al., 2019;</ref><ref type="bibr" target="#b4">Choy et al., 2019;</ref><ref type="bibr" target="#b2">Bai et al., 2020;</ref><ref type="bibr" target="#b17">Huang et al., 2021;</ref><ref type="bibr" target="#b0">Ao et al., 2021;</ref><ref type="bibr">Yu et al., 2021)</ref> focus on the learning of discrim-arXiv:2201.12094v1 [cs.CV] 28 Jan 2022 inative point features, while the 3D rigid transformation is estimated by some robust pose estimators <ref type="bibr" target="#b9">(Fischler &amp; Bolles, 1981;</ref><ref type="bibr" target="#b42">Yang et al., 2020)</ref>.</p><p>Feature-learning based methods concentrate on the extraction of geometric and other useful information of the point clouds to compose discriminative features. PPFNet <ref type="bibr" target="#b6">(Deng et al., 2018a;</ref> introduces PPF <ref type="bibr" target="#b8">(Drost et al., 2010)</ref> for feature encoding. 3DSmoothNet <ref type="bibr" target="#b14">(Gojcic et al., 2019)</ref> adopts smoothed density representations for 3D data and utilizes hierarchical 3D convolutions to get deep features. FCGF <ref type="bibr" target="#b4">(Choy et al., 2019)</ref> utilizes a sparse 3D fullyconvolutional network with metric learning to extend receptive field and extract geometric features. SpinNet <ref type="bibr" target="#b0">(Ao et al., 2021)</ref> proposes spatial cyclindrical convolutions to convert local patches into rotation-invariant features. Furthermore, some strategies are explored to boost the feature-learning based registration performance. D3Feat <ref type="bibr" target="#b2">(Bai et al., 2020)</ref>, PREDATOR <ref type="bibr" target="#b17">(Huang et al., 2021)</ref> and CoFiNet <ref type="bibr">(Yu et al., 2021)</ref> leverage the previous feature extractors ( <ref type="bibr" target="#b36">Thomas et al., 2019)</ref> by explicitly selecting salient points, overlapping points or leveraging a coarse-to-fine strategy to build correspondences. However, the neighborhood size of the features has been rarely studied. An inappropriate neighborhood of the interest point will degenerate the feature encoding ( <ref type="figure">Figure 1</ref> gives an example), while the points with small neighborhoods and locating in indistinguishable surfaces usually mislead the correspondence determination.</p><p>In this work, we propose a novel Neighborhood-aware Geometric Encoding Network (NgeNet) for accurate point cloud registration. NgeNet utilizes a multi-scale architecture to explicitly generate point-wise features with multiple neighborhood sizes and a geometric-guided encoding module to maximally utilize the geometric information. Specifically, a voting mechanism is designed to select a proper neighborhood size for each point and reject the spurious features on the indistinguishable surfaces. Experiments on indoor, outdoor and object-centric synthetic datasets show that NgeNet outperforms the classic methods in point cloud registration. Particularly, our method gains ?6% and ?4% performance enhancement (92.9% and 71.9% in Registration Recall) on indoor benchmarks compared with the-state-of-art methods. Furthermore, the proposed techniques are model-agnostic, able to be easily migrated to other networks.</p><p>In summary, our main contributions are as follows:</p><p>? A multi-scale architecture coupled with geometricguided encoding to produce multi-level features encoded with geometric and semantic information. ? A multi-level consistency voting to select proper neighborhood for each point and reject the spurious ones. ? The proposed techniques in our method are modelagnostic, able to be easily migrated to other backbone architectures and improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Point cloud encoding methods</head><p>The early point descriptors (encoding) are mainly handcrafted. 3D shape context <ref type="bibr" target="#b37">(Tombari et al., 2010)</ref> use surface normal as a local reference frame and accumulate points in the support into discrete bins to compose a describable vector. Fast point feature histogram (FPFH) <ref type="bibr" target="#b32">(Rusu et al., 2008;</ref><ref type="bibr" target="#b33">2009</ref>) discovers point pair relationship inside the support and cast the statistical information into multi-dimensional histograms. The point pair feature (PPF) <ref type="bibr" target="#b8">(Drost et al., 2010)</ref> discovers the relative position and orientation of two oriented points to create a global model description.</p><p>Recently, point cloud encoding based on deep learning has been widely used. PointNet <ref type="bibr" target="#b27">(Qi et al., 2017a)</ref> and PointNet++ <ref type="bibr" target="#b28">(Qi et al., 2017b)</ref> utilize multi-layer perceptron (MLP) and feature aggregation for point encoding. DGCNN  propose EdgeConv to explicitly constructs a local graph for point encoding. KP-Conv <ref type="bibr" target="#b36">(Thomas et al., 2019)</ref> and FCGF <ref type="bibr" target="#b4">(Choy et al., 2019)</ref> utilize kernel-based convolution or sparse convolution for point encoding. Based on the inherent order invariance of point clouds, Transformer methods <ref type="bibr" target="#b38">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b15">Guo et al., 2021;</ref><ref type="bibr" target="#b50">Zhao et al., 2021)</ref> employs attention for point encoding. Concurrently, the methods combining both hand-crafted and learning-based techniques have also been explored. PPFNet <ref type="bibr" target="#b7">(Deng et al., 2018b;</ref><ref type="bibr">a)</ref>, RPMNet <ref type="bibr" target="#b45">(Yew &amp; Lee, 2020)</ref> and ROPNet <ref type="bibr" target="#b52">(Zhu et al., 2021)</ref> integrates handcrafted PPF into deep learning architecture for robust point cloud registration. Nevertheless, for real-world large-scale point clouds, due to the limited GPU memory and computational resources, integrating geometric encoding into feature-learning architecture in a one-forward-pass manner (not in a patch-wise manner) is still a huge challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-scale architectures</head><p>Inspired by the great success of the various multi-scale analysis methods in classic image tasks <ref type="bibr">(Yu &amp; Koltun, 2015;</ref><ref type="bibr">Liu et al., 2016;</ref><ref type="bibr" target="#b21">Lin et al., 2017;</ref><ref type="bibr" target="#b11">Gao et al., 2019)</ref>, multi-scale neural networks have been adopted in 3D point cloud tasks, including semantic segmentation <ref type="bibr" target="#b30">(Qiu et al., 2021)</ref>, object classification <ref type="bibr" target="#b29">(Qin et al., 2019)</ref> and detection <ref type="bibr" target="#b19">(Kuang et al., 2020)</ref>. Until recently, multi-scale architecture has been employed in point cloud registration. HRegNet <ref type="bibr" target="#b24">(Lu et al., 2021)</ref> is an end-to-end registration method that utilizes multi-scale feature maps for transformation estimation from coarse to fine. MS-SVConv <ref type="bibr" target="#b16">(Horache et al., 2021)</ref>  High Level  <ref type="figure">Figure 2</ref>. The architecture of NgeNet. Inputted the source and target point clouds, NgeNet utilizes siamese shared layers with a novel geometric-encoding interaction module applied to the superpoints to encode the information of the point clouds, and multi-scale parallel decoding layers to extract multi-level point-wise features for each point cloud. A learning-free consistency voting mechanism is designed to select the feature with the appropriate neighborhood for each point and reject the spurious features on indistinguishable surfaces.</p><formula xml:id="formula_0">X O X S X Y S Y O Y F x ? F x F x F Y ? F Y F Y F x ? F x F x F Y ? F Y F Y y y ? y<label>3D</label></formula><formula xml:id="formula_1">F x ? X ? F x ? F Y ? Y ? F Y ? 1 =||y ? y ||, 2 =||y ? y ? ||, 3 =||y ? y ? ||</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point cloud registration</head><p>Given the source point cloud</p><formula xml:id="formula_2">X = {x i ? R 3 } i=1,2,??? ,N and target point cloud Y = {y j ? R 3 } j=1,2,??? ,M ,</formula><p>where N and M are the point number in X and Y, the aim of point cloud registration is to find a transformation T ? SE(3) that best aligns X and Y, which can be formulated as follows:</p><formula xml:id="formula_3">arg min T 1 |?| (i,j)?? ||T (x i ) ? y j || 2 ,<label>(1)</label></formula><p>where ? denotes the correspondence set and | ? | denotes the cardinal number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neighborhood analysis</head><p>An encoder-decoder network is usually used in a featurelearning based registration method. Generally, inputted the source point cloud X ? R N ?3 and the target point cloud Y ? R M ?3 , the network outputs features F X ? R N ?C and F Y ? R M ?C , with each row corresponding to Cdimensional feature of a point 1 . There are two ways to affect the receptive field (neighborhood size) in point convolutions. On the one hand, consecutive convolutional layers can implicitly increase the neighborhood size. <ref type="figure" target="#fig_0">Figure 3a</ref> gives out such an example, where the neighborhood size of the selected point x i is extended when the convolutional layer goes deeper. On the other hand, strided convolutional layers with the neighborhood size doubly extended are usually used in the encoder-decoder network to analogy the image downsampling in point cloud analysis, as illustrated in <ref type="figure" target="#fig_0">Figure 3b</ref>. When the network goes deeper, the receptive field of the sampled points will extend and include more structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head><p>NgeNet is an encoder-decoder network ( <ref type="figure">Figure 2</ref>). The basic convolution block in the encoder consists of a residual-style KPConv / strided KPConv layer, an instance norm layer and a Leaky ReLU layer (k=0.1). The upsampling block in the decoder adopts nearest searching for the feature interpolating. The unary block consists of a linear (MLP) layer, an instance norm layer and a Leaky ReLU layer (k=0.1), while the last unary block just consists of a linear layer.</p><p>NgeNet takes the source point cloud X and its initial descriptor F X , the target point cloud Y and its initial descriptor F Y as input (F X and F Y are initialized to the matrix of ones). A siamese multi-scale backbone (subsection 4.2.1) with geometric-guided encoding (subsection 4.2.2) and information cross interaction (subsection 4.2.3) is utilized to process the input data, and outputs each point cloud's corresponding high-, middle-and low-level</p><formula xml:id="formula_4">features (F h X , F m X , F l X and F h Y , F m Y , F l Y )</formula><p>, overlap scores (O X and O Y ) and saliency scores (S X and S Y ). The specified loss function is introduced for the network training (subsection 4.3). Here, each row vector of the feature matrices represents the Cdimensional feature of a point in the point cloud. Finally, a learning-free consistent voting mechanism (subsection 4.4) is proposed to select the most discriminative feature (with proper neighborhood) for each point or reject the points on the indistinguishable smooth surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Component</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">SIAMESE MULTI-SCALE BACKBONE</head><p>The siamese multi-scale backbone processes the inputted point cloud X and Y with shared weights. Here, without loss of generality, we utilize X's part as an example to deliberate the detailed implementation of the backbone.</p><p>Shared Encoder. As shown in <ref type="figure">Figure 2</ref>, the encoder adopts KPConv <ref type="bibr" target="#b36">(Thomas et al., 2019)</ref> as the basic unit. Specifically, 3 strided KPConv blocks are applied on the subsampled point clouds to extend the neighborhood in convolutions. The feature maps before each strided KPConv block are saved (denoted as F 1 X , F 2 X , F 3 X ) for the decoder to generate multi-scale features. Here, it should be noticed that the perception range of the neighborhood points for each point feature extended from F 1 X to F 3 X . After the encoder, superpoints X and its feature F en X ? R N ?Den are obtained. Parallel Decoder. The decoder accepts F 1 X , F 2 X , F 3 X and F inter X (see subsection 4.2.3) as input, and outputs the high-, middle-and low-level features of the points in X, i.e.,</p><formula xml:id="formula_5">F h X , F m X , F l X .</formula><p>For the convenience of further discussion, a basic operation is predefined as:</p><formula xml:id="formula_6">?(F 1 , F 2 , g) = cat[Up(g(F 2 )), F 1 ],<label>(2)</label></formula><p>where F 1 and F 2 are the inputted features, g is a function representing MLP or Identity layer, cat[?, ?] is the concatenation operation and Up(?) is the nearest upsampling. Then, the F l X , F m X and F h X are calculated as follows:</p><formula xml:id="formula_7">F l X =MLP 2 (?(F 1 X , F 2 X , MLP 1 )), F m X =MLP 5 (?(F 1 X , ?(F 2 X , F 3 X , MLP 3 ), MLP 4 )), F h X =MLP 8 (?(F 1 X , ?(F 2 X , ?(F 3 X , F inter X , Identity), MLP 6 ), MLP 7 )</formula><p>).</p><p>( <ref type="formula">3)</ref> The details are shown in <ref type="figure">Figure 2</ref>. Additionally, the overlap scores O X and saliency scores S X are calculated along with F h X , to provide the probability for each point in salient overlapping point sampling. However, different from PREDA-TOR <ref type="bibr" target="#b17">(Huang et al., 2021</ref>) that focuses on the point sampling (i.e overlap and saliency scores), NgeNet pay more attention to the encoding of point features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Cross</head><p>Interaction </p><formula xml:id="formula_8">F x ? ? ? ? ? F Y ? ? ? ? ? ? ? ? ? 2 ? ? 4 ? ? ? ? 3 ? ? k ? 10 ? ? ? ? 2 ? ? (a) (b) ? ? F x ? ? ? ? ? F Y ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">GEOMETRIC-GUIDED ENCODING</head><p>Geometric-guided encoding (GGE) module takes the superpoints and the corresponding latent features as input, and outputs geometry enhanced features <ref type="figure" target="#fig_2">(Figure 4a</ref>).</p><p>Normal vectors smoothing. Normal vectors are used to enhance the geometric encoding <ref type="bibr" target="#b32">(Rusu et al., 2008;</ref><ref type="bibr" target="#b33">2009</ref>). Specifically, a smoothing strategy is proposed here to ensure the correct geometric presentation of the normal vectors in our GGE. Firstly, the subsampled superpoints X are mapped back to the original X. Then, the normals N X for X are calculated with Open3D's classic API <ref type="bibr" target="#b51">(Zhou et al., 2018)</ref>. Finally, the normal vector of a certain point in X is calculated by averaging the normals of its surrounding points in X. That is, for point x i ? X , the smoothed normal vector N x is calculated as</p><formula xml:id="formula_9">N x i = 1 |J N i | xj ?J N i N xj ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_10">J N i = x j | ||x j ? x i || &lt; r N , x j ? X and r N is the radius of x i 's neighborhood.</formula><p>Geometric encoding. Inspired by <ref type="bibr" target="#b45">Yew &amp; Lee (2020)</ref>, the geometric features G x i for point x i ? X is constructed with PPF <ref type="bibr" target="#b8">(Drost et al., 2010)</ref>, i.e.,</p><formula xml:id="formula_11">PPF(x i , x j ) =(?(x j ? x i , N x i ), ?(x j ? x i , N x j ), ?(N x i , N x j ), ||x i ? x j || 2 ), G x j =f 1 (x i , x j ? x i , PPF(x i , x j )), G x i = max{G x j | x j ? J G i }.<label>(5)</label></formula><p>where ?(?, ?) ? [0, ?] denotes the angle between the two vectors, f 1 is implemented by PointNet <ref type="bibr" target="#b27">(Qi et al., 2017a)</ref>,</p><formula xml:id="formula_12">J G i = x j | ||x j ? x i || &lt; r G , r G is the radius of x i 's neighbor- hood, and max(?) means channel-wise max-pooling.</formula><p>Semantic encoding. Inspired by <ref type="bibr" target="#b17">Huang et al. (2021)</ref>, the dense connected graph network is introduced in GGE module to enhance the semantic features F en X (outputted from the encoder) as H X , which is constructed as follows,</p><formula xml:id="formula_13">H 0 X = F en X , H i X = GNN i (H i?1 X ), H X = MLP(cat(H 1 X , H 2 X , H 3 X )).<label>(6)</label></formula><p>Then we explicitly fuse the geometric features G X with the semantic features H X to generate our GGE features:</p><formula xml:id="formula_14">F gge X = f 2 (G X , H X ),<label>(7)</label></formula><p>where f 2 is implemented by PointNet, F gge X ? R N ?D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">INFORMATION CROSS INTERACTION</head><p>As shown in <ref type="figure">Figure 2</ref>, the information cross interaction module bridges the encoder and decoder, and further translates the F en</p><formula xml:id="formula_15">X and F en Y to information interacted features F inter X , F inter Y</formula><p>, which consists of two shared GGE module and one cross attention module <ref type="figure" target="#fig_2">(Figure 4b)</ref>.</p><p>The cross attention is a multi-head attention with residual connection. For X and F gge X , the information interacted feature F inter X is calculated as</p><formula xml:id="formula_16">head i = softmax( Q i ? K T i ? D i ) ? V i , F ca X = MLP 2 (F gge X + MLP 1 (cat(head 1 , ..., head k ))), F inter X = GGE(X , F ca X )<label>(8)</label></formula><p>where</p><formula xml:id="formula_17">Q i = F gge X ? W Q i , K i = F gge Y ? W K i , V i = F gge Y ? W V i (W Q i ? R D?Di , W K i ? R D?Di , W V i ? R D?Di are learnable weights matrices), k is the number of heads, D i = D/k, and GGE(?, ?) is the GGE module defined in subsection 4.2.2. F inter Y</formula><p>is obtained in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature loss. Circle loss for</head><formula xml:id="formula_18">feature F h X , F m X , F l X and F h Y , F m Y , F l Y<label>are</label></formula><p>calculated with the randomly sampled corresponding pairs (X c and Y c ) from X and Y. For example, the loss L h X (F) for F h X is defined as</p><formula xml:id="formula_19">L h X (F) = 1 L L i=1 log[1 + yj ?Pi exp(?? p ij (D h ij ? ?p)) ? y k ?Ni exp(?? n ik (?n ? D h ik ))],<label>(9)</label></formula><p>where L is the number of sampled corresponding pairs, </p><formula xml:id="formula_20">D h ij = ||F h xi ? F h yj<label>||</label></formula><formula xml:id="formula_21">(L h X (F) + L h Y (F))</formula><p>, and so are the L m (F) and L l (F). Overlap and saliency loss. Binary cross entropy loss is adopted for O X and S X , i.e.,</p><formula xml:id="formula_22">L X (O) = ? xi?X w O i [O xi log O xi + (1 ? O xi ) log(1 ? O xi )],<label>(10)</label></formula><p>Algorithm 1 Consistent Voting Mechanism</p><formula xml:id="formula_23">Input: X and F h X , F m X , F l X ; Y and F h Y , F m Y , F l Y . Parameter: d: parameter related to points density Output: F f in X for X and F f in Y for Y. 1: for all x i ? X do 2: Find y l = N N F l (x i , Y), y m = N N F m (x i , Y), y h = N N F h (x i , Y) 3: Compute d i1 = ||y l ? y m || 2 , d i2 = ||y l ? y h || 2 , d i3 = ||y m ? y h || 2 4: if d i2 &lt; d or d i3 &lt; d then 5: F f in xi = F h xi , F f in y h = F h y h 6: else if d i1 &lt; d then 7: F f in xi = F m xi , F f in y m = F m y m 8:</formula><p>else 9: </p><formula xml:id="formula_24">F f in xi = null 10: end if 11: end for L X (S) = ? xi?X w S i [S xi log S xi + (1 ? S xi ) log(1 ? S xi )],<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Consistent voting</head><p>Generally, an overlapping point with discriminative geometric structures is able to correctly match its corresponding point in another point cloud by features of different levels' neighborhood. <ref type="figure" target="#fig_5">Figure 5</ref> shows such an example, where x 1 's candidate points obtained from low-, middle-, high-level's feature matching are consistent, and x 2 's consistency exists in both low and middle feature levels, while x 3 get three different candidates with different level's feature matching.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>NgeNet was evaluated on 3DMatch <ref type="bibr" target="#b48">(Zeng et al., 2017)</ref>, 3DLoMatch <ref type="bibr" target="#b17">(Huang et al., 2021)</ref>, Odometry KITTI <ref type="bibr" target="#b13">(Geiger et al., 2012)</ref> and MVP-RG <ref type="bibr" target="#b25">(Pan et al., 2021)</ref> datasets, resulting in a detailed quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>NgeNet was implemented in PyTorch <ref type="bibr" target="#b26">(Paszke et al., 2019)</ref>, with knn parameter k of PPF set to 64, the dimension D of F gge set to 256 and the dimension C of F f in set to 32. Parameter d in voting was set to 2? voxel size. All experiments were conducted on a single RTX 3090 GPU. SGD optimizer with 0.98 momentum was used for network training, with initial learning rate set to 0.01 and the number of sampled point pairs L set to 256, 512 and 768 for 3DMatch (3DLoMatch), Odometry KITTI and MVP-RG, respectively. The scale factor ?, positive and negative margin ?p, ?n in loss function were set to 16, 0.1 and 1.4. For 3DMatch (3DLoMatch), Odometry KITTI and MVP-RG, the model was trained by 40, 150 and 200 epochs, respectively. Exponential decay was set to 0.95 after every epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">3DMatch and 3DLoMatch</head><p>3DMatch <ref type="bibr" target="#b48">(Zeng et al., 2017)</ref> and 3DLoMatch <ref type="bibr" target="#b17">(Huang et al., 2021)</ref> are two widely used indoor datasets that contain &gt;30% and 10 ? 30% partial overlapping scene pairs, respectively. In consistent with <ref type="bibr" target="#b17">Huang et al. (2021)</ref>, 46 scenes (20642 scan pairs) were used for training and 8 scenes (1331 scan pairs) were used for validation. 8 scenes (1279 and 1726 non-consecutive pairs for 3DMatch and 3DLoMatch) were used for testing. The Inlier Ratio (IR), Feature Matching Recall (FMR) and Registration Recall (RR) for each dataset were reported (The metric details are given in A.1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">PERFORMANCE</head><p>NgeNet was compared with the SOTA method 3DSN <ref type="bibr" target="#b14">(Gojcic et al., 2019)</ref>, FCGF <ref type="bibr" target="#b4">(Choy et al., 2019)</ref>, D3Feat <ref type="bibr" target="#b2">(Bai et al., 2020)</ref>, PREDATOR <ref type="bibr" target="#b17">(Huang et al., 2021)</ref> and CoFiNet <ref type="bibr">(Yu et al., 2021)</ref> on 3DMatch and 3DLoMatch datasets, whose results are summarized in <ref type="table" target="#tab_3">Table 1</ref>.  Furthermore, though NgeNet achieves a great performance improvement, its parameter number (8.86M) remains moderate among all the methods. Several difficult cases and failed cases can be found in <ref type="figure">Figure A2</ref> and A3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">NEIGHBORHOOD-AWARE FEATURES</head><p>Influence of voting. <ref type="figure" target="#fig_6">Figure 6</ref> shows an example of how consistent voting works. Consistent voting assigns the most discriminative feature of the high-, middle-, and low-level features to each point, which results in the pruning of massive mismatched correspondence and reserving the correct ones (green and blue lines).</p><p>Matched points distribution. <ref type="figure" target="#fig_7">Figure 7</ref> gives an example of the correctly matched points distribution on the overlap boundary and plane regions. As depicted in <ref type="figure" target="#fig_7">Figure 7a</ref>, the number of matched points with high-level features is less than that with middle-and low-level features on the boundary of overlap regions, indicating that an over-large neighborhood cannot produce discriminative features. Besides, as depicted in <ref type="figure" target="#fig_7">Figure 7b</ref>, there are few matched points on the plane surface, no matter the level of features. The phenomenon indicates that points on the plane surfaces are indistinguishable, most of which will be discarded in multiscale feature matching and consistent voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-level Middle-level</head><p>Low-level  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">ABLATION STUDY</head><p>Multi-scale architecture (ms), consistent voting (voting) and GGE modules are the three cornerstones of NgeNet. Here, ablation study was conducted on 3DMatch dataset under 5000 sampling points, where the NgeNet without ms, voting and GGE modules was considered as the base model. <ref type="table" target="#tab_5">Table  2</ref> summarizes the results of the ablation study, from which we can draw the conclusion that ms+voting promotes RR by 2.2%, GGE promotes RR by 1.9% and ms+voting+GGE promotes RR by 3.6%. To our surprise, training the network with multi-levels but only testing with high-level features (base+ms) also improves the RR by 1.0%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">FEATURES COMPARISON</head><p>The features outputted by NgeNet were compared with the ones of PREDATOR on registration task under different sampling strategies, whose experimental results are summarized in <ref type="table" target="#tab_6">Table 3</ref>. Here, the points were either randomly sampled (random) or sampled based on overlap and saliency score (prob.). As shown in <ref type="table" target="#tab_6">Table 3</ref>, NgeNet exceeds PREDATOR in IR, FMR and RR under both random and prob. sampling strategies, demonstrating that the features of Ngenet are more distinctive for registration. The FMR curve shown in <ref type="figure" target="#fig_8">Figure 8</ref>  consistently better than the other methods under different thresholds ? 1 and ? 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.">FLEXIBILITY AND EFFECTIVENESS</head><p>The flexibility and effectiveness of the proposed multi-scale architecture, consistent voting and GGE modules were validated by transferring them to other encoder-decoder architectures, such as FCGF <ref type="bibr" target="#b4">(Choy et al., 2019)</ref>, D3Feat <ref type="bibr" target="#b2">(Bai et al., 2020)</ref> and PREDATOR <ref type="bibr" target="#b17">(Huang et al., 2021)</ref>. An experiment has been conducted on the 3DMatch, and the results are shown in <ref type="table" target="#tab_7">Table 4</ref>. Here, it can be found that NgeNet's modules greatly improve the baseline methods' performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Odometry KITTI</head><p>Odometry KITTI <ref type="bibr" target="#b13">(Geiger et al., 2012)</ref> is an outdoor LIDAR dataset for autonomous driving. In consistent with <ref type="bibr" target="#b17">Huang et al. (2021)</ref>, the frame pair at most 10 meters were selected to form scan pairs. Then, the model was trained with sequence No.00-05 (1358 scan pairs), validated with sequence No.06-07 (180 scan pairs) and tested with sequence No.08-10 (555 pairs). Finally, the Relative Translation Error (RTE), Relative Rotation Error (RRE) and Registration Recall (RR) (see A.1.2) of the results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">PERFORMANCE</head><p>NgeNet was compared with the SOTA method 3DFeat-Net <ref type="bibr" target="#b44">(Yew &amp; Lee, 2018)</ref>, FCGF <ref type="bibr" target="#b4">(Choy et al., 2019)</ref>, D3Feat <ref type="bibr" target="#b2">(Bai et al., 2020)</ref>, PREDATOR <ref type="bibr" target="#b17">(Huang et al., 2021)</ref> and CoFiNet <ref type="bibr">(Yu et al., 2021)</ref>, whose results are summarized in <ref type="table" target="#tab_8">Table 5</ref>. Here, NgeNet achieve the best RR (99.8%), RTE  <ref type="bibr" target="#b17">(Huang et al., 2021)</ref>, and the recent end-to-end learning-based registration methods DCP <ref type="bibr" target="#b39">(Wang &amp; Solomon, 2019a)</ref>, RPM-Net <ref type="bibr" target="#b45">(Yew &amp; Lee, 2020)</ref>, RGM <ref type="bibr" target="#b10">(Fu et al., 2021)</ref> and GMCNet <ref type="bibr" target="#b25">(Pan et al., 2021)</ref>. For PREDATOR and NgeNet, both 768 correspondences were sampled for RANSAC. <ref type="table" target="#tab_9">Table 6</ref> summarizes the results, where NgeNet outperforms all the other methods. Particularly, NgeNet surpasses the end-to-end learning method in a remarkable margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed NgeNet, a multi-scale encoderdecoder network combined with geometric guided encoding and a consistent voting to produce neighborhood-aware discriminative features for accurate point cloud registration. Comprehensive experiments demonstrate the efficience and effectiveness of NgeNet. Especially, the proposed techniques in NgeNet can be transferred to other networks to expediently boost the baseline performance. In future, long-range geometric relationship and point sampling strategies will be further discussed and coupled with our neighborhood-aware features. Translation Error is also defined the same as that in Odometry KITTI dataset. Here we use L t to represent the translation error.</p><p>RMSE indicates the point-wise distance under the estimated and ground truth transformation:</p><formula xml:id="formula_25">L RM SE = 1 N N i=1 ||T * (x i ) ? T (x i )|| 2 , (A.7)</formula><p>where N denotes the number of points in source point cloud. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1. PPF MEMORY ANALYSIS</head><p>We compared the GPU memory consumption of PPF on the original points and the superpoints on 3DMatch train set. We first calculated the average and maximum points number of the original points and superpoints. Then, following PPFNet <ref type="bibr" target="#b7">(Deng et al., 2018b)</ref>, neighborhood points number k under the specific radius was set to 1024 for original points, while k was set to 64 in NgeNet for superpoints. The channels number for encoding PPF was marked as D p , where 64 and 256 were considered. The GPU memory consumption for the feature map (without considering gradient map or others) is shown in <ref type="table" target="#tab_3">Table A1</ref>. GPU memory consumption of PPF on superpoints is three orders of magnitude smaller than on original points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2. MULTI-LEVEL FEATURES COMPARISON</head><p>We conducted experiments to statistically analyze the distinctiveness of features across all three levels. Considered x i and its corresponding point y j , we found the nearest corresponding points y l , y m , y h by calculating Euclidean distance in the relevant feature space respectively. We calculated</p><formula xml:id="formula_26">d j l = ||y l ? y j || 2 , d jm = ||y m ? y j || 2 , d j h = ||y h ? y j || 2 . (A.8) If d jq &lt; 2 ? v,</formula><p>we regarded that x i belongs to correct matching points in level q(q ? {l, m, h}), where v denotes initial voxel size for X and Y. And we regarded x i belongs to best matching points in level q if d jq is the minimal value among d j l , d jm and d j h . We counted the average number of correct matching points and best matching points in different levels separately. The statistical histogram on 3DMatch and 3DLoMatch are illustrated in <ref type="figure">Figure A1</ref>. 5000 points were sampled for feature matching based on overlap and saliency scores, where 3509 and 2218 points were overlapping point averaged on 3DMatch and 3DLoMatch.</p><p>By comparing the blue bar chart in <ref type="figure">Figure A1</ref>(a), we conclude that high level features are better than middle and low level features for matching, while middle and low level features can still be used to help finding the correct corresponding points for considerable number of points. By comparing the orange bar chart, we conclude that nearly half number of best matching points use high-level features to find the optimal candidate point. However, by using middle and low level features, there are still nearly the same number (1270 + 786) of points in X can get the best corresponding points in Y. Similar results are shown in <ref type="figure">Figure A1(b)</ref>. In short, both middle and low level features are useful for matching. We report Registration Recall (RR) on 3DMatch and 3DLoMatch in scene level, with additional evaluation metrics including Relative Rotation Error (RRE) and Relative Translation Error (RTE). The results are shown in <ref type="table" target="#tab_5">Table A2</ref>. Successfully registered pairs in each scene are included to calculate RRE and RTE. We achieve the highest RR in most scenes and the second highest in few scenes. Despite significant improvement has been made in RR, which indicates taking more difficult pairs into account, there is a slight growth in RRE and RTE. It shows NgeNet is robust and accurate for registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4. PAIR RECALL ON 3DMATCH</head><p>Scene RR is our previous reported RR which calculates the average registration recall across scenes. However, the unbalanced sample numbers in each scene may be unfair for scene RR metrics. Here, we additionally report pair RR on 3DMatch, which calculates the average RR among all 1279 pairs without taking scene category into consideration. The results are summarized in <ref type="table" target="#tab_6">Table A3</ref>. Both scene RR and pair RR are improved with the proposed modules. The ground truth labels S X ? R N , S Y ? R M for saliency loss are obtained on the fly with feature matching. For S X , feature matching is conducted based on F h X and F h Y as follows:</p><formula xml:id="formula_27">M = F h X ? (F h Y ) T . (A.9)</formula><p>For point x i , the corresponding point index in Y is obtained by j = arg max M[i, :]. If (x i , y j ) belongs to correspondences set under the ground truth transformation, S Xi is set to 1, otherwise set to 0.</p><p>It's noticed that M ? R N ?M (in 3DMatch and Odemetry KITTI) is huge matrix where N and M are about in tens of thousands level. We ablated the saliency loss on 3DMatch dataset, and the results 2 can be seen in <ref type="table" target="#tab_7">Table A4</ref>. Saliency loss slightly improves the performance but brings huge additional memory consumption. Removing saliency loss is an option when we train point clouds with more points in the future and slight performance degradation is allowed. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the contributing neighborhood points in the point convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Geometric-guided encoding &amp; information interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2 , ?p and ?n denote positive and negative margin,? p ij = [D h ij ? ?p] + , ? n ik = [?n ? D h ik ] + ,P i and N i denote the matched and unmatched points in Y c for point x i ? X c , and ? is a scale factor. The loss L h Y (F) is defined in the same way. Then, let L h (F) = 1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where w O i and w S i are weighting factors for samples balance, O xi and S xi ? {0, 1} are the ground truth binary overlap score and saliency score. O xi is set to 1 if the distance between T * (x i ) and N N (T * (x i ), Y) is below the threshold, where T * is the ground truth transformation and N N (x, Y) operator reports the nearest neighbor of x in Y. S xi is set to 1 if the ground truth candidate point is matched for x i based on feature matching (see A.2.5). L Y (O) and L Y (S) are defined in the same way. Then, let L(O) = 1 2 (L X (O) + L Y (O)), L(S) = 1 2 (L X (S) + L Y (S)). Combined loss. The complete loss function of NgeNet is L = L h (F) + L m (F) + L l (F) + L(O) + L(S). (12)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Multi-level consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Correspondences constructed by multi-scale feature matching and consistent voting. 250 correspondences are randomly selected for convenience of visualization. Here, the correct correspondences obtained based on high-, middle-and low-level features are marked in green, blue and yellow, respectively. The incorrect correspondences are marked in red. The correspondences constructed from the voted neighborhood-aware features contain 11 points with middle-level features and 62 points with high-level features, having preserved most of the correct ones and rejected most of the specious features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Matched points distribution on boundary and plane surface with multi-level features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>demonstrates that the features of NgeNet are Feature matching recall in relation to inlier distance threshold ?1 (left) and inlier ratio threshold ?2 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>3DMatch (3509 / 5000 overlapping points) (b) 3DLoMatch (2218 / 5000 overlapping points) Figure A1. Multi-level features comparison on 3DMatch and 3DLoMatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>DeepGlint, Beijing, China; 2 School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; 3 Research Center for Mathematics and Interdisciplinary Sciences, Shandong University, Shandong, China. Correspondence to: Renmin Han &lt;hanrenmin@sdu.edu.cn&gt;.</figDesc><table><row><cell>OR: 0.7749</cell><cell cols="3">appropriate neighborhood</cell><cell>OR: 0.7544</cell></row><row><cell>Target</cell><cell></cell><cell></cell><cell></cell><cell>Source</cell></row><row><cell>OR: 0.4273</cell><cell cols="3">over-large neighborhood</cell><cell>OR: 0.4312</cell></row><row><cell>Sampled Point</cell><cell>Source</cell><cell>Target</cell><cell>Overlap Point</cell><cell>OR: Overlap Ratio</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Results on 3DMatch and 3DLoMatch under different numbers of sampling points. Taking x i as an example, consistent voting helps x i to select the proper feature among {F h xi , F m xi , F l xi , null} as its final feature F f in xi , where null means x i is a bad point and should not be used for correspondence building.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3DMatch</cell><cell></cell><cell cols="2">3DLoMatch</cell><cell></cell><cell></cell></row><row><cell># Samples</cell><cell cols="3">5000 2500 1000 500</cell><cell cols="4">250 5000 2500 1000 500</cell><cell cols="2">250 # Params ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Registration Recall (%) ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DSN</cell><cell>78.4</cell><cell>76.2</cell><cell cols="2">71.4 67.6 50.8 33.0</cell><cell>29.0</cell><cell cols="3">23.3 17.0 11.0</cell><cell>10.2M</cell></row><row><cell>FCGF</cell><cell>85.1</cell><cell>84.7</cell><cell cols="2">83.3 81.6 71.4 40.1</cell><cell>41.7</cell><cell cols="3">38.2 35.4 26.8</cell><cell>8.76M</cell></row><row><cell>D3Feat</cell><cell>81.6</cell><cell>84.5</cell><cell cols="2">83.4 82.4 77.9 37.2</cell><cell>42.7</cell><cell cols="3">46.9 43.8 39.1</cell><cell>27.3M</cell></row><row><cell>PREDATOR</cell><cell>89.0</cell><cell>89.9</cell><cell cols="2">90.6 88.5 86.6 59.8</cell><cell>61.2</cell><cell cols="3">62.4 60.8 58.1</cell><cell>7.43M</cell></row><row><cell>CoFiNet</cell><cell>89.3</cell><cell>88.9</cell><cell cols="2">88.4 87.4 87.0 67.5</cell><cell>66.2</cell><cell cols="3">64.2 63.1 61.0</cell><cell>5.48M</cell></row><row><cell cols="2">NgeNet (ours) 92.9</cell><cell>92.1</cell><cell cols="2">92.3 90.3 88.7 71.9</cell><cell>71.4</cell><cell cols="3">70.6 67.5 61.9</cell><cell>8.86M</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Feature Matching Recall (%) ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DSN</cell><cell>95.0</cell><cell>94.3</cell><cell cols="2">92.9 90.1 82.9 63.6</cell><cell>61.7</cell><cell cols="3">53.6 45.2 34.2</cell><cell>10.2M</cell></row><row><cell>FCGF</cell><cell>97.4</cell><cell>97.3</cell><cell cols="2">97.0 96.7 96.6 76.6</cell><cell>75.4</cell><cell cols="3">74.2 71.7 67.3</cell><cell>8.76M</cell></row><row><cell>D3Feat</cell><cell>95.6</cell><cell>95.4</cell><cell cols="2">94.5 94.1 93.1 67.3</cell><cell>66.7</cell><cell cols="3">67.0 66.7 66.5</cell><cell>27.3M</cell></row><row><cell>PREDATOR</cell><cell>96.6</cell><cell>96.6</cell><cell cols="2">96.5 96.3 96.5 78.6</cell><cell>77.4</cell><cell cols="3">76.3 75.7 75.3</cell><cell>7.43M</cell></row><row><cell>CoFiNet</cell><cell>98.1</cell><cell>98.3</cell><cell cols="2">98.1 98.2 98.3 83.1</cell><cell>83.5</cell><cell cols="3">83.3 83.1 82.6</cell><cell>5.48M</cell></row><row><cell cols="2">NgeNet (ours) 98.2</cell><cell>98.4</cell><cell cols="2">98.1 98.6 98.2 84.5</cell><cell>84.8</cell><cell cols="3">85.0 85.3 83.4</cell><cell>8.86M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Inlier Ratio (%) ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DSN</cell><cell>36.0</cell><cell>32.5</cell><cell cols="2">26.4 21.5 16.4 11.4</cell><cell>10.1</cell><cell>8.0</cell><cell>6.4</cell><cell>4.8</cell><cell>10.2M</cell></row><row><cell>FCGF</cell><cell>56.8</cell><cell>54.1</cell><cell cols="2">48.7 42.5 34.1 21.4</cell><cell>20.0</cell><cell cols="3">17.2 14.8 11.6</cell><cell>8.76M</cell></row><row><cell>D3Feat</cell><cell>39.0</cell><cell>38.8</cell><cell cols="2">40.4 41.5 41.8 13.2</cell><cell>13.1</cell><cell cols="3">14.0 14.6 15.0</cell><cell>27.3M</cell></row><row><cell>PREDATOR</cell><cell>58.0</cell><cell>58.4</cell><cell cols="2">57.1 54.1 49.3 26.7</cell><cell>28.1</cell><cell cols="3">28.3 27.5 25.8</cell><cell>7.43M</cell></row><row><cell>CoFiNet</cell><cell>49.8</cell><cell>51.2</cell><cell cols="2">51.9 52.2 52.2 24.4</cell><cell>25.9</cell><cell cols="3">26.7 26.8 26.9</cell><cell>5.48M</cell></row><row><cell cols="2">NgeNet (ours) 63.1</cell><cell>63.5</cell><cell cols="2">61.5 57.6 51.1 31.8</cell><cell>33.3</cell><cell cols="3">33.5 31.9 29.2</cell><cell>8.86M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>their</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">multi-level features F h X , F m X , F l X and F h Y , F m Y , F l Y as in-put, and outputs the final feature F f in X for X and F f in Y for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Y.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Thus, a simple yet effective consistent voting algorithm is proposed to select the points with the most discriminative features, which is deliberated in Algorithm 1, where the N N F (x, Y) operator reports the nearest neighbor of x in Y in feature F's space. The consistent voting process ac- cepts the target and source point cloud X and Y, and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 ,</head><label>1</label><figDesc>NgeNet outperforms all the other models on both two datasets. NgeNet achieved 92.9% and 71.9% RR on 3DMatch and 3DLoMatch, exceeding PREDATOR (90.6% and 62.4% RR on 3DMatch) by 5.9% and CoFiNet (89.3% and 67.5% RR on 3DLoMatch) by 4%. Moreover, NgeNet performs the best under various number of sampling points, demonstrating the robustness of the model.</figDesc><table><row><cell>True: 137</cell><cell>True: 98</cell><cell>True: 56</cell><cell>True: 73(11)</cell></row><row><cell>False: 113</cell><cell>False: 152</cell><cell>False: 194</cell><cell>False: 19</cell></row><row><cell>IR: 0.55</cell><cell>IR: 0.39</cell><cell>IR: 0.22</cell><cell>IR: 0.79</cell></row><row><cell></cell><cell></cell><cell>Voting</cell><cell></cell></row><row><cell>High-level</cell><cell>Middle-level</cell><cell>Low-level</cell><cell></cell></row><row><cell></cell><cell>Multi-scale feature matching</cell><cell>Final result</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on 3DMatch.</figDesc><table><row><cell cols="3">base ms voting GGE IR (%) ? FMR (%) ? RR (%) ?</cell></row><row><cell>50.3</cell><cell>96.3</cell><cell>89.3</cell></row><row><cell>56.6</cell><cell>97.2</cell><cell>90.3</cell></row><row><cell>60.8</cell><cell>98.1</cell><cell>91.5</cell></row><row><cell>54.4</cell><cell>97.4</cell><cell>91.2</cell></row><row><cell>63.1</cell><cell>98.2</cell><cell>92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Feature comparison with PREDATOR on 3DMatch.</figDesc><table><row><cell>Model</cell><cell cols="4">sampling IR (%) ? FMR (%) ? RR (%) ?</cell></row><row><cell>PREDATOR</cell><cell>random</cell><cell>51.6</cell><cell>96.5</cell><cell>86.0</cell></row><row><cell>NgeNet (ours)</cell><cell>random</cell><cell>54.2</cell><cell>97.8</cell><cell>90.1</cell></row><row><cell>PREDATOR</cell><cell>prob.</cell><cell>58.0</cell><cell>96.6</cell><cell>89.0</cell></row><row><cell>NgeNet (ours)</cell><cell>prob.</cell><cell>63.1</cell><cell>98.2</cell><cell>92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Results of transferring NgeNet's modules to other networks. The results reported in the published papers and the improvements are shown in brackets.</figDesc><table><row><cell>Model</cell><cell>IR (%) ?</cell><cell>FMR (%) ?</cell><cell>RR (%) ?</cell></row><row><cell>FCGF</cell><cell>46.3 (56.8)</cell><cell cols="2">94.9 (97.4) 85.0 (85.1)</cell></row><row><cell>FCGF + ms + voting</cell><cell>56.8 (+0.0)</cell><cell cols="2">97.8 (+0.4) 89.8 (+4.7)</cell></row><row><cell>FCGF + ms + voting + GGE</cell><cell>51.5 (-5.3)</cell><cell>95.9 (-1.5)</cell><cell>90.6 (+5.5)</cell></row><row><cell>D3Feat</cell><cell>42.9 (39.0)</cell><cell cols="2">95.9 (95.6) 83.2 (81.6)</cell></row><row><cell>D3Feat + ms + voting</cell><cell>48.9 (+9.9)</cell><cell cols="2">97.1 (+1.5) 87.0 (+5.4)</cell></row><row><cell>D3Feat + ms + voting + GGE</cell><cell cols="3">49.3 (+10.3) 97.2 (+1.6) 87.7 (+6.1)</cell></row><row><cell>PREDATOR</cell><cell>54.3 (58.0)</cell><cell cols="2">97.1 (96.6) 89.0 (89.0)</cell></row><row><cell>PREDATOR + ms + voting</cell><cell>60.0 (+2.0)</cell><cell cols="2">98.3 (+1.7) 89.7 (+0.7)</cell></row><row><cell>PREDATOR + ms + voting + GGE</cell><cell>62.5 (+4.5)</cell><cell cols="2">98.7 (+2.1) 90.7 (+1.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Results on Odometry KITTI dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">RTE (cm) ? RRE ( ? ) ? RR (%) ?</cell></row><row><cell>3DFeat-Net</cell><cell>25.9</cell><cell>0.57</cell><cell>96.0</cell></row><row><cell>FCGF</cell><cell>9.5</cell><cell>0.30</cell><cell>96.6</cell></row><row><cell>D3Feat</cell><cell>7.2</cell><cell>0.30</cell><cell>99.8</cell></row><row><cell>PREDATOR</cell><cell>6.8</cell><cell>0.27</cell><cell>99.8</cell></row><row><cell>CoFiNet</cell><cell>8.5</cell><cell>0.41</cell><cell>99.8</cell></row><row><cell>NgeNet (wo. GGF) (ours)</cell><cell>6.3</cell><cell>0.25</cell><cell>99.8</cell></row><row><cell>NgeNet (ours)</cell><cell>6.1</cell><cell>0.26</cell><cell>99.8</cell></row><row><cell cols="4">(6.1cm) and RRE (0.26 ? ). It should be noted that NgeNet</cell></row><row><cell cols="4">can achieve better RRE with 0.25 ? without GGE module.</cell></row><row><cell cols="4">Maybe normal vectors are not suitable for LIDAR dataset,</cell></row><row><cell cols="3">which has relative weak structure information.</cell><cell></cell></row><row><cell>5.4. MVP-RG</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MVP(multi-view partial)-RG dataset (Pan et al., 2021) is</cell></row><row><cell cols="4">constructed by 16 categories object-centric synthetic point</cell></row><row><cell cols="4">cloud. The density and unrestricted rotations ([0 ? , 360 ? ])</cell></row><row><cell cols="4">vary in different point cloud. Following Pan et al. (2021),</cell></row><row><cell cols="4">we report three metrics L R , L t , and L RM SE (see A.1.3).</cell></row><row><cell cols="4">We used the official training set (6400 pairs) for training and</cell></row><row><cell cols="3">used the official test set (1200 pairs) for testing.</cell><cell></cell></row><row><cell>5.4.1. PERFORMANCE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">NgeNet was compared with PREDATOR</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Results on MVP-RG.</figDesc><table><row><cell>Method</cell><cell>LR ( ? ) ?</cell><cell>Lt ?</cell><cell>LRMSE ?</cell></row><row><cell>DCP</cell><cell>30.73</cell><cell>0.273</cell><cell>0.634</cell></row><row><cell>RPM-Net</cell><cell>22.20</cell><cell>0.174</cell><cell>0.327</cell></row><row><cell>RGM</cell><cell>41.27</cell><cell>0.425</cell><cell>0.583</cell></row><row><cell>PREDAOTR</cell><cell>10. 58</cell><cell>0.067</cell><cell>0.125</cell></row><row><cell>GMCNet</cell><cell>16.57</cell><cell>0.174</cell><cell>0.246</cell></row><row><cell>NgeNet (ours)</cell><cell>7.99</cell><cell>0.048</cell><cell>0.093</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A1 .</head><label>A1</label><figDesc>GPU memory consumption of PPF</figDesc><table><row><cell>Points</cell><cell># points</cell><cell>k</cell><cell cols="2">D p Memory (MB)</cell></row><row><cell cols="4">original points 37001 (avg) 1024 64</cell><cell>9,250</cell></row><row><cell>superpoints</cell><cell>679 (avg)</cell><cell>64</cell><cell>64</cell><cell>10</cell></row><row><cell cols="4">original points 60000 (max) 1024 64</cell><cell>15,000</cell></row><row><cell>superpoints</cell><cell>1389 (max)</cell><cell>64</cell><cell>64</cell><cell>22</cell></row><row><cell cols="4">original points 37001 (avg) 1024 256</cell><cell>37001</cell></row><row><cell>superpoints</cell><cell>679 (avg)</cell><cell>64</cell><cell>256</cell><cell>40</cell></row><row><cell cols="4">original points 60000 (max) 1024 256</cell><cell>60,000</cell></row><row><cell>superpoints</cell><cell>1389 (max)</cell><cell>64</cell><cell>256</cell><cell>43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A2 .</head><label>A2</label><figDesc>Detailed results on the 3DMatch and 3DLoMatch datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3DMatch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3DLoMatch</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">Kitchen Home 1 Home 2 Hotel 1 Hotel 2 Hotel 3 Study MIT Lab Avg.</cell><cell cols="10">STD Kitchen Home 1 Home 2 Hotel 1 Hotel 2 Hotel 3 Study MIT Lab Avg.</cell><cell>STD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># Sample</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>449</cell><cell>106</cell><cell>159</cell><cell>182</cell><cell>78</cell><cell>26</cell><cell>234</cell><cell>45</cell><cell>160</cell><cell>128</cell><cell>524</cell><cell>283</cell><cell>222</cell><cell>210</cell><cell>138</cell><cell>42</cell><cell>237</cell><cell>70</cell><cell>216</cell><cell>140</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Relative Rotation Error (?) ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DSN</cell><cell>1.926</cell><cell>1.843</cell><cell>2.324</cell><cell>2.041</cell><cell>1.952</cell><cell>2.908</cell><cell>2.296</cell><cell>2.301</cell><cell cols="2">2.199 0.321</cell><cell>3.020</cell><cell>3.898</cell><cell>3.427</cell><cell>3.196</cell><cell>3.217</cell><cell>3.328</cell><cell>4.325</cell><cell>3.814</cell><cell cols="2">3.528 0.414</cell></row><row><cell>FCGF</cell><cell>1.767</cell><cell>1.849</cell><cell>2.210</cell><cell>1.867</cell><cell>1.667</cell><cell>2.417</cell><cell>2.024</cell><cell>1.792</cell><cell cols="2">1.949 0.236</cell><cell>2.904</cell><cell>3.229</cell><cell>3.277</cell><cell>2.768</cell><cell>2.801</cell><cell>2.822</cell><cell>3.372</cell><cell>4.006</cell><cell cols="2">3.147 0.394</cell></row><row><cell>D3Feat</cell><cell>2.016</cell><cell>2.029</cell><cell>2.425</cell><cell>1.990</cell><cell>1.967</cell><cell>2.400</cell><cell>2.346</cell><cell>2.115</cell><cell cols="2">2.161 0.183</cell><cell>3.226</cell><cell>3.492</cell><cell>3.373</cell><cell>3.330</cell><cell>3.165</cell><cell>2.972</cell><cell>3.708</cell><cell>3.619</cell><cell cols="2">3.361 0.227</cell></row><row><cell>PREDATOR</cell><cell>1.861</cell><cell>1.806</cell><cell>2.473</cell><cell>2.045</cell><cell>1.600</cell><cell>2.458</cell><cell>2.067</cell><cell>1.926</cell><cell cols="2">2.029 0.286</cell><cell>3.079</cell><cell>2.637</cell><cell>3.220</cell><cell>2.694</cell><cell>2.907</cell><cell>3.390</cell><cell>3.046</cell><cell>3.412</cell><cell cols="2">3.048 0.273</cell></row><row><cell>CoFiNet</cell><cell>1.986</cell><cell>1.777</cell><cell>2.213</cell><cell>1.982</cell><cell>1.820</cell><cell>1.709</cell><cell>2.575</cell><cell>1.956</cell><cell cols="2">2.002 0.261</cell><cell>3.097</cell><cell>2.946</cell><cell>3.488</cell><cell>2.750</cell><cell>3.009</cell><cell>3.462</cell><cell>4.123</cell><cell>3.293</cell><cell cols="2">3.271 0.401</cell></row><row><cell>NgeNet</cell><cell>1.679</cell><cell>1.699</cell><cell>2.166</cell><cell>1.501</cell><cell>1.665</cell><cell>2.371</cell><cell>2.145</cell><cell>1.869</cell><cell cols="2">2.115 0.286</cell><cell>2.507</cell><cell>2.635</cell><cell>3.278</cell><cell>2.492</cell><cell>2.601</cell><cell>2.755</cell><cell>3.204</cell><cell>3.050</cell><cell cols="2">3.264 0.296</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Relative Translation Error (m) ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DSN</cell><cell>0.059</cell><cell>0.070</cell><cell>0.079</cell><cell>0.065</cell><cell>0.074</cell><cell>0.062</cell><cell>0.093</cell><cell>0.065</cell><cell cols="2">0.071 0.010</cell><cell>0.082</cell><cell>0.098</cell><cell>0.096</cell><cell>0.101</cell><cell>0.080</cell><cell>0.089</cell><cell>0.158</cell><cell>0.120</cell><cell cols="2">0.103 0.024</cell></row><row><cell>FCGF</cell><cell>0.053</cell><cell>0.056</cell><cell>0.071</cell><cell>0.062</cell><cell>0.061</cell><cell>0.055</cell><cell>0.082</cell><cell>0.090</cell><cell cols="2">0.066 0.013</cell><cell>0.084</cell><cell>0.097</cell><cell>0.076</cell><cell>0.101</cell><cell>0.084</cell><cell>0.077</cell><cell>0.144</cell><cell>0.140</cell><cell cols="2">0.100 0.025</cell></row><row><cell>D3Feat</cell><cell>0.055</cell><cell>0.065</cell><cell>0.080</cell><cell>0.064</cell><cell>0.078</cell><cell>0.049</cell><cell>0.083</cell><cell>0.064</cell><cell cols="2">0.067 0.011</cell><cell>0.088</cell><cell>0.101</cell><cell>0.086</cell><cell>0.099</cell><cell>0.092</cell><cell>0.075</cell><cell>0.146</cell><cell>0.135</cell><cell cols="2">0.103 0.023</cell></row><row><cell>PREDATOR</cell><cell>0.048</cell><cell>0.055</cell><cell>0.070</cell><cell>0.073</cell><cell>0.060</cell><cell>0.065</cell><cell>0.080</cell><cell>0.063</cell><cell cols="2">0.064 0.010</cell><cell>0.081</cell><cell>0.080</cell><cell>0.084</cell><cell>0.099</cell><cell>0.096</cell><cell>0.077</cell><cell>0.101</cell><cell>0.130</cell><cell cols="2">0.093 0.016</cell></row><row><cell>CoFiNet</cell><cell>0.049</cell><cell>0.059</cell><cell>0.064</cell><cell>0.065</cell><cell>0.066</cell><cell>0.045</cell><cell>0.089</cell><cell>0.074</cell><cell cols="2">0.064 0.013</cell><cell>0.079</cell><cell>0.076</cell><cell>0.087</cell><cell>0.086</cell><cell>0.085</cell><cell>0.074</cell><cell>0.139</cell><cell>0.094</cell><cell cols="2">0.090 0.020</cell></row><row><cell>NgeNet</cell><cell>0.042</cell><cell>0.057</cell><cell>0.060</cell><cell>0.057</cell><cell>0.058</cell><cell>0.059</cell><cell>0.083</cell><cell>0.073</cell><cell cols="2">0.073 0.011</cell><cell>0.070</cell><cell>0.079</cell><cell>0.085</cell><cell>0.078</cell><cell>0.088</cell><cell>0.063</cell><cell>0.103</cell><cell>0.083</cell><cell cols="2">0.096 0.011</cell></row><row><cell cols="11">A.2.3. SCENE PERFORMANCE ON 3DMATCH AND 3DLOMATCH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A3 .</head><label>A3</label><figDesc>Ablation study on 3DMatch dataset. base ms voting GGE scene RR (%) ? pair RR (%) ?</figDesc><table><row><cell>89.3</cell><cell>91.2</cell></row><row><cell>90.3</cell><cell>92.2</cell></row><row><cell>91.5</cell><cell>93.1</cell></row><row><cell>91.2</cell><cell>92.8</cell></row><row><cell>92.9</cell><cell>94.1</cell></row><row><cell>A.2.5. SALIENCY LOSS</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A4 .</head><label>A4</label><figDesc>Performance on 3DMatch w/wo saliency loss under 5000 sampling points. Saliency loss IR (%) ? FMR (%) ? scene RR (%) ? pair RR (%) ? Memory (MB) ?</figDesc><table><row><cell>wo</cell><cell>62.4</cell><cell>98.4</cell><cell>92.4</cell><cell>93.6</cell><cell>3723</cell></row><row><cell>w</cell><cell>63.1</cell><cell>98.2</cell><cell>92.9</cell><cell>94.1</cell><cell>6646</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the convenience of following discussion, all the data objects are denoted by matrix but not set.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this supplementary material, we first introduce evaluation metrics in details in A.1. Then we provide additional results and analysis in A.2. Finally, we show qualitative registration visualizations on 3DMatch, 3DLoMatch, Odometry KITTI and MVP-RG in A.3, and some badcases in A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Evaluation metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1. 3DMATCH AND 3DLOMATCH</head><p>Inlier Ratio (IR) measures the proportion of correct correspondences among all the putative correspondences. Consider the putative correspondences (x i , y j ) ? C ij retrieved by mutually feature matching, inlier ratio counts the fraction of correct correspondences under the ground truth transformation T * :</p><p>[?] denotes the Iverson bracket and ? 1 =0.1m which is pre-defined during evaluation.</p><p>Feature Matching Recall (FMR) measures the percentage of the inlier ratio that is above a certain threshold (? 2 =5%), indicating the pair that is likely to be matched correctly by using some robust estimator such as RANSAC <ref type="bibr" target="#b9">(Fischler &amp; Bolles, 1981)</ref>:</p><p>Registration Recall (RR) indicates the proportion of successful aligned scan pairs, which directly evaluates the quality of the pairwise registration. Specifically, consider the ground truth correspondence set (x i , y j ) ? C * ij , it computes the root mean square error among C * ij under the predicted transformation T :</p><p>and calculates the fraction of alignments with RMSE &lt; 0.2m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2. ODOMETRY KITTI</head><p>Relative Translation Error (RTE) calculates the Euclidean distance between estimated transltion t and the ground truth translation t * :</p><p>Relative Rotation Error (RRE) measures the discrepancy between the estimated rotation R and the ground truth rotation matrix R * :</p><p>where trace( ? ) denotes the trace of the matrix.</p><p>Registration Recall (RR) measures the proportion of successful aligned scan pairs with both RRE and RTE below a certain threshold: RTE &lt; 2m,</p><p>It's noted that the final RTE and RRE are both averaged on the successful point cloud pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3. MVP-RG</head><p>Rotation Error is defined the same as that in Odometry KITTI dataset. To keep up with the symbols in MVP-RG dataset, we use L R instead of RRE to represent the rotation error.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a general surface descriptor for 3d point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spinnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11753" to="11762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointnetlk: Robust &amp; efficient point cloud registration using pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7163" to="7172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint learning of dense detection and description of 3d local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>D3feat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6359" to="6367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="606" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8958" to="8966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="602" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ppfnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust point cloud registration framework based on deep graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8893" to="8902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Res2net</surname></persName>
		</author>
		<title level="m">A new multi-scale backbone architecture. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense 3d reconstruction in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stereoscan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE intelligent vehicles symposium (IV)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="963" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5545" to="5554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pct</surname></persName>
		</author>
		<title level="m">Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">3d point cloud registration with multi-scale architecture and selfsupervised fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14533</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predator: Registration of 3d point clouds with low overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usvyatsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4267" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature-metric registration: A fast semi-supervised approach for robust point cloud registration without correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11366" to="11374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Voxel-fpn: Multi-scale voxel feature aggregation for 3d object detection from lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">704</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep hough voting for robust global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15994" to="16003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hregnet: A hierarchical network for large-scale outdoor lidar point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16014" to="16023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Robust partial-to-partial point cloud registration in a full range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15606</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<title level="m">Deep hierarchical feature learning on point sets in a metric space</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pointdan: A multi-scale 3d domain adaption network for point cloud representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02744</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic segmentation for real point cloud scenes via bilateral augmentation and adaptive fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1757" to="1767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient variants of the icp algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings third international conference on 3-D digital imaging and modeling</title>
		<meeting>third international conference on 3-D digital imaging and modeling</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE/RSJ international conference on intelligent robots and systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on robotics and automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simultaneous localisation and mapping at the level of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slam++</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1352" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sarode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pcrnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07906</idno>
		<title level="m">Point cloud registration network using pointnet encoding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unique shape context for 3d data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM workshop on 3D object retrieval</title>
		<meeting>the ACM workshop on 3D object retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3523" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Prnet: Self-supervised learning for partial-to-partial registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12240</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Teaser: Fast and certifiable point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlone</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="314" to="333" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Go-icp: A globally optimal solution to 3d icp point-set registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2241" to="2254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3dfeat-net: Weakly supervised local 3d features for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="607" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rpm-net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11824" to="11833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Geometric Encoding Network for Point Cloud Registration Yu, F. and Koltun, V. Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reliable coarse-to-fine correspondences for robust pointcloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cofinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual-lidar odometry and mapping: Low-drift, robust, and fast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2174" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Point</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transformer</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Open3d: A modern library for 3d data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Point cloud registration using representative overlapping points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?mez-Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02583</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Registration Recall (%) ?</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
