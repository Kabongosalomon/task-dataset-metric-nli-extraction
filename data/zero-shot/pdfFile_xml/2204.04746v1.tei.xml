<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CholecTriplet2021: A benchmark challenge for surgical action triplet recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinedu</forename><forename type="middle">Innocent</forename><surname>Nwoye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Alapatt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armine</forename><surname>Vardazaryan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Xia</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xia</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Lab for Medical Imaging and Digital Surgery</orgName>
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fucang</forename><surname>Jia</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Lab for Medical Imaging and Digital Surgery</orgName>
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derong</forename><surname>Yu</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Institue of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyan</forename><surname>Zheng</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Institue of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Duan</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">Argonne National Laboratory</orgName>
								<orgName type="institution">US h Digital Surgery, a Medtronic Company</orgName>
								<address>
									<addrLine>9700 S Cass Ave</addrLine>
									<postCode>60439</postCode>
									<settlement>Lemont, London</settlement>
									<region>IL</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Getty</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">Argonne National Laboratory</orgName>
								<orgName type="institution">US h Digital Surgery, a Medtronic Company</orgName>
								<address>
									<addrLine>9700 S Cass Ave</addrLine>
									<postCode>60439</postCode>
									<settlement>Lemont, London</settlement>
									<region>IL</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Sanchez-Matilla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Robu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Chen</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beerend</forename><surname>Gerats</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">Meander Medical Centre</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sista</forename><surname>Raviteja</surname></persName>
							<affiliation key="aff10">
								<orgName type="department">Indian Institute of Technology Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachana</forename><surname>Sathish</surname></persName>
							<affiliation key="aff10">
								<orgName type="department">Indian Institute of Technology Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Tao</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Institue of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Kondo</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Muroran Institute of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winnie</forename><surname>Pang</surname></persName>
							<affiliation key="aff12">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Ren</surname></persName>
							<affiliation key="aff18">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Ronald</forename><surname>Abbing</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">Meander Medical Centre</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Hasan Sarhan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bodenstedt</surname></persName>
							<affiliation key="aff13">
								<orgName type="department" key="dep1">Department for Translational Surgical Oncology</orgName>
								<orgName type="department" key="dep2">National Center for Tumor Diseases Partner Site Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithya</forename><surname>Bhasker</surname></persName>
							<affiliation key="aff13">
								<orgName type="department" key="dep1">Department for Translational Surgical Oncology</orgName>
								<orgName type="department" key="dep2">National Center for Tumor Diseases Partner Site Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Oliveira</surname></persName>
							<affiliation key="aff14">
								<orgName type="department">2Ai School of Technology</orgName>
								<orgName type="institution">IPCA</orgName>
								<address>
									<settlement>Barcelos</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff15">
								<orgName type="department" key="dep1">Life and Health Science Research Institute (ICVS)</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Braga</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff16">
								<orgName type="department" key="dep1">Algoritimi Center</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Guimeraes</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helena</forename><forename type="middle">R</forename><surname>Torres</surname></persName>
							<affiliation key="aff14">
								<orgName type="department">2Ai School of Technology</orgName>
								<orgName type="institution">IPCA</orgName>
								<address>
									<settlement>Barcelos</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff15">
								<orgName type="department" key="dep1">Life and Health Science Research Institute (ICVS)</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Braga</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff16">
								<orgName type="department" key="dep1">Algoritimi Center</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Guimeraes</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ling</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finn</forename><surname>Gaida</surname></persName>
							<affiliation key="aff21">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Czempiel</surname></persName>
							<affiliation key="aff21">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">L</forename><surname>Vila?a</surname></persName>
							<affiliation key="aff14">
								<orgName type="department">2Ai School of Technology</orgName>
								<orgName type="institution">IPCA</orgName>
								<address>
									<settlement>Barcelos</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morais</surname></persName>
							<affiliation key="aff14">
								<orgName type="department">2Ai School of Technology</orgName>
								<orgName type="institution">IPCA</orgName>
								<address>
									<settlement>Barcelos</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Fonseca</surname></persName>
							<affiliation key="aff16">
								<orgName type="department" key="dep1">Algoritimi Center</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Guimeraes</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruby</forename><forename type="middle">Mae</forename><surname>Egging</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">Meander Medical Centre</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inge</forename><forename type="middle">Nicole</forename><surname>Wijma</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">Meander Medical Centre</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibin</forename><surname>Bian</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Velmurugan</forename><surname>Balasubramanian</surname></persName>
							<affiliation key="aff10">
								<orgName type="department">Indian Institute of Technology Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debdoot</forename><surname>Sheet</surname></persName>
							<affiliation key="aff10">
								<orgName type="department">Indian Institute of Technology Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imanol</forename><surname>Luengo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbo</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Ding</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob-Anton</forename><surname>Aschenbrenner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Elini</forename><surname>Van Der Kar</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">Meander Medical Centre</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengya</forename><surname>Xu</surname></persName>
							<affiliation key="aff12">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mobarakol</forename><surname>Islam</surname></persName>
							<affiliation key="aff12">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalithkumar</forename><surname>Seenivasan</surname></persName>
							<affiliation key="aff12">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Jenke</surname></persName>
							<affiliation key="aff13">
								<orgName type="department" key="dep1">Department for Translational Surgical Oncology</orgName>
								<orgName type="department" key="dep2">National Center for Tumor Diseases Partner Site Dresden</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
							<affiliation key="aff19">
								<orgName type="department">Wellcome/EPSRC Center for Interventional and Surgical Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Mutter</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff21">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Mascagni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff20">
								<orgName type="institution">Fondazione Policlinico Universitario Agostino Gemelli IRCCS</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Seeliger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff21">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristians</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff21">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename><forename type="middle">&amp;amp;</forename><surname>Johnson</surname></persName>
						</author>
						<author>
							<affiliation key="aff17">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CholecTriplet2021: A benchmark challenge for surgical action triplet recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Context-aware decision support in the operating room can foster surgical safety and efficiency by leveraging real-time feedback from surgical workflow analysis. Most existing works recognize surgical activities at a coarse-grained level, such as phases, steps or events, leaving out fine-grained interaction details about the surgical activity; yet those are needed for more helpful AI assistance in the operating room. Recognizing surgical actions as triplets of instrument, verb, target combination delivers comprehensive details about the activities taking place in surgical videos. This paper presents CholecTriplet2021: an endoscopic vision challenge organized at MICCAI 2021 for the recognition of surgical action triplets in laparoscopic videos. The challenge granted private access to the large-scale CholecT50 dataset, which is annotated with action triplet information. In this paper, we present the challenge setup and assessment of the stateof-the-art deep learning methods proposed by the participants during the challenge. A total of 4 baseline methods from the challenge organizers and 19 new deep learning algorithms by competing teams are presented to recognize surgical action triplets directly from surgical videos, achieving mean average precision (mAP) ranging from 4.2% to 38.1%. This study also analyzes the significance of the results obtained by the presented approaches, performs a thorough methodological comparison between them, in-depth result analysis, and proposes a novel ensemble method for enhanced recognition. Our analysis shows that surgical workflow analysis is not yet solved, and also highlights interesting directions for future research on fine-grained surgical activity recognition which is of utmost importance for the development of AI in surgery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Activity recognition is the basis for the development of many potential applications in health, surveillance, manufacturing, sports, etc <ref type="bibr" target="#b0">(Avci et al., 2010)</ref>. In surgery, it can be leveraged to provide intra-operative context-awareness and decision support <ref type="bibr" target="#b36">(Maier-Hein et al., 2017a)</ref>. This could augment surgeons' capabilities, fostering safety and efficiency in the operating room (OR) <ref type="bibr" target="#b59">(Vercauteren et al., 2019)</ref>.</p><p>Despite the vast literature in medical computer vision, a majority of the works tackle activity recognition at a very coarsegrained level such as phase <ref type="bibr" target="#b57">(Twinanda et al., 2016)</ref>, which does not provide an accurate picture of activities taking place. As an example, the calot triangle dissection phase in cholecystectomy contains a multitude of finer actions that would be relevant to recognize. The finer division of activity recognition such as step <ref type="bibr" target="#b31">(Lecuyer et al., 2020)</ref>, gesture <ref type="bibr" target="#b10">(DiPietro et al., 2019)</ref> or action  recognition leaves out details about the anatomy and thus, does not provide comprehensive details to describe the tool-tissue interaction. The detection of the used surgical instruments and their target anatomy as well as their fine-grained interaction details is necessary for the development of artificial intelligence (AI) that is safe for the patient <ref type="bibr" target="#b41">(Nwoye, 2021)</ref>. This opens a vista of opportunities to develop methods that recognize the elements involved in tool-tissue interaction while accounting for the relationship and multiple instances.</p><p>In general computer vision, human activity is modeled as triplets human, verb, object providing full-scale and expressive details on Human-object interaction (HOI) <ref type="bibr" target="#b7">(Chao et al., 2015b)</ref>. Translating this formalism to surgical vision, however, was not achieved until very recently, when <ref type="bibr" target="#b42">Nwoye et al. (2020)</ref> presented the video recognition of surgical activities as triplets of the used instruments, actions performed, and the underlying target anatomy.</p><p>We present a critical study on surgical action triplet recognition in the form of a challenge termed CholecTriplet2021 (https://cholectriplet2021.grand-challenge.org), to pave the way for research targeting fine-grained and detailed recognition of surgical activities from videos. This international challenge was organized as part of the Endoscopic Vision (EndoVis) Grand Challenge  and hosted at MICCAI 2021. The challenge presented promising technologies for the detailed understanding of tool-tissue interactions in minimally invasive surgery. A total of 19 teams participated, the highest record since the inception of the EndoVis Grand Challenge series.</p><p>The challenge provided a platform for the scientific community to perform comparative benchmarking and validation of endoscopic vision algorithms in a promising direction in surgical activity recognition. We provided private access to a highquality large-scale surgical action triplet dataset, CholecT50 , for both method development and validation. In addition to these contributions brought by the event itself, the challenge report presented here offers contributions of its own. After individual descriptions for each approach, we highlight several trends in an in-depth methodological comparison, providing a comprehensive overview of possible strategies for tackling the surgical action triplet problem. To further facilitate future research efforts, we also compile the implementation details of all featured submissions. We then set a new upper baseline (+4.3% AP to challenge methods) for the surgical action triplet recognition problem, by proposing a simple but effective algorithm ensembling models featured in the challenge. Finally, results are analyzed in depth: our quantitative analysis considers multiple metrics to cover all aspects of the triplet recognition problem. A rich selection of qualitative results is presented as well, to better understand the behavior of all the methods.</p><p>The paper is organized as follows: we position our work with respect to the related literature in the next section. Afterward, we present the challenge overview and setup including the used dataset and a summary of participation. This is followed by the methods presented at the challenge including the evaluation protocols, results, and extensive analysis of their performance. We conclude by highlighting the benefits of this study, insights, and prospects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The CholecTriplet2021 challenge relates to several research topics, for which we present the relevant literature in the following paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Activity recognition</head><p>One of the key concerns of computer vision is the recognition of human activities; a task for which a wide variety of approaches has been proposed, both in medical and non-medical domains. In those approaches, the definition of this task can be more or less granular, which is a key factor in determining its difficulty as well as its utility. Early activity recognition work in general computer vision involved broad, coarse-grained classification tasks: the 2012 version of the PASCAL VOC <ref type="bibr" target="#b12">(Everingham et al., 2015)</ref> challenge proposed an action classification task on static images with 10 classes. HMDB-51 <ref type="bibr" target="#b29">(Kuehne et al., 2011)</ref> and UCF-101 <ref type="bibr" target="#b53">(Soomro et al., 2012)</ref> are collections of realistic action video clips extracted from YouTube. As datasets expanded, the number of classes increased; classes became more diverse but also finer-grained. For example, Kinetics <ref type="bibr" target="#b2">(Carreira and Zisserman, 2017)</ref> features three classes related to cycling ("riding a bike", "riding a mountain bike", "falling off a bike") while HMDB-51 only contains one <ref type="bibr" target="#b29">(Kuehne et al., 2011)</ref>. Generally speaking, the advantage of having more granular classes is that they enable more detailed and informative descriptions.</p><p>In surgical computer vision, fine-grained activity descriptions are particularly valuable: as key components in concepts for context-aware surgical systems <ref type="bibr">(Maier-Hein et al., 2017a,b)</ref>, activity recognition algorithms tie into clinical outcomes. However, descriptions achieved by surgical vision algorithms have historically been coarse. The task of surgical phase recognition <ref type="bibr" target="#b57">(Twinanda et al., 2016;</ref><ref type="bibr" target="#b9">Dergachyova et al., 2016;</ref><ref type="bibr" target="#b15">Funke et al., 2018;</ref><ref type="bibr" target="#b65">Yu et al., 2019;</ref><ref type="bibr" target="#b68">Zisimopoulos et al., 2018;</ref><ref type="bibr" target="#b17">Hajj et al., 2018;</ref><ref type="bibr" target="#b8">Czempiel et al., 2020)</ref>, one of the main research topics in this area, breaks down an entire surgery into a small number of chunks, each with a broadly defined role within the surgical procedure. Since phases can last several minutes, they may contain many individual actions; this lack of detail is ultimately what limits the utility of the phase information. For this reason, other studies addressed more granular classes: subdivisions of phases into steps can be found in <ref type="bibr" target="#b48">Ramesh et al. (2021)</ref>; <ref type="bibr" target="#b31">Lecuyer et al. (2020)</ref> and recognition of action verbs has are explored in <ref type="bibr" target="#b49">Rupprecht et al. (2016)</ref>; <ref type="bibr" target="#b27">Khatibi and Dezyani (2020)</ref>. In the previous edition of EndoVis, an action recognition task was featured , modeling tool-tissue interaction to a certain degree: each class was defined by one verb.</p><p>Overall, a shift towards finer-grained activity recognition has become a major focus in recent research on activity recognition, especially for surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Action triplet recognition</head><p>The finest level of granularity in visual activity understanding is currently achieved by decomposing actions into their individual components: who performs the action, what the action is, and what the action is performed on. This decomposition into action triplets of (subject, verb, object) is central in HOI <ref type="bibr" target="#b7">(Chao et al., 2015b)</ref> studies. <ref type="bibr" target="#b38">Mallya and Lazebnik (2016)</ref> used CNN features extracted from humans and objects detected in frames. <ref type="bibr" target="#b5">Chao et al. (2018)</ref> proposed a multi-stream architecture to model spatial relationships. <ref type="bibr" target="#b16">Gkioxari et al. (2018)</ref>'s method was built around a FasterRCNN object detector to locate humans. <ref type="bibr" target="#b46">Qi et al. (2018)</ref> introduced the Graph Parsing Neural Network (GPNN), representing human-object interactions with the adjacency matrix and node labels of a graph.</p><p>In surgical computer vision, action triplets <ref type="bibr" target="#b26">(Katic et al., 2014)</ref> in the form of surgical tool/instrument, action verb, target anatomy are used to describe tool-tissue interactions (TTI) <ref type="bibr" target="#b41">(Nwoye, 2021)</ref>. Early works used them as auxiliary annotations to improve surgical phase recognition <ref type="bibr" target="#b26">(Katic et al., 2014</ref><ref type="bibr" target="#b25">(Katic et al., , 2015</ref>. The first method to actually perform action triplet recognition from videos was introduced by <ref type="bibr" target="#b42">Nwoye et al. (2020)</ref> as the Tripnet, featuring verb and target detection using instrument class activation guidance, as well as triplet association using a 3D interaction space. A new model  with more advanced modeling of interactions between triplet components using an attention mechanism was later developed. <ref type="bibr" target="#b64">Xu et al. (2021)</ref> proposed a cross-domain method for automatic surgical captions that capture semantic relationships, similarly to action triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Action triplet datasets</head><p>The study of action triplet recognition requires datasets annotated with triplet components. In general computer vision, an early example is the HICO dataset <ref type="bibr" target="#b6">(Chao et al., 2015a)</ref>. CAD-120 <ref type="bibr" target="#b28">(Koppula et al., 2013)</ref> is annotated with object affordances. V-COCO <ref type="bibr" target="#b50">(Sadhu et al., 2021)</ref>, as an extension of the widely used MS-COCO <ref type="bibr" target="#b33">(Lin et al., 2014)</ref>, added visual semantic role labels; the provided bounding box annotations also enabled HOI spatial detection. HICO later received an update in the form of HICO-DET <ref type="bibr" target="#b5">(Chao et al., 2018)</ref>, similarly incorporating bounding boxes. HCVRD <ref type="bibr" target="#b67">(Zhuang et al., 2018)</ref> was proposed as a benchmark for detecting human-centered visual relationships, which includes action verbs among other types. Ambiguous-HOI  collects difficult examples from several of the datasets previously mentioned, in order to form a challenging HOI benchmark.</p><p>In the surgical domain, datasets offering action triplet annotations describing tool-tissue interaction are much more recent. In a challenge organized by <ref type="bibr" target="#b60">Wagner et al. (2021)</ref>, a cholecystectomy video dataset with annotations for four surgical action verbs was provided. The SARAS-ESAD dataset <ref type="bibr">(Bawa et al., 2021)</ref> featured more refined classes with actions described by both the verb and the anatomy; bounding boxes for 21 action classes were provided as well. The first dataset with full annotations for each triplet component was introduced in <ref type="bibr" target="#b42">Nwoye et al. (2020)</ref> as CholecT40. The expanded version renamed CholecT50  is employed in this challenge. Another dataset used in <ref type="bibr" target="#b64">Xu et al. (2021)</ref> incorporated surgical captions; despite not explicitly following the action triplet formalism, the level of detail offered is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Endoscopic vision challenges</head><p>As a relatively new problem, surgical action triplet recognition has only received little attention despite its potential. This is a major motivating factor for CholecTriplet2021: over the past few years, challenges have played an important role in the surgical computer vision community due to the exposure they can bring to interesting research topics, as well as their ability to introduce and compare a wide variety of original methods. For instance, the M2CAI 2016 challenge <ref type="bibr" target="#b55">(Stauder et al., 2016)</ref> featured two tasks -surgical phase recognition and tool presence detection for cholecystectomy. The first edition of the CATARACTS challenge <ref type="bibr" target="#b18">(Hajj et al., 2019)</ref>, involved tool recognition on cataract surgery videos; later editions became part of the EndoVis (Endoscopic Vision) challenge series. Each iteration of EndoVis featured multiple sub-challenges, some of which focused on surgical activity understanding: segmentation and tracking of tools for colorectal surgery videos (2015), phase recognition for colorectal surgery using video and sensor data (2017), and finally, phase, tool and action recognition for cholecystectomy videos .</p><p>Outside of EndoVis subchallenges, the SARAS-ESAD <ref type="bibr">(Bawa et al., 2021)</ref> organized within the MIDL 2020 challenge presents a benchmark for surgical action detection using a largescale video dataset (ESAD) offering another level of granularity ( verb, target as a single label) as well as bounding boxes for action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CholecTriplet challenge</head><p>The goal of this challenge is to assess AI solutions for finegrained surgical activity recognition. This recognition is modeled as a triplet <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref>, with the following notation: instrument, verb, target</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task</head><p>The task is to develop a machine learning method to recognize these triplets directly from unseen surgical videos. In modeling surgical action triplet, a prevailing problem to tackle is the simultaneous detection of the correct instruments, verbs, and targets in every image frame and resolving their association. This is challenging since the involvement of a component in a triplet can be visually subtle. Hence, the challenge also assesses the detection of these individual components for a more insightful analysis of a model's understanding of the triplet's composition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>105</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset</head><p>The dataset used for the challenge is CholecT50 , which consists of 50 video recordings of laparoscopic cholecystectomy that have been annotated with the binary presence of action triplets. At 1 frame per second (fps), the dataset reaches a total of 161K frames. CholecT50 consists of 100 action triplet classes composed from 6 instruments, 10 verbs, and 15 targets. The triplet labels we provided in form of a single binary presence for each triplet class instrument, verb, target taken as a whole. Binary labels for each component of the triplet were also provided.</p><p>On the data split, 45 out of 50 videos in the dataset were released to the participants for training and validation; those videos were part of the publicly released Cholec80 dataset <ref type="bibr" target="#b57">Twinanda et al. (2016)</ref>. The remaining 5 videos, which were not public, were withheld from the participants as the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Challenge design</head><p>The challenge was conducted as part of EndoVis grandchallenge  in MICCAI 2021. The challenge proposal went through two rounds of open review between November 2020 and February 2021. By early March 2021, a call for participation was circulated and the challenge officially started on March 15, 2021, with a public release of the training data. Participation took place by web registration and signing a non-disclosure contract on the use of the challenge dataset.</p><p>By the end of May, a Slack 1 channel was created for the registered participants and their teams. A blog was provided with snippets of code and instructions for getting started with the challenge. Reference to the baseline methods and code for some specialized functions were also provided. A Docker 2 submission template, development guide, and evaluation metrics were provided to the participants during their method development phase. Participating teams were allowed to develop novel methods, fine-tune a state-of-the-art method, or improve on existing solutions. Entrants were allowed to pre-train their model on any third-party publicly available dataset. Developed methods were intended to take sequential image frames from videos, process, and return a vector probability for the 100 triplet classes for each image. Due to the possibility of multiple instances of the triplets, both the triplets and their three components were cast as a multi-label classification problem. The output probability vectors for the three components of the triplets were derived using a filtering algorithm proposed in .</p><p>By the terminal point of the development phase, a validation process was initiated for users to ascertain that their Docker container had been built with the correct input/output format and was able to run without run-time errors on a set of randomly selected images from the training set. Teams were allowed to validate multiple times until an error-free Docker was obtained, but within a time-bound of three weeks which lasted till Sept 5, 2021. The final submission was performed once using only a validated Docker image container. These submissions were run and evaluated on the hidden test set by the challenge organizers with the outcome presented at the MICCAI 2021 satellite event. The challenge timeline is presented in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>We also provide a one-month post-challenge window (November 15 -December 15, 2021) during which all teams could re-evaluate their updated model if their prior submissions had inaccuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Method submission</head><p>All Docker submissions were collected as saved Docker images uploaded to Dropbox 3 . Only Dockers strictly adhering to a predefined Docker format and input/output requirements were considered as valid submissions. In practice, this involved passing several automatic checks including ensuring compatibility with a defined directory structure and naming format. In terms of output format, automatic checks were conducted to ensure that an output probability value between 0 and 1 was written for each input frame and consequently each available video. Additionally, each submitted method was required to pass a causality check on a predefined, hidden subset of frames to ensure that future frames were not utilized. During the validation phase, participants were expected to make corrections to the submissions based on automatically generated feedback that was shared by email to ensure that only validated Docker images were submitted for final evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Awards</head><p>The winning submission earned an NVIDIA GPU. Monetary prizes were also awarded to the top 3 competing teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Participation statistics</head><p>As presented in <ref type="figure" target="#fig_0">Fig. 2</ref>, an initial 44 teams registered by the end of the enrollment window, accounting for 106 approved individual participants out of 191 recorded registrations. During the Docker validation phase, 24 teams submitted a total of 105 Docker containers which were evaluated with a 41.9% success rate. On the deadline of September 15, 2021, a total of 19 teams' submissions were received for final evaluation. A baseline submission from the organizers brings this to a total of 20 teams. These teams were drawn from 10 countries across 3 continents. The demography of the final participating teams is presented in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline methods</head><p>Four baseline methods are provided by the challenge organizers (Team CAMMA): (1) MTL baseline <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref>, (2) Tripnet <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref>, (3) Attention Tripnet , and (4) Rendezvous . We present a brief overview of the baseline models below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">MTL baseline</head><p>The Multi-Task Learning baseline <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref> is built around a ResNet-18 backbone, which serves as the common visual feature extractor for three separate branches. Each branch is a 3-layer (2 convolutional, 1 fully connected) neural network responsible for recognizing one of the triplet components. The instrument branch differs from the other two with the addition of Global Max Pooling (GMP). A final fully-connected layer is used to combine the three components' prediction into a final triplet prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Tripnet</head><p>The Tripnet <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref> also relies on multi-task learning from a ResNet-18 backbone but provides a stronger baseline thanks to its two characteristics. The first one is the Class Activation Guide (CAG): here the instrument branch, called the Weakly Supervised Localization (WSL) module, generates per-instrument Class Activation Maps, which are forwarded to a subnetwork in charge of verb and target detections as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. These maps are concatenated to the verb and target features. This additional information on instrument positions, in the form of concatenated instrument activation maps, provides clues for better detection of verbs and targets, since those are located at the tooltips.</p><p>The second innovation of the Tripnet is the 3D Interaction Space (3DIS), which associates the triplet components. Log probabilities for each component are projected into a 3D grid of dimensions n I ? n V ? n T (number of instruments, verbs, and targets, respectively) by a trainable function. Each point in the 3D grid represents the probability of the triplet, as estimated by the 3DIS function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Attention Tripnet</head><p>The Attention Tripnet  enhances the previous Tripnet, by replacing the CAG with an attention model, the Class Activation Guided Attention Mechanism (CAGAM). While the CAG only proceeds by feature concatenation, the CAGAM computes additive enhancements for verb and target class maps based on the instrument maps. Those enhancements are obtained by two different attention mechanisms: a channel (instrument type) attention for verbs and a position attention for targets. Using attention in this manner explicitly steers the verb and target detections towards the correct locations, much more strongly than the concatenation in the Tripnet's CAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Rendezvous</head><p>Rendezvous , or simply RDV, is the strongest baseline model. In contrast to the Attention Tripnet, triplet association is performed using a Transformer-like model built around a Multi-Head of Mixed Attention (MHMA), instead of the 3DIS. The MHMA operates on a set of four features: H IVT , H I , H V , H T for global triplet, instrument, verb, and target features respectively. H IVT is drawn from a bottleneck layer, connected to an early layer of the ResNet-18 backbone. H I is taken from the WSL's output, and the last two are taken from the CAGAM.</p><p>Inside the MHMA, H IVT is processed by a self-attention head. A projection function maps it to three separate vectors -query, key, and value. A scaled dot product then transforms them into one refined feature. H I is fed to a cross-attention head: the key and value are obtained by projecting H I , but the query is the same one used in H IVT 's self-attention head. H V , H T 's cross-attention heads operate in the same manner as H I 's.</p><p>The resulting four refined features are concatenated; two convolutions and an AddNorm operation complete the MHMA, which outputs a refined version of H IVT . Inside Rendezvous, a stack of 8 MHMAs is employed; the output of this stack is used to make the final prediction on the triplet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Competing methods</head><p>Team 2Ai: Surgical video analysis using an ensemble of multitask recurrent convolutional networks (versions 1 and 2): Team 2Ai proposed a solution for this challenge consisting of an ensemble of multi-task recurrent convolutional networks. Each model architecture consists of a multi-task recurrent convolutional network with four heads, where each branch targets one of four tasks, namely surgical instrument detection, verb recognition, phase identification, and target recognition. To extract generic visual features, they used a shared feature extractor for all four branches. Specifically, for each of the instrument detection and target recognition branches, a fully connected layer is connected to the backbone to compute the signals for both tasks. Here, a sigmoid activation layer is applied to produce the final predictions. In the branches for surgical action and phase recognition, long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) are connected to the backbone to leverage the temporal context of the current frame. A sigmoid and softmax layers are added to the end of the last LSTM units in the action and phase recognition branches, respectively. Finally, binary cross-entropy is used as a loss function for the sur-gical instrument detection, verb recognition, and target recognition tasks, and cross-entropy is used for phase identification. The different networks, each with a different feature extraction backbone, are individually optimized and trained. Afterward, majority-vote ensembling is applied to combine the predictions resulting from the different networks. As a final step, a temporal smoothing technique is applied to each task to avoid temporally incoherent results. To meet the challenge output requirements, the individual signals are finally represented as surgical actions triplets. 2Ai version 2 is submitted post-challenge to correct the error in the output format of the initial model by changing the final output mapping from binary to probability scores. In this revised version, the ensemble is performed using an average of probabilities outputs from all the networks.</p><p>Team ANL-Triplet: Exploiting temporal information for triplet recognition:</p><p>Team ANL-Triplet used three ResNet-18 backbones <ref type="bibr" target="#b19">(He et al., 2016)</ref> for instrument, verb and target prediction. In a multi-task setup, these component predictions are concatenated before using a single convolutional layer and a fully connected layer to recognize the action triplets represented in the frame. To improve verb and target recognition performance, these models are initialized using the learned instrument model weight. Additionally, an independent ResNet-34 is trained to incorporate temporal information by predicting action triplets directly from features extracted from the current frame and 7 previous frames. In both triplet recognition approaches, a smoothing factor of 0.2, 0.2 2 is used for predictions that predicted two and one components correctly, respectively, reducing the penalty for semantically closer predictions. The final prediction is made by taking the average of the two triplet predictions. Team Band of Broeders: YOLOV5 for surgical action triplet detection:</p><p>The Band of Broeders utilized manually generated bounding box annotations of the surgical triplets to train a YOLOV5 based object detector for action triplet prediction. They chose the YOLOv5 4 neural network for its state-of-the-art performance on various object detection datasets. Their network consists of three stages: (1) the backbone CSP-DenseNet <ref type="bibr" target="#b61">(Wang et al., 2020a)</ref>, which is used for its gradient efficiency, (2) the neck Path Aggregation Network (PA-Net) , which is used for multi-scale feature extraction, and (3) the head which produces the final bounding box predictions. These components allow for the building of neural networks with large receptive fields and a multi-scale view of the frame enabling the detection of objects of various sizes. Team CAMP: Phase-guided temporal endoscopic action triplet classification (EndoVisNet):</p><p>Team CAMP's methodology is based on the idea that the presence of an action in a given frame implies that only a smaller set of actions could occur in the immediate frames that follow. To leverage this temporal property, visual features from both the input frame and its t preceding frames are extracted using the SlowFast <ref type="bibr" target="#b14">(Feichtenhofer et al., 2019)</ref> network to ensure that the extracted features are relevant and independent of the movement speed in an input video. After extraction, the temporal features are pooled into 1 dimension to make a final prediction. Feature classification is modeled using a similar approach to Tripnet <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref>, leveraging a multi-task learning approach, but with an extra branch for the phase detection, which is trained jointly on labels extracted from the Cholec80 dataset <ref type="bibr" target="#b57">(Twinanda et al., 2016)</ref>. The final action triplet classification scores are computed as a learned linear combination of the instrument, verb, and target prediction scores. Team Casia Robotics: Triplet translation embedding network with coordinate attention:</p><p>Team Casia Robotics' submission (see <ref type="figure" target="#fig_2">Fig. 4</ref>) builds on the work <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref> and replaces the introduced 3D interaction space with a translation embedding module, following <ref type="bibr" target="#b66">Zhang et al. (2017)</ref>. Here, the translation embedding module couples the instrument, target, and verb features to produce the final triplet predictions. Specifically, they used a ResNet-18 <ref type="bibr" target="#b19">(He et al., 2016)</ref> backbone for extracting spatial features followed by three sub-networks with convolutional layers and fully-connected layers that are used to predict instruments, verbs, and targets, respectively. To model the complex relationship between triplets, they modeled projection matrices, W i and W t , that translate learned representations from the feature space 4 https://github.com/ultralytics/yolov5 to a shared relation space. The interaction relation between the three learned component feature vectors f t (target), f i (instrument/tool), f v (verb) is then represented as:</p><formula xml:id="formula_0">(W t f t ? W i f i ). f v .</formula><p>All invalid component combinations are then masked out to only generate predictions for the 100 relevant triplet classes. Further, a coordinate attention mechanism <ref type="bibr" target="#b21">(Hou et al., 2021a)</ref> is used to incorporate positional information into each subnetwork to more accurately locate each component of the triplet.</p><p>Team Ceaiik: Spatio-temporal learning of action triplets in surgical videos:</p><p>Team Ceaiik's model architecture design is based on the triplet classification as a composition of three individual tasks namely instrument, verb, and target classifications <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref>. In the first stage of training, the model utilizes a ResNet-50 <ref type="bibr" target="#b19">(He et al., 2016)</ref> to extract spatial features based on the results of <ref type="bibr" target="#b39">Mishra et al. (2017)</ref>; <ref type="bibr" target="#b40">Mondal et al. (2019)</ref> and employs 3 classification heads to predict instrument, verb, and target class probabilities. A final classification head is employed to utilize the previously computed component probabilities, which are aggregated to make a triplet prediction. Further, in the second stage of training, an effort is made to incorporate certain temporal properties of action triplets in their model design. Keeping the learned ResNet-50 weights frozen from the first stage of training, the model extracts visual features which are passed through an LSTM module <ref type="bibr" target="#b20">(Hochreiter and Schmidhuber, 1997</ref>) before instrument, verb, target, and triplet prediction are performed using a similar approach to the first stage. Team CITI SJTU: Action triplet recognition via convolutional LSTMs and multi-task learning:</p><p>Team CITI SJTU focused their method on modeling the sub-components and temporal coherence when predicting action triplets. Their multi-task deep learning network includes four branches with one main triplet branch and three auxiliary branches generating the recognition results for instruments, verbs, and targets, respectively. All the branches share the same ResNet-50 <ref type="bibr" target="#b19">(He et al., 2016)</ref> network for feature extraction, followed by two convolutional Long Short Term Memory (Con-vLSTM) layers <ref type="bibr" target="#b52">(Shi et al., 2015)</ref> for modeling spatial-temporal relationships. After training, they used the triplet prediction branch to obtain the final triplet prediction for inference. The three auxiliary branches allow them to add fine-grained information for training to significantly boost triplet recognition per-formance. Team Digital Surgery: Temporal ensemble triplet action recognition (TE-TAR):</p><p>Team Digital Surgery's proposed model, TE-TAR, consists of an ensemble of encoders, an LSTM model <ref type="bibr" target="#b20">(Hochreiter and Schmidhuber, 1997)</ref>, and a classification layer. The ensemble of encoders is composed of four HRNet32 backbones <ref type="bibr" target="#b62">(Wang et al., 2020b</ref>) and a classification head that efficiently combines multi-scale information. Each ensemble is trained with a different subset of data for learning more diverse features. For each image, four features are generated in total (i.e., one per encoder), which are combined by summation. In addition, to allow the model to estimate the action of the instrument (verb) and the anatomy where the instrument is applied (target), they proposed the use of temporal information, using a sliding window approach with a window length of 10 frames. Features are encoded by the ensemble for all frames in the window and then fed to an LSTM. As they formulated the action triplet task as a classification problem, the final triplets are estimated by feeding the aggregated features to a linear classification layer. Team Lsgroup: Feature fusion and weak locational information calculating in triplet classification multi-task:</p><p>Team Lsgroup submitted a multi-task learning network with four sub-networks that are used to classify each component of the action triplet and the triplet itself. Their model is based on two fundamental assumptions: (1) The type and location of surgical instruments are critical factors for determining the verb component of the triplet <ref type="bibr" target="#b42">(Nwoye et al., 2020)</ref>, and <ref type="formula">(2)</ref> it is important to combine the feature of the instrument, verb, and target <ref type="bibr" target="#b42">(Nwoye et al., 2020;</ref><ref type="bibr" target="#b24">Jin et al., 2021)</ref>. Therefore, their approach is composed of a CNN backbone (feature extraction), weak instrument localization, component feature fusion sub-network, and triplet-possibility mapping using weighted averaging. Firstly, a ResNet-18 backbone is trained to jointly learn to perform instrument, verb, and target prediction using three classifier heads. To focus the predictions on the relevant portions of the image, weak instrument localization features <ref type="bibr" target="#b43">(Nwoye et al., 2019)</ref> are concatenated to the learned features for verb and target heads before performing the final classification. The predicted probability vectors for each of the three tasks are then concatenated before a single fully connected layer is used to map this vector to the final triplet prediction vector. Team HFUT-MedIA: Correlation embedded multi-task network (COEMNet):</p><p>Team HFUT-MedIA participated with a correlation embedded multi-task network, named COEMNet (see <ref type="figure" target="#fig_3">Fig. 5</ref>). First, a multi-task learning network is trained for instrument, verb, and target recognition tasks. The learned features for instrument prediction are leveraged for better recognition of verbs and targets. Secondly, the correlation between all classes is modeled as a graph and used the statistical co-occurrence frequencies as the adjacency matrix. A multi-layer graph convolutional network (GCN) is deeply integrated into their end-to-end network to efficiently learn the label classifiers and embed correlation information into feature representation <ref type="bibr" target="#b63">(Wang et al., 2020c)</ref>. Since associating the predicted instruments, verbs, and targets to form the right triplet predictions is complex when there are multiple triplet annotations in a single frame, a triplet adjustment task is added to the proposed network to minimize the association errors. The learned adjustment factors are applied to the original triplet prediction for more accurate recognition. Team HFUT-NUS: Multi-task learning-based surgical interaction triplet recognition:</p><p>Team HFUT-NUS's method utilizes a ResNet-18 <ref type="bibr" target="#b19">(He et al., 2016)</ref> backbone to extract features from endoscopic images followed by three parallel fully connected layers that are used to predict instrument, verb, and target, respectively. To further improve the precision of surgical interaction triplet detection, an attention mechanism is adopted to weigh the importance of instrument, verb, and target components. Finally, the weighted instrument vector, verb vector, and target vector are spliced together and fed into a fully connected layer with a sigmoid activation to predict the final surgical action triplet. Team J&amp;M: Surgical action triplet recognition with Efficient-MSTCN:</p><p>Team J&amp;M's submission is a 2-stage network that is trained to first extract relevant spatial features and then utilize the temporal context of each frame to make action triplet predictions for a frame. In their proposed model, EfficientNetV2-M <ref type="bibr" target="#b56">(Tan and Le, 2021)</ref> is used as the feature extractor. It is trained to make predictions using a single frame. Afterward, the weights are frozen and a Multi-Stage Temporal Convolutional Network (MS-TCN) <ref type="bibr" target="#b13">(Farha and Gall, 2019)</ref> is employed to refine the model predictions. Team Med Recognizer: Surgical action triplet recognition via temporal memory relation gradient network:</p><p>Team Med Recognizer's competing model is a temporal memory relation gradient network, in which a stem module first extracts spatial features from each frame, and then splits into three branches, with different temporal supportive information integrated to represent the action triplet. The temporal lengths are set as 10, 5, and 5 for 'verb', 'instrument', and 'target', respectively, given that different tasks require different amounts of temporal context. The spatial-temporal feature extraction model is developed based on <ref type="bibr" target="#b24">Jin et al. (2021)</ref>, which is originally designed for phase recognition, to leverage the long-range and multi-scale temporal patterns in the video. The features are then fed into the classifiers to generate the prediction probabilities of the three tasks. The obtained probabilities from three branches are then integrated to produce the final one. Postprocessing methods are then applied to account for the label imbalance, where higher weights are assigned to the classes with fewer data samples. The temporal information is also further utilized to weight the predicted probabilities of the previous frames when producing the results of the current frame. Dropout is used at both the training and the testing time.</p><p>Team MMLAB: Temporal triplet net for triplet presence detection in surgical videos:</p><p>Team MMLAB proposed the Temporal Triplet Net (TTN) which consists of DenseNet <ref type="bibr" target="#b23">(Huang et al., 2017)</ref> and a Graph Convolutional Network (GCN) that utilizes both spatial and temporal features for the recognition of action triplets in surgical videos. The DenseNet is used as an image classification model to extract spatial features for each labeled frame for an input video. To utilize the temporal information, the GCN is incorporated to capture the temporal relationships among the continuous frames of a video sequence using the features extracted for each frame. Specifically, the representation for each frame is regarded as the node of the graph, and the similarity between each pair of nodes is regarded as the edge of the graph. Team NCT-TSO: Multi-task learning framework for action triplet recognition:</p><p>Team NCT-TSO's method is similar to <ref type="bibr" target="#b42">Nwoye et al. (2020)</ref> designed for the tasks of verb, target, and triplet recognition. In their proposed method, a separate model is trained for instrument recognition using the Cholec80 dataset <ref type="bibr" target="#b57">(Twinanda et al., 2016)</ref>. The instrument recognition model is based on a convolutional neural network (CNN) which uses the ResNet-50 <ref type="bibr" target="#b19">(He et al., 2016)</ref> as its backbone and spatial pooling to learn classspecific feature maps of the instruments in a weakly supervised manner <ref type="bibr" target="#b11">(Durand et al., 2017)</ref>. These instrument maps are subsequently fed to the verb and target paths of the triplet recognition network. The verb and target paths share the same ResNet-50 model as their backbone, followed by two convolutional layers for each path. The verb and target paths are also trained to learn class-specific feature maps and use wildcat spatial pooling (Durand et al., 2017) on these maps for the prediction of labels. The instrument logits from the pre-trained instrument model, along with the verb and target logits, are subsequently used to learn the triplets using a 3D interaction space as proposed in the work of <ref type="bibr" target="#b42">Nwoye et al. (2020)</ref>. Team SIAT CAMI: Multi-task mutual Channel Recurrent Net for Fine-grained Surgical Triplet Recognition:</p><p>Team SIAT CAMI used MT-MCLNet (see <ref type="figure" target="#fig_4">Fig. 6</ref>), a multitask surgical triplet recognition network with multi-label mutual channel loss <ref type="bibr" target="#b4">(Chang et al., 2020)</ref> to extract local finegrained features and aggregate temporal information on verb and triplet branch. When a sequence of video images is fed into the network, the backbone ResNet-50 module <ref type="bibr" target="#b19">(He et al., 2016)</ref> first extracts a global 2048-dimension spatial feature for each image in the sequence. To extend the global features to each task, different 1 ? 1 convolutions are applied to generate 2040dimension features for instrument and target branches, 2000 for verb and triplet. Then, the 2040/2000-dimension spatial features are fed into two LSTM modules to capture 512-dimension temporal motion information. Then, 4 fully connected layers are utilized to produce the final classification output. The overall loss function is a weighted sum of standard cross-entropy loss and a mutual channel loss, which is intended to encourage learning a diverse range of discriminative features. Team SJTU-IMR: Tracking surgical actions with transformers and action label-guided fine-grained information aggregation:</p><p>Although each triplet is unique, different triplets may share certain elements. Team SJTU-IMR hypothesized that this observation is critical to modeling instrument-tissue interactions as it can establish a link between different actions with shared elements. To this end, they presented a two-stage transformer- based learning framework to solve the surgical action tracking problem. Specifically, the features learned from the first stage, which conducts frame-level action recognition via a Swin-S network architecture , are taken as the input to the second stage, which performs sequence-level action recognition via masked transformers <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref>. At both stages, the same multi-task learning strategy of combining coarse-grained action recognition with fine-grained information aggregation is employed. Their fine-grained information aggregation is guided by coarse-grained action labels so that the networks can put more emphasis on features modeling instrumenttissue interactions. Team SK: Action triplet recognition with weakly-supervised attention of surgical instruments:</p><p>Team SK followed an instrument-centric approach to action triplet recognition by first performing instrument recognition that is then used to condition the verb, target, and consequently, triplet recognition tasks. Using the first 4 convolutional layers of a ResNet-18 <ref type="bibr" target="#b19">(He et al., 2016)</ref> as their backbone, the first subnetwork generates attention maps using 2 convolutional layers (layer 5 of ResNet-18 + 1x1 convolution) to localize the position of the instruments in the image. These attention maps are implicitly learned through weak supervision with instrument labels. A second sub-network then uses a convolutional layer to predict a fixed number of channels corresponding to each instrument which is multiplied by the previously learned attention maps to appropriately weigh different parts of the image based on instrument location. Finally, a 1x1 convolution, global average pooling, and softmax operation are performed to obtain the triplet probabilities. Team Trequartista: Phase-aware multitasking action recognition model with adjustment for low-data triplet classes:</p><p>Team Trequartista's entry used multi-task learning on 5 tasks, triplet, instrument, verb, target, and phase prediction. The phase prediction ground truth made use of Cholec80 annotations <ref type="bibr" target="#b57">(Twinanda et al., 2016)</ref>. Using a ResNet-18 and ResNet-34 <ref type="bibr" target="#b19">(He et al., 2016)</ref> as their primary models, their work focuses on finding the optimal hyperparameters and appropriately post-processing their predictions specifically for the challenge metric (mAP). Following an observation that predicting underrepresented classes has a strongly negative effect on the overall metric, the triplet predictions probability for the 63 least represented triplets are adjusted to compensate for this effect based on the following heuristic: P(Triplet) = P(Instrument) * 0.03P(Verb) + 0.97P(Target). (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Theoretical comparative analysis</head><p>Presented methods can be broadly classified into 5 categories: A. Multi-task learning: Rank 1-5, 7-8, 10-12, 14-19:</p><p>Multi-task learning methods aim to improve triplet recognition performance by learning a shared representational space by training a model to perform multiple related tasks. Unsurprisingly, given the availability of additional annotated information, all but 3 teams employed multi-task learning in their model design. The most common formulation is to learn to predict the triplet components, instrument-verb-target, in addition to the triplet itself. Interestingly, this formulation has come in two flavors: 1) Predicting the triplet as an explicitly modeled association of the 3 predicted components (2Ai, ANL Triplet, CAMP, HFUT-MedIA, HFUT-NUS, lsgroup, Med Recognizer, NCT-TSO, SJTU-IMR, and 2) Predicting the triplets and 3 components using a shared backbone that implicitly learns useful features from the other tasks for triplet recognition (CITI SJTU, Trequartista, SIAT-CAMI). Given that surgical phases <ref type="bibr" target="#b57">(Twinanda et al., 2016)</ref> are defined by and consequently signal the occurrence of certain actions, surgical phase recognition is highly related to the task of action triplet recognition. Three teams (Trequartista, 2Ai, CAMP) leverage this fact by incorporating spatial phase annotations for the challenge training videos that are publicly available in the Cholec80 dataset <ref type="bibr" target="#b57">(Twinanda et al., 2016)</ref>. All three methods do so in multi-task learning setups, with phase and triplet component recognition included as relevant tasks to boost triplet recognition performance. Finally, the Band of Broeders 5 entry treated the triplet recognition tasks as an object detection task, positing that localization information is critical to effectively recognizing the action triplets represented in an image. They manually generated bounding boxes and instrument labels, which are then assigned to their corresponding triplets, to facilitate model training by learning to both detect and localize triplets. B. Temporal modeling: Rank 2, 4-6, 8-11, 18-19:</p><p>While surgical action triplet recognition can be done on single frames, temporal models processing information from several frames at a time are interesting solutions to investigate: object permanence, motion, and surgical workflow are indeed temporal concepts that can inform action recognition. The many teams (11 out of 19) choosing this direction proposed a diverse range of temporal modeling methods, which can be described according to three main traits. The first is the choice of temporal architecture. Some methods used non-trainable operations to aggregate information from static image models across multiple frames (ANL-Triplet, LSGroup, CAMP, MMLAB); most entrants, however, resort to sequence models running on features extracted by CNN. Among them, LSTM-based recurrent neural networks are a clear trend, with various forms appearing in 6 submissions (2AI, CEAIIK, Digital Surgery, CITI-SJTU, Med Recognizer, SIAT-CAMI). Team CITI-SJTU in particular used a pair of ConvLSTMs. Besides LSTMs, other more recent models appeared as well: TCNs (J&amp;M, Med Recognizer), and Transformers (SJTU-IMR).</p><p>The second notable trait is the model's temporal range. In some methods, this range is very short: 2 frames for team LS-Group, 4 frames for team CEAIIK, 5 frames for team ANL-Triplet, and team MMLAB. Longer periods are featured in submissions by team 2AI (10 frames), team Digital Surgery (10 frames), team CITI-SJTU (16 frames), and team CAMP (32 5 non-competing method as it uses private annotations frames); team Med Recognizer used "short video clips" for the SV-RCNet, 10-frame clips for the verb memory bank, and 5frame clips for the instrument memory bank. Team SJTU-IMR feed their model in chunks of 200 frames, which can cover large workflow sections. One team, J&amp;M, used concatenated features from all video frames, making the entire history available to the model at any given timestep.</p><p>The third trait distinguishing temporal methods is end-toend training. Some methods feature temporal layers that are trained separately (CEAIIK, Digital Surgery, J&amp;M, Med Recognizer, SJTU-IMR), while others (2AI, ANL-Triplet, CITI-SJTU, SIAT-CAMI, LSGroup, CAMP) trained all parts simultaneously.</p><p>C. Attention mechanism/transformer: <ref type="bibr">Rank 3,</ref><ref type="bibr">5,</ref><ref type="bibr">9,</ref><ref type="bibr">10,</ref><ref type="bibr">12,</ref><ref type="bibr">14:</ref> Attention mechanisms are methods designed to modulate a model's input or internal representation, to highlight parts that are important for making predictions. The use of attention can greatly improve surgical action triplet recognition, as shown by  in the two approaches of Attention Tripnet and Rendezvous -these two models reappear in this challenge as baselines. All methods from the challenge featured in this category used some form of spatial attention since areas surrounding tooltips are particularly informative. A few of these methods explicitly used instrument maps as the source of attention (Attention Tripnet, RDV, LSGroup, SK). The other forms of spatial attention featured are team SIAT-CAMI's "channelwise attention", team Casia Robotics's "coordinate attention" <ref type="bibr" target="#b22">(Hou et al., 2021b)</ref> and team SJTU-IMR's Swin-S vision Transformer. Two entries used additional attention components of different types: semantic, in the RDV baseline's multi-head of mixed attention; and temporal, in the masked Transformer appearing in the second stage of team SJTU-IMR's method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Graph convolutional networks: Rank 3, 13:</head><p>Two teams, HFUT-MedIA and MMLAB, made use of GCN and CNN to better recognize action triplets represented in a given image, using two different strategies. Team MMLab used a GCN primarily to leverage temporal relationships using spatial features extracted using a CNN in the first stage of training. Here, the extracted features for each frame are treated as a graph node and the similarity between each pair of nodes features an edge between two nodes. Team HFUT-MedIA, in contrast, does not base its graph on CNN-extracted spatial features but rather used it to effectively incorporate triplet co-occurrence distribution statistics into various stages of a discriminative CNN to predict action triplet probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ensemble models: Rank 1, 6, 16:</head><p>Ensembling is a commonly used strategy for limiting noisy predictions, by combining outputs from independently trained models. Three teams employed this type of approach, with distinct variations on the ensembling concept. The choice of operation for merging outputs varies between teams: summation (Digital Surgery), averaging (2AI version 2), majority vote (2AI version 1), ad-hoc heuristics (Trequartista). Early or late fusion is another differentiating trait: teams Trequartista and 2AI merged final probabilities, while team Digital Surgery combined features from various feature extractors before applying an LSTM model. Finally, various ensemble sizes and architectures are used: 3 ResNets (Trequartista), 4 HRNets (Digital Surgery), and an ensemble of 6 CNNs (team 2AI). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ensemble predictions</head><p>For optimal performance on the dataset and as a summary of the challenge benchmark study, we ensemble the predictions of 7 top models (with triplet recognition AP above 30.0%): Trequartista, HFUT-MedIA, SIAT-CAMI, RDV, ANL-Triplet, CITI-SJTU, and Digital Surgery. This helps minimize errors due to noise, bias, and variance while improving the stability, reliability, and accuracy of predictions. In this work, we experiment with 6 ensemble methods.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, we start with simple averaging of the probability scores of the different models <ref type="figure" target="#fig_5">(Fig. 7a</ref>). This is extended to weighted averaging <ref type="figure" target="#fig_5">(Fig. 7b)</ref> where the weights are computed as the performance ratio of each model against the others. We also perform soft voting <ref type="figure" target="#fig_5">(Fig. 7c</ref>) between the maximum (presence) and minimum (absence) probability scores per class at a threshold of 0.5. Our first trainable method uses a two-layer network to learn a deep ensemble <ref type="figure" target="#fig_5">(Fig. 7d</ref>) of the challenge networks predictions. Lastly, we focus on learning the model averaging weights and in turn use this for a deep weighted ensemble <ref type="figure" target="#fig_5">(Fig. 7e</ref>). In this case, we learn two types of weights: (1) a vector of N weights for the N models, and (2) a matrix of N ? C weights for per class (C) weights of N models. The latter option, which is a deep per-class weighted ensemble, is designed to utilize the strength of each model in recognizing the different categories of the triplets. All the deep ensemble models are trained on triplets probability (Y IVT ) predictions of the selected models on the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Implementation details</head><p>The implementation details of all submissions as well as the baselines are summarized in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Metrics</head><p>To evaluate the performance of the presented models on surgical action triplet recognition, we use the average precision (AP) metric, computed as the area under the precision-recall curve; this is recommended as the standard practice for the CholecT50 dataset. Using AP metrics, we access the models' capacity for predicting the correct triplet components: AP I , AP V , AP T , and the correct triplet associations: AP I V, AP I T , AP I VT . The AP I VT evaluates the complete instrument-verbtarget combination and hence serves as the primary metric in the challenge. We also analyze the quality of the model predictions by computing the topK performance scores where K ? <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20]</ref> and the average over topK@ <ref type="bibr">[5:20]</ref> at intervals of 5 steps. All the evaluation scores are computed using the ivtmetrics library 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation protocol</head><p>We maintain an online inference strategy to represent the real-time usage of developed methods in the OR. For uniform evaluation of all models -including those not multitasking the triplet components -we collect only the probability scores for triplet classes (Y IVT ) per frame as the model output. The individual component predictions are filtered from Y IVT following the disentanglement function proposed in . The video-specific AP scores are computed per category in a test video, then averaged categorically across all test videos. We obtain the mean AP by averaging the category APs.</p><p>For method ranking, the challenge evaluation is performed on 94 valid triplet classes excluding the 6 null classes in the dataset. The null classes, which include grasper, <ref type="bibr">null-verb, null-target , bipolar, null-verb, null-target , hook, null-verb, null-target , scissors, null-verb, null-target , clipper, nullverb, null-target , and irrigator, null-verb, null-target , are</ref> possible only when an instrument is idle or performing an action that is not part of the 100 considered triplet classes. A total absence of a triplet is marked null-instrument,null-verb,nulltarget , which is not an explicit class in a multi-label classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and discussion</head><p>In this section, we provide a quantitative and qualitative overview of all methods featured in the challenge, including baselines and models evaluated post-challenge. In total there are 24 models: 4 are baseline models from the challenge organizers, 19 are competing models and 1 is a post-challenge submission. The analysis of their results builds a foundation for the CholecT50 dataset, establishing it as a validated reference benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Summary of the quantitative results</head><p>For a concise overview, we first summarize the AP scores on both the component detection and triplet association in <ref type="table" target="#tab_5">Table  4</ref>. On the instrument component, half of the presented models achieved an AP score higher than 70% with the highest score of 82.1% by team SIAT-CAMI and the average performance (with standard deviation) of 62.5?18.9%. These results suggest the tremendous progress made in the use of deep learning models for surgical instrument recognition in laparoscopic videos, as 18 out of the 24 presented methods recognized the instruments at a performance higher than 50%. The verb recognition peaked at 52.9% AP with only four teams achieving a higher than 50% score. It can be observed that these four teams either leveraged temporal information or exploited phase labels, which is also a temporal task. This suggests a strong correlation between surgical phases and actions and the quality of temporal feature modeling in tracking activity workflow. Improving the verb performance is a promising direction for future work likely by exploring a better-conditioned range of temporal dependencies attuned to triplet timings. The bulk of the AP scores  Average and standard deviation (std) 62.5?18.9 37.7?12.41 30.2?11.0 26.3?8.9 26.5?11.3 23.3?9.9 bold = best score and underlined = second best. Not eligible for award: ? post-challenge submission, ? organizers' baselines, ? used non-public third-party dataset.</p><p>from the competing teams falls within 30-40% with a mean of 37.7?12.1%.</p><p>Target, being the most difficult component, was recognized at a maximum AP of 46.4% by team Trequartista. The low performance on this sub-task can be attributed to the instrumentcentric nature of the underlying target which makes its recognition very challenging. The mean performance at the challenge was 30.2?11.0%. As evidenced by the submissions, tackling surgical target recognition still is not straightforward.</p><p>On the association part, an interesting observation is that top-performing models recognized the instrument-target (AP IT ) pair better than they recognized the instrument-verb (AP IV ) pair, despite the larger number of classes for the former. Their drop in performance from AP T to AP IT is much lower than from AP V to AP IV . These deep learning models were more likely to recognize the correct operating instruments given an underlying target (a spatial relationship) than given their actions (a temporal relationship). The reverse is the case for lower-performing models. Here, it is easier to recognize instrument-verb pairs as most instruments perform specific actions and the verbs have a lesser number of classes than targets, and therefore their combinations (IV or IT). Many instruments can act on a wide range of targets -including unintended contact -making the problem more challenging for less advanced models. Overall these observations give a different perspective when interpreting the strength of the proposed models in understanding tool-tissue interactions. As shown in <ref type="table" target="#tab_5">Table 4</ref>, team SIAT-CAMI obtained the highest AP IT score while the baseline Rendezvous  still retained the state-of-the-art (SOTA) performance on AP IV . The complete triplet recognition was best achieved at an AP of 38.1% by team Trequartista topping the challenge leader-board. The second was a model submitted post-challenge by 2AI as an improvement of their challenge competing method. Meanwhile, team SIAT-CAMI claimed the runner-up prize with an AP of 35.8% while team HFUT-MedIA took the third-place prize with an AP of 32.9%. Three other teams (CITI-SJTU, ANL Triplet, and Digital Surgery) and the baseline Rendezvous achieved an AP higher than 30% which are promising performances for 100 class triplet recognition tasks. The mean performance for the triplet recognition recorded at the CholecTriplet 2021 challenge was 23.3?9.9% suggesting room for improvement on this challenging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">TopK accuracy on surgical action triplet recognition</head><p>Due to the large number of classes and the high semantic overlap in the triplet classes, we also evaluate the topK of the presented models. This metric measures the ability of a model to predict the exact triplets within its top K confidence scores. We analyze the top 5, 10, 15, 20, and average across these four thresholds, topK@[5:20] as shown in <ref type="table" target="#tab_6">Table 5</ref>. The obtained results describe the model's confidence in its predictions with the Rendezvous model obtaining a 69.35% accuracy score as the best model when the top 5 confident predictions are considered. Similarly, HFUT-MedIA, Attention Tripnet, and Tripnet models produce the best results at top 10, 15, and 20 confidence scores respectively.</p><p>On average, the Rendezvous model can correctly recognize the triplets at a performance of 84.23% when top confident predictions are taken into account. The average performance of all the presented models at the challenge is 53.1?18.3%. On this metric, it is not surprising to see the challenge winner, Trequartista, scoring lower in top K accuracy because the model uses a mathematical operation to suppress the less confident predictions. These low confidence scores, when taken into account by the AP metric, lower the performance of other models.</p><p>With topK focusing only on top confident predictions, the models are not penalized by their less confident predictions. This accuracy metric is highly informative and more usable when thresholding predictions to binary values to obtain the unique IDs of the predicted triplets. It also suggests that most of the presented models would ordinarily obtain higher scores with fewer triplet classes, less class similarity, and less semantic overlap. Meanwhile, the top K accuracy increases with the K tolerance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Per-class component detection AP</head><p>Beyond the broad overview of the ranked performances, we present a detailed analysis of per-class performance for each component task.</p><p>On surgical instrument presence detection, the most frequently used instruments, hook and grasper, are the most correctly detected, as shown in <ref type="table" target="#tab_7">Table 6</ref>. Their recognition APs are above 90% for half of the methods and their overall mean performances are above 82%. The suction irrigation device (irrigator) is only used when the field is unclear, resulting in a low usage frequency and mean performance of 15.2%. The scissors with the highest standard deviation of ?33.2 is the most complicated instrument to recognize as it often confounded with other instruments such as grasper, bipolar, and clipper. <ref type="table" target="#tab_8">Table 7</ref> presents the per-class performance for the verbs. The most frequently used verbs such as grasp, retract, dissect are detected above 50.0% by the top models and above 40% on average. Verbs such as dissect, coagulate, clip, cut, which have the strongest affinity with a particular instrument class, are detected above 70% by the top models and with a higher average challenge performance. This confirms that triplets are instrument-centric. The average performance for cut is approximately 50% of the top team score. This is likely affected by the low detection of the performing instrument, scissors. In cases where an instrument has multiple frequent verbs, the performance tends to spread out over those verbs according to their prevalence: for example retract ? grasp pack for grasper, aspirate irrigate for irrigator, etc. Irrigate is the least detected verb, likely due to its temporal nature as it can only be distinguished from aspirate based on the temporal dynamics of the fluid. Remarkably, team HFUT-MedIA leveraged graph convolution networks, notable for temporal action detection, to better detect this verb.</p><p>Finally, we analyze per-class performance on target recognition which appears to be the most challenging component to correctly detect. As shown in <ref type="table" target="#tab_9">Table 8</ref>, the gallbladder and specimen-bag are the most recognized targets, with the top models exceeding 90.0% AP. Their average performance across all methods is above 64.0%.</p><p>Other targets such as liver, cystic-duct, abdominal wall and cavity, are moderately detected. This is likely due to their obvious nature and clearer boundaries compared to the less detected ones. Interactions with those are easier to ascertain than interactions with much smaller structures such as cystic-artery and other blood-vessels. Within the cystic-pedicle, the cystic-duct is the most detected tubular structure. The cystic artery in itself is hard to differentiate from other blood vessels. This shows how deceptively complicated the task of anatomical target detection is. The peritoneum which covers the entire cavity appears as a transparent layer making it difficult to identify. The heavily super-classed omentum, peritoneum and gut are the least detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Result ranking stability</head><p>To measure the rank stability, we employ Wilcoxon signedrank test as a non-parametric alternative to the dependent samples t-test since our data is not multivariate normal and teams' predictions could present many outliers. Using this, we test each team's method against a null hypothesis (H 0 ) to ascertain the statistical significance of its performance over others. The    H 0 states that the difference between the proposed method and the alternative methods has a mean signed-rank of zero. Each H 0 test of one team against another produces a p-value between 0 and 1 as a measure of its level of statistical significance with a smaller value showing stronger evidence to reject the null hypothesis. Typically, a p ? 0.05 is considered statistically significant.</p><p>To perform this analysis, we sample N = 30 random batches of 100 consecutive frames to simulate a video clip evaluation and perform the Wilcoxon test by iteratively using each competing team as a proposed method against the rest of the teams as its alternatives. The obtained p-values, tabulated in Table 9, show that closely-ranked teams do not significantly improve each other methods (accept H 0 ), whereas distantly ranked teams show a significant difference in their model performances (reject H 0 ). At a 5% confidence level, we conclude that the CholecTriplet2021 challenge has assembled teams with both similar and diverse methods in both modeling and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Model ensemble results</head><p>Table 10 demonstrates that combining decisions from multiple models indeed helps to improve their overall performance. A simple averaging leads to an additional 0.8% gain in triplet recognition AP and +1.1% AP when the models' contributions are weighted by their individual strengths. Although an ensemble builds a strong learner from a group of weak learners, voting is surprisingly ineffective in this regard. In comparison to non-trainable alternatives, training ensemble techniques help to better minimize noise, bias, and variance errors, resulting in superior performances. Learning a new averaging weight appears more efficient than training totally new prediction models (Deep ensemble), as shown in <ref type="table" target="#tab_2">Table 10</ref>. Learning the model weights per task category rather than general class weights per model is even more fascinating and reaches the highest mAP of 42.4% for surgical action triplet recognition. In general, performance improvement with the model ensemble is obtained across all the six sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Qualitative results</head><p>To better analyze the quality of the detections, we visualize the top K predictions for each model on sample frames with K triplet instances. An easy example in <ref type="figure">Fig. 8</ref> showcases an image frame with a triplet of the best-recognized instrument (hook), the best recognized verb (dissect), and best-recognized target (gallbladder). This case is correctly detected by ? 88% of the teams. A moderately difficult case of a frame with multiple triplets recorded seven incorrect predictions. An image frame showing a single triplet involving a cystic-artery proves to be difficult for more than half of the teams. The incorrect prediction is mostly on the target component of the triplet.</p><p>Finally, we analyze the triplet recognition showing a sequential flow of tool-activity recognition. Here, we group all the triplets by their instruments and use a range of colors to show their variations and transitions in temporal order. The ground-truth flow shows the level of simplicity and complexity of many triplets with regard to their affinities with instruments. As shown in <ref type="figure">Fig. 11</ref>, some triplets can be easily inferred by the instrument information such as scissors (likely cutting either cystic-artery, cystic-duct, blood-vessels or some adhesion), clipper (likely clipping the respective anatomical targets). However, instruments such as grasper, hook, bipolar, etc, perform multiple triplets which are not easily deduced from instrument presence alone. The overcrowded coloring of grasper shows that it is used for various actions/targets and often for short sequences/intervals. To better understand the model's capacity to consistently approximate the ground-truth labels, we present their utilized methodologies in the first 8 columns. Even with the complexity of actions performed by the grasper, it is better recognized by top methods. Triplets with bipolar appear very complex to all the methods. A closer attempt is by team ANL-Triplet which leveraged temporal modeling.  <ref type="figure">Fig. 8</ref>. Qualitative results visualizing triplet predictions: a cross-section of teams' top k predictions on an input image depicting k action triplets: (a.) easy case, (b.) moderate case, and (c.) difficult case. ? = post-challenge submission, ? = organizers' baselines, ? = used non-public third-party dataset. green = incorrect prediction red = incorrect prediction. Triplets with the hook are less difficult, however, their transition is mostly unclear to all the models. Triplets with scissors are the best-approximated predictions due to their limited variability. Specifically, models utilizing attention and/or temporal modeling are the best at this triplet. The use of phase labels mimics methods utilizing temporal modeling. Triplets with the clipper are another class with limited variability and are closely approximated by most of the models. Triplets with the irrigator, although limited in variability, appear very confusing to all the presented methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Limitations</head><p>This challenge and analysis present several limitations, which should be addressed in future iterations. Firstly, as with most challenges, given that the participants are not constrained in terms of modeling, comparisons made between different modeling approaches must be treated as indications rather than facts. Participating submissions employed varying degrees of focus on components such as hyperparameter tuning, thus limiting their comparability. We also note that different submissions use a wide range of parameter counts, potentially due to resource constraints or research priorities, which could greatly affect model performance. Another limitation of this challenge was the enforcement of the causality constraint. Participants were asked to ensure that their submission only made use of past frames to make predictions; however, enforcing this at inference time is computationally impractical as each input image must be processed individually along with its entire temporal context to ensure that there is no leakage of information. Alternative strategies could have been to decide on a fixed and limited temporal context (n frames) that would be used to make a prediction or to allow acausal predictions. Bearing in mind that the value of these systems may lie in real-time systems, we opted to allow an unlimited past context and designed a hidden subset of the test set that was used to test causality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>As the finest-grained and most comprehensive description of surgical activities for computer vision, surgical action triplets carry significant clinical value. Before this challenge, however, this problem received little attention from the community; in that regard, CholecTriplet2021 was a success. With a recordhigh 19 submissions, three of which surpassed the state of the art, this event featured a diverse range of approaches, providing a solid methodological foundation for future research efforts. Most importantly, this challenge showed that surgical action triplet recognition remains an open challenge, with several promising directions to explore. The use of attention, already studied before CholecTriplet2021, can be expanded, as shown by several submissions. This challenge also saw the very first uses of graph convolutions and temporal models for surgical action triplets. Finally, future work should focus on refining the spatial aspect of this problem: locating action triplets in addition to simply detecting their presence would provide much richer information on the surgery. In that sense, full bounding box annotations would be a considerable step forward for research on tool-tissue interaction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>CholecTriplet2021 challenge timeline and activities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of Tripnet: a baseline method for action triplet recognition built on ResNet-18 backbone, weakly supervised localization (WSL) of surgical instruments, class activation guide (CAG) for verb-target recognition, and 3D interaction space (3DIS) for triplet association.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Overview of the Casia Robotics submission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Overview of the HFUT-MedIA submission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Overview of the SIAT CAMI submission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Ensemble method for the model predictions: (a) averaging, (b) weighted averaging, (c) majority soft voting, (d) deep ensemble, and (e) deep weighted ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.001 0.002 0.001 0.005 0.600 0.699 0.001 0.001 0.039 0.699 0.009 0.001 0.802 -0.374 0.096 0.023 0.001 J &amp; M 0.005 0.001 0.005 0.001 0.006 0.524 0.682 0.001 0.001 0.096 0.495 0.003 0.002 0.785 0.374 -0.2100 0.053 0.001 CITI-SJTU 0.001 0.001 0.053 0.001 0.151 0.080 0.053 0.001 0.001 0.399 0.101 0.119 0.001 0.219 0.096 0.210 -0.322 0.001 ANL-Triplet 0.001 0.001 0.452 0.001 0.374 0.018 0.001 0.001 0.001 0.509 0.009 0.399 0.001 0.011 0.023 0.053 0.322 -0.001 2Ai 0.001 0.624 0.001 0.001 0.001 0.001 0.001 0.009 -0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 Significant p-value ? 0.05 Not significant p-value &gt; 0.05</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1. Introduction &lt; bipolar, coagulate, liver &gt; &lt; grasper, retract, liver &gt; &lt;irrigator, aspirate, fluid &gt; &lt; grasper, retract, gallbladder &gt; &lt; hook, dissect, gallbladder &gt;Fig. 1. A cross-section of CholecT50 dataset for the CholecTriplet2021 challenge. Represented surgical action triplets are illustrated for different timepoints during a laparoscopic cholecystectomy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Demography of final participating teams. ? Some teams have affiliations in multiple countries.</figDesc><table><row><cell></cell><cell>Asia</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe</cell><cell></cell><cell></cell><cell>North America</cell></row><row><cell cols="9">China Singapore Japan India Germany UK Netherlands Portugal France</cell><cell>USA</cell></row><row><cell>8</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>3</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>A complete cross-view of the MICCAI EndoVis CholecTriplet 2021 participating teams including their affiliations and presented methodologies.</figDesc><table><row><cell>Team</cell><cell>Affiliation(s)</cell><cell>Method</cell></row><row><cell>1 2Ai</cell><cell>Applied Artificial Intelligence Laboratory, Portugal</cell><cell>Surgical video analysis using an ensemble of multi-task recurrent</cell></row><row><cell></cell><cell></cell><cell>convolutional networks (versions 1 &amp; 2)</cell></row><row><cell>2 ANL-Triplet</cell><cell>Argonne National Laboratory, USA</cell><cell>ANL-Triplet: Exploiting temporal information for triplet recognition</cell></row><row><cell cols="2">3 Band of Broeders Meander Medical Centre, The Netherlands</cell><cell>YoloV5 for surgical action triplet detection</cell></row><row><cell></cell><cell>Johnson &amp; Johnson, USA</cell><cell></cell></row><row><cell>4 CAMMA</cell><cell>University of Strasbourg, France (Organizers)</cell><cell>MTL Baseline, Tripnet, Attention Tripnet, and Rendezvous</cell></row><row><cell>5 CAMP</cell><cell>Technical University of Munich, Germany</cell><cell>EndoVisNet: Phase-guided temporal endoscopic action triplet</cell></row><row><cell></cell><cell></cell><cell>classification</cell></row><row><cell>6 Casia Robotics</cell><cell cols="2">Institute of Automation, Chinese Academy of Sciences, China Triplet translation embedding network with coordinate attention</cell></row><row><cell>7 Ceaiik</cell><cell>Indian Institute of Technology Kharagpur, India</cell><cell>Spatio-temporal learning of action triplets in surgical videos</cell></row><row><cell>8 CITI SJTU</cell><cell>Shanghai Jiao Tong University, China</cell><cell>Action triplet recognition via convolutional LSTMs and multi-task</cell></row><row><cell></cell><cell></cell><cell>learning</cell></row><row><cell>9 Digital Surgery</cell><cell>Digital Surgery, a Medtronic Company, London, UK</cell><cell>TE-TAR: Temporal ensemble triplet action recognition</cell></row><row><cell></cell><cell>Wellcome/EPSRC Center for Interventional and Surgical</cell><cell></cell></row><row><cell></cell><cell>Science, University College London, UK</cell><cell></cell></row><row><cell>10 Lsgroup</cell><cell>Xiamen University, China</cell><cell>Feature fusion and weak locational information calculating in triplet</cell></row><row><cell></cell><cell></cell><cell>classification multi-task</cell></row><row><cell>11 HFUT-MedIA</cell><cell>Hefei University of Technology, China</cell><cell>COEMNet: Correlation embedded multi-task network</cell></row><row><cell>12 HFUT-NUS</cell><cell>National University of Singapore, Singapore</cell><cell>Multi-task learning based surgical interaction triplet recognition</cell></row><row><cell></cell><cell>Heifei University of Technology, China</cell><cell></cell></row><row><cell>13 J&amp;M</cell><cell>Johnson &amp; Johnson, US</cell><cell>Surgical action triplet recognition with Efficient-MSTCN</cell></row><row><cell></cell><cell>Meander Medical Centre, The Netherlands</cell><cell></cell></row><row><cell cols="2">14 Med Recognizer Chinese University of Hong Kong, China</cell><cell>Surgical action triplet recognition via temporal memory relation</cell></row><row><cell></cell><cell></cell><cell>gradient network</cell></row><row><cell>15 MMLAB</cell><cell>National University of Singapore, Singapore</cell><cell>Temporal triplet net for triplet presence detection in surgical videos</cell></row><row><cell>16 NCT-TSO</cell><cell>National Center for Tumor Diseases</cell><cell>Multi-task learning framework for action triplet recognition</cell></row><row><cell></cell><cell>Partner Site Dresden, Germany</cell><cell></cell></row><row><cell>17 SIAT CAMI</cell><cell>Shenzhen Institute of Advanced Technology,</cell><cell>Multi-task mutual channel recurrent net for fine-grained surgical</cell></row><row><cell></cell><cell>Chinese Academy of Sciences, China</cell><cell>triplet recognition</cell></row><row><cell>18 SJTU-IMR</cell><cell>Shanghai Jiao Tong University, China</cell><cell>Tracking surgical actions with transformers and action label-guided</cell></row><row><cell></cell><cell></cell><cell>fine-grained information aggregation</cell></row><row><cell>19 SK</cell><cell>Muroran Institute of Technology, Japan</cell><cell>Action triplet recognition with weakly-supervised attention of</cell></row><row><cell></cell><cell></cell><cell>surgical instruments</cell></row><row><cell>20 Trequartista</cell><cell>University of Chicago, USA</cell><cell>Phase-aware multitasking action recognition model with adjustment</cell></row><row><cell></cell><cell></cell><cell>for low-data triplet classes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Methodological and implementation details.</figDesc><table><row><cell>Team</cell><cell cols="2">Architecture Backbone</cell><cell cols="2">Multi-task Temporal</cell><cell>Attention</cell><cell>Output</cell><cell>Post-</cell><cell>Data</cell><cell>Transfer</cell><cell cols="2">Optimizer Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>component</cell><cell></cell><cell>components</cell><cell>processing</cell><cell>augmentation</cell><cell>learning</cell><cell></cell><cell></cell></row><row><cell>Trequartista</cell><cell>CNN</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>N/A</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>63 rare triplets</cell><cell>Fast AI default</cell><cell>Cholec80</cell><cell>Adam</cell><cell>Focal Loss +</cell></row><row><cell></cell><cell></cell><cell>ResNet34</cell><cell>verb</cell><cell></cell><cell></cell><cell>for phases,</cell><cell>inferred using</cell><cell>image augmentation</cell><cell>phase</cell><cell></cell><cell>pair Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>Instruments, verbs,</cell><cell>ad-hoc formula.</cell><cell></cell><cell></cell><cell></cell><cell>focusing on</cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>targets, triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ranking</cell></row><row><cell></cell><cell></cell><cell></cell><cell>phase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2AI</cell><cell>CNN ensemble-</cell><cell>EfficientNetB0</cell><cell>Instrument</cell><cell>LSTM</cell><cell>None</cell><cell>Probability vectors</cell><cell>Temporal</cell><cell>crop, horizontal flip;</cell><cell>Cholec80</cell><cell>SGD</cell><cell>CE (Cross-</cell></row><row><cell></cell><cell>LSTM</cell><cell>EfficientNetB4</cell><cell>verb</cell><cell></cell><cell></cell><cell>for phases,</cell><cell>smoothing</cell><cell>color jitter, 10 deg.</cell><cell>phase</cell><cell></cell><cell>entropy)</cell></row><row><cell></cell><cell></cell><cell>ResNet50</cell><cell>target</cell><cell></cell><cell></cell><cell>instruments, verbs,</cell><cell></cell><cell>rotation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ResNest50</cell><cell>phase</cell><cell></cell><cell></cell><cell>targets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ResNest101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>SENet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIAT-CAMI</cell><cell>CNN-LSTM</cell><cell>ResNet50</cell><cell>Instrument</cell><cell>LSTM</cell><cell>Channel-wise</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>crop,</cell><cell>N/A</cell><cell>Adam</cell><cell>Mutual</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell>attention</cell><cell>for Instruments,</cell><cell></cell><cell>horizontal flip,</cell><cell></cell><cell></cell><cell>channel loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell>rotation, color jitter</cell><cell></cell><cell></cell><cell>+ CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HFUT-MedIA</cell><cell>CNN-GCN</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>N/A</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>rotation, flip, color</cell><cell>Imagenet</cell><cell>AdamW</cell><cell>Weighted</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell></cell><cell>for triplets</cell><cell></cell><cell>jitter, gaussian blur,</cell><cell></cell><cell></cell><cell>CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>erasing, cutmix</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RDV</cell><cell>CNN-CAGAM-</cell><cell>ResNet18</cell><cell>instrument</cell><cell>N/A</cell><cell>Spatial</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>Random scaling,</cell><cell>Imagenet</cell><cell>Momentum</cell><cell>Weighted CE</cell></row><row><cell></cell><cell>MHMA</cell><cell></cell><cell>verb</cell><cell></cell><cell>(CAGAM) +</cell><cell>for Instruments,</cell><cell></cell><cell>brightness</cell><cell></cell><cell>SGD</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell>semantic</cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell>(MHMA)</cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CITI-SJTU</cell><cell cols="2">CNN-ConvLSTM ResNet50</cell><cell>Instrument</cell><cell>ConvLSTM</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>horizontal flip,</cell><cell>Imagenet</cell><cell>SGD</cell><cell>Weighted CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell></cell><cell>for Instruments,</cell><cell></cell><cell>rotation, color jitter</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ANL Triplet</cell><cell>CNN</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>Previous 4</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>Temporal</cell><cell>FastAI</cell><cell>Unspecified</cell><cell>AdamW</cell><cell>CE</cell></row><row><cell></cell><cell></cell><cell>ResNet34</cell><cell>verb</cell><cell>frames injection</cell><cell></cell><cell>for Instruments,</cell><cell>averaging</cell><cell></cell><cell>pretraining for</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell>Resnet34</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Digital Surgery</cell><cell>CNN ensemble-</cell><cell>HRNet32</cell><cell>N/A</cell><cell>LSTM</cell><cell>N/A</cell><cell>Probability vector</cell><cell>N/A</cell><cell>Illumination, color,</cell><cell>Imagenet</cell><cell>Adam</cell><cell>Smoothed</cell></row><row><cell></cell><cell>LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>for triplets</cell><cell></cell><cell>blur, noise</cell><cell></cell><cell></cell><cell>NLL +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>flattened CE</cell></row><row><cell>Casia Robotics</cell><cell>CNN</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>N/A</cell><cell>Coordinate</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>rotation, flip, patch</cell><cell>N/A</cell><cell>Adam</cell><cell>CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell>attention</cell><cell>for Instruments,</cell><cell></cell><cell>masking</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSGroup</cell><cell>CNN (4</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>Previous frame</cell><cell>Class Active</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>horizontal flip</cell><cell>Imagenet.</cell><cell>SGD</cell><cell>CE</cell></row><row><cell></cell><cell>subnetworks)</cell><cell></cell><cell>verb</cell><cell>injection</cell><cell>Mapping</cell><cell>for triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>J&amp;M</cell><cell>CNN-TCN</cell><cell>EfficientNetV2-</cell><cell>N/A</cell><cell>MS-TCN / MS-</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>N/A</cell><cell>Imagenet</cell><cell>SGD</cell><cell>CE</cell></row><row><cell></cell><cell></cell><cell>M</cell><cell></cell><cell>TCN++</cell><cell></cell><cell>for triplets</cell><cell></cell><cell></cell><cell></cell><cell>(EfficientNetV</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2-M), Adam</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(MS-TCN)</cell><cell></cell></row><row><cell cols="2">Attention Tripnet CNN-CAGAM-</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>N/A</cell><cell>Spatial</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>Flip, brightness</cell><cell>Imagenet</cell><cell>Momentum</cell><cell>Weighted CE</cell></row><row><cell></cell><cell>3DIS</cell><cell></cell><cell>verb</cell><cell></cell><cell>(CAGAM)</cell><cell>for Instruments,</cell><cell></cell><cell></cell><cell></cell><cell>SGD</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CEAIIK</cell><cell>CNN-LSTM</cell><cell>ResNet50</cell><cell>instrument</cell><cell>LSTM</cell><cell>N/A</cell><cell>Probability vector</cell><cell>N/A</cell><cell>Blur, brightness,</cell><cell>N/A</cell><cell>Adam</cell><cell>Multi-label</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell></cell><cell>for triplets</cell><cell></cell><cell>contrast, rotation,</cell><cell></cell><cell></cell><cell>soft margin</cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sun flare</cell><cell></cell><cell></cell><cell>loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SJTU-IMR</cell><cell>2-stage</cell><cell>Swin-S</cell><cell>instrument</cell><cell>Masked</cell><cell>Transformer</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>crop, flip, color shift,</cell><cell>N/A</cell><cell>AdamW</cell><cell>CE</cell></row><row><cell></cell><cell>Transformer</cell><cell></cell><cell>verb</cell><cell>transformers</cell><cell>multi-head</cell><cell>for triplets</cell><cell></cell><cell>temporal warp</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell>(stage 2)</cell><cell>self-attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tripnet</cell><cell cols="2">CNN-CAG-3DIS ResNet18</cell><cell>instrument</cell><cell>N/A</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>Invalid triplet</cell><cell>Flip, brightness</cell><cell>Imagenet</cell><cell>Momentum</cell><cell>Weighted CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell></cell><cell>for Instruments,</cell><cell>masking</cell><cell></cell><cell></cell><cell>SGD</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SK</cell><cell>CNN</cell><cell>Resnet18</cell><cell>Instrument</cell><cell>N/A</cell><cell>Instrument</cell><cell>Probability vector</cell><cell>N/A</cell><cell>Shift, scaling,</cell><cell>Imagenet</cell><cell>Adam</cell><cell>Weighted CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell>maps</cell><cell>for instruments,</cell><cell></cell><cell>rotation, color jitter,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell>blur, noise, crop</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMLAB</cell><cell cols="2">CNN-Graph conv DenseNet121</cell><cell>No</cell><cell>Previous 4</cell><cell>N/A</cell><cell>Probability vector</cell><cell>N/A</cell><cell>Horizontal flip</cell><cell>Imagenet.</cell><cell>SGD</cell><cell>CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>frames injection</cell><cell></cell><cell>for triplets</cell><cell></cell><cell></cell><cell></cell><cell>(DenseNet),</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Adam (GCN)</cell><cell></cell></row><row><cell cols="2">Band of Broeders YOLOv5 object</cell><cell cols="2">CSP-DenseNet Triplet</cell><cell>N/A</cell><cell>N/A</cell><cell>Bounding boxes,</cell><cell>Overlapping</cell><cell>YOLOv5</cell><cell>COCO</cell><cell>SGD</cell><cell>CE</cell></row><row><cell></cell><cell>detector</cell><cell></cell><cell>detection &amp;</cell><cell></cell><cell></cell><cell>triplet class</cell><cell>bounding</cell><cell>augmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>localization</cell><cell></cell><cell></cell><cell>probabilities,</cell><cell>boxes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>objectness</cell><cell>suppression</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>confidence scores</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MTL baseline</cell><cell>CNN</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>N/A</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>Flip, brightness</cell><cell>Imagenet</cell><cell>Momentum</cell><cell>Weighted CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell></cell><cell>for Instruments,</cell><cell></cell><cell></cell><cell></cell><cell>SGD</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NCT-TSO</cell><cell>CNN</cell><cell>ResNet50</cell><cell>Target</cell><cell>N/A</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>Invalid triplet</cell><cell>N/A</cell><cell>Cholec80</cell><cell>Adam</cell><cell>Weighted CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>triplet</cell><cell></cell><cell></cell><cell>for triplets</cell><cell>masking</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HFUT-NUS</cell><cell>CNN</cell><cell>ResNet18</cell><cell>Instrument</cell><cell>N/A</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>Horizontal flip,</cell><cell>N/A</cell><cell>Adam</cell><cell>Weighted CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell></cell><cell>for Instruments,</cell><cell></cell><cell>rotation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAMP</cell><cell>CNN</cell><cell>SlowFast50</cell><cell>Instrument,</cell><cell>SlowFast</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>Scale, crop,</cell><cell>Training on</cell><cell>Adam</cell><cell>Weighted CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verb</cell><cell></cell><cell></cell><cell>for Instruments,</cell><cell></cell><cell>horizontal flip</cell><cell>Phase labels</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell></cell><cell></cell><cell>verbs, targets,</cell><cell></cell><cell></cell><cell>from Cholec80</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>phase</cell><cell></cell><cell></cell><cell>triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Med Recognizer</cell><cell>SVRCNet-</cell><cell>ResNet50</cell><cell>Instrument</cell><cell>LSTM, Memory</cell><cell>N/A</cell><cell>Probability vectors</cell><cell>N/A</cell><cell>Crop. flip</cell><cell>Cholec80</cell><cell>SGD</cell><cell>CE</cell></row><row><cell></cell><cell>TMRNet</cell><cell></cell><cell>verb</cell><cell>bank, temporal</cell><cell></cell><cell>for triplets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>target</cell><cell>variation layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance summary of the presented methods across all task divisions</figDesc><table><row><cell></cell><cell></cell><cell>Component detection</cell><cell></cell><cell></cell><cell>Triplet association</cell><cell></cell><cell>Challenge</cell></row><row><cell>Team</cell><cell>AP I</cell><cell>AP V</cell><cell>AP T</cell><cell>AP IV</cell><cell>AP IT</cell><cell>AP IVT</cell><cell>ranking</cell></row><row><cell>Trequartista</cell><cell>79.9</cell><cell>52.9</cell><cell>46.4</cell><cell>39.0</cell><cell>41.9</cell><cell>38.1</cell><cell>1</cell></row><row><cell>2AI: Version 2  ?</cell><cell>79.8</cell><cell>50.1</cell><cell>42.8</cell><cell>35.2</cell><cell>42.4</cell><cell>36.9</cell><cell>-</cell></row><row><cell>SIAT CAMI</cell><cell>82.1</cell><cell>51.5</cell><cell>45.5</cell><cell>37.1</cell><cell>43.1</cell><cell>35.8</cell><cell>2</cell></row><row><cell>HFUT-MedIA</cell><cell>77.1</cell><cell>46.7</cell><cell>37.8</cell><cell>33.1</cell><cell>35.9</cell><cell>32.9</cell><cell>3</cell></row><row><cell>RDV (CAMMA)  ?</cell><cell>77.5</cell><cell>47.5</cell><cell>37.7</cell><cell>39.4</cell><cell>39.6</cell><cell>32.7</cell><cell>-</cell></row><row><cell>CITI SJTU</cell><cell>67.8</cell><cell>37.1</cell><cell>34.8</cell><cell>29.9</cell><cell>33.0</cell><cell>32.0</cell><cell>4</cell></row><row><cell>ANL Triplet</cell><cell>73.6</cell><cell>47.3</cell><cell>40.5</cell><cell>32.6</cell><cell>37.1</cell><cell>31.9</cell><cell>5</cell></row><row><cell>Digital Surgery</cell><cell>80.8</cell><cell>50.0</cell><cell>41.1</cell><cell>35.1</cell><cell>35.7</cell><cell>31.7</cell><cell>6</cell></row><row><cell>Casia Robotics</cell><cell>72.6</cell><cell>43.9</cell><cell>31.2</cell><cell>30.7</cell><cell>30.6</cell><cell>26.7</cell><cell>7</cell></row><row><cell>Lsgroup</cell><cell>73.8</cell><cell>44.3</cell><cell>34.9</cell><cell>31.4</cell><cell>31.9</cell><cell>26.3</cell><cell>8</cell></row><row><cell>J&amp;M</cell><cell>69.4</cell><cell>46.7</cell><cell>39.2</cell><cell>28.9</cell><cell>28.8</cell><cell>25.6</cell><cell>9</cell></row><row><cell>Attention-Tripnet (CAMMA)  ?</cell><cell>77.1</cell><cell>43.4</cell><cell>30.0</cell><cell>32.3</cell><cell>29.7</cell><cell>25.5</cell><cell>-</cell></row><row><cell>Ceaiik</cell><cell>68.9</cell><cell>40.5</cell><cell>30.9</cell><cell>27.5</cell><cell>28.4</cell><cell>25.2</cell><cell>10</cell></row><row><cell>SJTU-IMR</cell><cell>72.6</cell><cell>42.5</cell><cell>34.1</cell><cell>29.2</cell><cell>26.4</cell><cell>24.8</cell><cell>11</cell></row><row><cell>Tripnet (CAMMA)  ?</cell><cell>74.6</cell><cell>42.9</cell><cell>32.2</cell><cell>27.0</cell><cell>28.0</cell><cell>23.4</cell><cell>-</cell></row><row><cell>SK</cell><cell>52.6</cell><cell>30.4</cell><cell>20.2</cell><cell>25.8</cell><cell>21.0</cell><cell>18.4</cell><cell>12</cell></row><row><cell>MMLAB</cell><cell>50.1</cell><cell>31.8</cell><cell>31.6</cell><cell>20.6</cell><cell>22.1</cell><cell>18.1</cell><cell>13</cell></row><row><cell>Band of Broeders  ?</cell><cell>63.4</cell><cell>35.2</cell><cell>26.2</cell><cell>19.7</cell><cell>18.6</cell><cell>16.0</cell><cell>-</cell></row><row><cell>MTL baseline (CAMMA)  ?</cell><cell>48.6</cell><cell>27.8</cell><cell>19.8</cell><cell>18.5</cell><cell>15.5</cell><cell>13.7</cell><cell>-</cell></row><row><cell>NCT-TSO</cell><cell>27.3</cell><cell>15.7</cell><cell>12.3</cell><cell>13.6</cell><cell>11.7</cell><cell>10.4</cell><cell>14</cell></row><row><cell>2AI: Version 1</cell><cell>46.2</cell><cell>24.4</cell><cell>20.5</cell><cell>13.3</cell><cell>12.3</cell><cell>10.0</cell><cell>15</cell></row><row><cell>HFUT-NUS</cell><cell>34.1</cell><cell>20.2</cell><cell>13.5</cell><cell>16.0</cell><cell>11.2</cell><cell>09.8</cell><cell>16</cell></row><row><cell>CAMP</cell><cell>30.4</cell><cell>19.5</cell><cell>11.8</cell><cell>13.2</cell><cell>09.7</cell><cell>09.3</cell><cell>17</cell></row><row><cell>Med Recognizer</cell><cell>20.6</cell><cell>12.8</cell><cell>10.1</cell><cell>07.0</cell><cell>04.5</cell><cell>04.2</cell><cell>18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Top K accuracy of the triplet predictions</figDesc><table><row><cell>Team</cell><cell>Top 5</cell><cell>Top 10</cell><cell>Top 15</cell><cell cols="2">Top 20 Top {5:20}</cell></row><row><cell>RDV  ?</cell><cell>69.35</cell><cell>84.38</cell><cell>89.93</cell><cell>93.24</cell><cell>84.23</cell></row><row><cell>Tripnet  ?</cell><cell>67.89</cell><cell>83.99</cell><cell>90.76</cell><cell>93.65</cell><cell>84.07</cell></row><row><cell>HFUT-MedIA</cell><cell>65.05</cell><cell>85.35</cell><cell>91.75</cell><cell>93.59</cell><cell>83.94</cell></row><row><cell>Attention-Tripnet  ?</cell><cell>66.86</cell><cell>82.49</cell><cell>91.85</cell><cell>93.25</cell><cell>83.61</cell></row><row><cell>Trequartista</cell><cell>68.50</cell><cell>82.40</cell><cell>88.24</cell><cell>92.29</cell><cell>82.86</cell></row><row><cell>Ceaiik</cell><cell>66.02</cell><cell>81.34</cell><cell>89.74</cell><cell>93.40</cell><cell>82.63</cell></row><row><cell>Digital-Surgery</cell><cell>65.97</cell><cell>81.56</cell><cell>88.78</cell><cell>92.92</cell><cell>82.31</cell></row><row><cell>HFUT-NUS</cell><cell>65.71</cell><cell>84.18</cell><cell>88.68</cell><cell>90.43</cell><cell>82.25</cell></row><row><cell>SIAT-CAMI</cell><cell>66.58</cell><cell>81.93</cell><cell>88.59</cell><cell>91.84</cell><cell>82.24</cell></row><row><cell>SJTU-IMR</cell><cell>66.50</cell><cell>81.88</cell><cell>84.19</cell><cell>84.89</cell><cell>79.37</cell></row><row><cell>ANL-Triplet</cell><cell>52.12</cell><cell>83.65</cell><cell>89.19</cell><cell>91.37</cell><cell>79.08</cell></row><row><cell>CITI-SJTU</cell><cell>54.95</cell><cell>78.61</cell><cell>88.96</cell><cell>92.41</cell><cell>78.73</cell></row><row><cell>SK</cell><cell>48.08</cell><cell>79.46</cell><cell>90.41</cell><cell>92.12</cell><cell>77.52</cell></row><row><cell>2AI: Version 2  ?</cell><cell>64.87</cell><cell>76.17</cell><cell>82.54</cell><cell>86.26</cell><cell>77.46</cell></row><row><cell>Casia-Robotics</cell><cell>59.11</cell><cell>75.03</cell><cell>84.44</cell><cell>90.41</cell><cell>77.25</cell></row><row><cell>MMLAB</cell><cell>60.53</cell><cell>76.57</cell><cell>82.72</cell><cell>86.67</cell><cell>76.62</cell></row><row><cell>Lsgroup</cell><cell>62.88</cell><cell>73.03</cell><cell>78.64</cell><cell>81.98</cell><cell>74.13</cell></row><row><cell>J&amp;M</cell><cell>56.09</cell><cell>66.36</cell><cell>72.36</cell><cell>76.90</cell><cell>67.93</cell></row><row><cell>MTL-baseline  ?</cell><cell>45.59</cell><cell>52.45</cell><cell>56.65</cell><cell>59.77</cell><cell>53.62</cell></row><row><cell>Med-Recognizer</cell><cell>30.26</cell><cell>43.69</cell><cell>58.24</cell><cell>68.05</cell><cell>50.06</cell></row><row><cell>Band-of-Broeders  ?</cell><cell>39.86</cell><cell>40.34</cell><cell>41.14</cell><cell>48.27</cell><cell>42.40</cell></row><row><cell>2AI</cell><cell>29.44</cell><cell>30.18</cell><cell>32.11</cell><cell>41.14</cell><cell>33.22</cell></row><row><cell>CAMP</cell><cell>08.73</cell><cell>17.91</cell><cell>21.25</cell><cell>25.71</cell><cell>18.40</cell></row><row><cell>NCT-TSO</cell><cell>04.88</cell><cell>11.78</cell><cell>16.59</cell><cell>21.93</cell><cell>13.80</cell></row><row><cell cols="6">Average and standard deviation 53.1?18.3 67.5?22.3 73.9?23.3 77.9?22.0 68.1?21.2</cell></row></table><note>bold = best score and underlined = second best. Not eligible for award: ? organizers' baselines, ? post-challenge submission, ? used non-public third-party dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Per-class performance on instrument presence detection</figDesc><table><row><cell>Team</cell><cell cols="2">Grasper Bipolar Hook Scissors Clipper Irrigator Mean</cell></row><row><cell>SIAT CAMI</cell><cell>95.8 92.8 97.5 94.9 81.1 28.7</cell><cell>82.1</cell></row><row><cell>Digital Surgery</cell><cell>94.9 94.2 98.4 92.8 85.7 16.6</cell><cell>80.8</cell></row><row><cell>Trequartista</cell><cell>95.1 91.3 98.1 86.3 81.5 25.4</cell><cell>79.9</cell></row><row><cell>2AI: Version 2  ?</cell><cell>96.8 88.2 98.3 88.4 81.5 23.5</cell><cell>79.8</cell></row><row><cell>RDV  ?</cell><cell>95.1 90.1 98.2 89.0 79.5 10.6</cell><cell>77.5</cell></row><row><cell>HFUT-MedIA</cell><cell>93.0 83.1 95.9 84.7 81.4 22.2</cell><cell>77.1</cell></row><row><cell>Attention Tripnet  ?</cell><cell>95.4 87.9 98.6 88.5 78.8 10.8</cell><cell>77.1</cell></row><row><cell>Tripnet  ?</cell><cell>86.7 82.3 97.6 79.4 80.3 19.4</cell><cell>74.6</cell></row><row><cell>Lsgroup</cell><cell>91.6 85.3 96.8 76.3 76.5 14.0</cell><cell>73.8</cell></row><row><cell>ANL Triplet</cell><cell>88.3 68.6 96.6 84.0 82.4 19.8</cell><cell>73.6</cell></row><row><cell>Casia Robotics</cell><cell>92.0 88.2 97.7 67.7 72.1 15.7</cell><cell>72.6</cell></row><row><cell>SJTU-IMR</cell><cell>85.7 89.7 97.4 65.7 76.6 18.7</cell><cell>72.6</cell></row><row><cell>J&amp;M</cell><cell>91.7 72.9 96.6 48.4 76.7 28.7</cell><cell>69.4</cell></row><row><cell>Ceaiik</cell><cell>88.1 84.9 98.0 52.5 66.8 21.1</cell><cell>68.9</cell></row><row><cell>CITI SJTU</cell><cell>92.4 92.1 66.6 62.7 78.4 12.6</cell><cell>67.8</cell></row><row><cell>Band of Broeders  ?</cell><cell>91.6 55.5 94.2 66.8 66.8 03.6</cell><cell>63.4</cell></row><row><cell>SK</cell><cell>57.3 72.5 30.6 61.5 70.6 22.3</cell><cell>52.6</cell></row><row><cell>MMLAB</cell><cell>86.9 44.2 88.8 28.6 31.0 19.5</cell><cell>50.1</cell></row><row><cell>MTL baseline  ?</cell><cell>81.5 58.9 93.2 13.8 37.8 04.4</cell><cell>48.6</cell></row><row><cell>2AI: Version 1</cell><cell>83.1 57.7 91.4 11.7 28.1 03.5</cell><cell>46.2</cell></row><row><cell>HFUT-NUS</cell><cell>49.8 80.0 58.3 03.3 06.5 05.9</cell><cell>34.1</cell></row><row><cell>CAMP</cell><cell>61.2 15.6 72.0 03.5 20.4 08.9</cell><cell>30.4</cell></row><row><cell>NCT-TSO</cell><cell>39.0 81.7 25.7 05.1 05.6 05.7</cell><cell>27.3</cell></row><row><cell>Med Recognizer</cell><cell>56.3 09.2 47.0 03.0 04.1 03.3</cell><cell>20.6</cell></row><row><cell cols="3">Average and standard deviation 82.9?16.7 73.6?23.1 84.7?22.4 56.6?33.2 60.4?28.4 15.2?8.1 62.5?18.9</cell></row></table><note>bold = best score and underlined = second best. Not eligible for award: ? organizers' baselines, ? post-challenge submission, ? used non-public third-party dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Per-class performance on verb recognition.</figDesc><table><row><cell>Team</cell><cell>Grasp</cell><cell>Retract</cell><cell cols="2">Dissect Coagulate</cell><cell>Clip</cell><cell>Cut</cell><cell cols="2">Aspirate Irrigate</cell><cell>Pack</cell><cell>Null</cell><cell>Mean</cell></row><row><cell>Trequartista</cell><cell>54.0</cell><cell>55.5</cell><cell>79.6</cell><cell>70.6</cell><cell>80.7</cell><cell>81.6</cell><cell>20.9</cell><cell>01.8</cell><cell>48.8</cell><cell>30.2</cell><cell>52.9</cell></row><row><cell>SIAT CAMI</cell><cell>56.0</cell><cell>46.7</cell><cell>69.2</cell><cell>72.8</cell><cell>81.2</cell><cell>74.2</cell><cell>28.1</cell><cell>01.9</cell><cell>48.7</cell><cell>31.7</cell><cell>51.5</cell></row><row><cell>2AI: Version 2  ?</cell><cell>56.0</cell><cell>46.8</cell><cell>78.3</cell><cell>68.5</cell><cell>80.6</cell><cell>72.8</cell><cell>17.6</cell><cell>01.8</cell><cell>44.8</cell><cell>28.6</cell><cell>50.1</cell></row><row><cell>Digital Surgery</cell><cell>53.2</cell><cell>45.7</cell><cell>68.3</cell><cell>70.6</cell><cell>86.2</cell><cell>74.1</cell><cell>18.4</cell><cell>03.0</cell><cell>49.3</cell><cell>26.4</cell><cell>50.0</cell></row><row><cell>RDV  ?</cell><cell>52.4</cell><cell>48.5</cell><cell>73.2</cell><cell>69.5</cell><cell>80.2</cell><cell>70.5</cell><cell>09.9</cell><cell>01.2</cell><cell>37.1</cell><cell>28.2</cell><cell>47.5</cell></row><row><cell>ANL Triplet</cell><cell>44.6</cell><cell>60.7</cell><cell>73.7</cell><cell>62.2</cell><cell>82.4</cell><cell>69.1</cell><cell>11.4</cell><cell>00.9</cell><cell>37.0</cell><cell>26.9</cell><cell>47.3</cell></row><row><cell>HFUT-MedIA</cell><cell>56.8</cell><cell>41.8</cell><cell>68.2</cell><cell>64.0</cell><cell>81.6</cell><cell>67.2</cell><cell>17.2</cell><cell>16.2</cell><cell>20.4</cell><cell>30.3</cell><cell>46.7</cell></row><row><cell>J&amp;M</cell><cell>47.4</cell><cell>44.9</cell><cell>75.0</cell><cell>71.6</cell><cell>77.9</cell><cell>36.4</cell><cell>37.8</cell><cell>11.2</cell><cell>37.7</cell><cell>24.3</cell><cell>46.7</cell></row><row><cell>Lsgroup</cell><cell>50.3</cell><cell>46.0</cell><cell>73.5</cell><cell>65.1</cell><cell>76.6</cell><cell>63.1</cell><cell>17.3</cell><cell>00.5</cell><cell>21.1</cell><cell>25.3</cell><cell>44.3</cell></row><row><cell>Casia Robotics</cell><cell>46.8</cell><cell>43.7</cell><cell>72.1</cell><cell>65.8</cell><cell>71.2</cell><cell>63.8</cell><cell>12.0</cell><cell>08.4</cell><cell>23.5</cell><cell>27.8</cell><cell>43.9</cell></row><row><cell>Attention Tripnet  ?</cell><cell>53.2</cell><cell>39.4</cell><cell>71.4</cell><cell>65.1</cell><cell>79.2</cell><cell>68.4</cell><cell>09.1</cell><cell>02.8</cell><cell>18.1</cell><cell>22.9</cell><cell>43.4</cell></row><row><cell>Tripnet  ?</cell><cell>48.9</cell><cell>48.0</cell><cell>70.2</cell><cell>67.5</cell><cell>79.4</cell><cell>60.2</cell><cell>19.6</cell><cell>00.9</cell><cell>08.7</cell><cell>21.5</cell><cell>42.9</cell></row><row><cell>SJTU-IMR</cell><cell>57.6</cell><cell>47.1</cell><cell>74.8</cell><cell>69.9</cell><cell>76.6</cell><cell>43.0</cell><cell>14.5</cell><cell>00.4</cell><cell>10.4</cell><cell>26.1</cell><cell>42.5</cell></row><row><cell>Ceaiik</cell><cell>50.0</cell><cell>43.5</cell><cell>75.4</cell><cell>61.7</cell><cell>66.1</cell><cell>40.8</cell><cell>14.1</cell><cell>03.3</cell><cell>23.1</cell><cell>23.1</cell><cell>40.5</cell></row><row><cell>CITI SJTU</cell><cell>54.5</cell><cell>45.2</cell><cell>72.9</cell><cell>66.9</cell><cell>67.0</cell><cell>04.2</cell><cell>12.1</cell><cell>00.5</cell><cell>20.6</cell><cell>23.7</cell><cell>37.1</cell></row><row><cell>Band of Broeders  ?</cell><cell>44.1</cell><cell>35.9</cell><cell>68.9</cell><cell>55.0</cell><cell>67.9</cell><cell>52.0</cell><cell>02.7</cell><cell>00.4</cell><cell>01.2</cell><cell>20.0</cell><cell>35.2</cell></row><row><cell>MMLAB</cell><cell>49.0</cell><cell>38.0</cell><cell>61.0</cell><cell>43.3</cell><cell>32.4</cell><cell>19.1</cell><cell>11.5</cell><cell>03.9</cell><cell>33.8</cell><cell>23.4</cell><cell>31.8</cell></row><row><cell>SK</cell><cell>47.4</cell><cell>23.0</cell><cell>53.3</cell><cell>50.8</cell><cell>70.7</cell><cell>02.3</cell><cell>17.9</cell><cell>00.5</cell><cell>12.7</cell><cell>22.7</cell><cell>30.4</cell></row><row><cell>MTL baseline  ?</cell><cell>37.7</cell><cell>38.7</cell><cell>66.6</cell><cell>47.2</cell><cell>39.7</cell><cell>12.5</cell><cell>05.0</cell><cell>00.3</cell><cell>09.3</cell><cell>17.9</cell><cell>27.8</cell></row><row><cell>2AI: Version 1</cell><cell>43.5</cell><cell>34.9</cell><cell>58.1</cell><cell>39.9</cell><cell>28.9</cell><cell>13.6</cell><cell>02.5</cell><cell>00.4</cell><cell>01.2</cell><cell>18.7</cell><cell>24.4</cell></row><row><cell>HFUT-NUS</cell><cell>40.9</cell><cell>21.7</cell><cell>39.4</cell><cell>59.6</cell><cell>06.4</cell><cell>04.5</cell><cell>03.6</cell><cell>01.1</cell><cell>04.2</cell><cell>18.1</cell><cell>20.2</cell></row><row><cell>CAMP</cell><cell>42.6</cell><cell>26.0</cell><cell>45.5</cell><cell>21.4</cell><cell>23.5</cell><cell>06.2</cell><cell>06.3</cell><cell>00.3</cell><cell>04.7</cell><cell>16.9</cell><cell>19.5</cell></row><row><cell>Naive CNN  ?</cell><cell>34.4</cell><cell>34.2</cell><cell>55.5</cell><cell>16.4</cell><cell>07.8</cell><cell>03.4</cell><cell>06.3</cell><cell>00.8</cell><cell>03.4</cell><cell>19.0</cell><cell>18.3</cell></row><row><cell>NCT-TSO</cell><cell>24.9</cell><cell>16.5</cell><cell>21.5</cell><cell>55.6</cell><cell>05.2</cell><cell>05.6</cell><cell>07.2</cell><cell>00.6</cell><cell>00.8</cell><cell>17.8</cell><cell>15.7</cell></row><row><cell>Med Recognizer</cell><cell>35.6</cell><cell>21.1</cell><cell>32.3</cell><cell>07.4</cell><cell>01.7</cell><cell>01.9</cell><cell>02.5</cell><cell>00.4</cell><cell>01.9</cell><cell>21.8</cell><cell>12.8</cell></row><row><cell>Average and standard deviation</cell><cell cols="10">47.8?7.8 40.0?11.2 64.3?15.2 58.0?16.3 60.1?28.1 42.0?29.2 13.3?8.5 2.6?3.9 21.6?16.7 23.9?4.3</cell><cell>37.7?12.1</cell></row></table><note>bold = best score and underlined = second best. Not eligible for award: ? post-challenge submission, ? organizers' baselines, ? used non-public third-party dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Per-class performance on target recognition. ?</figDesc><table><row><cell>Team</cell><cell>gallb ladd er</cell><cell>cysti c-du ct</cell><cell>cysti c-art ery</cell><cell>bloo d-ve ssel</cell><cell>fluid</cell><cell>abdo mina l-wa ll or cavit y</cell><cell>liver</cell><cell>ome ntum</cell><cell>perit oneu m</cell><cell>gut</cell><cell>spec imen -bag</cell><cell>null</cell><cell>Mea n</cell></row><row><cell>Trequartista</cell><cell>91.4</cell><cell>66.8</cell><cell>30.6</cell><cell>33.0</cell><cell>20.9</cell><cell>60.8</cell><cell>82.5</cell><cell>00.7</cell><cell>37.5</cell><cell>08.4</cell><cell>89.2</cell><cell>30.2</cell><cell>46.4</cell></row><row><cell>SIAT CAMI</cell><cell>89.4</cell><cell>64.3</cell><cell>31.2</cell><cell>15.4</cell><cell>28.1</cell><cell>67.3</cell><cell>79.5</cell><cell>00.6</cell><cell>32.6</cell><cell>16.3</cell><cell>85.4</cell><cell>31.7</cell><cell>45.5</cell></row><row><cell>2AI: Version 2  ?</cell><cell>89.4</cell><cell>67.3</cell><cell>26.1</cell><cell>33.0</cell><cell>17.6</cell><cell>43.1</cell><cell>75.0</cell><cell>00.8</cell><cell>24.4</cell><cell>14.8</cell><cell>89.2</cell><cell>28.6</cell><cell>42.8</cell></row><row><cell>Digital Surgery</cell><cell>93.3</cell><cell>61.2</cell><cell>29.4</cell><cell>02.1</cell><cell>18.4</cell><cell>30.9</cell><cell>82.5</cell><cell>00.6</cell><cell>39.9</cell><cell>10.5</cell><cell>92.6</cell><cell>26.4</cell><cell>41.1</cell></row><row><cell>ANL TRIPLET</cell><cell>86.5</cell><cell>65.5</cell><cell>26.9</cell><cell>27.4</cell><cell>11.4</cell><cell>35.9</cell><cell>69.9</cell><cell>00.4</cell><cell>33.9</cell><cell>18.3</cell><cell>78.6</cell><cell>26.9</cell><cell>40.5</cell></row><row><cell>J&amp;M</cell><cell>87.3</cell><cell>43.1</cell><cell>27.3</cell><cell>03.8</cell><cell>37.8</cell><cell>56.7</cell><cell>83.0</cell><cell>00.3</cell><cell>14.5</cell><cell>15.2</cell><cell>73.6</cell><cell>24.3</cell><cell>39.2</cell></row><row><cell>HFUT-MedIA</cell><cell>85.0</cell><cell>61.0</cell><cell>24.5</cell><cell>25.0</cell><cell>17.2</cell><cell>26.5</cell><cell>69.1</cell><cell>00.9</cell><cell>15.5</cell><cell>08.1</cell><cell>86.1</cell><cell>30.3</cell><cell>37.8</cell></row><row><cell>RDV  ?</cell><cell>88.2</cell><cell>54.7</cell><cell>32.0</cell><cell>18.3</cell><cell>09.9</cell><cell>27.7</cell><cell>70.9</cell><cell>00.5</cell><cell>25.2</cell><cell>08.4</cell><cell>83.5</cell><cell>28.2</cell><cell>37.7</cell></row><row><cell>Lsgroup</cell><cell>88.3</cell><cell>51.4</cell><cell>21.6</cell><cell>22.6</cell><cell>17.3</cell><cell>24.9</cell><cell>63.4</cell><cell>00.5</cell><cell>20.4</cell><cell>08.1</cell><cell>70.7</cell><cell>25.3</cell><cell>34.9</cell></row><row><cell>CITI SJTU</cell><cell>84.8</cell><cell>55.2</cell><cell>23.6</cell><cell>04.8</cell><cell>12.1</cell><cell>44.8</cell><cell>64.7</cell><cell>01.0</cell><cell>06.7</cell><cell>13.5</cell><cell>78.1</cell><cell>23.7</cell><cell>34.8</cell></row><row><cell>SJTU-IMR</cell><cell>83.6</cell><cell>54.7</cell><cell>24.7</cell><cell>08.9</cell><cell>14.5</cell><cell>34.2</cell><cell>67.0</cell><cell>00.8</cell><cell>09.6</cell><cell>05.2</cell><cell>76.2</cell><cell>26.1</cell><cell>34.1</cell></row><row><cell>Tripnet  ?</cell><cell>82.2</cell><cell>49.4</cell><cell>24.7</cell><cell>07.0</cell><cell>19.6</cell><cell>16.0</cell><cell>68.6</cell><cell>01.2</cell><cell>09.1</cell><cell>02.4</cell><cell>80.6</cell><cell>21.5</cell><cell>32.2</cell></row><row><cell>MMLAB</cell><cell>79.6</cell><cell>45.3</cell><cell>18.1</cell><cell>14.2</cell><cell>11.5</cell><cell>10.4</cell><cell>51.3</cell><cell>00.6</cell><cell>26.9</cell><cell>16.0</cell><cell>77.6</cell><cell>23.4</cell><cell>31.6</cell></row><row><cell>Casia Robotics</cell><cell>84.3</cell><cell>48.9</cell><cell>16.7</cell><cell>05.9</cell><cell>12.0</cell><cell>21.2</cell><cell>59.5</cell><cell>03.9</cell><cell>06.0</cell><cell>12.1</cell><cell>71.8</cell><cell>27.8</cell><cell>31.2</cell></row><row><cell>Ceaiik</cell><cell>84.3</cell><cell>44.7</cell><cell>21.0</cell><cell>05.4</cell><cell>14.1</cell><cell>21.4</cell><cell>55.1</cell><cell>01.2</cell><cell>12.0</cell><cell>08.7</cell><cell>76.0</cell><cell>23.1</cell><cell>30.9</cell></row><row><cell>Attention Tripnet  ?</cell><cell>77.4</cell><cell>42.5</cell><cell>23.1</cell><cell>13.8</cell><cell>09.1</cell><cell>22.2</cell><cell>41.0</cell><cell>02.0</cell><cell>18.5</cell><cell>03.6</cell><cell>79.7</cell><cell>22.9</cell><cell>30.0</cell></row><row><cell>Band of Broeders  ?</cell><cell>83.9</cell><cell>42.6</cell><cell>08.2</cell><cell>01.7</cell><cell>02.7</cell><cell>03.7</cell><cell>74.6</cell><cell>00.3</cell><cell>03.7</cell><cell>01.1</cell><cell>67.1</cell><cell>20.0</cell><cell>26.2</cell></row><row><cell>2AI: Version 1</cell><cell>76.9</cell><cell>22.1</cell><cell>05.8</cell><cell>01.7</cell><cell>02.5</cell><cell>00.9</cell><cell>45.6</cell><cell>00.3</cell><cell>03.7</cell><cell>01.1</cell><cell>63.7</cell><cell>18.7</cell><cell>20.5</cell></row><row><cell>SK</cell><cell>71.5</cell><cell>33.4</cell><cell>12.4</cell><cell>03.9</cell><cell>17.9</cell><cell>03.5</cell><cell>24.9</cell><cell>01.1</cell><cell>02.7</cell><cell>03.5</cell><cell>42.4</cell><cell>22.7</cell><cell>20.2</cell></row><row><cell>MTL baseline  ?</cell><cell>76.2</cell><cell>21.3</cell><cell>11.2</cell><cell>01.9</cell><cell>05.0</cell><cell>03.0</cell><cell>42.5</cell><cell>00.3</cell><cell>04.9</cell><cell>02.2</cell><cell>48.3</cell><cell>17.9</cell><cell>19.8</cell></row><row><cell>HFUT-NUS</cell><cell>52.6</cell><cell>12.3</cell><cell>05.2</cell><cell>03.1</cell><cell>03.6</cell><cell>03.8</cell><cell>40.9</cell><cell>01.1</cell><cell>04.7</cell><cell>02.5</cell><cell>11.7</cell><cell>18.1</cell><cell>13.5</cell></row><row><cell>NCT-TSO</cell><cell>40.9</cell><cell>10.6</cell><cell>12.1</cell><cell>16.2</cell><cell>07.2</cell><cell>00.9</cell><cell>33.7</cell><cell>00.3</cell><cell>02.5</cell><cell>00.7</cell><cell>02.8</cell><cell>17.8</cell><cell>12.3</cell></row><row><cell>CAMP</cell><cell>58.6</cell><cell>10.8</cell><cell>05.0</cell><cell>02.7</cell><cell>06.3</cell><cell>01.5</cell><cell>14.6</cell><cell>00.6</cell><cell>02.6</cell><cell>04.3</cell><cell>16.6</cell><cell>16.9</cell><cell>11.8</cell></row><row><cell>Med Recognizer</cell><cell>56.4</cell><cell>12.0</cell><cell>04.9</cell><cell>01.6</cell><cell>02.5</cell><cell>00.9</cell><cell>10.1</cell><cell>00.5</cell><cell>03.0</cell><cell>00.7</cell><cell>05.3</cell><cell>21.8</cell><cell>10.1</cell></row><row><cell>Average and standard deviation</cell><cell cols="12">78.8?13.5 43.4?19.0 19.4?9.1 11.4?10.3 13.3?8.5 23.4?20.3 57.1?21.3 0.9?0.7 15.0?12.3 7.7?5.7 64.5?27.9 23.9?4.3</cell><cell>30.2?11.0</cell></row></table><note>bold = best score and underlined = second best. Not eligible for award: ? post-challenge submission, ? organizers' baselines, ? used non-public third-party dataset. ? shows only the targets in test videos.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Wilcoxon signed-rank test of the competing teams for rank stability.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Rank Stability</cell><cell></cell><cell cols="7">? Wilcoxon signed-rank test</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Proposed method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p-value</cell><cell>SK</cell><cell>MedR</cell><cell>Digital Surgery</cell><cell>Band of Broeders</cell><cell>Trequartista</cell><cell>MMLAB</cell><cell>Lsgroup</cell><cell>CAMP</cell><cell>NCT-TSO</cell><cell>HFUT-MedIA</cell><cell>Casia Robotics</cell><cell>SIAT-CAMI</cell><cell>HFUT-NUS</cell><cell>Ceaiik</cell><cell>SJTU-IMR</cell><cell>J &amp; M</cell><cell>CITI-SJTU</cell><cell>ANL-Triplet</cell></row><row><cell>method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Alternative</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Performance summary of the ensemble methods in comparison with the top 7 methods at the challenge. = best score and underlined = second best. ? = organizers' baseline</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell cols="3">Component detection</cell><cell cols="3">Triplet association</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AP I</cell><cell>AP V</cell><cell>AP T</cell><cell>AP IV</cell><cell>AP IT</cell><cell>AP IVT</cell></row><row><cell></cell><cell>Trequartista</cell><cell></cell><cell>79.9</cell><cell>52.9</cell><cell>46.4</cell><cell>39.0</cell><cell>41.9</cell><cell>38.1</cell></row><row><cell></cell><cell>SIAT CAMI</cell><cell></cell><cell>82.1</cell><cell>51.5</cell><cell>45.5</cell><cell>37.1</cell><cell>43.1</cell><cell>35.8</cell></row><row><cell></cell><cell>HFUT-MedIA</cell><cell></cell><cell>77.1</cell><cell>46.7</cell><cell>37.8</cell><cell>33.1</cell><cell>35.9</cell><cell>32.9</cell></row><row><cell>Top Challenge Teams</cell><cell>RDV  ?</cell><cell></cell><cell>77.5</cell><cell>47.5</cell><cell>37.7</cell><cell>39.4</cell><cell>39.6</cell><cell>32.7</cell></row><row><cell></cell><cell>CITI SJTU</cell><cell></cell><cell>67.8</cell><cell>37.1</cell><cell>34.8</cell><cell>29.9</cell><cell>33.0</cell><cell>32.0</cell></row><row><cell></cell><cell>ANL Triplet</cell><cell></cell><cell>73.6</cell><cell>47.3</cell><cell>40.5</cell><cell>32.6</cell><cell>37.1</cell><cell>31.9</cell></row><row><cell></cell><cell>Digital Surgery</cell><cell></cell><cell>80.8</cell><cell>50.0</cell><cell>41.1</cell><cell>35.1</cell><cell>35.7</cell><cell>31.7</cell></row><row><cell></cell><cell>Averaging</cell><cell></cell><cell>82.4</cell><cell>52.9</cell><cell>44.7</cell><cell>40.4</cell><cell>43.5</cell><cell>38.9</cell></row><row><cell></cell><cell>Weighted Averaging</cell><cell></cell><cell>82.5</cell><cell>53.1</cell><cell>44.9</cell><cell>40.5</cell><cell>43.8</cell><cell>39.2</cell></row><row><cell>Model Ensemble</cell><cell>Soft Voting Deep ensemble</cell><cell></cell><cell>79.6 71.4</cell><cell>46.7 37.3</cell><cell>42.6 28.6</cell><cell>35.3 30.5</cell><cell>39.8 30.5</cell><cell>35.1 30.3</cell></row><row><cell></cell><cell>Deep weighted ensemble</cell><cell></cell><cell>81.9</cell><cell>51.5</cell><cell>44.0</cell><cell>39.3</cell><cell>43.1</cell><cell>40.5</cell></row><row><cell></cell><cell cols="2">Deep per-class weighted ensemble</cell><cell>81.4</cell><cell>52.2</cell><cell>46.4</cell><cell>40.0</cell><cell>42.9</cell><cell>42.4</cell></row><row><cell>Image + ground truth</cell><cell>Team</cell><cell>Top prediction</cell><cell>Team</cell><cell cols="2">Top prediction</cell><cell>Team</cell><cell cols="2">Top prediction</cell></row><row><cell></cell><cell>2AI: Version 1</cell><cell>Hook, dissect, gallbladder</cell><cell>CITI SJTU</cell><cell cols="2">Hook, dissect, gallbladder</cell><cell>CAMMA: MTL  ?</cell><cell cols="2">Hook, dissect, gallbladder</cell></row><row><cell></cell><cell>2AI: Version 2  ?</cell><cell>Hook, dissect, gallbladder</cell><cell>Digital Surgery</cell><cell cols="2">Hook, dissect, gallbladder</cell><cell>NCT-TSO</cell><cell cols="2">Hook, dissect, gallbladder</cell></row><row><cell></cell><cell>ANL Triplet</cell><cell>Hook, dissect, gallbladder</cell><cell>HFUT-MedIA</cell><cell cols="2">Hook, dissect, gallbladder</cell><cell>CAMMA: RDV  ?</cell><cell cols="2">Hook, dissect, gallbladder</cell></row><row><cell>(a.)</cell><cell cols="2">CAMMA: Attention Tripnet  ? Hook, dissect, gallbladder Band of Broeders Hook, dissect, gallbladder</cell><cell>HFUT-NUS J&amp;M</cell><cell cols="3">Grasper, retract, gallbladder SIAT CAMI Hook, dissect, gallbladder SJTU-IMR</cell><cell cols="2">Hook, dissect, gallbladder Hook, dissect, gallbladder</cell></row><row><cell></cell><cell>CAMP</cell><cell cols="2">Grasper, retract, cystic-plate Lsgroup</cell><cell cols="2">Hook, dissect, gallbladder</cell><cell>SK</cell><cell cols="2">Hook, cut, peritoneum</cell></row><row><cell></cell><cell>Casia Robotics</cell><cell>Hook, dissect, gallbladder</cell><cell>Med Recognizer</cell><cell cols="2">Hook, dissect, gallbladder</cell><cell>Trequartista</cell><cell cols="2">Hook, dissect, gallbladder</cell></row><row><cell></cell><cell>Ceaiik</cell><cell>Hook, dissect, gallbladder</cell><cell>MMLAB</cell><cell cols="2">Hook, dissect, gallbladder</cell><cell>CAMMA: Tripnet  ?</cell><cell cols="2">Hook, dissect, gallbladder</cell></row><row><cell>(b.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>bold</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .</head><label>11</label><figDesc>Qualitative results regarding the quality of triplet recognition on the CholecT50 dataset. The results are obtained on a concatenation of 3 different surgical videos from the testing set. The methodologies employed by each model are indicated in columns 1-7. The triplet flows are categorized by their instrument, and the color shades illustrate their varying interactions (verbs) on different targets.</figDesc><table><row><cell>TEAM Groundtruth</cell><cell>Multi-task Learning</cell><cell>Temporal Modeling</cell><cell>Attention Mechanism</cell><cell>Graph Convolution</cell><cell>Ensemble Methods</cell><cell>+ Phase Labels</cell><cell>+ Spatial Labels</cell><cell>Grasper</cell><cell>Action Triplets Performed Using Instrument: Bipolar Hook Scissors Clipper</cell><cell>Irrigator</cell></row><row><cell>Trequartista</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2AI (version 2)  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIAT-CAMI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HFUT-MedIA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RDV  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CITI SJTU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ANL Triplet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Digital Surgery</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Casia Robotics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lsgroup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>J&amp;M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Attention Tripnet  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ceaiik</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SJTU-IMR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tripnet  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMLAB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Band of Broeders  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MTL Baseline  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NCT-TSO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2Ai</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HFUT-NUS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAMP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Med Recognizer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Not eligible for award: ? post-challenge submission, ? organizers' baselines, ? used non-public third-party dataset.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.slack.com 2 https://hub.docker.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.dropbox.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://pypi.org/project/ivtmetrics</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The organizers would like to thank the IHU and IRCAD research teams for their help with the initial data annotation during the CONDOR project. We also thank the EndoVis 2021 organizing committee for providing the platform for this challenge. Specifically, we thank Stefanie Speidel, Lena Maier-Hein, and Danail Stoyanov. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activity recognition using inertial sensing for healthcare, wellbeing and sports applications: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Perianu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marin-Perianu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Havinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23th International conference on architecture of computing systems 2010, VDE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2021. The SARAS endoscopic surgeon action detection (ESAD) dataset: Challenges and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Bawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaping&amp;apos;a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Skarga-Bandurova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oleari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leporini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Landolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stabile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Muradore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03178</idno>
		<ptr target="https://arxiv.org/abs/2104.03178" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.502</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2017.502</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4683" to="4695" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2018.00048</idno>
		<idno>doi:10.1109/WACV.2018.00048</idno>
		<ptr target="https://doi.org/10.1109/WACV.2018.00048" />
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting><address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-03-12" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HICO: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.122</idno>
		<idno>doi:10.1109/ICCV.2015.122</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.122" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tecno: Surgical phase recognition with multi-stage temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_33</idno>
		<idno>doi:10. 1007/978-3-030-59716-0\_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_33" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2020 -23rd International Conference</title>
		<meeting><address><addrLine>Lima, Peru</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-10-04" />
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic data-driven real-time segmentation and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dergachyova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulm?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Morandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-016-1371-x</idno>
		<idno>doi:10.1007/s11548-016-1371-x</idno>
		<ptr target="https://doi.org/10.1007/s11548-016-1371-x" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1081" to="1089" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmenting and classifying activities in robot-assisted surgery with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waldram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-014-0733-5</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal coherence-based self-supervised learning for laparoscopic workflow analysis, in: OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, -and -Skin Image Analysis -First International Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01201-4_11</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">5th International Workshop, CARE 2018, 7th International Workshop, CLIP 2018, Third International Workshop</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
	<note>Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00872</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE Computer Society</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monitoring tool usage in surgery videos using boosted convolutional and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2018.05.001</idno>
		<idno>doi:10.1016/j.media. 2018.05.001</idno>
		<ptr target="https://doi.org/10.1016/j.media.2018.05.001" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Anal</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="203" to="218" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CATARACTS: challenge on automatic tool annotation for cataract surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marsalkaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Dedmari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prellberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ara?jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2018.11.008</idno>
		<idno>doi:10.1016/j. media.2018.11.008</idno>
		<ptr target="https://doi.org/10.1016/j.media.2018.11.008" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coordinate attention for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13713" to="13722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coordinate attention for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/html/Hou_Coordinate_Attention_for_Efficient_Mobile_Network_Design_CVPR_2021_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="13713" to="13722" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal memory relation network for workflow recognition from surgical video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1911" to="1923" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lapontospm: an ontology for laparoscopic surgeries and its application to surgical phase recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Julliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>M?ller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gibaud</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-015-1222-1</idno>
		<idno>doi:10.1007/s11548-015-1222-1</idno>
		<ptr target="https://doi.org/10.1007/s11548-015-1222-1" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1427" to="1434" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge-driven formalization of laparoscopic surgeries for rule-based intraoperative context-aware assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>M?ller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-07521-1_17</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">Information Processing in Computer-Assisted Interventions -5th International Conference, IPCAI 2014</title>
		<meeting><address><addrLine>Fukuoka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-06-28" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Proposing novel methods for gynecologic surgical action recognition on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khatibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dezyani</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-020-09540-y</idno>
		<idno>doi:10.1007/s11042-020-09540-y</idno>
		<ptr target="https://doi.org/10.1007/s11042-020-09540-y" />
	</analytic>
	<monogr>
		<title level="j">Multim. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="30111" to="30133" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from RGB-D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364913478446</idno>
		<idno>doi:10.1177/ 0278364913478446</idno>
		<ptr target="https://doi.org/10.1177/0278364913478446" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Robotics Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126543</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<editor>Metaxas, D.N., Quan, L., Sanfeliu, A., Gool, L.V.</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011-11-06" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Iccv</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126543</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">6126543</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Assisted phase and step annotation for surgical videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ragot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-02108-8</idno>
		<idno>doi:10.1007/s11548-019-02108-8</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-02108-8" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="673" to="680" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01018</idno>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Detailed_2D-3D_Joint_Representation_for_Human-Object_Interaction_CVPR_2020_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10163" to="10172" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<idno>doi:10.1007/ 978-3-319-10602-1\_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1_48" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<editor>Fleet, D.J., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09-06" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Surgical data science for next-generation interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashizume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kati?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kranzfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?rz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41551-017-0132-7</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Surgical data science: Enabling next-generation surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning models for actions and personobject interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_25</idno>
		<idno>doi:10.1007/ 978-3-319-46448-0\_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-0_25" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Welling, M.</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands, Oc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="414" to="428" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning latent temporal connectionism of deep residual visual abstractions for identifying surgical tools in laparoscopy procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sathish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multitask learning of temporal connectionism in convolutional networks using a joint distribution loss function to simultaneously identify tools and phase in surgical videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sathish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep Learning Methods for the Detection and Recognition of Surgical Tools and Activities in Laparoscopic Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nwoye</surname></persName>
		</author>
		<ptr target="http://icube-publis.unistra.fr/8-Nwoy21" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recognition of instrument-tissue interactions in endoscopic videos via action triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weakly supervised convolutional lstm approach for tool tracking in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1059" to="1067" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Data splits and metrics for benchmarking methods on surgical action triplet datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rendezvous: Attention mechanisms for the recognition of surgical action triplets in endoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seeliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2022.102433</idno>
		<ptr target="https://doi.org/10.1016/j.media.2022.102433" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102433</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision</title>
		<editor>Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Proceedings, Part IX</title>
		<idno type="DOI">10.1007/978-3-030-01240-3_25</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="407" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-task temporal convolutional networks for joint recognition of surgical phases and steps in gastric bypass procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dall&amp;apos;alba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sensor substitution for video-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2016.7759769</idno>
		<idno>doi:10.1109/IROS.2016</idno>
		<ptr target="https://doi.org/10.1109/IROS.2016.7759769" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2016</title>
		<meeting><address><addrLine>Daejeon, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-10-09" />
			<biblScope unit="page" from="5230" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual semantic role labeling for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="5589" to="5600" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<title level="m">CVPR2021/html/Sadhu_Visual_Semantic_Role_Labeling_for_ Video_Understanding_CVPR_2021_paper.html</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems 28</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>ArXiv abs/1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Endoscopic vision challenge 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mathis-Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scheikl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandes-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alapatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jarc</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4572973</idno>
		<idno>doi:10.5281/zenodo</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4572973" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The TUM lapchole dataset for the M2CAI 2016 workflow challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ostler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kranzfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feu?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09278</idno>
		<ptr target="http://arxiv.org/abs/1610.09278" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR.</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Cai4cai: the rise of contextual artificial intelligence in computer-assisted interventions. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="198" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Comparative validation of machine learning algorithms for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>M?ller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kisilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>M?ndermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Lubotsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davitashvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Capek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Disch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Twick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirtac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hosgor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bolmgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siemens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Von Frankenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mathis-Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14956</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>surgical workflow and skill analysis with the heichole benchmark</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cspnet: A new backbone that can enhance learning capability of cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="390" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-label classification with label graph superimposing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12265" to="12272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning domain adaptation with model calibration for surgical report generation in robotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA48506.2021.9561569</idno>
		<idno>doi:10.1109/ICRA48506.2021.9561569</idno>
		<ptr target="https://doi.org/10.1109/ICRA48506.2021.9561569" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation, ICRA 2021</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-05-30" />
			<biblScope unit="page" from="12350" to="12356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Learning from a tiny dataset of manual annotations: a teacher/student approach for surgical phase recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>IPCAI</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5532" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hcvrd: a benchmark for large-scale human-centered visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deepphase: Surgical phase recognition in CATARACTS videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giataganas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberola-L?pez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_31</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2018 -21st International Conference</title>
		<editor>Fichtinger, G.</editor>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Nwoye : Conceptualization, Data Curation, Data Analysis and Interpretation, Methodology, Software, Investigation, Validation, Evaluation, Formal Analysis, Visualization, Writing -Original Draft, Writing -Review &amp; Editing, Challenge Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Resources</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Alapatt : Conceptualization, Investigation, Validation, Evaluation, Formal Analysis, Visualization, Writing -Original Draft, Writing -Review &amp; Editing, Challenge Organization</title>
		<imprint/>
	</monogr>
	<note>Resources</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Data Curation, Investigation, Formal Analysis, Writing -Original Draft, Writing -Review &amp; Editing, Visualization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<title level="m">Conceptualization, Investigation, Validation, Writing -Review &amp; Editing</title>
		<meeting><address><addrLine>Challenge Organization, Resources</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Getty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanchez-Matilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gerats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raviteja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sathish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Abbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bhasker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gaida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vila?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Egging</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Wijma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aschenbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Methodology, Software, Writing -Review &amp; Editing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seeliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<title level="m">Data Curation, Writing -Review &amp; Editing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<title level="m">Data Curation, Writing -Review &amp; Editing, Supervision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Writing -Review &amp; Editing, Supervision, Challenge Organization, Resources, Funding Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conceptualization</title>
		<imprint/>
	</monogr>
	<note>Project Administration</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
