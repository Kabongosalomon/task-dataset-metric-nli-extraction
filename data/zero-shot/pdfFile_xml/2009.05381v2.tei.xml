<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Encoding for Video Retrieval by Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">FEBRUARY 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xirong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Dual Encoding for Video Retrieval by Text</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2021">FEBRUARY 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video retrieval</term>
					<term>cross-modal representation learning</term>
					<term>dual encoding</term>
					<term>hybrid space learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method. Code is available at https://github.com/danieljf24/hybrid space.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HIS paper targets at the task of video retrieval by text, where a query is described exclusively in the form of a natural-language sentence, with no visual example attached. The task is scientifically interesting and challenging as it requires establishing proper associations between visual and linguistic information presented in the temporal order.</p><p>Retrieving unlabeled videos by text attracts initial attention in the form of zero-example multimedia event detection, where the goal is to retrieve video shots showing specific events such as parking a vehicle, dog show and birthday party, but with no training videos provided <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. All these methods are concept based, representing the video content by automatically detected concepts, which are used to match with textual descriptions of a target event. An attractive property of the conceptbased representation is its good interpretability <ref type="bibr" target="#b6">[7]</ref>, as each of its dimensions has explicit meanings. The concept-based tradition continues in the era of deep learning. For the NIST TRECVID AVS challenge <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, a leading benchmark evaluation for video retrieval by text, we observe that the top performers in the previous years (2016-2018) are mostly concept based <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>. However, the concept-based paradigm faces intrinsic difficulties including how to specify a set of proper concepts, how to train good classifiers for these concepts, and more crucially how to select relevant and detectable concepts for both video and query representation <ref type="bibr" target="#b5">[6]</ref>. These difficulties remain largely unresolved to this day.</p><p>Not surprisingly, efforts towards concept-free representation have been made. In the context of cross-modal retrieval between image and text <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, Canonical Correlation Analysis (CCA) has been frequently used to linearly project both visual and textual features into a common space. From the viewpoint of neural networks, CCA es-arXiv:2009.05381v2 [cs.CV] 18 Feb 2021 sentially applies a fully connected (FC) layers (with no bias term) on the visual side and another FC layer on the textual side. With the quick development of deep neural networks in both computer vision and natural language processing, the simple FC layer has now been replaced by more advanced embedding networks <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>. Still, we see a common pattern among the varied solutions. That is, first encoding videos and textual queries, and then mapping them into a common space where the video-text similarities can be computed directly. Therefore, what matters for this paradigm are forms of video encoding, text encoding and common space learning.</p><p>For video encoding, a popular solution is to first extract frame features from videos by pre-trained CNN models, and then aggregate them into a video-level feature by mean pooling <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b23">[23]</ref>, max pooling <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b24">[24]</ref>, NetVLAD <ref type="bibr" target="#b19">[19]</ref>, RNN <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> or self-attention mechanisms <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>. As for text encoding, while bag-of-words remains common <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[27]</ref>, deep networks are in an increasing use. Given a specific sentence, a typical approach is to first quantize each of its word by word embedding and then aggregate wordlevel features into a sentence-level feature by max pooling <ref type="bibr" target="#b20">[20]</ref>, Fisher Vector <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, NetVLAD <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b24">[24]</ref>, RNN <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b30">[30]</ref> or Graph Convolutional Network (GCN) <ref type="bibr" target="#b22">[22]</ref>. Instead of using one specific encoding strategy, Dong et al. <ref type="bibr" target="#b31">[31]</ref> and Li et al. <ref type="bibr" target="#b32">[32]</ref> utilize multiple text encoders including bag-of-words, word2vec and GRU. However, they simply use mean pooling for video encoding. In contrast to the existing works, we propose dual multi-level encoding for both videos and text in advance to common space learning. As exemplified in <ref type="figure" target="#fig_0">Fig. 1</ref>, the new encoding strategy is crucial for describing complex queries and video content.</p><p>Our hypothesis is that a given video/query has to be first encoded into a powerful representation of its own. We consider such a decomposition crucial as it allows us to design an encoding network that jointly exploits multiple encoding strategies including mean pooling, recurrent neural networks and convolutional networks. In our design, the output of a specific encoding block is not only used as input of a follow-up encoding block, but also re-used via skip connections to contribute to the final output. It generates new, higher-level features progressively. These features, generated at distinct levels, are powerful and complementary to each other, allowing us to obtain effective video (and text) representations by very simple concatenation.</p><p>Philosophically, our dual encoding model is linked to Allan Paivio's dual-coding theory of cognition <ref type="bibr" target="#b33">[33]</ref>. Supported by evidence from psychological research, the dual-coding theory postulates that verbal and visual information are processed along distinct channels with separate representations in the human mind. Later, these representations are used for retrieving information previously stored in the mind. In a similar spirit, the dual encoding model stores video and textual information learned from training data in separate representations and recall them in the inference stage.</p><p>For common space learning, the state-of-the-art relies on constructing a latent space <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, as such a space can be optimized in an end-to-end manner, and thus permits superior performance against the concept-based alternative. This, however, comes at the cost of losing interpretability. Different from the concept space, each dimension of the latent space is not directly interpretable. Hence, what a model has truly learned is often agnostic. In order to combine the merits of the latent space and the concept space, we propose to train the dual encoding network with hybrid space learning. In particular, a latent space and a concept space are simultaneously learned, for better performance and better interpretability.</p><p>In sum, this paper makes the following contributions.</p><p>? We propose a novel dual network that encodes an input, let it be a query sentence or a video, in a similar manner. By jointly exploiting multi-level encodings, the network explicitly and progressively learns to represent global, local and temporal patterns in videos and sentences. Moreover, dual encoding is orthogonal to common space learning, allowing us to flexibly embrace stateof-the-art common space learning algorithms. ? We propose a new hybrid space learning to learn a hybrid common space for video-text similarity prediction, which inherits the high performance of the latent space and the good interpretability of the concept space. ? We conduct extensive experiments on four challenging video datasets, i.e., MSR-VTT <ref type="bibr" target="#b34">[34]</ref>, TRECVID AVS 2016-2018 <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, VATEX <ref type="bibr" target="#b37">[37]</ref> and MPII-MD <ref type="bibr" target="#b38">[38]</ref>. Dual encoding, trained by hybrid space learning, is a new state-of-the-art for video retrieval by text.</p><p>The architecture of our model is based on existing components including mean feature pooling, GRU and 1D-CNN. Our novelty is the integration of these components into a dual multi-level encoding architecture for video / text representation learning. We consider the novelty of the system architecture to be more than the sum of its parts. The Dual Encoding network effectively combines these vanilla components into a powerful solution that is competitive with recent Transformer based methods <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. Also notice that while we use two pre-trained CNNs to extract frame-level features, the two features are concatenated to form a single vector before feeding it into our network. The multi-level features combined in Dual Encoding are generated by the network itself. Hence, our method essentially uses a single visual feature as input, and thus differs fundamentally from previous works that rely on the ensemble of diverse models / features.</p><p>A preliminary version of this work was published at CVPR 2019 <ref type="bibr" target="#b41">[41]</ref>. The journal extension improves over the conference paper mainly in two aspects. Technically, we introduce hybrid space learning. Compared to the latent space learning used in <ref type="bibr" target="#b41">[41]</ref>, the new learning algorithm leads to better video retrieval performance. With the learned concept space, the model's interpretability is also improved. Experimentally, our evaluation has been substantially expanded in terms of datasets and competitive baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept based Methods</head><p>Since 2016 the TRECVID starts a new challenge for video retrieval by text, known as Ad-hoc Video Search (AVS) <ref type="bibr" target="#b7">[8]</ref>. The majority of the top ranked solutions for this challenge depend on visual concept classifiers to describe video content and linguistic rules to detect concepts in textual queries <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b42">[42]</ref>. Then the similarity between a textual query and a specific video is typically computed by concept matching. For instance, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b42">[42]</ref> utilize multiple pre-trained Convolutional Neural Network (CNN) models to detect main objects and scenes in video frames. As for query representation, the authors design relatively complex linguistic rules to extract relevant concepts from a given query. Ueki et al. <ref type="bibr" target="#b12">[12]</ref> come with a much larger concept bank consisting of more than 50k concepts. In addition to pre-trained CNN models, they train SVM classifiers to automatically annotate the video content. In <ref type="bibr" target="#b43">[43]</ref>, Snoek et al. utilize a model called VideoStory <ref type="bibr" target="#b6">[7]</ref> to represent videos and then embed them into a concept space by a linear transformation, while they still represent textual query by selecting concepts based on part-of-speech tagging heuristically. Consequently, the video-text similarity is implemented as the cosine similarity in terms of their concept vectors.</p><p>We argue that such a concept-based paradigm has a fundamental disadvantage. That is, it is very difficult to describe the rich sequential information within both video and query using a few selected concepts. Despite the disadvantage, such a paradigm also has its merit where representing video and textual query by concepts make it somewhat interpretable. In this work, we also integrate such interpretation merit into our proposed model, allowing the model to match videos and text in the concept space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent Space based Methods</head><p>Latent space based methods first encode video and textual queries and then map them into a common latent space where the video-text similarity can be measured directly <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b46">[46]</ref>. For these methods, what matters are forms of video encoding, text encoding and similarity learning. So we review recent work in these aspects.</p><p>For video encoding, a typical approach is to first extract visual features from video frames by pre-trained CNN models, and subsequently aggregate the frame-level features into a video-level feature. The de facto choice is mean pooling <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b31">[31]</ref> or max pooling <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b28">[28]</ref>. To explicitly model the temporal information, Torabi et al. <ref type="bibr" target="#b26">[26]</ref> use a Long Short-Term Memory (LSTM), where frame-level features are sequentially fed into the LSTM, and the mean pooling of the hidden vectors at each step is used as the video feature. Besides using a Gated Recurrent Unit (GRU) to model the temporal dependency of video frames, Yang et al. <ref type="bibr" target="#b21">[21]</ref> additionally utilize the multi-head self-attention mechanism <ref type="bibr" target="#b47">[47]</ref> to learn the frame-wise correlation thus enhance the video representation. In addition to the framelevel visual features, we also notice efforts on utilizing features extracted from other channels such as audio <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b28">[28]</ref> and motion <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b28">[28]</ref>. However, they still use mean pooling, max pooling or NetVLAD to aggregate different features into video-level features.</p><p>For text encoding, while bag-of-words (BoW) remains common <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b48">[48]</ref>, deep networks are in increasing use. The common way by deep learning techniques is to first represent each word of the textual query by word2vec models pre-trained on large-scale text corpora, and then aggregate them by max pooling in <ref type="bibr" target="#b20">[20]</ref>, Fisher Vector in <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref> or NetVLAD in <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b24">[24]</ref>. Despite their good performance, the main drawback of these methods is ignoring the sequential order in text. Recurrent neural networks (RNN), known to be effective for modeling sequence oder dependency, are also dominated <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b49">[49]</ref>. Recursive neural networks are investigated in <ref type="bibr" target="#b44">[44]</ref> for vectorizing subjectverb-object triplets extracted from a given sentence. Variants of recurrent neural networks are being exploited, see the usage of LSTM, bidirectional LSTM, GRU, bidirectional GRU in <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b17">[17]</ref> and <ref type="bibr" target="#b45">[45]</ref>, respectively. For instance, Mithun et al. <ref type="bibr" target="#b17">[17]</ref> utilize the last hidden state of the GRU as the text representation. The work <ref type="bibr" target="#b31">[31]</ref> and its extension <ref type="bibr" target="#b32">[32]</ref> explore a joint use of multiple text encoding strategies including BoW, word2vec and GRU, and found it is beneficial for video retrieval. However, as aforementioned, those works simply employ mean pooling for video encoding. Recently, Chen et al. <ref type="bibr" target="#b22">[22]</ref> utilize graph convolutional network to model the connection between words. But it requires text to be well annotated with semantic role relation annotations, which may be not suitable for a new scenario with different linguistic expression patterns. By contrast, our proposed text encoding trained in an end-to-end manner, without using such semantic role annotations. Moreover, different from the above works, this paper aims to explicitly and progressively exploit global, local and temporal patterns in both videos and textual queries.</p><p>For similarity learning, lots of works <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b44">[44]</ref> typically project encoded videos and text into a common latent space and triplet ranking loss variants are dominantly used for model training. After being projected in a latent space, the video-text similarity can be measured by a standard similarity metric, e.g., cosine similarity. Besides the triplet ranking loss, Zhang et al. <ref type="bibr" target="#b18">[18]</ref> additionally employ contrastive loss and reconstruction loss to further constrain the latent space. Recently, we notice an increasing used of learning multiple common latent spaces <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b28">[28]</ref> instead of only one latent space. For instance, Miech et al. <ref type="bibr" target="#b28">[28]</ref> utilize four different features, i.e., appearance, motion, face and audio features, to represent videos, and learn one latent space for each video feature. The final video-text similarity is the fusion of their similarities in the four latent spaces. Wray et al. <ref type="bibr" target="#b24">[24]</ref> decompose text into nouns and non-noun words, and respectively project them into two different latent spaces. With the similar idea of <ref type="bibr" target="#b24">[24]</ref>, Chen et al. <ref type="bibr" target="#b22">[22]</ref> decompose text into events, actions and entities, and three latent spaces are learned for video-text matching.</p><p>Different from the existing works where learned common spaces are latent spaces without no explicit interpretability, we propose to learn a hybrid space consisting of a latent space and a concept space, which inherits the merits of high performance of latent space based methods and interpretability of concept based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-modal Fusion Methods</head><p>In contrast to latent space based methods, cross-modal fusion methods do not construct an explicit latent space. Instead, they use a cross-modal fusion subnetwork that takes text and videos as input and directly produces similarity scores <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b49">[49]</ref>. For instance, Yu et al. <ref type="bibr" target="#b30">[30]</ref> fuse the video and text representation into a 3D tensor using a soft attention, and a convolutional network is further employed to directly predict the similarity based on the fused feature. Although these methods <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b49">[49]</ref> are effective, their retrieval efficiency are somewhat low as video and text are coupled with each other. By contrast, our proposed method maps videos and text into a common space by a separated branch respectively, which makes videos and text decoupled. So, all candidate videos can be mapped in the common space off-line, which is efficient for large-scale video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Transformer-based Methods</head><p>Transformers <ref type="bibr" target="#b47">[47]</ref> are a new genre of deep neural networks that stack multiple self-attended layers. By selfsupervised learning with masked language modeling on very large-scale corpora, BERT-like Transformer models have shown superior performance over RNNs on multiple NLP tasks <ref type="bibr" target="#b50">[50]</ref>. Unsurprisingly, we see an increasing effort on re-purposing such types of models for video retrieval by text <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b52">[52]</ref>. In Loko? et al. <ref type="bibr" target="#b51">[51]</ref> and Zhao et al. <ref type="bibr" target="#b52">[52]</ref>, a pre-trained BERT is adopted to encode the textual query. Gabeur et al. <ref type="bibr" target="#b40">[40]</ref> also use BERT for query representation, and go one step further by proposing a multi-modal Transformer (MMT) with four stacked Transformer layers to jointly encode diverse video features for video representation. Zhu and Yang propose ActBERT to encode global actions, local regional objects, and text descriptions in a unified framework <ref type="bibr" target="#b39">[39]</ref>. We empirically show in Section 5.2.3 that our proposed method, although utilizing relatively simple encoders such as Bag-of-Words, GRU and 1D-CNN, compares favorably to the Transformer based methods. Moreover, our method can be extended with ease to incorporate the Transformers for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE Dual Encoding NETWORK</head><p>Both video and sentence are essentially a sequence of items, let it be frames or words. Such a property motivates us to design a dual encoding network to handle the two distinct modalities. Specifically, given a video v and a sentence s, the proposed network encodes them in parallel, in advance to common space learning. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, multi-level encodings are performed for each modality. The encoding results are then combined, denoted as ?(v) and ?(s), to describe the two modalities in a coarse-to-fine fashion. In our design, ?(v) and ?(s) are not directly used for cross-modal matching. So the two vectors do not have to reside in the same feature space and can have distinct dimensions, giving them sufficient freedom to become powerful representations of the corresponding modalities.</p><p>In what follows we first depict the network at the video side. We then specify choices that are unique at the text side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video-side Multi-level Encoding</head><p>For a given video, we extract uniformly a sequence of n frames with a pre-specified interval of 0.5 seconds. Per frame we extract deep features using a pre-trained Ima-geNet CNN, as commonly used for video content analysis <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b44">[44]</ref>. Consequently, the video is described by a sequence of feature vectors {v 1 , v 2 , . . . , v n }, where v t indicates the deep feature vector of the t-th frame. Notice that 3D CNNs <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref> can also be used for feature extraction when treating segments of frames as individual items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Level 1. Global Encoding by Mean Pooling</head><p>According to our literature review, mean pooling, which represents a video by simply averaging the features of its frames, is arguably the most popular choice for textvideo retrieval. By definition, mean pooling captures visual patterns that repeatedly present in the video content. These patterns tend to be global. We use f 1 v to indicate the encoding result at this level, that is:</p><formula xml:id="formula_0">f 1 v = 1 n n t=1 v t .<label>(1)</label></formula><p>3.1.2 Level 2. Temporal-Aware Encoding by biGRU Bi-directional recurrent neural network <ref type="bibr" target="#b55">[55]</ref> is known to be effective for making use of both past and future contextual information of a given sequence. We hypothesize that such a network is also effective for modeling the video temporal information. We adopt a bidirectional GRU (biGRU) <ref type="bibr" target="#b56">[56]</ref>, which has less parameters than the bidirectional LSTM and thus requires less amounts of training data. A biGRU consists of two separated GRU layers, i.e., a forward GRU and a backward GRU. The forward GRU is used to encode frame features in normal order, while the backward GRU encodes frame features in reverse order. Let ? ? h t and ? ? h t be their corresponding hidden states at a specific time step t = 1, . . . , n. The hidden states are generated as</p><formula xml:id="formula_1">? ? h t = ? ?? ? GRU (v t , ? ? h t?1 ), ? ? h t = ? ?? ? GRU (v n+1?t , ? ? h t?1 ),<label>(2)</label></formula><p>where ? ?? ? GRU and ? ?? ? GRU indicate the forward and backward GRUs, with past information carried by</p><formula xml:id="formula_2">? ? h t?1 and ? ? h t?1 , respectively. Concatenating ? ? h t and ? ? h t , we obtain the biGRU output h t v = [ ? ? h t , ? ? h t ].</formula><p>The size of the hidden vectors in the forward and backward GRUs is empirically set to 512. Accordingly, the size of h t v is 1,024. Putting all the output together, we obtain a feature map</p><formula xml:id="formula_3">H v = {h 1 v , h 2 v , .</formula><p>.., h n v }, with a size of 1, 024?n. The biGRU based encoding, denoted f <ref type="bibr" target="#b1">(2)</ref> v , is obtained by applying mean pooling on H v along the row dimension, that is</p><formula xml:id="formula_4">f 2 v = 1 n n t=1 h t v .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Level 3. Local-Enhanced Encoding by biGRU-CNN</head><p>The previous layer treats the output of biGRU at each step equally. To enhance local patterns that help discriminate between videos of subtle difference, we build convolutional networks on top of biGRU. In particular, we adapt 1D CNN originally developed for sentence classification <ref type="bibr" target="#b57">[57]</ref>. The input of our CNN is the feature map H v generated by the previous biGRU module. Let Conv1d k,r be a 1D convolutional block that contains r = 512 filters of size k, with k ? 2. Feeding H v , after zero padding, into Conv1d k,r produces a n?r feature map. Non-linearity is introduced by applying the ReLU activation function on the feature map. As n varies for videos, we further apply max pooling to compress the feature map to a vector c k of fixed length r. More formally we express the above process as</p><formula xml:id="formula_5">c k v = max-pooling(ReLU (Conv1d k,r (H v ))).<label>(4)</label></formula><p>A filter with k = 2 allows two adjacent rows in H v to interact with each other, while a filter of larger k means more adjacent rows are exploited simultaneously. In order to generate a multi-scale representation, we deploy multiple 1D convolutional blocks with k = 2, 3, 4, 5. Their output is concatenated to form the biGRU-CNN based encoding, i.e.,</p><formula xml:id="formula_6">f 3 v = [c 2 v , c 3 v , c 4 v , c 5 v ].<label>(5)</label></formula><p>As</p><formula xml:id="formula_7">f 1 v , f 2 v , f 3 v</formula><p>are obtained sequentially at different levels by specific encoding strategies, we consider it reasonable to presume that the three encoding results are complementary to each other, with some redundancy. Hence, we obtain multi-level encoding of the input video by concatenating the output from all the three levels, namely</p><formula xml:id="formula_8">?(v) = [f 1 v , f 2 v , f 3 v ].<label>(6)</label></formula><p>In fact, this concatenation operation, while being simple, is a common practice for feature combination <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>.</p><p>It is worth noting that we have described how to combine three encoders, i.e., mean pooling, biGRU and biGRU-CNN, which naturally capture global, temporal and local patterns. In principle, other encoders, e.g., Transformers, that are complementary to the existing encoders can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text-side Multi-level Encoding</head><p>The above encoding network, after minor modification, is also applicable for the text modality.</p><p>Given a sentence s of length m, we represent each of its words by a one-hot vector. Accordingly, a sequence of one-hot vectors {w 1 , w 2 , . . . , w m } is generated, where w t indicates the vector of the t-th word. Global encoding f 1 s is obtained by averaging all the individual vectors in the sequence. This amounts to the classical bag-of-words representation.</p><p>For biGRU based encoding, each word is first converted to a dense vector by multiplying its one-hot vector with a word embedding matrix. We initialize the matrix using a word2vec [60] model provided by <ref type="bibr" target="#b31">[31]</ref>, which trained word2vec on English tags of 30 million Flickr images. The rest is mostly identical to the video counterpart. We denote the biGRU based encoding of the sentence as f 2 s . Similarly, we have the biGRU-CNN based encoding of the sentence as f 3 s . Here, we utilize three 1D convolutional blocks with k = 2, 3, 4. Multi-level encoding of the sentence is obtained by concatenating the encoding results from all the three levels in the dual network, i.e.,</p><formula xml:id="formula_9">?(s) = [f 1 s , f 2 s , f 3 s ].<label>(7)</label></formula><p>As ?(v) and ?(s) have not been correlated, they are not directly comparable. For video-text similarity computation, the vectors need to be projected into a common space, the learning algorithm for which will be presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HYBRID SPACE LEARNING</head><p>We propose to train our dual encoding network with a hybrid space learning algorithm. The hybrid space consists of a latent space which aims for good performance and a concept space which is meant for good interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning a Latent Space</head><p>Network. Given the encoded video vector ?(v) and the sentence vector ?(s), we project them into a latent space by affine transformations. From the neural network viewpoint, an affine transformation is essentially a Fully Connected (FC) layer. We additionally use a Batch Normalization (BN) layer after the FC layer, as we find this trick beneficial. Putting everything together, we obtain the video feature vector f (v) and sentence feature vector f (s) in the latent space as:</p><formula xml:id="formula_10">f (v) = BN(W 1 ?(v) + b 1 ), f (s) = BN(W 2 ?(s) + b 2 ),<label>(8)</label></formula><p>where W 1 and W 2 parameterize the FC layers on each side, with b 1 and b 2 as bias terms.</p><p>To measure the video-text similarity sim lat (v, s) in the latent space, we use popular cosine similarity between f (v) and f (s):</p><formula xml:id="formula_11">sim lat (v, s) = f (v) ? f (s) f (v) f (s) .<label>(9)</label></formula><p>In our preliminary experiment, we also tried the Manhattan and Euclidean distance, but found them less effective than the cosine similarity. Loss. A desirable similarity function shall make relevant video-sentence pairs near and irrelevant pairs far away in the latent space. Therefore, we use the improved triplet ranking loss <ref type="bibr" target="#b61">[61]</ref>, which penalizes the model according to the hardest negative examples in the mini-batch. Concretely, given a relevant video-sentence pair (v, s) in a mini-batch, its loss L lat (v, s) is:</p><formula xml:id="formula_12">L lat (v, s) = max(0, m + sim lat (v, s ? ) ? sim lat (v, s)) +max(0, m + sim lat (v ? , s) ? sim lat (v, s)),<label>(10)</label></formula><p>where m is the margin constant, while s ? and v ? respectively indicate a negative sentence sample for v and a negative video sample for s. The two negatives are not randomly sampled. Instead, the most similar yet negative sentence and video in the current mini-batch are chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning a Concept Space</head><p>As multiple concepts can be used simultaneously to describe a specific video or sentence, learning a concept space can be naturally formulated as a multi-label classification problem.</p><p>Network. Suppose the size of the concept vocabulary is K. In order to project ?(v) and ?(s) into a K-dimensional concept space, we adopt a network similar to the network used for latent space learning. That is,</p><formula xml:id="formula_13">g(v) = ?(BN (W 3 ?(v) + b 3 )), g(s) = ?(BN (W 4 ?(s) + b 4 )),<label>(11)</label></formula><p>Note that different from Eq. 8, we additionally use a sigmoid activation ? to produce probabilistic output. For a specific concept indexed by i = 1, . . . , K, we use g(v) i to denote the probability of the concept being relevant with respect to the video v. In a similar vein we define g(s) i . Consider the absolute scale of elements in the concept vectors matters, cosine similarity which mainly considers the direction of feature vectors is suboptimal to measure similarities between concept vectors. Viewing the two concept vectors g(v) and g(s) as (unnormalized) histograms, we Training video: <ref type="figure">Fig. 3</ref>. An illustration of extracting concept-level annotations from sentence descriptions of training videos. Instead of binary labels, we extract frequency-based soft labels to better reflect the importance of a specific concept for a given video. E.g., the concept dance appears in all the five sentences of the example video, so its y value is 1, whilst the concept wedding occurring twice has a y value of 0.4</p><p>. use generalized Jaccard similarity to compute the video-text similarity sim con (v, s) in the concept space, i.e.,</p><formula xml:id="formula_14">sim con (v, s) = K i=1 min(g(v) i , g(s) i ) K i=1 max(g(v) i , g(s) i ) .<label>(12)</label></formula><p>Concept-level annotations. Per training video, we extract automatically its concept-level annotations from the associated sentence descriptions as follows. Assume that for a specific training video v, we have access to p sentences, {s 1 , . . . , s p }, that describe the video content. The relevance of a specific concept w.r.t v is determined by its occurrence in the p sentences. Li et al. <ref type="bibr" target="#b62">[62]</ref> suggest that a concept appearing in multiple sentences is usually more important than those presented once. Hence, instead of binary labels, we obtain soft labels based on concept frequency. Specifically, let y be a K-dimensional ground-truth vector for v. The value of its i-th dimension, i.e., y i , is defined as the frequency of the i-th concept divided by the maximum frequency of all concepts within the p sentences, see <ref type="figure">Fig. 3</ref>. Accordingly, we extend a relevant video-sentence pair (v, s) to a triplet training instance (v, s, y) to supervise concept space learning.</p><p>Loss. For multi-label classification, the binary crossentropy (BCE) loss is common. In our context, the loss for a given video-sentence pair (v, s) with respect to their shared ground-truth y is computed as</p><formula xml:id="formula_15">L bce (v, s, y) = ?( 1 K K i=1 [yi log(g(v)i) + (1 ? yi) log(1 ? g(v)i)] + 1 K K i=1 [yi log(g(s)i) + (1 ? yi) log(1 ? g(s)i)]).<label>(13)</label></formula><p>We expect the concept space to be used not only for interpretability but also for improving video-text matching. So in addition to the BCE loss, we also minimize the improved triplet ranking loss in the concept space. That is,</p><formula xml:id="formula_16">L con,rank (v, s) = max(0, m + sim con (v, s ? ) ? sim con (v, s)) +max(0, m + sim con (v ? , s) ? sim con (v, s)).<label>(14)</label></formula><p>The concept space is learned by minimizing the following combined loss:</p><formula xml:id="formula_17">L con (v, s, y) = L bce (v, s, y) + L con,rank (v, s).<label>(15)</label></formula><p>Both L bce and L con,rank matter in Eq. 15. Without L bce , Eq. 15 is boiled down to learning another latent space that not only lacks interpretability but also makes sim con (v, c) less complementary to sim lat (v, c) for video-text similarity computation. Meanwhile, using L bce alone makes the concept space suboptimal for video-text matching and sensitive to the concept annotation quality, c.f. the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Learning of the Two Spaces</head><p>The dual encoding network is trained by minimizing the combination of the latent-space loss L lat and the conceptbased loss L con . In particular, given a training set D = {(v, s, y)}, we have</p><formula xml:id="formula_18">argmin ? (v,s,y)?D L lat (v, s) + L con (v, s, y),<label>(16)</label></formula><p>where ? denotes all the trainable parameters in the whole model. Except for image CNNs used for video feature extraction, the dual encoding network is trained in an endto-end manner.</p><p>In order to conceptually explain why hybrid space learning helps, we provide a toy example in <ref type="figure">Fig. 4</ref> that shows potential weakness when using a latent space or a concept space alone. As the latent space considers only relative positions per triplet, triplets that are semantically close, i.e., red and light-red triplets in <ref type="figure">Fig. 4(a)</ref>, could be distant from each other. By contrast, each dimension of the concept space, which corresponds to a unique concept by definition, acts as an anchor to pinpoint the absolute position of a sample w.r.t. the concept. <ref type="figure">Fig. 4(b)</ref> shows the embedding space w.r.t concept i and concept j, where the concept i alone is insufficient to discern samples nears its origin. Other dimensions have to be taken into account. Nonetheless, as the concept set is pre-specified, the other dimensions are not necessarily discriminative, see the vertical axis corresponding to concept j. The hybrid space learning gives us the possibility of combining the best of the two spaces, while overcoming their weaknesses. In addition, the combination of videotext similarities in the two distinct spaces can be viewed as a lightweight instantiation of model ensemble <ref type="bibr" target="#b63">[63]</ref>, which typically improves over the base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Video-Text Similarity Computation</head><p>Once the model is trained, the final similarity between a video v and a sentence s is computed as the sum of their latent-space similarity and concept-space similarity, namely</p><formula xml:id="formula_19">sim(v, s) = ? ? sim lat (v, c) + (1 ? ?) ? sim con (v, c),<label>(17)</label></formula><p>where ? is a hyper-parameter to balance the importance of two spaces, ranging within [0, 1]. Note that raw values of sim lat (v, s) and sim con (v, s) reside in distinct scales. Hence, they are rescaled separately by min-max normalization before being combined. Also note that in the inference stage, the multi-level encoding at the video side can be performed independently. Hence, for a large-scale video collection, their hybrid-space features can be pre-computed, allowing us to answer ad-hoc queries on the fly.  <ref type="figure">Fig. 4. A toy example</ref> showing potential weakness of a latent space and a concept space in two dimensions. Markers with the same color are relevant, while negative samples are with gray background. Red and light-red are semantically closer than the blue. Given a specific triplet, only their relative positions matter in the latent space. So even with zero triplet ranking loss, the red triplet can be projected into a region that is closer to the blue triplet rather than the light-red triplet. In the concept space, wherein each dimension corresponds to a distinct concept, the absolute positions of the triplets now matter. Samples positive w.r.t a specific concept are to be projected to one end, while negatives are to be placed near the origin. As the region around the origin naturally lacks discriminability, other dimensions have to be considered. However, as the concepts are pre-specified, they are not necessarily optimal to describe the video content. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>Our evaluation is organized as follows. Firstly in Section 5.1, we compare the proposed Dual Encoding model (with its best setup) against the state-of-the-art on four datasets, i.e., MSR-VTT <ref type="bibr" target="#b34">[34]</ref>, TRECVID AVS 2016-2018 <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, VATEX <ref type="bibr" target="#b37">[37]</ref> and MPII-MD <ref type="bibr" target="#b38">[38]</ref>. Constructed independently by the dataset developers, the first three datasets consist of short web videos with very diverse content, while the last dataset contains video clips from 72 movies. Second, in order to verify the influence of major components in the proposed model, Section 5.2 presents an ablation study on the MSR-VTT dataset. Additional experiments concerning the effect of the hyper-parameter ? and the concept annotation quality on the performance and the efficiency of our model are provided in the appendix. Before proceeding to the experiments, we detail common implementations regarding text preprocessing, video features, concept vocabulary extraction, and model training. For sentence preprocessing, we first convert all words to the lowercase and then replace words that occurring less than five times in the training set with a special token. For video features, on VATEX we adopt 1,024-d I3D <ref type="bibr" target="#b54">[54]</ref> video features provided by the dataset developers <ref type="bibr" target="#b37">[37]</ref>. As for the other datasets, we extract frame-level ResNeXt-101 <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b65">[65]</ref> and ResNet-152 <ref type="bibr" target="#b66">[66]</ref> using an open-source toolbox 1 . The two feature vectors are concatenated to obtain a combined 4,096d CNN feature, which we refer to as ResNeXt-ResNet.</p><p>To obtain the concept vocabulary, we conduct part-ofspeech tagging by NLTK toolkit on all training sentences, and only keep the nouns, verbs and adjectives. All the English stopwords also removed. Besides, we also lemmatize the words, making dog and dogs to be a same concept. Finally, the top K = 512 frequent words are selected as the final concept vocabulary.</p><p>The proposed model is implemented using PyTorch (http://pytorch.org). Following <ref type="bibr" target="#b61">[61]</ref>, the parameter m for the improved triplet ranking loss is set to 0.2. The weight ? in the combined similarity is empirically set to 0.6. We 1. https://github.com/xuchaoxi/video-cnn-feat learn from our earlier studies <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b41">[41]</ref> that setting the dimensionality of the common space to 2, 048 is a good practice. Hence, we let the overall dimensionality of the hybrid space be 2, 048. Recall that the concept space is 512dimensional. Accordingly, the dimensionality of the latent space is 2, 048 ? 512 = 1, 536. We use stochastic gradient descent with Adam <ref type="bibr" target="#b67">[67]</ref>. The mini-batch size is 128. With an initial learning rate of 0.0001, we take an adjustment schedule similar to <ref type="bibr" target="#b31">[31]</ref>. That is, once the validation loss does not decrease in three consecutive epochs, we divide the learning rate by 2. Early stop occurs if the validation performance does not improve in ten consecutive epochs. The maximal number of epochs is 50. In practice, early stop occurs typically after 15 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with the State-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Experiments on MSR-VTT</head><p>Data. The MSR-VTT dataset <ref type="bibr" target="#b34">[34]</ref>, originally developed for video captioning, consists of 10k web video clips and 200k natural sentences describing the visual content of the clips. The number of sentences per clip is 20. For this dataset, we notice there are three distinct editions of data partition in the literature <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b34">[34]</ref>. The official partition <ref type="bibr" target="#b34">[34]</ref> uses 6,513 clips for training, 497 clips for validation, and the remaining 2,990 clips for testing. For the partition by <ref type="bibr" target="#b28">[28]</ref>, there are 6,656 clips for training and 1,000 clips for testing. The partition of <ref type="bibr" target="#b30">[30]</ref> uses 7,010 and 1,000 clips for training and testing, respectively. As the last two data partitions provide no validation set, we build a validation set by randomly sample 1,000 clips from MSR-VTT with <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref> excluded, respectively. For a comprehensive evaluation, our experiments are performed on all the three data partitions.</p><p>Performance Metrics. We use rank-based metrics, namely R@K (K = 1, 5, 10), Median rank (Med r) and mean Average Precision (mAP) to evaluate the performance. R@K is the percentage of test queries for which at least one relevant item is found among the top-K retrieved results. Med r is the median rank of the first relevant item in the search results. Higher R@K, mAP and lower Med r mean better performance. For overall comparison, we report the Sum of all Recalls (SumR).</p><p>Baselines. The following thirteen state-of-the-art models are compared:</p><p>? VSE++ [61]: A state-of-the-art text-image retrieval model, which is commonly used as the strong baseline model for text-video retrieval. We replace its image-side branch with mean pooling on frame-level feature followed by a FC layer.</p><p>? W2VV <ref type="bibr" target="#b31">[31]</ref>: Learn to project text into a visual feature space by minimizing the distance of relevant video-text pairs in the visual space. Multiple text encoding strategies including BoW, word2vec and GRU are jointly used for text encoding.</p><p>? MEE <ref type="bibr" target="#b28">[28]</ref>: Use four different features to represent videos, and learn one latent space for each video feature. The weighted sum of similarities in four latent spaces is regarded as the final video-text similarity.</p><p>? W2VV++ <ref type="bibr" target="#b32">[32]</ref>: An improved version of W2VV, it employs a better sentence encoding strategy and an improved triplet ranking loss.</p><p>? CE <ref type="bibr" target="#b19">[19]</ref>: Use a collaborative gating to fuse multiple features to obtain a strong video representation.</p><p>? TCE <ref type="bibr" target="#b21">[21]</ref>: Utilize a latent semantic tree <ref type="bibr" target="#b69">[69]</ref> augmented encoder to represent text, and a GRU with multi-head selfattention mechanism <ref type="bibr" target="#b47">[47]</ref> to encode videos.</p><p>? HGR <ref type="bibr" target="#b22">[22]</ref>: Utilize graph convolutional network to model the connection between words, and project text and videos into three latent spaces. ? Mithun et al. <ref type="bibr" target="#b17">[17]</ref>: Project videos and text into two latent spaces, and a weighted triplet ranking loss is used for training.</p><p>? Francis et al. <ref type="bibr" target="#b68">[68]</ref>: Fuse multimodal features, i.e., counting, activity and concpet features to obtain stronger video and text representations.</p><p>? JPoSE <ref type="bibr" target="#b24">[24]</ref>: Decompose text into nouns and non-noun words, and respectively project them into two different latent spaces.</p><p>? CT-SAN <ref type="bibr" target="#b49">[49]</ref>: Learn to directly predict the similarity based on the fused video-text features without learning a common space, and a concept word detector is used for enhancing the video representation.</p><p>? JSFusion <ref type="bibr" target="#b30">[30]</ref>: With the same idea of <ref type="bibr" target="#b49">[49]</ref> that direly predicts the video-text similarity, a stronger joint sequence fusion is employed to fuse video and text features. For a direct comparison, we cite numbers from the original papers whenever applicable. Meanwhile, we notice that video features used by specific papers vary. So, to make the comparison fairer, we have re-trained the following seven models which have been open-sourced, i.e., W2VV, MEE, VSE++, W2VV++, CE, TCE and HGR, using the same ResNeXt-ResNet feature 2 .</p><p>Results. <ref type="table" target="#tab_1">Table 1</ref> summarizes the performance comparison on three different data partitions of MSR-VTT. Though our goal is video retrieval by text, which corresponds to text-to-video retrieval in the table, video-to-text retrieval is also included for completeness. Note that the number of the candidate videos/sentences to be retrieved in the official partition is larger than that in the other two partitions. Hence, the official partition is more challenging. As a consequence, for all the models, their performance scores on the official partition are lower than their counterparts on the other partitions. Consider the results using the same video features, our dual encoding model achieves the best overall performance.</p><p>Among the results marked with asterisks (*), CE* which utilizes seven video features performs the best. Still, the proposed Dual Encoding model using only two visual features outperforms CE* on the first two data partitions, while slightly worse on Test1k-Yu. Note that on this test set, Dual Encoding was trained on the corresponding training set of 7,010 videos as specified by <ref type="bibr" target="#b30">[30]</ref>, whilst CE* was trained on an enlarged set of 9,000 videos. The results demonstrate the effectiveness of our proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experiments on TRECVID AVS 2016-2018</head><p>Data. IACC.3 dataset is the largest test bed for video retrieval by text to this date, which developed for TRECVID (Ad-hoc Video Search) AVS 2016, 2017 and 2018 task <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>. The dataset contains 4,593 Internet Archive videos with duration ranging from 6.5 minutes to 9.5 minutes and a mean duration of almost 7.8 minutes. Shot boundary detection results in 335,944 shots in total. Given an adhoc query, e.g., Find shots of military personnel interacting with protesters, the task is to return for the query a list of 1,000 shots from the test collection ranked according to their likelihood of containing the given query. Per year TRECVID specifies 30 distinct queries of varied complexity. As TRECVID does not specify training data for the AVS task, we train the dual encoding network using the joint collection of MSR-VTT and the TGIF <ref type="bibr" target="#b70">[70]</ref> which contains 100K animated GIFs and 120K sentences describing visual content of the GIFs. Although animated GIFs are a very different domain, TGIF was constructed in a way to resemble usergenerated video clips, e.g., with cartoon, static, and textual content removed. For IACC.3, MSR-VTT and TGIF, we use the ResNeXt-ResNet video feature.</p><p>Performance Metrics. We utilize inferred Average Precision (infAP), the official performance metric used by the TRECVID AVS task. The overall performance is measured by averaging infAP scores over the queries. Their values are reported in percentage (%). Note that the TRECVID ground truth is partially available at the shot-level. The task organizers employ a pooling strategy to collect the ground truth, i.e., a pool of candidate shots are formed by collecting the top-1000 shots from each submission and a random subset is selected for manual verification. The ground truth thus favors official participants. As the top ranked items found by our method can be outside of the subset, infAP scores of our method are likely to be underestimated.</p><p>Baselines. We include the top 3 entries of each year, i.e., <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b73">[73]</ref> for 2016, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b43">[43]</ref> for 2017 and <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b74">[74]</ref> for 2018. Besides we include publications on the tasks, i.e., <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b42">[42]</ref>. The most of above methods are concept based, except <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b74">[74]</ref>. Among them, <ref type="bibr" target="#b71">[71]</ref>   fuses three W2VV++ variants with different settings, <ref type="bibr" target="#b72">[72]</ref> uses two attention networks, besides the classical conceptbased representation, while <ref type="bibr" target="#b74">[74]</ref> is based on VSE++. Notice that visual features and training data used by these methods vary, meaning the comparison and consequently conclusions drawn from this comparison is at a system level. So for a more conclusive comparison, we re-train VSE++ <ref type="bibr" target="#b61">[61]</ref>, W2VV <ref type="bibr" target="#b31">[31]</ref>, W2VV++ <ref type="bibr" target="#b32">[32]</ref> and CE <ref type="bibr" target="#b19">[19]</ref> using the same training data and the ResNeXt-ResNet feature. Results. <ref type="table" target="#tab_2">Table 2</ref> shows the performance of different methods on the TRECVID AVS 2016, 2017 and 2018 tasks, and the overall performance is the mean score of the three years. The proposed method again performs the best, with infAP of 15.2, 23.1 and 12.1 respectively. While <ref type="bibr" target="#b71">[71]</ref> has a same infAP of 12.1 on the TRECVID AVS 2018 task, their solution ensembles three models. Their best single model, i.e., W2VV++ <ref type="bibr" target="#b32">[32]</ref> which uses the same training data and the same ResNeXt-ResNet feature, has a lower infAP of 10.6. Given the same training data and feature, the proposed method outperforms VSE++, W2VV, CE with a clear margin. These results confirm the effectiveness of our dual encoding for large-scale video retrieval by text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Experiments on VATEX</head><p>Data. VATEX <ref type="bibr" target="#b37">[37]</ref> a large-scale multilingual video description dataset. Each video, collected for YouTube, has a duration of 10 seconds. Per video there are 10 English sentences and 10 Chinese sentences to describe the corresponding  video content. Here, we only use the English sentences. We adopt the dataset partition provided by <ref type="bibr" target="#b22">[22]</ref>, i.e., 25,991 video clips for training, 1,500 clips for validation and 1,500 clips for testing, where validation and test set are obtained by randomly split the official validation set of 3,000 clips into two equal parts.</p><p>Performance Metrics. R@K (K = 1, 5, 10) and SumR are used as the performance metrics.</p><p>Baselines. For method comparison, we consider HGR <ref type="bibr" target="#b22">[22]</ref>, the first work reporting video retrieval performance on VATEX. We also compare the VSE++ <ref type="bibr" target="#b61">[61]</ref>, W2VV <ref type="bibr" target="#b31">[31]</ref>, W2VV++ <ref type="bibr" target="#b32">[32]</ref> and CE <ref type="bibr" target="#b19">[19]</ref>.</p><p>Results. <ref type="table" target="#tab_3">Table 3</ref> summarizes the performance, where all the models use the same I3D <ref type="bibr" target="#b54">[54]</ref> video features. Among them, VSE, VSE++ and W2VV++ are all use mean pooling over the frame-level feature to encode videos, while our dual encoding model explores multi-level features to represent videos, consistently achieving better performance. The result shows the benefit of using the multi-level feature for video representation. Although HGR utilizes the extra semantic role annotation of sentences for text representation, our dual encoding model still slightly outperforms HGR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Experiments on MPII-MD Data.</head><p>To evaluate the effectiveness of our proposed methods for a specific video domain, we conduct the experiment on MPII-MD [38] a movie description dataset. We use the official data partition, that is, 56,828, 4,929 and 6,580 movie clips   for training, evaluation and testing, respectively. Each movie clip is associated with one or two textual descriptions. Performance Metrics. Performance of R@1, R@5, R@10 and SumR are reported.</p><p>Baselines. In this experiments, we compare the VSE++ <ref type="bibr" target="#b61">[61]</ref>, W2VV <ref type="bibr" target="#b31">[31]</ref>, W2VV++ <ref type="bibr" target="#b32">[32]</ref> and CE <ref type="bibr" target="#b19">[19]</ref>. All the models are trained using the ResNeXt-ResNet feature.</p><p>Results. <ref type="table" target="#tab_4">Table 4</ref> summarizes the performance on the MPII-MD dataset. Our proposed dual encoding model outperforms the other counterparts. It is worth noting that the performance of all models are lower on MPII-MD than that on MSR-VTT, we attribute it to the challenging nature of movie retrieval by vague descriptions on MPII-MD. See <ref type="figure" target="#fig_4">Fig. 5</ref> for some qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>In this section, we evaluate the viability of each component on MSR-VTT, with performance reported on the full-size test set unless otherwise stated. A comparison with recent multi-modal Transformer methods is also conducted. In addition, we investigate the effectiveness of our multi-level text encoding for image-text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Multi-level Encoding versus Single-Level Encoding</head><p>To exam the usefulness of each encoding component in the dual encoding network, we conduct an ablation study as follows. Given varied combinations of the encoding components, seven models are trained. <ref type="table" target="#tab_5">Table 5</ref> summarizes the choices of video and text encodings and the corresponding performance.</p><p>Among the individual encoders, biGRU-CNN is found to be the most effective. As more encoding layers are included, the overall performance goes up. For the last four models which combine the output from previous layers, they all outperform the corresponding counterpart using the output of a specific layer. E.g., the model with Level 1 + 2 encoding strategy outperforms the ones with Level 1 or Level 2. The results suggest that features of different levels are complementary to each other. The full multi-level encoding setup, i.e., Level 1 +2 + 3, performs the best.</p><p>We also investigate single-side encoding, that is, videoside multi-level encoding with mean pooling on the text side, and text-side multi-level encoding with mean pooling on the video side. These two strategies obtain SumR of 194.5 and 191, respectively. The lower scores justify the necessity of dual encoding. The result also suggests that video-side multi-level encoding is more beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Hybrid Space versus Single Space</head><p>In order to verify the effectiveness of the hybrid space, we have re-trained Dual Encoding with two alternative spaces, i.e., fully latent space (the CVPR version <ref type="bibr" target="#b41">[41]</ref>) and fully concept space, respectively. <ref type="table" target="#tab_6">Table 6</ref> summarizes their performance on MSR-VTT.</p><p>Our model with the hybrid space consistently outperforms the other two counterparts with a clear margin, which shows the effectiveness of hybrid space for video-text retrieval. Among them, although the model with the concept space is able to give some interpretation of the retrieval model, its performance is the worst. The latent space counterpart gives better performance than the concept space, but lacks interpretability. Moreover, simply increasing the dimensionality of the latent space, from 1,536 to 2,048, does not improve the performance. By contrast, the hybrid space strikes a proper balance between the retrieval performance and the interpretability.</p><p>To justify the necessity of the combined loss for concept space learning, we also report the performance of the hybrid space that excludes the triplet ranking loss from Eq. 15. This variant suffers a noticeable performance decrease in terms of For each query, the top 3 ranked videos and the ground-truth video (marked with red ticks) are shown. In case the ground-truth video is among the top three, the fourth video will be included as well. By definition, each query has only one ground-truth video. Number on the left hand side of each video indicates the video's rank in the retrieval result. Below a specific query are its predicted concepts, visualized in the form of a tag cloud, bigger font meaning larger predicted scores. Next to the videos are their predicted concepts. Putting these tag clouds together helps us better understand the video retrieval results.</p><p>SumR, from 211.7 to 183.3. The result shows the importance of considering the triplet ranking loss for learning a concept space that is beneficial for video-text matching.</p><p>Interpreting retrieval results with predicted concepts. <ref type="figure" target="#fig_5">Fig. 6</ref> shows some examples returned by our proposed dual encoding. Although only one correct video is annotated for each query, the top retrieved videos for Q1 (Query 1), Q2 and Q3 are typically relevant to the given query to some extent. In Q4, as the word beach is used to describe the object ball, the former is less important than the latter in this query. However, the predicted concept vector of Q4 shows that the model over emphasizes beach, as visualized in the tag cloud. This explains that the top 2 retrieved videos are all about activities on beach. Meanwhile, for the truly relevant video, which is ranked at the position of 32, the predicted concepts are dance and group. Although these concepts are semantically relevant to the video content, they are irrelevant for the query. For Q5, concepts predicted our model, e.g., cartoon and tree, are not precise enough to capture santa claus the key role in the query. So our model also fails to answer this query. In general, we find concepts predicted by our dual encoding model reasonable, and useful for understanding the retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Dual Encoding versus-and-with Transformers</head><p>Dual Encoding versus multi-modal Transformers. We compare with two recent multi-modal Transformers, i.e., Act-  BERT <ref type="bibr" target="#b39">[39]</ref> and MMT <ref type="bibr" target="#b40">[40]</ref>, that have been evaluated on specific test sets of MSR-VTT. As <ref type="table" target="#tab_8">Table 7</ref> shows, Dual Encoding clearly outperforms ActBERT and is comparable to MMT in terms of the overall performance (better on Test1k-Miech and worse on Test1k-Yu). It is worth pointing out that MMT uses a diverse set of seven video features that describe varied aspects of the video content including motion, audio, scene, OCR, face, speech, and visual appearance features. By contrast, Dual Encoding uses only two regular 2D-CNN features. Moreover, on Test1k-Yu <ref type="bibr" target="#b30">[30]</ref>, Dual Encoding was trained on the corresponding training set of 7,010 videos, whilst MMT was trained on an enlarged set of 9,000 videos. We consider in this context that Dual Encoding compares favorably to MMT. In addition, <ref type="table" target="#tab_9">Table 8</ref> shows a comparison concerning model complexity. Note that ActBERT is not included, as that method remains closed-source, making a precise estimation of its model complexity impossible. Dual Encoding is smaller (28.1% less parameters) and much faster (71.2% less FLOPs) than MMT. Dual Encoding with Transformers. Dual Encoding is a generic framework that allows pre-trained Transformers to be included with ease. As illustrated in <ref type="figure" target="#fig_6">Fig. 7</ref>, we extend the Dual Encoding network by adopting MMT trained on the HowTo100M dataset <ref type="bibr" target="#b20">[20]</ref> as the visual Transformer and BERT <ref type="bibr" target="#b76">[76]</ref> trained on Wikipedia and book corpora <ref type="bibr" target="#b77">[77]</ref> as the textual Transformer. The two Transformers are integrated as is without fine tuning. Their outputs, i.e., a 3,584-d video feature vector and a 1,024-d sentence vector, are concatenated with the video-side and text-side multi-level encodings, respectively, in advance to hybrid space learning. As shown in <ref type="table" target="#tab_8">Table 7</ref>   built upon the relatively simple mean feature pooling, GRU and 1D-CNN encoders, remain competitive and can be used together with the Transformers to maximize the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Multi-level Encoding for Image-Text Retrieval</head><p>Setup. We investigate if the VSE++ model <ref type="bibr" target="#b61">[61]</ref> can be improved in its original context of image-text retrieval, when replacing its textual encoding module, which is a GRU, by the proposed multi-level encoding module. To that end, we fix all other choices, adopting the exact evaluation protocol of <ref type="bibr" target="#b61">[61]</ref>. That is, we use the same data split, where the training / validation / test test has 30,000 / 1,000 / 1,000 images for Flickr30K, and 82,783 / 5,000 / 5,000 images for MSCOCO. We also use the same VGGNet feature provided by <ref type="bibr" target="#b61">[61]</ref>. Performance of R@1, R@5 and R@10 are reported. On MSCOCO, the results are reported by averaging over 5 folds of 1,000 test images. Results. <ref type="table" target="#tab_11">Table 9</ref> shows the performance of image-text retrieval on Flickr30k and MSCOCO. Integrating text-side multi-level encoding into VSE++ brings improvements on both datasets. The results suggest that the proposed textside multi-level encoding is also beneficial for VSE++ in its original context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUMMARY AND CONCLUSIONS</head><p>For video retrieval by text, this paper proposes a Dual Encoding network with hybrid space learning. By jointly exploiting multiple encoding strategies at different levels, the proposed network encodes both videos and text into powerful dense representations. Followed by hybrid space learning, these representations can be transformed to perform sequence-to-sequence cross-modal matching effectively. Extensive experiments on four video datasets, i.e., MSR-VTT, TRECVID AVS 2016-2018, VATEX, and MPII-MD, support the following conclusions. Among the three levels of encoding, biGRU-CNN that builds a 1D convolutional network on top of bidirectional GRU is the most effective when used alone. Video-side multi-level encoding is more beneficial when compared with its text-side counterpart. Our multilevel encoding modules also show competitive performance against recent multi-modal Transformers. Compared with the widely used latent space learning, our hybrid space learning not only improves the retrieval performance but also enhances the interpretability of what the dual encoding network has learned. For state-of-the-art performance, we recommend Dual Encoding with hybrid space learning for video-text matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>Effect of ? on the retrieval performance. The influence of the hyper-parameter ? in Eq. 17 is studied as follows. We try ? with its value ranging from 0.1 to 0.9 with an interval of 0.1. As shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, when the ? is larger than 0.2, the performance of our model with the hybrid space are all over 200 on MSR-VTT, which consistently outperform the counterparts using the latent space or the concept space alone. The results show that our hybrid space is not very sensitive to this parameter.</p><p>Effect of the concept annotation quality on the retrieval performance. The concept annotations used to supervise the hybrid space learning process are automatically extracted from video descriptions. In order to explore how their quality affects the retrieval performance, we re-train the model using concept annotations with varied levels of simulated noise. Concretely, suppose a specific training video is originally associated with q concepts. Given a noise level of h ? {0.1, 0.2, ? ? ? , 1}, we replace a random subset of q ? h concepts in the original annotations by the same number of concepts selected from the remaining K ? q concepts (recall that K is the size of the concept vocabulary). A larger h means lower annotation quality. As shown in <ref type="figure">Fig. 9(a)</ref>, the retrieval performance is hardly affected by the concept annotation quality at a large range of noise levels. We attribute this result to two factors. First, the hybrid-space similarity is dominated by the latent-space similarity, which is independent of concept annotations by definition. Hence, while the retrieval performance of the concept-space similarity degenerates as more noise is added, the performance   <ref type="figure">Fig. 9</ref>. The influence of the concept annotation quality on the overall performance of text-to-video and video-to-text retrieval. Per noise level, hybrid space learning is performed with L con,rank (a) and without L con,rank (b), respectively. The curve of "hybrid-space similarity" shows how the noise affects the model performance. To further reveal the effect of the noise on the individual spaces, we plot performance curves when using the latent-space similarity and conceptspace similarity separately. The use of L con,rank allows our model to be highly insensitive to the concept annotation quality.</p><p>of the latent-space similarity is largely stable. Second, the use of L con,rank maintains to a large extent the effectiveness of the concept space for cross-modal matching. Note the extreme case of h = 1 at the right end of the curves, where the concept annotations become fully random and consequently the correspondence between the individual dimensions and concepts is completely lost. By minimizing L con,rank , the "concept" space remains a latent space that can be used for matching. Without the loss, the space will be meaningless, see <ref type="figure">Fig. 9(b)</ref>. Therefore, the retrieval performance of our model is highly insensitive to the concept annotation quality. Although L bce is relatively sensitive to the quality of annotation, we consider the loss necessary as it produces an auxiliary common space not only interpretable but also more complementary to the latent space, see the better performance of the hybrid space against the fully latent space in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>Efficiency Test. Recall that the dual encoding network is designed to represent both videos and sentences into a common space respectively. Once the network is trained, representing them in the common space can be performed independently. This means we can process large-scale videos offline and answer ad-hoc queries on the fly. Specifically, given a natural-sentence query, it takes approximately 0.2 seconds to retrieve videos from the largest IACC.3 dataset, which consists of 335,944 videos. The performance is tested on a normal computer with 64G RAM and a GTX 1080TI GPU. The retrieval speed is adequate for instant response.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Showcase of video retrieval by text with and without the proposed encoding. The symbol indicates encoding by mean pooling. Numbers in the third column are the rank of the relevant video returned by retrieval models subject to specific query / video encoding strategies. The retrieval model with dual encoding successfully answers this complex query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A conceptual diagram of the proposed dual encoding network for video retrieval by text. Given a video v and a sentence s, the network performs in parallel multi-level encodings, i.e., mean pooling, biGRU and biGRU-CNN, eventually representing the two input by two combined vectors ?(v) and ?(s), respectively. The vectors are later projected into a hybrid common space which consists of a latent space and a concept space. Once the network is trained, encoding at each side is performed independently, meaning we can process large-scale videos offline and answer ad-hoc queries on the fly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Miech et al.<ref type="bibr" target="#b20">[20]</ref>: Project videos and text into a common space by a gated embedding module respectively. The model is pre-trained on large-scale video-text dataset HowTo100M<ref type="bibr" target="#b20">[20]</ref> and fine-tuned on MSR-VTT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Selected examples of movie retrieval by text on MPII-MD. The top retrieved shots, though not being ground truth, appear to be correct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Selected examples of text-to-video retrieval by our model on MSR-VTT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Integrating pre-trained visual / textual Transformers into the Dual Encoding network. The outputs of the two transformers are respectively concatenated with the video-side and text-side multi-level encodings in advance to hybrid space learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>The influence of the parameter ? of Eq. 17 on our proposed model. Performance are evaluated on MSR-VTT. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 similairty latent-space similarity concept-space similarity (a) With L con,rank 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 similairty latent-space similarity concept-space similarity (b) Without L con,rank</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Sentence descriptions:? people dancing at a formal party ? people are dancing at a wedding ? reception party with dancing and hip hop ? a man is dancing for a song in front of others ? people at a wedding reception do the whip nae nae dance</figDesc><table><row><cell cols="2">Extracted soft labels:</cell></row><row><cell>dance</cell><cell>5/5=1.0</cell></row><row><cell>people</cell><cell>3/5=0.6</cell></row><row><cell>wedding</cell><cell>2/5=0.4</cell></row><row><cell>?</cell><cell>?</cell></row><row><cell>car</cell><cell>0/5=0.0</cell></row><row><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 State-of-the-art on MSR-VTT.</head><label>1</label><figDesc>Larger R@{1,5,10}, mAP and smaller Med r indicate better performance. Symbol asterisk (*) indicates numbers directly cited from the original papers, and the others are obtained by our re-training given the same ResNeXt-ResNet feature. On all the three distinct editions of data partition, the proposed Dual Encoding model obtains the best overall performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">Text-to-Video Retrieval</cell><cell></cell><cell></cell><cell cols="3">Video-to-Text Retrieval</cell><cell></cell><cell>SumR</cell></row><row><cell></cell><cell cols="5">R@1 R@5 R@10 Med r mAP</cell><cell cols="5">R@1 R@5 R@10 Med r mAP</cell><cell></cell></row><row><cell>Official full-size test set [34]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Francis et al. * [68]</cell><cell>6.5</cell><cell>19.3</cell><cell>28.0</cell><cell>42</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mithun et al. * [17]</cell><cell>7.0</cell><cell>20.9</cell><cell>29.7</cell><cell>38</cell><cell>-</cell><cell>12.5</cell><cell>32.1</cell><cell>42.4</cell><cell>16</cell><cell>-</cell><cell>144.6</cell></row><row><cell>TCE* [21]</cell><cell>7.7</cell><cell>22.5</cell><cell>32.1</cell><cell>30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HGR* [22]</cell><cell>9.2</cell><cell>26.2</cell><cell>36.5</cell><cell>24</cell><cell>-</cell><cell>15.0</cell><cell>36.7</cell><cell>48.8</cell><cell>11</cell><cell>-</cell><cell>172.4</cell></row><row><cell>CE* [19]</cell><cell>10.0</cell><cell>29.0</cell><cell>41.2</cell><cell>16</cell><cell>-</cell><cell>15.6</cell><cell>40.9</cell><cell>55.2</cell><cell>8.3</cell><cell>-</cell><cell>191.9</cell></row><row><cell>W2VV [31]</cell><cell>1.1</cell><cell>4.7</cell><cell>8.1</cell><cell>236</cell><cell>3.7</cell><cell>17.0</cell><cell>37.9</cell><cell>49.1</cell><cell>11</cell><cell>7.6</cell><cell>117.9</cell></row><row><cell>MEE [28]</cell><cell>6.8</cell><cell>20.7</cell><cell>31.1</cell><cell>28</cell><cell>14.7</cell><cell>13.4</cell><cell>32.0</cell><cell>44.0</cell><cell>14</cell><cell>6.6</cell><cell>148.0</cell></row><row><cell>CE [19]</cell><cell>7.9</cell><cell>23.6</cell><cell>34.6</cell><cell>23</cell><cell>16.5</cell><cell>11.0</cell><cell>31.9</cell><cell>46.1</cell><cell>13</cell><cell>6.8</cell><cell>155.1</cell></row><row><cell>VSE++ [61]</cell><cell>8.7</cell><cell>24.3</cell><cell>34.1</cell><cell>28</cell><cell>16.9</cell><cell>15.6</cell><cell>36.6</cell><cell>48.6</cell><cell>11</cell><cell>7.4</cell><cell>167.9</cell></row><row><cell>TCE [21]</cell><cell>9.3</cell><cell>27.3</cell><cell>38.6</cell><cell>19</cell><cell>18.7</cell><cell>15.1</cell><cell>36.8</cell><cell>50.2</cell><cell>10</cell><cell>8.0</cell><cell>177.3</cell></row><row><cell>W2VV++ [32]</cell><cell>11.1</cell><cell>29.6</cell><cell>40.5</cell><cell>18</cell><cell>20.6</cell><cell>17.5</cell><cell>40.2</cell><cell>52.5</cell><cell>9</cell><cell>8.5</cell><cell>191.4</cell></row><row><cell>HGR [22]</cell><cell>11.1</cell><cell>30.5</cell><cell>42.1</cell><cell>16</cell><cell>20.8</cell><cell>18.7</cell><cell>44.3</cell><cell>57.6</cell><cell>7</cell><cell>9.9</cell><cell>204.4</cell></row><row><cell>Dual Encoding</cell><cell>11.6</cell><cell>30.3</cell><cell>41.3</cell><cell>17</cell><cell>21.2</cell><cell>22.5</cell><cell>47.1</cell><cell>58.9</cell><cell>7</cell><cell>10.5</cell><cell>211.7</cell></row><row><cell>Test1k-Miech [28]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JPoSE* [24]</cell><cell>14.3</cell><cell>38.1</cell><cell>53.0</cell><cell>9</cell><cell>-</cell><cell>16.4</cell><cell>41.3</cell><cell>54.4</cell><cell>8.7</cell><cell>-</cell><cell>217.5</cell></row><row><cell>MEE* [28]</cell><cell>16.8</cell><cell>41.0</cell><cell>54.4</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TCE* [21]</cell><cell>17.1</cell><cell>39.9</cell><cell>53.7</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE* [19]</cell><cell>18.2</cell><cell>46.0</cell><cell>60.7</cell><cell>7</cell><cell>-</cell><cell>18.0</cell><cell>46.0</cell><cell>60.3</cell><cell>6.5</cell><cell>-</cell><cell>249.2</cell></row><row><cell>W2VV [31]</cell><cell>2.7</cell><cell>12.5</cell><cell>17.3</cell><cell>83</cell><cell>7.9</cell><cell>17.3</cell><cell>42.0</cell><cell>53.5</cell><cell>9</cell><cell>29.3</cell><cell>145.3</cell></row><row><cell>MEE [28]</cell><cell>15.7</cell><cell>39.0</cell><cell>52.3</cell><cell>9</cell><cell>27.1</cell><cell>15.3</cell><cell>41.9</cell><cell>54.5</cell><cell>8</cell><cell>28.1</cell><cell>218.7</cell></row><row><cell>VSE++ [61]</cell><cell>17.0</cell><cell>40.9</cell><cell>52.0</cell><cell>10</cell><cell>16.9</cell><cell>18.1</cell><cell>40.4</cell><cell>52.1</cell><cell>9</cell><cell>29.2</cell><cell>220.5</cell></row><row><cell>CE [19]</cell><cell>17.8</cell><cell>42.8</cell><cell>56.1</cell><cell>8</cell><cell>30.3</cell><cell>17.4</cell><cell>42.9</cell><cell>56.1</cell><cell>8</cell><cell>29.8</cell><cell>233.1</cell></row><row><cell>TCE [21]</cell><cell>17.0</cell><cell>44.7</cell><cell>58.3</cell><cell>7</cell><cell>30.0</cell><cell>15.1</cell><cell>43.3</cell><cell>58.2</cell><cell>7</cell><cell>28.3</cell><cell>236.6</cell></row><row><cell>W2VV++ [32]</cell><cell>21.7</cell><cell>48.6</cell><cell>60.9</cell><cell>6</cell><cell>34.4</cell><cell>18.6</cell><cell>46.4</cell><cell>59.1</cell><cell>6</cell><cell>31.7</cell><cell>255.3</cell></row><row><cell>HGR [22]</cell><cell>22.9</cell><cell>50.2</cell><cell>63.6</cell><cell>5</cell><cell>35.9</cell><cell>20.0</cell><cell>48.3</cell><cell>60.9</cell><cell>6</cell><cell>33.2</cell><cell>265.9</cell></row><row><cell>Dual Encoding</cell><cell>23.0</cell><cell>50.6</cell><cell>62.5</cell><cell>5</cell><cell>36.1</cell><cell>25.1</cell><cell>52.1</cell><cell>64.6</cell><cell>5</cell><cell>37.7</cell><cell>277.9</cell></row><row><cell>Test1k-Yu [30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CT-SAN* [49]</cell><cell>4.4</cell><cell>16.6</cell><cell>22.3</cell><cell>35</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>JSFusion* [30]</cell><cell>10.2</cell><cell>31.2</cell><cell>43.2</cell><cell>13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TCE* [21]</cell><cell>16.1</cell><cell>38.0</cell><cell>51.5</cell><cell>10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Miech et al. * [20]</cell><cell>14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE* [19]</cell><cell>20.9</cell><cell>48.8</cell><cell>62.4</cell><cell>6</cell><cell>-</cell><cell>20.6</cell><cell>50.3</cell><cell>64.0</cell><cell>5.3</cell><cell>-</cell><cell>267.0</cell></row><row><cell>W2VV [31]</cell><cell>1.9</cell><cell>9.9</cell><cell>15.2</cell><cell>79</cell><cell>6.8</cell><cell>17.3</cell><cell>39.3</cell><cell>50.2</cell><cell>10</cell><cell>27.8</cell><cell>133.8</cell></row><row><cell>VSE++ [61]</cell><cell>16.0</cell><cell>38.5</cell><cell>50.9</cell><cell>10</cell><cell>27.4</cell><cell>16.2</cell><cell>39.3</cell><cell>51.2</cell><cell>10</cell><cell>27.4</cell><cell>212.1</cell></row><row><cell>MEE [28]</cell><cell>14.6</cell><cell>38.4</cell><cell>52.4</cell><cell>9</cell><cell>26.1</cell><cell>15.2</cell><cell>40.9</cell><cell>53.8</cell><cell>9</cell><cell>27.9</cell><cell>215.3</cell></row><row><cell>W2VV++ [32]</cell><cell>19.0</cell><cell>45.0</cell><cell>58.7</cell><cell>7</cell><cell>31.8</cell><cell>16.9</cell><cell>42.7</cell><cell>54.6</cell><cell>8</cell><cell>29.0</cell><cell>236.9</cell></row><row><cell>CE [19]</cell><cell>17.2</cell><cell>46.2</cell><cell>58.5</cell><cell>7</cell><cell>30.3</cell><cell>15.8</cell><cell>44.9</cell><cell>59.2</cell><cell>7</cell><cell>30.4</cell><cell>241.8</cell></row><row><cell>TCE [21]</cell><cell>17.8</cell><cell>46.0</cell><cell>58.3</cell><cell>7</cell><cell>31.1</cell><cell>18.9</cell><cell>43.5</cell><cell>58.8</cell><cell>7</cell><cell>31.4</cell><cell>243.3</cell></row><row><cell>HGR [22]</cell><cell>21.7</cell><cell>47.4</cell><cell>61.1</cell><cell>6</cell><cell>34.0</cell><cell>20.4</cell><cell>47.9</cell><cell>60.6</cell><cell>6</cell><cell>33.4</cell><cell>259.1</cell></row><row><cell>Dual Encoding</cell><cell>21.1</cell><cell>48.7</cell><cell>60.2</cell><cell>6</cell><cell>33.6</cell><cell>21.7</cell><cell>49.4</cell><cell>61.6</cell><cell>6</cell><cell>34.7</cell><cell>262.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 State-of-the-art on the TRECVID AVS 2016 / 2017 / 2018.</head><label>2</label><figDesc>Symbol asterisk (*) indicates numbers directly cited from the original papers. The proposed dual encoding model consistently perform the best.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TRECVID edition</cell><cell></cell></row><row><cell></cell><cell>2016</cell><cell>2017</cell><cell>2018</cell><cell>OVERALL</cell></row><row><cell>Top-3 TRECVID finalists:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rank 1*</cell><cell cols="3">5.4 [10] 20.6 [43] 12.1 [71]</cell><cell>-</cell></row><row><cell>Rank 2*</cell><cell cols="3">5.1 [11] 15.9 [12] 8.7 [72]</cell><cell>-</cell></row><row><cell>Rank 3*</cell><cell cols="3">4.0 [73] 12.0 [13] 8.2 [74]</cell><cell>-</cell></row><row><cell>Literature methods:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VideoStory* [7], [75]</cell><cell>8.7</cell><cell>15.0</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Markatopoulou et al. * [42] 6.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [19]</cell><cell>7.4</cell><cell>14.5</cell><cell>8.6</cell><cell>10.2</cell></row><row><cell>VSE++ [61]</cell><cell>13.5</cell><cell>16.3</cell><cell>10.6</cell><cell>13.5</cell></row><row><cell>W2VV [31]</cell><cell>14.9</cell><cell>19.8</cell><cell>10.3</cell><cell>15.0</cell></row><row><cell>W2VV++ [32]</cell><cell>15.1</cell><cell>21.3</cell><cell>10.6</cell><cell>15.7</cell></row><row><cell>Dual Encoding</cell><cell>15.2</cell><cell>23.1</cell><cell>12.1</cell><cell>16.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 State-of-the-art on VATEX.</head><label>3</label><figDesc>Our proposed model performs the best.</figDesc><table><row><cell>Method</cell><cell cols="3">Text-to-Video</cell><cell cols="3">Video-to-Text</cell><cell>SumR</cell></row><row><cell></cell><cell cols="3">R@1 R@5 R@10</cell><cell cols="3">R@1 R@5 R@10</cell><cell></cell></row><row><cell>W2VV [31]</cell><cell>14.6</cell><cell>36.3</cell><cell>46.1</cell><cell>39.6</cell><cell>69.5</cell><cell>79.4</cell><cell>285.5</cell></row><row><cell>VSE++ [61]</cell><cell>31.3</cell><cell>65.8</cell><cell>76.4</cell><cell>42.9</cell><cell>73.9</cell><cell>83.6</cell><cell>373.9</cell></row><row><cell>CE [19]</cell><cell>31.1</cell><cell>68.7</cell><cell>80.2</cell><cell>41.3</cell><cell>71.0</cell><cell>82.3</cell><cell>374.6</cell></row><row><cell>W2VV++ [32]</cell><cell>32.0</cell><cell>68.2</cell><cell>78.8</cell><cell>41.8</cell><cell>75.1</cell><cell>84.3</cell><cell>380.2</cell></row><row><cell>HGR [22]</cell><cell>35.1</cell><cell>73.5</cell><cell>83.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Dual Encoding</cell><cell>36.8</cell><cell>73.6</cell><cell>83.7</cell><cell>46.8</cell><cell>75.7</cell><cell>85.1</cell><cell>401.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 State-of-the-art on MPII-MD.shots retrieved from the MPII-MD test set by our model Query:</head><label>4</label><figDesc>Our proposed model performs the best. They wrap their arms around each other Query: In a restaurant, Someone sits at a table with the guy</figDesc><table><row><cell>Method</cell><cell cols="2">Text-to-Video</cell><cell></cell><cell cols="2">Video-to-Text</cell><cell></cell><cell>SumR</cell></row><row><cell></cell><cell cols="3">R@1 R@5 R@10</cell><cell cols="3">R@1 R@5 R@10</cell><cell></cell></row><row><cell>W2VV++ [32]</cell><cell>0.3</cell><cell>1.0</cell><cell>1.7</cell><cell>0.1</cell><cell>0.7</cell><cell>1.3</cell><cell>5.1</cell></row><row><cell>VSE++ [61]</cell><cell>0.2</cell><cell>0.9</cell><cell>1.6</cell><cell>0.8</cell><cell>2.2</cell><cell>3.6</cell><cell>9.3</cell></row><row><cell>W2VV [31]</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>1.3</cell><cell>4.0</cell><cell>6.1</cell><cell>12.3</cell></row><row><cell>CE [19]</cell><cell>0.9</cell><cell>3.1</cell><cell>5.7</cell><cell>1.1</cell><cell>3.6</cell><cell>5.8</cell><cell>20.2</cell></row><row><cell>Dual Encoding</cell><cell>1.7</cell><cell>4.8</cell><cell>7.0</cell><cell>1.4</cell><cell>4.7</cell><cell>7.0</cell><cell>26.6</cell></row><row><cell cols="4">Top-5 Query: The car stops before a chain link fence Query: Someone smiles</cell><cell></cell><cell></cell><cell cols="2">Ground truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 Effectiveness of Dual Encoding.</head><label>5</label><figDesc>The overall performance, as indicated by Sum of Recalls, goes up as more encoding layers are added. Dual encoding exploiting all the three levels is the best.</figDesc><table><row><cell>Encoding strategy</cell><cell></cell><cell cols="3">Text-to-Video Retrieval</cell><cell></cell><cell></cell><cell cols="3">Video-to-Text Retrieval</cell><cell></cell><cell>SumR</cell></row><row><cell></cell><cell cols="5">R@1 R@5 R@10 Med r mAP</cell><cell cols="5">R@1 R@5 R@10 Med r mAP</cell><cell></cell></row><row><cell>Level 1 (Mean pooling)</cell><cell>9.7</cell><cell>26.8</cell><cell>37.0</cell><cell>23</cell><cell>18.5</cell><cell>17.7</cell><cell>40.0</cell><cell>51.7</cell><cell>10</cell><cell>8.3</cell><cell>182.9</cell></row><row><cell>Level 2 (biGRU)</cell><cell>10.3</cell><cell>28.5</cell><cell>39.4</cell><cell>19</cell><cell>19.7</cell><cell>18.4</cell><cell>41.9</cell><cell>54.3</cell><cell>8</cell><cell>9.3</cell><cell>192.7</cell></row><row><cell>Level 3 (biGRU-CNN)</cell><cell>11.1</cell><cell>30.1</cell><cell>41.6</cell><cell>17</cell><cell>20.9</cell><cell>18.1</cell><cell>41.6</cell><cell>55.3</cell><cell>8</cell><cell>9.6</cell><cell>197.9</cell></row><row><cell>Level 1 + 2</cell><cell>10.6</cell><cell>28.8</cell><cell>39.2</cell><cell>20</cell><cell>19.9</cell><cell>19.1</cell><cell>43.1</cell><cell>54.5</cell><cell>8</cell><cell>9.2</cell><cell>195.3</cell></row><row><cell>Level 1 + 3</cell><cell>11.5</cell><cell>30.0</cell><cell>40.8</cell><cell>18</cell><cell>20.9</cell><cell>19.8</cell><cell>42.7</cell><cell>55.2</cell><cell>8</cell><cell>9.5</cell><cell>200.1</cell></row><row><cell>Level 2 + 3</cell><cell>11.4</cell><cell>30.6</cell><cell>41.7</cell><cell>17</cell><cell>21.2</cell><cell>19.9</cell><cell>44.3</cell><cell>55.8</cell><cell>8</cell><cell>10.1</cell><cell>203.8</cell></row><row><cell>Level 1 + 2 + 3</cell><cell>11.6</cell><cell>30.3</cell><cell>41.3</cell><cell>17</cell><cell>21.2</cell><cell>22.5</cell><cell>47.1</cell><cell>58.9</cell><cell>7</cell><cell>10.5</cell><cell>211.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 Performance of Dual encoding with distinct common spaces.</head><label>6</label><figDesc>Dataset: MSR-VTT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 Comparison with Transformer-based multi-modal methods for video retrieval by text.</head><label>7</label><figDesc>Dataset: MSR-VTT. Even though Dual Encoding uses relatively simple 2D CNN video features, it outperforms MMT on Test1k-Miech<ref type="bibr" target="#b28">[28]</ref>, while less effective on Test1k-Yu<ref type="bibr" target="#b30">[30]</ref>. The inclusion of Transformer features as shown inFig. 7makesDual Encoding top the performance table consistently on all the three test sets.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">Official full-size test set [34]</cell><cell></cell><cell></cell><cell cols="2">Test1k-Miech [28]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test1k-Yu [30]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">R@1 R@5 R@10 Med r mAP</cell><cell cols="5">R@1 R@5 R@10 Med r mAP</cell><cell cols="5">R@1 R@5 R@10 Med r mAP</cell></row><row><cell>ActBERT [39], HowTo100M pre-training</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.6</cell><cell>23.4</cell><cell>33.1</cell><cell>36</cell><cell>-</cell></row><row><cell>MMT [40]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.3</cell><cell>49.1</cell><cell>63.9</cell><cell>6</cell><cell>-</cell><cell>24.6</cell><cell>54.0</cell><cell>67.1</cell><cell>4</cell><cell>-</cell></row><row><cell>MMT [40], HowTo100M pre-training</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.6</cell><cell>57.1</cell><cell>69.6</cell><cell>4</cell><cell>-</cell></row><row><cell>Dual Encoding</cell><cell>11.6</cell><cell>30.3</cell><cell>41.3</cell><cell>17</cell><cell>21.2</cell><cell>23.0</cell><cell>50.6</cell><cell>62.5</cell><cell>5</cell><cell>36.1</cell><cell>21.1</cell><cell>48.7</cell><cell>60.2</cell><cell>6</cell><cell>33.6</cell></row><row><cell>Dual Encoding, with Transformer features only</cell><cell>8.5</cell><cell>25.1</cell><cell>36.1</cell><cell>22</cell><cell>17.4</cell><cell>16.8</cell><cell>41.6</cell><cell>55.8</cell><cell>8</cell><cell>28.7</cell><cell>15.0</cell><cell>40.7</cell><cell>54.2</cell><cell>8</cell><cell>27.6</cell></row><row><cell>Dual Encoding, with Transformer features included</cell><cell>13.3</cell><cell>34.0</cell><cell>45.7</cell><cell>13</cell><cell>23.8</cell><cell>25.7</cell><cell>52.9</cell><cell>66.4</cell><cell>5</cell><cell>38.9</cell><cell>32.4</cell><cell>62.3</cell><cell>72.8</cell><cell>3</cell><cell>46.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 Comparison with MMT in terms of model size and computation overhead at the inference stage.</head><label>8</label><figDesc>For each model, we measure the amount of FLOPs it takes to encode a given video-text pair. The computational cost of video feature extraction is excluded as that step is typically performed once in an offline mode. Numbers in parentheses indicate relative changes against MMT.</figDesc><table><row><cell>Model</cell><cell cols="2">Model Complexity</cell></row><row><cell></cell><cell cols="2">Parameters (M) FLOPs (G)</cell></row><row><cell>MMT</cell><cell>133.4</cell><cell>12.64</cell></row><row><cell>Dual Encoding</cell><cell>95.9 (? 28.1%)</cell><cell>3.64 (? 71.2%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>, the inclusion of the Transformer features clearly and consistently boosts the retrieval performance on all the three test sets. Meanwhile, using the Transformer features alone is less effective than the previous Dual Encoding model. Hence, our multi-level encoding modules, while ... ...</figDesc><table><row><cell>Video</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Video-side Multi-</cell><cell></cell></row><row><cell></cell><cell>level Encoding</cell><cell></cell></row><row><cell></cell><cell>Visual</cell><cell></cell></row><row><cell></cell><cell>Transformer</cell><cell>Hybrid Space</cell></row><row><cell>Text</cell><cell>Text-side Multi-</cell><cell>Learning</cell></row><row><cell>A</cell><cell>level Encoding</cell><cell></cell></row><row><cell>boy</cell><cell></cell><cell></cell></row><row><cell>... ...</cell><cell>Textual</cell><cell></cell></row><row><cell>trampoline</cell><cell>Transformer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 Performance of image-text retrieval on Flickr30k and MSCOCO.</head><label>9</label><figDesc>The proposed text-side multi-level encoding (MLE) is beneficial for VSE++<ref type="bibr" target="#b61">[61]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Text-to-Image</cell><cell cols="3">Image-to-Text</cell><cell>SumR</cell></row><row><cell></cell><cell cols="3">R@1 R@5 R@10</cell><cell cols="3">R@1 R@5 R@10</cell><cell></cell></row><row><cell>On Flickr30k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VSE++</cell><cell>23.1</cell><cell>49.2</cell><cell>60.7</cell><cell>31.9</cell><cell>58.4</cell><cell>68.0</cell><cell>291.3</cell></row><row><cell>VSE++, MLE</cell><cell>24.7</cell><cell>52.3</cell><cell>65.1</cell><cell>35.1</cell><cell>62.2</cell><cell>71.3</cell><cell>310.7</cell></row><row><cell>On MSCOCO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VSE++</cell><cell>33.7</cell><cell>68.8</cell><cell>81.0</cell><cell>43.6</cell><cell>74.8</cell><cell>84.6</cell><cell>389.6</cell></row><row><cell>VSE++, MLE</cell><cell>34.8</cell><cell>69.6</cell><cell>82.6</cell><cell>46.7</cell><cell>76.2</cell><cell>85.8</cell><cell>395.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. MEE and CE employ a separated branch to handle the two video features respectively, while others utilize the concatenated feature as the whole input.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-shot video retrieval using content and concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirajkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1857" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Composite concept discovery for zero-shot video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-shot event detection using multi-modal fusion of weakly supervised concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bondugula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2665" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic concept discovery for large-scale zero-shot event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2234" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event detection with zero example: select the right and suppress the wrong concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video2vec embeddings recognize events when examples are scarce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2089" to="2103" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TRECVID 2016: Evaluating video search, video event detection, localization, and hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quenot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Video indexing, search, detection, and description with focus on TRECVID</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Renoust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NII-HITACHI-UIT at TRECVID 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klinkigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ITI-CERTH participation in TRECVID 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Markatopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moumtzidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mironidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ioannidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Avgerinakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andreadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Waseda Meisei at TRECVID 2017: Ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vireo @ TRECVID 2017: Video-to-text, ad-hoc video search and video hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Waseda Meisei at TRECVID 2018: Ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakagome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the role of correlation and abstraction in cross-modal multimedia retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crossmedia semantic representation via bi-directional learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="877" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICMR</title>
		<imprint>
			<biblScope unit="page" from="19" to="27" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="374" to="390" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tree-augmented cross-modal encoding for complex-query video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1339" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning joint representations of videos and sentences with web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkil?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="651" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple parts-of-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dl-61-86 at TRECVID 2017: Video-to-text description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Som-hunter: Video browsing with relevance-to-som feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kratochv?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mejzl?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loko?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="790" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Find and focus: Retrieve and localize video events with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="200" to="216" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="471" to="487" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting visual features from text for image and video caption retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3377" to="3388" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">W2VV++: fully deep learning for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1786" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual coding theory: Retrospect and current status</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paivio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">255</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">TRECVID 2017: Evaluating adhoc and instance video search, events detection, video captioning and hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TRECVID 2018: Benchmarking video activity detection, video captioning and matching, video storytelling linking and video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, high-quality multilingual dataset for videoand-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3202" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ActBERT: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9346" to="9355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Query and keyframe representations for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Markatopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="407" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">University of Amsterdam and Renmin university at TRECVID 2017: Searching video, detecting events and describing video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2346" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Polysemous visual-semantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1979" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A graph-based framework to bridge movies and synopses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4592" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">VideoStory: A new multimedia embedding for few-example recognition and translation of events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A W2VV++ case study with automated and interactive text-to-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sou?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mejzl?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia, 2020</title>
		<imprint>
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stacked convolutional deep encoding network for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1746" to="1751" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">VSE++: Improved visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in BMVC</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Measuring and predicting tag importance for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2423" to="2436" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Combining classifiers: A theoretical framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Shuffled ImageNet banks for video event detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fusion of multimodal embeddings for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">TGIF: A new dataset and benchmark on animated GIF description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Renmin University of China and Zhejiang Gongshang University at TRECVID 2018: Deep cross-modal embeddings for videotext retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Informedia@TRECVID 2018: Ad-hoc video search with discrete and continuous representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaibhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">NTU ROSE lab at TRECVID 2018: Ad-hoc video search and video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bastan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Query understanding is key for zero-example video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Tensorflow code and pre-trained models for bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/bert" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards storylike visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
