<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Judging a Book by its Cover</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kenji Iwana</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><surname>Tahseen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raza</forename><surname>Rizvi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">German Research Center for Artificial Intelligence</orgName>
								<address>
									<settlement>Kaiserlautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheraz</forename><surname>Ahmed</surname></persName>
							<email>sheraz.ahmed@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="department">German Research Center for Artificial Intelligence</orgName>
								<address>
									<settlement>Kaiserlautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
							<email>andreas.dengel@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="department">German Research Center for Artificial Intelligence</orgName>
								<address>
									<settlement>Kaiserlautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
							<email>uchida@human.ait.kyushu-u.ac.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Advanced Information Technology</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<settlement>Fukuoka</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Kaiserslautern University of Technology</orgName>
								<address>
									<settlement>Kaiserlautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Judging a Book by its Cover</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Book covers communicate information to potential readers, but can that same information be learned by computers? We propose using a deep Convolutional Neural Network (CNN) to predict the genre of a book based on the visual clues provided by its cover. The purpose of this research is to investigate whether relationships between books and their covers can be learned. However, determining the genre of a book is a difficult task because covers can be ambiguous and genres can be overarching. Despite this, we show that a CNN can extract features and learn underlying design rules set by the designer to define a genre. Using machine learning, we can bring the large amount of resources available to the book cover design process. In addition, we present a new challenging dataset that can be used for many pattern recognition tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>"Don't judge a book by its cover" is a common English idiom meaning not to judge something by its outward appearance. Although, it still happens when a reader encounters a book. The cover of a book is often the first interaction and it creates an impression on the reader. It starts a conversation with a potential reader and begins to draw a story revealing the contents within. But, what does the book cover say? What are the clues that the book cover reveals? While the visual clues can communicate information to humans, we explore the possibility of using computers to learn about a book by its cover. Machine learning provides the ability to use a large amount of resources to the world of design. By bridging the gap between design and machine learning, we hope to use a large dataset to understand the secrets of visual design.</p><p>We propose a method deriving a relationship between book covers and their genre automatically. The goal is to determine if genre information can be learned based on the visual aspects of a cover created by the designer. This research can aid the design process by revealing underlying information, help promotion and sales processes by providing automatic genre suggestion, and be used in computer vision fields. The difficulty of this task is that books come with a wide variety of book covers and styles, including nondescript and misleading covers. Unlike other object detection and classification tasks, genres are not concretely defined. Another problem is that there is a massive amount of books exist and it is not suitable for exhaustive search methods.</p><p>To tackle this task, we present the use of an artificial neural network. The concept of neural networks and neural coding is to use interconnected nodes to work together to capture information. Early neural network-like models such as multilayer perception learning were invented in the 1970s but fell out of favor <ref type="bibr" target="#b0">[1]</ref>. More recently, artificial neural networks have been a focus of state-of-the-art research because of their successes in pattern recognition and machine learning. Their successes are in part due to the increase in data availability, increase in processing power, and introduction of GPUs <ref type="bibr" target="#b1">[2]</ref>.</p><p>Convolutional Neural Networks (CNN) <ref type="bibr" target="#b2">[3]</ref>, in particular, are multilayer neural networks that utilize learned convolutional kernels, or filters, as a method of feature extraction. The general idea is to use learned features rather than pre-designed features as the feature representation for image recognition. Recent deep CNNs combine multiple convolutional layers along with fully-connected layers. By increasing the depth of the network, higher level features can be learned and discriminative parts of the images are exaggerated <ref type="bibr" target="#b3">[4]</ref>. These deep CNNs have had successes in many fields including digitrecognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> and large-scale image recognition <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>The contribution of this paper is to demonstrate that connections between book genres can be learned using only the cover images. To solve this task, we used the concept of transfer learning and developed a CNN based system for book cover genre classification. AlexNet <ref type="bibr" target="#b7">[8]</ref> pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> is adapted for the task of genre recognition. We also reveal the relationships automatically learned between genres and book covers.</p><p>Secondly, we created a large dataset containing 137,788 books in 32 classes made of book cover images, title text, author text, and category membership. This dataset is very challenging and can be used for a variety of tasks some of which include text recognition, font analysis, and genre prediction. Furthermore, although AlexNet pre-trained on ImageNet has already achieved state-of-the-art results on document classification <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we had a limited accuracy which indicates the high level of difficulty of the proposed dataset.</p><p>The remaining of this paper is organized as follows. Section II provides related works in design learning with machine learning. Section III elaborates on CNNs and the details of the proposed method. In Section IV, we confirm the proposed method and analyzed the experimental results. The book cover designed principles learned by the CNN is detailed in Section V. Finally, Section VI draws the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Visual design is intentional and serves a purpose. It has a rich history and exploring the purposes of design has been extensively analyzed by designers <ref type="bibr" target="#b11">[12]</ref> but is a relatively new field in machine learning.</p><p>Techniques have been used to identify artistic styles and qualities of paintings and photographs <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Gatys, et al. <ref type="bibr" target="#b13">[14]</ref> used deep CNNs to learn and copy the artistic style of paintings. Similarly, the goal of this trial is to learn the stylistic qualities of the work, but we go beyond to learn the underlying meaning behind the style.</p><p>In the field of genre classification, there have been attempts to classify music by genre <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. It was also done in the fields of paintings <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref> and text <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, most of these methods use designed features or features specific to the task. In a more general sense, document classification tackles a similar problem in that it classes documents into architectural categories. In particular, deep CNNs have been successful in document classification <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Harley et al. <ref type="bibr" target="#b22">[23]</ref>, used a region-based CNNs to guide the document classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CONVOLUTIONAL NEURAL NETWORKS</head><p>Modern CNNs are made up of three components: convolutional layers, pooling layers, and fully-connected layers. The convolutional layers consist of feature maps produced by repeatedly applying filters across the input. The filters represent shared weights and are trained using backpropagation. The feature maps resulting from the applied filters are down-sampled by a max pooling layer to reduce redundancy improving the computational time for future layers. Finally, the last few layers of a CNN are made up of fully-connected layers. These layers are given a vector representation of the images from a preceding pooling layer and continue like standard feedforward neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AlexNet</head><p>The network used for our book cover classification is inspired from the work of Krizhevsky et al. <ref type="bibr" target="#b7">[8]</ref> We used a pre-trained network on ImageNet <ref type="bibr" target="#b8">[9]</ref>. By pre-training AlexNet on a very large dataset such as ImageNet, its possible to take advantage of the learned features and transfer it to other applications. Initializing a network with transferred features has shown to improve generalization <ref type="bibr" target="#b23">[24]</ref>. To accomplish this, we remove the original softmax output layer for the 1,000class classification of ImageNet and replace it with a 30class softmax for the experiment. Subsequently, the training is continued using the pre-trained parameters as an initialization.</p><p>The network architecture is as follows. The network consists of a total of eight layers, where the first five are convolutional layers followed by three fully-connected layers. Of the five convolutional layers, the first and second layers are made of 96 filters of size 11 ? 11 ? 3 stride 4, and 5 ? 5 ? 48 stride 1 respectively and are response-normalized. The last three convolutional layers have 384, 384, and 256 nodes and use filters of size 3 ? 3 ? 192. These last three convolutional layers do not use any normalization or pooling. The final three fully-connected layers have 4,096 nodes each. Both the convolutional layers and the fully-connected layers have Rectified Linear Unit (ReLU) activation functions. Dropout with a keep probability of 0.5 is used for the first two fullyconnected layers.</p><p>The model was trained with gradient decent with an initial learning rate of 0.01, after which, the learning rate was divided by 10 every 100,000 iterations. The reported results were taken after 450,000 iterations. Also, a weight decay of 0.0005 and momentum of 0.9 was implemented. The update rule for each weight w is defined as <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_0">v i+1 = 0.9v i ? 0.0005 w i ? ?L ?w wi (1) w i+1 = w i + v i+1 .<label>(2)</label></formula><p>B. LeNet For a comparison, we trained a network similar to a LeNet <ref type="bibr" target="#b2">[3]</ref>. This CNN used input images, that were scaled to 56px by 56px, in batches of 200. There were three convolutional layers with 32 nodes, 64 nodes, and 128 nodes respectively. Each convolutional layer uses a filter size of 5 ? 5 ? 1 at stride 1 and were proceeded by maxpooling layers of 2 ? 2 stride 1. The network concluded with a 1024 node full-convolutional layer and a softmax output layer. Each layer used ReLU activations and a constant learning rate of 0.0001. Dropout with a keep probability of 0.5 was used after the fullyconnected layer. Finally, the network was trained for 30,000 iterations of using an Adam optimizer <ref type="bibr" target="#b24">[25]</ref>. The modified LeNet was trained on the same training set and tested with the same test set as the AlexNet experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset preparation</head><p>The dataset was collected from the book cover images and genres listed by Amazon.com <ref type="bibr" target="#b25">[26]</ref>. The full dataset contains 137,788 unique book cover images in 32 classes as well as the title, author, and subcategories for each respective book. Each book's class is defined as the top categories under "Books" in the Amazon.com marketplace. However, for the experiment we refined the dataset into 30 classes of 1,900 books in each class. The 30 classes, or genres, used in the experiment are listed in <ref type="table" target="#tab_0">Table I</ref>. To equalize the number of books in each class, books were chosen at random to be included in the experiment. The two categories, "Gay &amp; Lesbian" and "Education &amp; Teaching," were not used for the experiment because they only contain 1,341 and 1,664 books respectively, thus not having enough representation in the dataset.</p><p>Also, when the dataset was collected, each book was assigned to only a single category. If the book belonged to multiple categories, one was chosen at random. We randomized and split the dataset into 90% training set and 10% test set. No pruning of cover images and no class membership corrections were done. In addition, we resized all of the images to fit 227px by 227px by 3 color channels for the input of the AlexNet and 56px by 56px by 3 color channels for LeNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>The pre-trained AlexNet with transfer learning resulted in a test set Top 1 classification accuracy of 24.7% , 33.1% for Top 2, and 40.3% for Top 3 which are 7.4, 5.0, and 4.0 times better than random chance respectively. As comparison, using the modified LeNet, we had a Top 1 accuracy of 13.5%, Top 2 accuracy of 21.4%, and Top 3 accuracy of 27.8%. The AlexNet performed much better on this dataset than the LeNet. Considering that CNN solutions are state-of-the-art for image and document recognition, the results show that classification of book cover designs is possible, although a very difficult task. <ref type="table" target="#tab_0">Table I</ref> shows the individual Top 1 accuracies for each genre. In every class except "Christian Books &amp; Bibles," the AlexNet performed better. For most cases, AlexNet had more than twice as good Top 1 accuracy compared to LeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis</head><p>In general, most cover images have either a strong activation toward a single class or are ambiguous and could be part of many classes at once. <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of books classified in the "Cookbooks, Food &amp; Wine" category. When the cover contained an image of food, the CNN predicted the correct class and with a high probability. But, the covers with more ambiguous images resulted in a low confidence. The misclassified examples in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref> failed for understandable reasons; the first two are ambiguous and can reasonably be classified as "Self-Help" and "Science &amp; Math" respectively.   The final example had a strong probability of being in "Comics &amp; Graphic Novels" and "Children's Books" because the cover image features an illustration of a vehicle. Many books contain misleading covers like these examples and correct classification would be difficult even for a human without reading the text. <ref type="figure" target="#fig_1">Figure 2</ref> reveals another example of misleading cover images, but for the "Biographies &amp; Memoirs" category. The difficulty of this category comes from a high rate of sharing qualities with other categories causing substantial ambiguity of the genre itself. A high number of misclassifications from the "Biographies &amp; Memoirs" category went into "History." However, <ref type="figure" target="#fig_1">Fig. 2</ref> shows that most of those misclassifications could be considered to be part of both categories. We also observed a similar relationship between "Comics &amp; Graphic Novels" and "Children's Books" and between "Medical Books" and "Science &amp; Math." This shows that the AlexNet network was able to automatically learn relationships between categories based solely on the cover images.</p><p>From visualizing the softmax activations in <ref type="figure" target="#fig_2">Fig. 3</ref>, we can see an overview of the probability of class membership as determined by the network for each of the book covers. The figure clearly shows the large central cluster of difficult covers as well as the confident correctly classified covers near each axis. For classes such as "Politics &amp; Social Sciences" and "Christian Books &amp; Bibles," the strong softmax responses are sparse and it is reflected in their very low recognition accuracy. Conversely, the densely activated axes have high recognition accuracies indicating that they have unique visual relationships to their genre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BOOK COVER DESIGN PRINCIPLES</head><p>Analysis of the results reveals that AlexNet was able to learn certain high-level features of each category. Some of these correlated features may be objects such as portraits for "Biographies &amp; Memoirs" or food for "Cookbooks, Food &amp; Wine." Other times it is colors, layout, or text. In this section, we explore the design principles that the CNN was able to automatically learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Color Matters</head><p>In the absence of distinguishable features, the CNN has to rely on color alone to classify covers. Because of this, many classes get associated to certain colors for books with limited features. Shown in <ref type="figure">Fig. 4</ref>, the AlexNet relates white to "Self-Help," yellow to "Religion &amp; Spirituality," green to "Science &amp; Math," blue to "Computers &amp; Technology," red to "Medical Books," and black to "Biographies &amp; Memoirs." Although, book covers were also important for classification. For example, "Cookbooks, Food &amp; Wine" often features food and are commonly by shades of beige and tan ( <ref type="figure">Fig. 5</ref>). Likewise, there is a high representation of gardening books in the "Crafts, Hobbies &amp; Home" class, therefore, green books are commonly classified in that genre. Also, the tone of the book can define the mood, so "Children's Books" commonly have designs with yellow or bright backgrounds and "Science Fiction &amp; Fantasy" books usually have black or dark backgrounds. The AlexNet was able to successfully capture the mood of book genres by grouping books of certain moods to respective genres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Objects Matter</head><p>The image on book covers is usually the thing that first attracts potential readers to a book. It should be no surprise that the object featured on the cover has an effect on how it gets classified. What is surprising about the results of our experiment is how the network is able to distinguish different genres but with common objects. For instance, featuring people on the cover is common among many genres, but the type of person or how the person is dressed determines how the book gets classified. <ref type="figure">Figure 6</ref> shows four genres that centrally display humans, but have discriminating features that make the classes separable.</p><p>The structure and layout of the book cover also makes a difference in the classification. Books with rectangular title boards, no matter the color, tended to be classified as "Law" and books with a large landscape photographs tended to be "Travel" <ref type="figure">(Fig. 7)</ref>. This trend continued to other categories, such as "Cookbooks, Food &amp; Wine" with a central image of food stretching to the edges of the cover, "Biographies &amp; Memoirs" featuring close-up shots of people, and reference and textbooks containing solid color bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Text Matters</head><p>Another interesting design principle captured by the AlexNet is the text qualities and the font properties. The best example of this is "Mystery, Thriller &amp; Suspense," shown in <ref type="figure">Fig. 8</ref>. Despite having a similar color pallet and image content to "Romance" and "Science Fiction &amp; Fantasy," the common thread in many of the classified "Mystery, Thriller &amp; Suspense" books was large overlaid sans serif text. <ref type="figure">Figure 8</ref> also shows that "Calendars" often de-emphasize the title text so the focus is on the cover image. On the other hand, the figure also shows that "Literature &amp; Fiction" often uses expressive fonts to reveal messages about the book. The text style on the cover of a book affects the classification, revealing that relationships between text style and genre exist.</p><p>In particular, of the 30 classes, "Test Preparation" had the highest recognition rate at 68.9%, much higher than the overall accuracy. The reason behind this high accuracy is that "Test Preparation" book covers are often formulaic. They tend to have an acronym in large letters (e.g. "SAT," "GRE," "GMAT," etc.) near the top with horizontal or vertical stripes and possibly a small image of people. The large text is important because when compared to other non-fiction and reference classes, the presence of large acronyms is the most discriminating factor. <ref type="figure" target="#fig_5">Figure 9</ref> shows books from other categories that were incorrectly classified as "Test Preparation." These examples follow the design rules similar to many other "Test Preparation" books, but the actual content of the text reveals the books as other classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we presented the application of machine learning to predict the genre of a book based on its cover image. We showed that it is possible to draw a relationship between book cover images and genre using automatic recognition. Using a CNN model, we categorized book covers into genres and the results of using AlexNet with transfer learning had an accuracy of 24.7% for Top 1, 33.1% for Top 2, and 40.3% for Top 3 in 30-class classification. The 5-layer LeNet had a lower accuracy of 13.5.7% for Top 1, 21.4% for Top 2, and 27.8% for Top 3. Using the pre-trained AlexNet had a dramatic effect on the accuracy compared to the LeNet.</p><p>However, classification of books based on the cover image is a difficult task. We revealed that many books have cover images with few visual features or ambiguous features causing for many incorrect predictions. While uncovering some of the design rules found by the CNN, we found that books can have also misleading covers. In addition, because books can be part of multiple genres, the CNN had a poor Top 1 performance. To overcome this, experiments can be done using multi-label classification.</p><p>Future research will be put into further analysis of the characteristics of the classifications and the features determined by the network in an attempt to design a network that is optimized for this task. Increasing the size of the network or tuning the hyperparameters may improve the performance. In addition, the book cover dataset we created can be used for other tasks as it contains other information such as title, author, and category hierarchy. Genre classification can also be done using supplemental information such as textual features alongside the cover images. We hope to design more robust models to better capture the essence of cover design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Sample test set images from the "Cookbooks, Food &amp; Wine" category. The top row shows the cover images and the bottom row shows their respective softmax activations from AlexNet. The blue bar is the correct class and the red bars are the other classes. Only the top 5 highest activations are displayed. (a) is examples of correctly classified books and (b) is examples of books belonging to "Cookbooks, Food &amp; Wine" that were misclassified as other classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The "Biographies &amp; Memoirs" book covers that were classified by AlexNet as "History." While misclassified, many of these books also can relate to "History" despite the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of the output layer softmax activations of AlexNet. Each point is a 30-dimensional vector where each dimension is the probability of each output class. For visualization purposes, the points are mapped into 2-dimensional subspace with PCA. The arrows represent the axes of each class. The class ground truth is represented by colors, chosen at random. Sample images with high activations from each class are enlarged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 :Fig. 5 : 6 :</head><label>456</label><figDesc>Book covers from genres with particular color associations. Each example was correctly classified by the AlexNet. classifying simple book covers by color alone causes many misclassifications to occur.The color association does not only restrict itself to simple book covers. Despite having active book covers, the tone of "Cookbooks, Food &amp; Wine" Book covers that were successfully classified by the common moods or color pallets of respective genres. Correctly classified book covers that feature different aspects of humans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Examples of layout considerations as determined by the AlexNet for "Law" and "Travel.". Book covers showing text and font differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Books from other categories that were classified as "Test Preparation." The correct labels for the books from left to right are "Sports &amp; Outdoors," "Parenting &amp; Relationships," "Medical Books," "Health, Fitness &amp; Dieting," "Health, Fitness &amp; Dieting," and "Cookbooks, Food &amp; Wine."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Top 1 Genre Accuracy Comparison</figDesc><table><row><cell></cell><cell cols="2">LeNet</cell><cell cols="2">AlexNet</cell></row><row><cell>Genre</cell><cell cols="2">Top 1 Top 3</cell><cell cols="2">Top 1 Top 3</cell></row><row><cell>Arts &amp; Photography</cell><cell>5.8</cell><cell>11.6</cell><cell>12.1</cell><cell>31.1</cell></row><row><cell>Biographies &amp; Memoirs</cell><cell>5.3</cell><cell>18.4</cell><cell>13.2</cell><cell>29.5</cell></row><row><cell>Business &amp; Money</cell><cell>10.0</cell><cell>25.3</cell><cell>12.6</cell><cell>25.8</cell></row><row><cell>Calendars</cell><cell>18.9</cell><cell>37.9</cell><cell>47.9</cell><cell>65.3</cell></row><row><cell>Children's Books</cell><cell>24.7</cell><cell>42.1</cell><cell>42.1</cell><cell>61.6</cell></row><row><cell>Comics &amp; Graphic Novels</cell><cell>15.8</cell><cell>33.7</cell><cell>47.4</cell><cell>67.9</cell></row><row><cell>Computers &amp; Technology</cell><cell>29.5</cell><cell>42.8</cell><cell>44.7</cell><cell>59.5</cell></row><row><cell>Cookbooks, Food &amp; Wine</cell><cell>14.2</cell><cell>32.6</cell><cell>43.7</cell><cell>57.4</cell></row><row><cell>Crafts, Hobbies &amp; Home</cell><cell>7.4</cell><cell>22.1</cell><cell>17.4</cell><cell>36.8</cell></row><row><cell>Christian Books &amp; Bibles</cell><cell>8.4</cell><cell>23.7</cell><cell>7.4</cell><cell>26.3</cell></row><row><cell>Engineering &amp; Transportation</cell><cell>10.0</cell><cell>21.1</cell><cell>20.0</cell><cell>34.7</cell></row><row><cell>Health, Fitness &amp; Dieting</cell><cell>4.2</cell><cell>15.8</cell><cell>12.6</cell><cell>29.5</cell></row><row><cell>History</cell><cell>6.3</cell><cell>16.8</cell><cell>12.6</cell><cell>27.9</cell></row><row><cell>Humor &amp; Entertainment</cell><cell>5.3</cell><cell>16.3</cell><cell>10.5</cell><cell>22.6</cell></row><row><cell>Law</cell><cell>14.7</cell><cell>25.8</cell><cell>25.3</cell><cell>38.4</cell></row><row><cell>Literature &amp; Fiction</cell><cell>3.2</cell><cell>12.1</cell><cell>11.1</cell><cell>22.6</cell></row><row><cell>Medical Books</cell><cell>12.6</cell><cell>30.0</cell><cell>19.5</cell><cell>36.8</cell></row><row><cell>Mystery, Thriller &amp; Suspense</cell><cell>23.7</cell><cell>40.0</cell><cell>34.2</cell><cell>48.9</cell></row><row><cell>Parenting &amp; Relationships</cell><cell>14.7</cell><cell>35.3</cell><cell>24.2</cell><cell>39.5</cell></row><row><cell>Politics &amp; Social Sciences</cell><cell>3.7</cell><cell>18.4</cell><cell>6.8</cell><cell>21.6</cell></row><row><cell>Reference</cell><cell>13.2</cell><cell>26.8</cell><cell>20.0</cell><cell>34.2</cell></row><row><cell>Religion &amp; Spirituality</cell><cell>8.4</cell><cell>27.9</cell><cell>16.3</cell><cell>31.6</cell></row><row><cell>Romance</cell><cell>27.4</cell><cell>43.2</cell><cell>45.3</cell><cell>60.5</cell></row><row><cell>Science &amp; Math</cell><cell>8.4</cell><cell>26.3</cell><cell>14.2</cell><cell>29.5</cell></row><row><cell>Science Fiction &amp; Fantasy</cell><cell>14.7</cell><cell>33.2</cell><cell>35.8</cell><cell>52.6</cell></row><row><cell>Self-Help</cell><cell>13.7</cell><cell>31.6</cell><cell>14.2</cell><cell>33.2</cell></row><row><cell>Sports &amp; Outdoors</cell><cell>5.3</cell><cell>16.8</cell><cell>14.7</cell><cell>28.4</cell></row><row><cell>Teen &amp; Young Adult</cell><cell>7.9</cell><cell>17.4</cell><cell>12.1</cell><cell>28.4</cell></row><row><cell>Test Preparation</cell><cell>47.9</cell><cell>56.8</cell><cell>68.9</cell><cell>78.4</cell></row><row><cell>Travel</cell><cell>19.5</cell><cell>33.7</cell><cell>33.2</cell><cell>48.4</cell></row><row><cell>Total Average</cell><cell>13.5</cell><cell>27.8</cell><cell>24.7</cell><cell>40.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was partially supported by MEXT-Japan (Grant No. 26240024) and the Institute of Decision Science for a Sustainable Society, Kyushu University, Fukuoka, Japan.</p><p>All book cover images are copyright Amazon.com, Inc. The display of the images are transformative and are used as fair use for academic purposes.</p><p>The book cover database is available at https://github.com/ uchidalab/book-dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High performance convolutional neural networks for document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Int. Workshop Frontiers in Handwriting Recognition. Suvisoft</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conf. Comput. Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Comp. Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conf. Comput. Vision and Patern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepdocclassifier: Document classification with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Capobianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Document Anal. and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1111" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3168" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graphic Design History: A Critical Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcvarish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recognizing image style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.3715</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assoc. Computing Mach. Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic genre classification using large high-level musical feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Soc. of Music Inform. Retrieval</title>
		<imprint>
			<biblScope unit="page" from="525" to="530" />
			<date type="published" when="2004" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Content-based methods for the management of digital music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2000 IEEE Int. Conf. Acoustics, Speech, and Signal Process</title>
		<meeting>2000 IEEE Int. Conf. Acoustics, Speech, and Signal ess</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2437" to="2440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classifying paintings by artistic genre: An analysis of features &amp; classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zujovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gandy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Int. Workshop Multimedia Signal Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to classify documents according to genre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushmerick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. for Inform. Sci. and Technology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1506" to="1518" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stable classification of text genres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Petrenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="385" to="393" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional nets for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ufkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Document Anal. and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Amazon.com: Online shopping for electronics, apparel, computers, books, dvds &amp; more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amazon</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="http://www.amazon.com/,ac-cessed" />
		<imprint>
			<date type="published" when="2015-10-27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
