<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhe</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alibaba</forename><surname>Group</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern deep-learning-based lane detection methods are successful in most scenarios but struggling for lane lines with complex topologies. In this work, we propose Cond-LaneNet, a novel top-to-down lane detection framework that detects the lane instances first and then dynamically predicts the line shape for each instance. Aiming to resolve lane instance-level discrimination problem, we introduce a conditional lane detection strategy based on conditional convolution and row-wise formulation. Further, we design the Recurrent Instance Module(RIM) to overcome the problem of detecting lane lines with complex topologies such as dense lines and fork lines. Benefit from the end-to-end pipeline which requires little post-process, our method has real-time efficiency. We extensively evaluate our method on three benchmarks of lane detection. Results show that our method achieves state-of-the-art performance on all three benchmark datasets. Moreover, our method has the coexistence of accuracy and efficiency, e.g. a 78.14 F1 score and 220 FPS on CULane. Our code is available at https://github.com/aliyun/ conditional-lane-detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Artificial intelligence technology is increasingly being used in the driving field, which is conducive to autonomous driving and the Advanced Driver Assistance System(ADAS). As a basic problem in autonomous driving, lane detection plays a vital role in applications such as vehicle real-time positioning, driving route planning, lanekeeping assist, and adaptive cruise control.</p><p>Traditional lane detection methods usually rely on handcrafted operators to extract features <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>, and then fit the line shape through post-processing such as Hough transform <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43]</ref> and Random Sampling Consensus (RANSAC) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b14">15]</ref>. However, traditional methods faile in maintaining robustness in real scene since the hand-crafted models cannot cope with the diversity of lane lines in different scenarios <ref type="bibr" target="#b26">[27]</ref>. Recently, most studies about lane detection have focused on deep learning <ref type="bibr" target="#b33">[34]</ref>. Early deep-learning-based methods detect lane lines through segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>. Recently, various methods such as anchor-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39]</ref>, rowwise detection methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref>, and parametric prediction methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref> have been proposed and continue to refresh the accuracy and efficiency.</p><p>Although deep-learning-based lane detection methods have made great progress <ref type="bibr" target="#b41">[42]</ref>, there are still many challenges.</p><p>A common problem for lane detection is instance-level discrimination. Most lane detection methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39]</ref> predict lane points first and then aggregate the points into lines. But it is still a common challenge to assign different points to different lane instances <ref type="bibr" target="#b33">[34]</ref>. A simple solution is to label the lane lines into classes of a fixed number(e.g. labeled as 0, 1, 2, 3 if the maximum lane number is 4) and make a multi-class classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref>. But the limitation is that only a predefined, fixed number of lanes can be detected <ref type="bibr" target="#b26">[27]</ref>. To overcome this limitation, the post-clustering strategy is investigated <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref>. However, this strategy is struggling for some cases such as dense lines. Another approach is anchor-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref>. But it is not flexible to predict the line shape due to the fixed shape of the anchor <ref type="bibr" target="#b38">[39]</ref>.</p><p>Another challenge is the detection of lane lines with complex topologies, such as fork lines and dense lines, as is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Such cases are common in driving sce-narios, e.g. fork lines usually appear when the number of lanes changes. Homayounfar et al. <ref type="bibr" target="#b9">[10]</ref> proposed an offline lane detection method for HDMap(High Definition Map) which can deal with the fork lines. However, there are few studies on the perception of lane lines with complex topologies for real-time driving scenarios.</p><p>The lane detection task is similar to instance segmentation, which requires assigning different pixels to different instances. Recently, some studies <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref> have investigated the conditional instance segmentation strategy, which is also promising for lane detection tasks. However, it is inefficient to directly apply this strategy to lane detection, since the constraint for the mask is not completely consistent with specifying the line shape. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In this work, we propose a novel lane detection framework called CondLaneNet. Aiming to resolve the lane instance-level discrimination problem, we propose the conditional lane detection strategy inspired by CondInst <ref type="bibr" target="#b34">[35]</ref> and SOLOv2 <ref type="bibr" target="#b37">[38]</ref>. Different from the instance segmentation tasks, we focus the optimization on specifying the lane line shape based on the row-wise formulation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>. Moreover, we design the Recurrent Instance Module(RIM) to deal with the detection of lane lines with complex topologies such as the dense lines and fork lines. Besides, benefit from the end-to-end pipeline that requires little postprocess, our method achieves real-time efficiency. The contributions of this work are summarised as follows:</p><p>? We have greatly improved the ability of lane instancelevel discrimination by the proposed conditional lane detection strategy and row-wise formulation.</p><p>? We solve the problem of detecting lane lines with complex topologies such as dense lines and fork lines by the proposed RIM.</p><p>? Our CondLaneNet framework achieves state-of-the-art performance on multiple datasets, e.g. an 86.10 F1 score(4.6% higher than SOTA) on CurveLanes and a 79.48 F1 score(3.2% higher than SOTA) on CULane. Moreover, the small version of our CondLaneNet has high efficiency while ensuring high accuracy, e.g. a 78.14 F1 score and 220 FPS on CULane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section introduces the recent deep-learning-based lane detection methods. According to the strategy of line shape description, current methods can be divided into four categories: segmentation-based methods, anchorbased methods, row-wise detection methods, and parametric prediction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Segmentation-based Methods</head><p>Segmentation-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref> are most common and have achieved impressive perfor-mance. Different from general semantic segmentation tasks, lane detection requires instance-level discrimination. Early methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref> used a multi-class classification strategy for lane instance discrimination. As explained in the previous section, this strategy is inflexible. For higher instance accuracy, the post-clustering strategy <ref type="bibr" target="#b3">[4]</ref> was widely applied <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref>. Considering that the segmentation-based methods generally predict a down-scaled mask, some methods <ref type="bibr" target="#b18">[19]</ref> predict an offset map for refinement. Recently, some studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref> indicated that it is inefficient to describe the lane line as a mask because the emphasis of segmentation is obtaining accurate classification per pixel rather than specifying the line shape. To overcome this problem, anchor-based methods and row-wise detection methods were proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anchor-based Methods</head><p>Anchor-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39</ref>] take a top-to-down pipeline and focus the optimization on the line shape by regressing the relative coordinates. The predefined anchors can reduce the impact of the no-visual-clue problem <ref type="bibr" target="#b31">[32]</ref> and improve the ability of instance discrimination. Due to the slender shape of lane lines, the widely used boxanchor in object detection <ref type="bibr" target="#b6">[7]</ref> cannot be used directly. Point-LaneNet <ref type="bibr" target="#b1">[2]</ref> and CurveLane <ref type="bibr" target="#b38">[39]</ref> used vertical lines as anchors. LaneATT <ref type="bibr" target="#b31">[32]</ref> designed anchors with a slender shape and achieves state-of-the-art performance on multiple datasets. However, the fixed anchor shape results in a low degree of freedom in describing the line shape <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Row-wise Detection Methods</head><p>Row-wise detection methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref> make good use of the shape prior and predict the line location for each row. In the training phase, the constraint on the overall line shape is realized through the location constraint of each row. Based on the continuity and consistency of the predicted locations from row to row, shape constraints can be added to the model <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Besides, in terms of efficiency, some recent row-wise detection methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b10">11]</ref> have achieved advantages. However, instance-level discrimination is still the main problem for row-wise formulation. As the widely used post-clustering module <ref type="bibr" target="#b3">[4]</ref> in segmentationbased methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref> cannot be directly integrated into the row-wise formulation, row-wise detection methods still take the multi-class classification strategy for lane instance discrimination. Considering the impressive performance on accuracy and efficiency, we also adopt the row-wise formulation and propose some novel strategies to overcome the instance-level discrimination problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Parametric Prediction Methods</head><p>Different from the above methods which predict points, parametric prediction methods directly output parametric  <ref type="figure">Figure 2</ref>. The structure of our CondLaneNet framework. The backbone adopts standard ResNet <ref type="bibr" target="#b7">[8]</ref> and FPN <ref type="bibr" target="#b22">[23]</ref> for multi-scale feature extraction. The transformer encoder module <ref type="bibr" target="#b36">[37]</ref> is added for more efficient context feature extraction. The proposal head is responsible for detecting the proposal points which are located at the start point of the line. Meanwhile, a parameter map that contains the dynamic convolution kernels is predicted. The conditional shape head predicts the row-wise location, the vertical range, and the offset map to describe the shape for each line. To address the cases of dense lines and fork lines, the RIM is designed.</p><p>lines expressed by curve equation. PolyLaneNet <ref type="bibr" target="#b30">[31]</ref> firstly proposed to use a deep network to regress the lane curve equation. LSTR <ref type="bibr" target="#b24">[25]</ref> introduced transformer [37] to lane detection task and get 420fps detection speed. However, the parametric prediction methods have not surpassed other methods in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Given an input image I ? R C?H?W , the goal of our CondLaneNet is to predict a collection of lanes L = {l 1 , l 2 , ..., l N }, where N is the total number of lanes. Generally, each lane l k is represented by an ordered set of coordinates as follows.</p><formula xml:id="formula_0">l k = [(x k1 , y k1 ), (x k2 , y k2 ), ..., (x kN k , y kN k )] (1)</formula><p>Where k is the index of lane and N k is the max number of sample points of the kth lane.</p><p>The overall structure of our CondLaneNet is shown in <ref type="figure">Figure 2</ref>. This section will first present the conditional lane detection strategy, then introduce the RIM(Recurrent Instance Module), and finally detail the framework design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional Lane Detection</head><p>Focusing on the instance-level discrimination ability, we propose the conditional lane detection strategy based on conditional convolution -a convolution operation with dynamic kernel parameters <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40]</ref>. The conditional detection process <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref> has two steps: instance detection and shape prediction, as is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The instance detection step predicts the object instance and regresses a set of dynamic kernel parameters for each instance. In the shape prediction step, conditional convolutions are applied to specify the instance shape. This process is conditioned on the dynamic kernel parameters. Since each instance corresponds to a set of dynamic kernel parameters, the shapes can be predicted instance-wisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance 1 Instance 2 Instance 3</head><p>Step 2: Shape prediction Step 1: Instance detection <ref type="bibr">Instance</ref>  Step 2: Shape prediction Step 1: Instance detection This strategy has achieved impressive performance on instance segmentation tasks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref>. However, directly applying the conditional instance segmentation strategy to lane detection is blunt and inappropriate. On the one hand, the segmentation-based shape description is inefficient for lane lines due to the excessively high degree of freedom <ref type="bibr" target="#b29">[30]</ref>. On the other hand, the instance detection strategy for general objects is not suitable for slender and curved objects due to the inconspicuous visual characteristic of the border and the central. Our conditional lane detection strategy improves shape prediction and instance detection to address the above problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Shape Prediction</head><p>We improve the row-wise formulation <ref type="bibr" target="#b29">[30]</ref> to predict the line shape based on our conditional shape head, as is shown in <ref type="figure">Figure 2</ref>. In the row-wise formulation, we predict the lane location on each row and then aggregate the locations to get the lane line in the order from bottom to top, based on the prior of the line shape. Our row-wise formulation has three components: the row-wise location, the vertical range, and the offset map. The first two outputs are basic elements for most row-wise detection methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>. Besides, we predict an offset map as the third output for further refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear</head><p>Positive Negetive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Row-wise location</head><p>Vertical range Row-wise Location As is shown in <ref type="figure" target="#fig_2">Figure 4</ref>, we divide the input image into grids of shape Y ? X and predict a corresponded location map, which is a feature map of shape 1 ? Y ? X output by the proposed conditional shape head. On the location map, each row has an abscissa indicating the location of the lane line.</p><p>To get the row-wise location, a basic approach is to process the X-classes classification in each row. In inference time, the row-wise location is determined by picking the most responsive abscissa in each row. However, a common situation is that the line location is between the two grids, and both the two grids should have a high response. To overcome this problem, we introduce the following formulation.</p><p>For each row, we predict the probability that the lane line appears in each grid.</p><formula xml:id="formula_1">p i = sof tmax(f i loc )<label>(2)</label></formula><p>Where i represents the ith row, f i loc is the feature vector of the ith row of location map f loc , p i is the probability vector for the ith row.</p><p>The final row-wise location is defined as the expected abscissa.</p><p>E</p><formula xml:id="formula_2">(x i ) = j j ? p ij<label>(3)</label></formula><p>Where E(x i ) is the expected abscissa, p ij is the probability of the lane line passing through the coordinate (j, i).</p><p>In the training phase, L1-loss is applied.</p><formula xml:id="formula_3">row = 1 N v i?V |E(x i ) ? x i |<label>(4)</label></formula><p>Where V represents the vertical range of the labeled line, N v is the number of valid rows.</p><p>Vertical Range The vertical lane range is determined by row-wisely predicting whether the lane line passes through the current row, as is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. We add a linear layer and perform binary-classification row by row. We use the feature vector of each row in the location map as the input. The softmax-cross-entropy loss is adopted to guide the training process.</p><formula xml:id="formula_4">range = i (?y i gt log(v i ) ? (1 ? y i gt )log(1 ? v i )) (5)</formula><p>Where v i represents the predicted positive probability for ith row and y i gt is the groundtruth of ith row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offset Map The row-wise location defined in Equation 3</head><p>points to the abscissa of the vertex on the left side of the grid, rather than the precise location. Thus, we add the offset map to predict the offset in the horizontal direction near the row-wise location for each row. We use L1-loss to constrain the offset map as follows.</p><formula xml:id="formula_5">of f set = 1 N ? (j,i)?? ? ij ? ? ij<label>(6)</label></formula><p>where? ij and ? ij are the predicted offset and the label offset on coordinate (j, i). We define ? as the area near the lane line with a fixed width. N ? is the number of pixels in ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape Description Each output lane line is represented</head><p>as an ordered set of coordinates. For kth line, the coordinate (x i k , y i k ) of the ith row is represented as follows.</p><formula xml:id="formula_6">y i k = H/Y ? i x i k = W/X ? (loc i k + ?(loc i k , i))<label>(7)</label></formula><p>Where i ? v k min , v k max , v k min and v k max are respectively the minimum and maximum values of the predicted vertical range, loc k i is rounded down from E k i , ?(?) is the predicted offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Instance Detection</head><p>We design the proposal head for instance detection, as is shown in <ref type="figure">Figure 2</ref>. For general conditional instance segmentation methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref>, the instance is detected in an end-to-end pipeline by predicting the central of each object. However, it is hard to predict the central for the slender and curved lines because the visual characteristic of the line central is not obvious.</p><p>We detect the lane instance by detecting the proposal point located at the start point of the line. The start point has a more clear definition and more obvious visual characteristic than the central. We follow CenterNet <ref type="bibr" target="#b4">[5]</ref> and predict a proposal heatmap to detect the proposal points. To constraint the proposal heatmap, we adopt focal loss following CornerNet <ref type="bibr" target="#b19">[20]</ref> and CenterNet <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_7">point = ?1 N p xy (1 ?P xy ) ? log(P xy ) P xy = 1 (1 ? P xy ) ? (P xy ) ? log(1 ?P xy ) otherwise (8)</formula><p>Where P xy is the label at coordinate (x, y) andP xy is the predicted value at coordinate (x, y) of the proposal heatmap. N p is the number of proposal points in the input image.</p><p>Besides, we regress the dynamic kernel parameters by predicting a parameter map following CondInst <ref type="bibr" target="#b34">[35]</ref> and SOLOv2 <ref type="bibr" target="#b37">[38]</ref>. The constraints of the parameter map are constructed through the constraints on the line shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Instance Module</head><p>In the proposal head described above, each proposal point is bound to a lane instance. However, in practice, multiple lane lines can fall in the same proposal point such as the fork lanes. To deal with the above cases, we propose the Recurrent Instance Module(RIM). <ref type="figure">Figure 5</ref>. The Recurrent Instance Module. In this figure, h and c are the short-term memory and long-term memory respectively, f is the input feature vector, s is the output state logit, k is the output kernel parameter vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM cell</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fc module</head><formula xml:id="formula_8">!"# ? !"# ? ! !"# LSTM cell Fc module ? !$# ! ? ! ! !$# ? !$# ! ! !$# !$#</formula><p>The structure of the proposed RIM is shown in <ref type="figure">Figure 5</ref>. Based on LSTM(Long Short-term Memory) <ref type="bibr" target="#b8">[9]</ref>, the RIM recurrently predicts a state vector s i and a kernel parameter vector k i . We define s i as two-dimensional logits that indicate two states: "continue" or "stop". The vector k i contains the kernel parameters for subsequent instance-wise dynamic convolution. In the inference phase, the RIM recurrently predicts the lane-wise kernel parameters bound to the same proposal point until the state is "stop". As is shown in <ref type="figure">Figure 2</ref>, RIM is added for each proposal point. Therefore, each proposal point can guide the shape prediction of multiple lane instances.</p><p>We adopt cross-entropy loss to constrain the state output as follows.</p><formula xml:id="formula_9">state = 1 N s i ? [y i ? log(s i ) + (1 ? y i ) ? log(1 ? s i )]<label>(9)</label></formula><p>Where s i is the output of softmax operation for ith state, result y i is the ground truth for the ith state and N s is the total number of the state outputs in a batch.</p><p>In the training phase, the total loss is defined as follows. total = point +? row +? range +? of f set +? state <ref type="formula">(10)</ref> The hyperparameters ?, ?, ? and ? are set to 1.0, 1.0, 0.4 and 1.0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>The overall architecture is shown in <ref type="figure">Figure 2</ref>. We adopt ResNet <ref type="bibr" target="#b7">[8]</ref> as the backbone and add a standard FPN <ref type="bibr" target="#b22">[23]</ref> module to provide integrated multi-scale features. The proposal head detects the lane instances by predicting the proposal heatmap of shape 1 ? H p ? W p . Meanwhile, a parameter map of shape C p ? H p ? W p that contains the dynamic kernel parameters is predicted. For the instance with the proposal point located at (x p , y p ), the corresponding dynamic kernel parameters are contained in the C p dimensional kernel feature vector at (x p , y p ) on the parameter map. Further, given the kernel feature vector, the RIM recurrently predicts the dynamic kernel parameters. Finally, the conditional shape head predicts the line shape instancewisely conditioned on the dynamic kernel parameters.  Our framework requires a strong capability of context feature fusion. For example, the prediction of the proposal point is based on the features of the entire lane line which generally has an elongated shape and long-range. Therefore, we add a transformer encoder structure to the last layer of the backbone for the fusion of contextual information. We retain the two-dimensional spatial features in the encoder layer and use convolutions for feature extraction. The structure of the transformer encoder used in our framework is shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>To extensively evaluate the proposed method, we conducte experiments on three benchmarks: CurveLanes <ref type="bibr" target="#b38">[39]</ref>, CU-Lane <ref type="bibr" target="#b27">[28]</ref>, and TuSimple <ref type="bibr" target="#b35">[36]</ref>. CurveLanes is a recently proposed benchmark with cases of complex topologies such as fork lines and dense lines. CULane is a widely used large lane detection dataset with 9 different scenarios. TuSimple is another widely used dataset of highway driving scenes. The details of the three datasets are shown in Tab. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics</head><p>For CurveLanes and CULane, we adopte the evaluation metrics of SCNN <ref type="bibr" target="#b27">[28]</ref> which utilizes the F1 measure as the metric. IoU between the predicted lane line and GT label is taken for judging whether a sample is true positive (TP) or false positive (FP) or false negative (FN). IoU of two lines is defined as the IoU of their masks with a fixed line width. Further, F1-measure is calculated as follows:</p><formula xml:id="formula_10">P recision = T P T P + F P (11) Recall = T P T P + F N (12) F 1 = 2 ? P recision ? Recall P recision + Recall<label>(13)</label></formula><p>For TuSimple dataset <ref type="bibr" target="#b35">[36]</ref>, there are three official indicators: false-positive rate (FPR), false-negative rate (FNR), and accuracy.</p><formula xml:id="formula_11">accuracy = clip C clip clip S clip<label>(14)</label></formula><p>Where C clip is the number of correctly predicted lane points and S clip is the total number of lane points of a clip. Lane with accuracy greater than 85% is considered as a truepositive otherwise false positive or false negative. Besides, the F1 score is also reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation details</head><p>We fix the large, medium, and small versions of our Cond-LaneNet for all three datasets. The difference between the three models is shown in <ref type="table" target="#tab_4">Table 2</ref>. For all three datasets, input images are resized to 800?320 pixels during training and testing. Since there are no cases of fork lines in CULane and TuSimple, RIM is only applied for the Curve-Lanes dataset. In the optimizing process, we use Adam optimizer <ref type="bibr" target="#b17">[18]</ref> and step learning rate decay <ref type="bibr" target="#b25">[26]</ref> with an initial learning rate of 3e-4. For each dataset, we train on the training set without any extra data. We respectively train 14, 16 and 70 epochs for CurveLanes, CULane and TuSimple with a batchsize of 32. The results are reported on the test set for CULane and TuSimple. For CurveLanes, we report the results on the validation set following CurveLane <ref type="bibr" target="#b38">[39]</ref>.</p><p>All the experiments were computed on a machine with an RTX2080 GPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The visualization results on the CurveLanes, CULane, and TuSimple datasets are shown in the <ref type="figure" target="#fig_5">Figure 7</ref>. The results show that our method can cope with complex line topologies. Even for the cases of dense lines and fork lines, our method can also successfully discriminate the instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>F1 Precision Recall FPS GFlops(G) SCNN <ref type="bibr" target="#b27">[28]</ref> 65  Tusimple The results on TuSimple are shown in <ref type="table">Table  5</ref>. Relatively, the gap between different methods on this dataset is smaller, due to the smaller amount of data and more single scenes. Our method achieves a new state-ofthe-art F1 score of 97.24. Besides, the small version of our method gets a 97.01 F1 score with 220 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study of Improvement Strategies</head><p>We performed ablation experiments on the CurveLanes dataset based on the small version of our CondLaneNet. The results are shown in Tabel 6. We take the lane detection model based on the original conditional instance segmentation strategy <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref> (as is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.a) as the baseline. The first row shows the results of the baseline.</p><p>In the second row, the proposed conditional lane detection strategy is applied and the lane mask expression is replaced by the row-wise formulation(as is shown in 3.b). In the third row, the offset map for post-refinement is added. In the fourth row, the transformer encoder is added and the offset map is removed. The fifth row presents the result of the model with the row-wise formulation, the offset map, and the transformer encoder. In the last row, RIM is added. Comparing the first two rows, we can see that the proposed conditional lane detection strategy has significantly improved the performance. Comparing the results of the 2nd and the 3rd row, the 4th and the 5th row, we can see the positive effect of the offset map. Moreover, the transformer encoder plays a vital role in our framework, which can be indicated by comparing the 2nd and the 4th row, the 3rd and the 5th row. Besides, RIM designed for the fork lines and dense lines also improves the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study of Transformer Encoder</head><p>This section further analyzes the function of the transformer encoder which indicates a vital role in the previous experiments. Our method first detects instances by detect- ing the proposal points and then predicts the shape for each instance. The accuracy of the proposal points greatly affects the final accuracy of the lane lines. We design different control groups to compare the accuracy of the proposal points and lane lines on CurveLanes. We define the proposal points which locate in the eight neighborhoods of the groundtruth points as the true-positive samples. Considering the function of RIM, the proposal point corresponding to multiple lines are regarded as multiple different proposal points. We report the F1 score of the proposal points and lane lines, as is shown in <ref type="table">Table 7</ref>.</p><p>The first row shows the results of the small, medium and large versions of the standard CondLaneNet. In the second row, the transformer encoder is removed. In the third row, we hack the inference process of the second row by replacing the proposal heatmap with the proposal heatmap output by the standard model(the first row). For the small version, removing the encoder leads to a significant drop for both proposal points and lanes. However, using the proposal heatmap of the standard model, the results on the third row are close to the first row.</p><p>The above results prove that the function of the encoder is mainly to improve the detection of the proposal points, which rely on contextual features and global information. Besides, the contextual features can be more fully refined in deeper networks. Therefore, for the medium and large versions, the improvement of the encoder is far less than the small version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, We proposed CondLaneNet, a novel topto-down lane detection framework that detects the lane instances first and then instance-wisely predict the shapes. Aiming to resolve the instance-level discrimination problem, we proposed the conditional lane detection strategy based on conditional convolution and row-wise formulation. Moreover, we designed RIM to cope with complex lane line topologies such as dense lines and fork lines. Our CondLaneNet framework refreshed the state-of-the-art performance on CULane, CurveLanes, and TuSimple. Moreover, on CULane and CurveLanes, the small version of our CondLaneNet not only surpassed other methods in accuracy, but also presented real-time efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Scenes of lane lines with complex topologies. It is challenging to cope with the scenes such as the dense lines(the first row) and the fork lines(the second row). Different instances are represented by different colors in thisfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The difference between conditional instance segmentation and the proposed conditional lane detection strategy. Our CondLaneNet detects the start point of the lane lines to detect the instance and uses the row-wise formulation to describe the line shape instead of the mask. The overlapping lines can be distinguished based on the proposed RIM, which will be detailed in Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The process of parsing the row-wise location and the vertical range from the location map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Flatten</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The structure of the transformer encoder. The ?, and ? respectively represent matrix addition, dot-product operation and element-wise product operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visualization results on CurveLanes(the first row), CULane(the middle row) and TuSimple(the last row) datasets. Different lane instances are represented by different colors.SOTA). Since our model can deal with cases of the fork and dense lane lines, there is a significant improvement in the recall indicator. Correspondingly, false-positive results will increase, resulting in a decrease in the precision indicator.CULane The results of our CondLaneNet and other stateof-the-art methods on CULane are shown in Tabel 4. Our method achieves a new state-of-the-art result of a 79.48 F1 score, which has increased by 3.19%. Moreover, our method achieves the best performance in eight of nine scenarios, showing robustness to different scenarios. For some hard cases such as curve and night, our methods have obvious advantages. Besides, the small version of our Cond-LaneNet gets a 78.14 F1 score with a speed of 220 FPS, 1.12 higher and 8.5? speed than LaneATT-L. Compared with LaneATT-S, CondLaneNet-S achieves a 4.01 % F1 score improvement with similar efficiency. In most scenarios of CULane, the small version of our CondLaneNet exceeds all previous methods in the F1 measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Backbone Proposal head</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Conditional shape head</cell></row><row><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Convolution+</cell></row><row><cell>Encoder</cell><cell></cell><cell></cell><cell>Row-wise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN+ReLU</cell></row><row><cell></cell><cell></cell><cell>Proposal heatmap Parameter map</cell><cell>? location Location maps</cell><cell>?</cell><cell>Linear</cell><cell>Vertical range</cell><cell>?</cell><cell>? Offset maps</cell><cell>Convolution Conditional convolution Kenerl parameters</cell></row><row><cell>Input image</cell><cell>RIM</cell><cell>RIM RIM</cell><cell cols="2">convolution Conditional</cell><cell></cell><cell></cell><cell cols="2">Shared branch convolution Conditional</cell><cell>for location map Kenerl parameters for offset map</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>1.</cell></row></table><note>Details of three datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Difference of different versions of our CondLaneNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different methods on CurveLanes.</figDesc><table><row><cell></cell><cell>.02</cell><cell>76.13</cell><cell>56.74</cell><cell></cell><cell>328.4</cell></row><row><cell>Enet-SAD [12]</cell><cell>50.31</cell><cell>63.60</cell><cell>41.60</cell><cell></cell><cell>3.9</cell></row><row><cell>PointLaneNet [2]</cell><cell>78.47</cell><cell>86.33</cell><cell>72.91</cell><cell></cell><cell>14.8</cell></row><row><cell>CurveLane-S [39]</cell><cell>81.12</cell><cell>93.58</cell><cell>71.59</cell><cell></cell><cell>7.4</cell></row><row><cell cols="2">CurveLane-M [39] 81.80</cell><cell>93.49</cell><cell>72.71</cell><cell></cell><cell>11.6</cell></row><row><cell cols="2">CurveLane-L [39] 82.29</cell><cell>91.11</cell><cell>75.03</cell><cell></cell><cell>20.7</cell></row><row><cell>CondLaneNet-S</cell><cell>85.09</cell><cell>87.75</cell><cell cols="2">82.58 154</cell><cell>10.3</cell></row><row><cell>CondLaneNet-M</cell><cell>85.92</cell><cell>88.29</cell><cell cols="2">83.68 109</cell><cell>19.7</cell></row><row><cell>CondLaneNet-L</cell><cell>86.10</cell><cell>88.98</cell><cell>83.41</cell><cell>48</cell><cell>44.9</cell></row><row><cell cols="6">CurveLanes The comparison results on CurveLanes are</cell></row><row><cell cols="6">shown in Tabel 3. CurveLanes contains cases of lane</cell></row><row><cell cols="6">lines with complex topologies such as curve, fork, and</cell></row><row><cell cols="6">dense lanes. Our large version of CondLaneNet achieves</cell></row><row><cell cols="6">a new state-of-the-art F1 score of 86.10, 4.63% higher than</cell></row><row><cell cols="6">CurveLane-L. Our small version of CondLaneNet still has</cell></row><row><cell cols="6">a performance of an 85.09 F1 score (3.40% higher than</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Comparison of different methods on CULane. Comparison of different methods on TuSimple.</figDesc><table><row><cell>Category</cell><cell cols="7">Total Normal Crowded Dazzle Shadow No line Arrow Curve Cross Night</cell><cell cols="2">FPS GFlops(G)</cell></row><row><cell>SCNN [28]</cell><cell>71.60</cell><cell>90.60</cell><cell>69.70</cell><cell>58.50</cell><cell>66.90</cell><cell>43.40</cell><cell>84.10 64.40 1990 66.10</cell><cell>7.5</cell><cell>328.4</cell></row><row><cell>ERFNet-E2E [41]</cell><cell>74.00</cell><cell>91.00</cell><cell>73.10</cell><cell>64.50</cell><cell>74.10</cell><cell>46.60</cell><cell>85.80 71.90 2022 67.90</cell><cell></cell><cell></cell></row><row><cell>FastDraw [29]</cell><cell></cell><cell>85.90</cell><cell>63.60</cell><cell>57.00</cell><cell>69.90</cell><cell>40.60</cell><cell>79.40 65.20 7013 57.80</cell><cell>90.3</cell><cell></cell></row><row><cell>ENet-SAD [12]</cell><cell>70.80</cell><cell>90.10</cell><cell>68.80</cell><cell>60.20</cell><cell>65.90</cell><cell>41.60</cell><cell>84.00 65.70 1998 66.00</cell><cell>75</cell><cell>3.9</cell></row><row><cell>UFAST-ResNet34 [30]</cell><cell>72.30</cell><cell>90.70</cell><cell>70.20</cell><cell>59.50</cell><cell>69.30</cell><cell>44.40</cell><cell cols="2">85.70 69.50 2037 66.70 175.0</cell><cell></cell></row><row><cell>UFAST-ResNet18 [30]</cell><cell>68.40</cell><cell>87.70</cell><cell>66.00</cell><cell>58.40</cell><cell>62.80</cell><cell>40.20</cell><cell cols="2">81.00 57.90 1743 62.10 322.5</cell><cell></cell></row><row><cell>ERFNet-IntRA-KD [11]</cell><cell>72.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100.0</cell><cell></cell></row><row><cell>CurveLanes-NAS-S [39]</cell><cell>71.40</cell><cell>88.30</cell><cell>68.60</cell><cell>63.20</cell><cell>68.00</cell><cell>47.90</cell><cell>82.50 66.00 2817 66.20</cell><cell></cell><cell>9.0</cell></row><row><cell cols="2">CurveLanes-NAS-M [39] 73.50</cell><cell>90.20</cell><cell>70.50</cell><cell>65.90</cell><cell>69.30</cell><cell>48.80</cell><cell>85.70 67.50 2359 68.20</cell><cell></cell><cell>35.7</cell></row><row><cell cols="2">CurveLanes-NAS-L [39] 74.80</cell><cell>90.70</cell><cell>72.30</cell><cell>67.70</cell><cell>70.10</cell><cell>49.40</cell><cell>85.80 68.40 1746 68.90</cell><cell></cell><cell>86.5</cell></row><row><cell>LaneATT-Small [32]</cell><cell>75.13</cell><cell>91.17</cell><cell>72.71</cell><cell>65.82</cell><cell>68.03</cell><cell>49.13</cell><cell>87.82 63.75 1020 68.58</cell><cell>250</cell><cell>9.3</cell></row><row><cell>LaneATT-Medium [32]</cell><cell>76.68</cell><cell>92.14</cell><cell>75.03</cell><cell>66.47</cell><cell>78.15</cell><cell>49.39</cell><cell>88.38 67.72 1330 70.72</cell><cell>171</cell><cell>18.0</cell></row><row><cell>LaneATT-Large [32]</cell><cell>77.02</cell><cell>91.74</cell><cell>76.16</cell><cell>69.47</cell><cell>76.31</cell><cell>50.46</cell><cell>86.29 64.05 1264 70.81</cell><cell>26</cell><cell>70.5</cell></row><row><cell>CondLaneNet-Small</cell><cell>78.14</cell><cell>92.87</cell><cell>75.79</cell><cell>70.72</cell><cell>80.01</cell><cell>52.39</cell><cell>89.37 72.40 1364 73.23</cell><cell>220</cell><cell>10.2</cell></row><row><cell>CondLaneNet-Medium</cell><cell>78.74</cell><cell>93.38</cell><cell>77.14</cell><cell>71.17</cell><cell>79.93</cell><cell>51.85</cell><cell>89.89 73.88 1387 73.92</cell><cell>152</cell><cell>19.6</cell></row><row><cell>CondLaneNet-Large</cell><cell>79.48</cell><cell>93.47</cell><cell>77.44</cell><cell>70.93</cell><cell>80.91</cell><cell>54.13</cell><cell>90.16 75.21 1201 74.80</cell><cell>58</cell><cell>44.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Ablation study of the improvement strategies on Curve-Lanes base on the small version of our CondLaneNet.</figDesc><table><row><cell>Baseline Row-wise Offset Encoder RIM ? ? ? ? ? ? ? ? ? ? ? ? ?</cell><cell>F1 score 72.19 80.09(+7.9) 81.24(+9.05) 81.85(+9.66) 83.41(+11.22) 85.09(+12.90)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Line P. point Line P. point Line Standard 88.35 85.09 88.99 85.92 89.54 86.10 S. w/o encoder 85.51 82.97 88.68 85.91 89.33 85.98 Hacked 88.05 84.39 88.90 85.93 89.37 85.99Table 7. Ablation study of the transformer encoder module on CurveLanes.</figDesc><table><row><cell>Model</cell><cell>Small</cell><cell>Medium</cell><cell>Large</cell></row><row><cell>Target</cell><cell>P. point</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking with ransac and kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monson</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3261" to="3264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointlanenet: Efficient end-to-end cnns for accurate real-time lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2563" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reliable multilane detection and classification by utilizing cnn as a regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shriyash</forename><surname>Chougule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Koznek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asad</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Schulze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">El-gan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N?ra</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dagmapper: Learning to map by discovering lane topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inter-region affinity distillation for road marking segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12486" to="12495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilane detection in urban driving environments using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Nam</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Woo</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1297" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">New lane model and distance transform for lane detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Vaudrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Analysis of Images and Patterns</title>
		<meeting>the International Conference on Computer Analysis of Images and Patterns</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1044" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computer visionbased multiple-lane detection on straight road and in a curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Analysis and Signal Processing</title>
		<meeting>the International Conference on Image Analysis and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="114" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking in challenging scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuwhan</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Key points estimation and point instance segmentation approach for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghwuy</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06604</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tae-Hee Lee, Hyun Seok Hong, Seung-Hoon Han, and In So Kweon. Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1947" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Line-cnn: End-to-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining statistical hough transform and particle filter for robust lane detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentin</forename><surname>W?rg?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Markeli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="993" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Endto-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11582" to="11591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ultra fast structureaware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="276" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Polylanenet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paix?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivera-Santos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12035</idno>
		<title level="m">Keep your eyes on the lane: Attention-guided lane detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel curve lane detection based on improved river flow and ransa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danya</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International IEEE Conference on Intelligent Transportation Systems</title>
		<meeting>the International IEEE Conference on Intelligent Transportation Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A review of lane detection methods based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jigang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tusimple lane detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SOLOv2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17721" to="17732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lanesensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesoo</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duck Hoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1006" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of autonomous driving: Common practices and emerging technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekim</forename><surname>Yurtsever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="58443" to="58469" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A novel lane detection based on geometrical model and gabor filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqiang</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
