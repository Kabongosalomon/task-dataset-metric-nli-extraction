<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONVIFORMERS: CONVOLUTIONALLY GUIDED VISION TRANSFORMER A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Vaishnav</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial and Natural Intelligence Toulouse Institute</orgName>
								<orgName type="institution">Universit? de Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dpt. of Cognitive Linguistic &amp; Psychological Sciences</orgName>
								<orgName type="institution" key="instit1">Carney Institute for Brain Science</orgName>
								<orgName type="institution" key="instit2">Brown University</orgName>
								<address>
									<postCode>02912</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Centre de Recherche Cerveau et Cognition</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Universit? de Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial and Natural Intelligence Toulouse Institute</orgName>
								<orgName type="institution">Universit? de Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dpt. of Cognitive Linguistic &amp; Psychological Sciences</orgName>
								<orgName type="institution" key="instit1">Carney Institute for Brain Science</orgName>
								<orgName type="institution" key="instit2">Brown University</orgName>
								<address>
									<postCode>02912</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iva?</forename><surname>Felipe Rodr?guez</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dpt. of Cognitive Linguistic &amp; Psychological Sciences</orgName>
								<orgName type="institution" key="instit1">Carney Institute for Brain Science</orgName>
								<orgName type="institution" key="instit2">Brown University</orgName>
								<address>
									<postCode>02912</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial and Natural Intelligence Toulouse Institute</orgName>
								<orgName type="institution">Universit? de Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dpt. of Cognitive Linguistic &amp; Psychological Sciences</orgName>
								<orgName type="institution" key="instit1">Carney Institute for Brain Science</orgName>
								<orgName type="institution" key="instit2">Brown University</orgName>
								<address>
									<postCode>02912</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CONVIFORMERS: CONVOLUTIONALLY GUIDED VISION TRANSFORMER A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformers are nowadays the de-facto choice for image classification tasks. There are two broad categories of classification tasks, fine-grained and coarse-grained. In fine-grained classification, the necessity is to discover subtle differences due to the high level of similarity between sub-classes. Such distinctions are often lost as we downscale the image to save the memory and computational cost associated with vision transformers (ViT). In this work, we present an in-depth analysis and describe the critical components for developing a system for the fine-grained categorization of plants from herbarium sheets. Our extensive experimental analysis indicated the need for a better augmentation technique and the ability of modern-day neural networks to handle higher dimensional images. We also introduce a convolutional transformer architecture called Conviformer which, unlike the popular Vision Transformer (ConViT), can handle higher resolution images without exploding memory and computational cost. We also introduce a novel, improved pre-processing technique called PreSizer to resize images better while preserving their original aspect ratios, which proved essential for classifying natural plants. With our simple yet effective approach, we achieved SoTA on Herbarium 202x and iNaturalist 2019 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial Intelligence is about to transform botany by facilitating the identification of both extant and extinct speciesincluding a novel, unknown species. One of the first attempts to automate leaf classification includes the work by <ref type="bibr" target="#b0">[1]</ref> using a probabilistic neural network <ref type="bibr" target="#b1">[2]</ref> which calculated decision boundary following the Bayes strategy. An early attempt to use a convolutional architecture for plant classification includes the work of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. A pre-processing step was required in <ref type="bibr" target="#b2">[3]</ref> to tackle the challenges posed by the dataset, such as blurring and illumination changes. Later, a multi-layered perceptron (MLP) based approach was tested by <ref type="bibr" target="#b4">[5]</ref>. Most algorithms during the early stage followed two steps, feature extraction and classification. <ref type="bibr" target="#b5">[6]</ref> used information such as sepal length, sepal width, petal length, and petal width as a feature extractor and used it in a multi-layered feed-forward network for IRIS plant identification. Unlike the approaches mentioned above, modern deep learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> used the images to extract the features and do the classification because of the availability of increasingly large image datasets such as iNaturalist <ref type="bibr" target="#b11">[12]</ref> and Herbarium <ref type="bibr" target="#b12">[13]</ref>.</p><p>Vision Transformers are one widely accepted class of architectures that have taken over the field of computer vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. These architectures rely on multiple blocks of self-attention modules and feed-forward networks to compute the interaction between all the pairs of input tokens passed to the network. Their ability to model contentbased interaction is a computationally intensive task and is quadratically associated with the length of the input sequence. To avoid memory explosion and the cost of training such transformer architectures, input images are usually downscaled before passing. However, this downscaling operation negatively impacts more challenging tasks, such as those presented in fine-grained classification. Unlike coarse-grained classification, sub-classes in a fine-grained classification task possess threefold challenges. First, there exists a very high level of similarity between species of different sub-class <ref type="figure" target="#fig_0">(Figure 1(a)</ref>); second, there is high variation between samples of the same sub-class <ref type="figure" target="#fig_0">(Figure 1(b)</ref>); and lastly, often, these datasets are limited in samples because of the need for a professional to annotate the data. Previous work has already shown that leaf classification, for instance, requires high-resolution images (see <ref type="bibr" target="#b17">[18]</ref>for instance).</p><p>Here, we propose a novel plug-and-play module for vision transformer architectures that augments its ability to handle higher resolution input images with insignificant computational cost. The need to process high-resolution images is especially problematic for transformer networks because of their large memory footprint, which grows as a function of the image resolution. Our proposed architecture effectively blends the benefits of convolutional with transformer-based architectures. Initial studies <ref type="bibr" target="#b18">[19]</ref> have found the positive impact of merging convolutional layers with transformer networks. We also demonstrate the need for higher resolution images for better classification. A secondary contribution of this work includes a novel resizing technique (PreSizer) to preserve the aspect ratio of the herbarium images -which proved to be necessary for the fine-grained recognition of plant species. Our comprehensive experimental analysis shows how critical ingredients like regularization, dropout and hierarchical labels alter the model's performance and proposes a classification recipe to be followed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions are as follows:</head><p>? A novel Conviformers architecture that combines a convolutional module as a front-end with a vision transformer module as a back-end. ? A generic approach that allows the transformer models to attend to input images of any resolution with minimal memory requirement. ? Empirical evidence exhibiting the need for higher image resolution to achieve better classification accuracy. ? A recipe after analyzing multiple components and their contribution to learning suitable discriminative features for classification. ? A novel pre-processing technique PreSizer to resize images preserving the aspect ratio and improving recognition accuracy for fine-grained recognition. ? A top entry uni-model performance on the Herbarium-202x and iNaturalist 2019.</p><p>Four broad categories of research work incorporate convolutions with transformer architectures. First, inserting few attention modules in between residual blocks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>; second, inserting attention modules at the end <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>; third, augmenting convolution layers with attention modules in parallel <ref type="bibr" target="#b26">[27]</ref>; and lastly substituting convolution layers by attention modules <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Still, there persisted a wide gap between the learnability of vision transformers and CNNs. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> analysed the potential weaknesses in directly applying transformers from NLP and proposed a combination of transformers along with convolutional networks. <ref type="bibr" target="#b30">[31]</ref> proposed Convolutional vision Transformers (CvT) and presented convolutional-based patch projection of image tokens along with hierarchical design. Another alternative, LocalViT <ref type="bibr" target="#b36">[37]</ref> proposed depthwise convolution to capture local features; Levit <ref type="bibr" target="#b33">[34]</ref> on the other hand, enhanced the inference speed of ViT by first passing the images from a series of convolutional blocks and passing its output to the ViT. A network proposed by <ref type="bibr" target="#b37">[38]</ref> incorporated locality without convolutions with the help of enhanced local self-attention using Hadamard attention and ghost head. Hadamard attention enabled maintaining higher-order mapping considering neighboring attention, while ghost heads increased the channel capacity by combining attention maps. A vanilla transformer model learns attention maps independently at each layer. To improve the quality of the attention maps, evolving attention transformers were proposed in <ref type="bibr" target="#b38">[39]</ref>. A chain of residual convolutional modules is used to guide attention mechanisms to interact between the maps in different layers. However, all these architectures require the input image to be smaller in resolution because of the increasing computational complexity associated with increasing input resolution.</p><p>Recently researchers have focused on a fifth direction where they focus on augmenting attention layers soft convolutions inductive bias. <ref type="bibr" target="#b15">[16]</ref> replaced the first few self-attention layers of ViT <ref type="bibr" target="#b14">[15]</ref> with gated positional self-attention to mimic the locality of convolutional layers. Their architecture uses a gating parameter to control how much to emphasize spatial positions vs. image content. This architecture starts with a convolutional layer as a default using the gates, which feed into self-attention modules. We base our work on this model because of its sample efficiency in learning and the ability to switch from attention to convolutions. Unlike other convolutional augmented transformer architectures proposed in the literature, we propose a plug-and-play convolutional module in this work. This module can be attached as a front-end to any vision transformers.</p><p>Transformer are also gaining popularity in fine-grained classification tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. This classification field can be divided into localization methods and feature encoding methods. In the localization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> method, the focus is to localize discriminative areas of the image that are later used for classification. Whereas in the feature encoding <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> method, the focus is on learning relevant higher-order features. Vision transformers backbones have been used in localizing discriminative regions in methods like FFVT <ref type="bibr" target="#b7">[8]</ref>, TransFG <ref type="bibr" target="#b9">[10]</ref>, RAMS-Trans <ref type="bibr" target="#b39">[40]</ref> and AF-Trans <ref type="bibr" target="#b10">[11]</ref>. A plug-in module, PiM <ref type="bibr" target="#b8">[9]</ref>, can be used either with the convolutional network or with ViT, where the emphasis is given to the strength of the attention maps instead modulated features map for classification. All of these networks work on a fixed input resolution input image. On the contrary, we propose a general purpose design that removes the constraints of working with fixed image resolution with transformer architecture.</p><p>3 Dataset iNaturalist 2019: This dataset is composed of 268,243 images containing 1,010 classes at the species level. As we move from the coarse level, there are 3 classes for Kingdom, 4 classes for Phylum, 9 classes for Class, 34 classes for Order, 57 classes for the family, 72 classes for genus and 1,010 classes for Species. The iNaturalist <ref type="bibr" target="#b11">[12]</ref> dataset is a long-tailed large-scale real-world datasets. For a fair comparison, we used official splits of training and validation images in this paper.</p><p>Herbarium 2021: Herbarium sheets are a vital source of botanical research. This dataset contains information about taxon, families and orders for 2.5M images. Herbarium sheets provided in the dataset belong to 451 families and 81 orders. At the finest level, there are 64500 species from the Americas and Oceania. This long-tailed dataset contains 3 images per species on the lower bound and &gt;100 as the upper bound. It is a part of a project spearheaded by the New York Botanical Garden. This dataset <ref type="bibr" target="#b12">[13]</ref> is a split training and test set with an 80%/20% ratio. We further divided the training dataset into a training and a test split with a 90%/10% ratio. We did not use the novel data augmentation technique mentioned in Section 4 while training the model on Herbarium 2021.</p><p>Herbarium 2022: All experiments described here use the Herbarium 2022 dataset 1 . It contains 15,501 species of vascular plants from North America, collected across 60 different botanical institutions worldwide. The distribution of class labels across the 1.05M images is long-tailed. There are as few as 7 samples per taxa and as many as 100 for some taxa. Training, validation and test distribution are the same as above.</p><p>In addition to class labels, additional meta-data is available for the family, genus and species. The family and genus level hierarchy has a unique name. In contrast, at the species level, they are unique at their genus level, and there could be similar species names for the different genera. There are 272 families and 2564 genera in the Herbarium-2022 dataset. This dataset is skewed at all three levels and has a long-tailed distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data prepossessing</head><p>Images in the Herbarium 2022 dataset have different resolutions, with their longest side always 1000 pixels. Working with such higher dimensional images is also computationally expensive. One possible widely accepted solution is to either resize the images or use padding to make them symmetric. However, we introduce a technique, PreSizer, that maintains the aspect ratio of the images in the dataset and makes their resolution symmetrical. Our first observation was that most images have very distinctive borders and contain irrelevant information for the prediction. So we remove the boundary pixels of a width of 20px from all four sides. Next, we find the shorter edge of the image, and along this side, we pad it to make it equal to the longer side by reflecting the image. Following this approach makes our images of the resolution 1000?1000 as the longer side is always 1000.</p><p>We use these resized and augmented images with contextually relevant information for the training at a resolution of 512?512 and central cropping to 448?448. <ref type="figure" target="#fig_1">Figure 2</ref> shows an illustrative example using PreSizer strategy. As we can see, the images are smooth around the edges removing artifacts where we carry out reflection and do not introduce any discontinuity. We also used the Scikit-Learn <ref type="bibr" target="#b46">[47]</ref> pre-processing library to encode the different labels in this given dataset. Resizing the image when padding equally on both sides. When we follow equal padding around the sides, we lose relevant content. (c) Resizing the image following the PreSizer technique in which we remove the boundary pixels of width 20 pixels from all the sides and append the mirror reflection of the image to make it square.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conviformers</head><p>We proposed an architecture Conviformers <ref type="figure">(Figure 3</ref>). It combines ConViT architecture that can perform at par or even better with standalone transformer <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b47">48]</ref> architectures in a lower data regime. ConViT tries to get the best from convolutions as well as transformers. It is initialized with a soft convolutional inductive bias using a learnable parameter which can also learn to switch to self-attention if needed. A gated positional self-attention (GPSA) learns a parameter lambda controlling the balance between convolutionally initialized content-based attention and self-attention. This gating parameter pays less attention to the convolutional position throughout the training than self-attention. However, the early layers of the model still use convolutional inductive bias to help in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Conviformer:</head><p>Convolutionally guided vision transformer model. We selected the base architecture as ConViT and added a convolutional frontend to reduce memory demands with increasing image resolution.</p><p>Architecture: ConViT require the input image divided in 14 ? 14 non-overlapping patches of 16?16?3. These patches are embedded in D emb = 64N h dimensional vector using convolutional stem with N h as number of heads in the model. These patches are passed through 10 GPSA layers with convolutional initialization and two self-attention layers, followed by a two-layer Feed-Forwards network with GELU activation. To predict the input class ConViT uses an extra class token similar to BERT <ref type="bibr" target="#b48">[49]</ref>, appended after the last GPSA layer. We use the base variant of the model, which has 16 heads (N h ) and a 768-dimensional embedding vector (D emb ).</p><p>We add a convolutional block ( <ref type="figure" target="#fig_3">Figure 4</ref> This feature map (z ? R B?tp?64 ; B is the batch size) is then sent to the subsequent attention blocks of ConViT, where the model's regular mechanisms continue to classify the images. In <ref type="table" target="#tab_0">Table 1</ref> we compare the number of patches needed to be processed while using ConViT with Conviformer.  6 Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Resolution</head><p>Our first experiment demonstrates the importance of input image resolution for a fine-grained classification task. For this experiment, we selected four resolutions (224?224, 448?448, 512?512 and 600?600). We trained ConViT for the smallest resolution image and Conviformer for the rest. All the models are trained from scratch for 300 epochs with default hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Herbarium-2021</head><p>We first trained our model on the Herbarium-2021 dataset. We divided the dataset into training and validation, with 2M samples for training and 200k samples for validation purposes. We let the model train for 300 epochs with crossentropy loss and default hyperparameters as mentioned on their GitHub 2 . We did not use PreSizer as described in section 4 in this training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Herbarium-2022</head><p>We used the pre-trained weights of Herbarium 2021 to initialize the model for current training instead of random initialization. The impact of pre-training granularity is discussed in prior works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. Our model is trained in three stages. In the first stage, we trained the model with 90% of the training dataset for 300 epochs. In the second stage, we included all the training datasets available for another 10 epochs. Finally, we modified the loss function with triplet loss and trained the model for 10 more epochs. Our model used images after they were passed through the PreSizer and resized to 512?512 and center cropped to 448?448. We trained the model for 320 epochs, this time using their default hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss:</head><p>We use three other losses to train the model initialized with the pre-training on the previous year's Herbarium-2021 challenge. The first model is trained using cross-entropy (CE) loss (equation 1) at the taxa level for the first 300 epochs. Once this training is complete, we took triplet loss <ref type="bibr" target="#b51">[52]</ref> as a regularization technique to train the model for another 10 epochs further. Triplet loss (eq. 2) helps us to bring together the samples of the same taxa in their embedding space and separate different taxa apart from each other.</p><formula xml:id="formula_0">L CE = N n=1 ?w yn log exp(x n,yn ) C c=1 exp(x n,c )<label>(1)</label></formula><p>where x is the input, y is the target, w is the weight, C is the number of classes, and N is the minibatch dimension.</p><formula xml:id="formula_1">L Trip ((x, x + , x ? ); g) = max(? + ||g(x) ? g(x + )|| 2 2 ? ||g(x) ? g(x ? )|| 2 2 , 0)<label>(2)</label></formula><p>where x is an anchor sample, x + is another instance from the same class as the anchor, while the negative x ? is from a different class than the anchor. The final loss function becomes L CE + L T rip .</p><p>Hierarchical labels This dataset has an enormous amount of relevant information which can further be used for classification, such as hierarchical labels for each sample image. These labels provide meta information related to family, species, and category labels. In Herbarium-2022, there are 272 types of families and 2564 types of genera in the dataset. Metadata also contains information about the phylogenetic distances between two genera. We used this information and explored the model's performance in different scenarios.</p><p>In the first set of experiments, we used the two hierarchical labels, i.e., family and genus and trained the model with cross-entropy (CE) loss to better regularize the architecture. To do the same, we pass the extracted features of the network to the taxonomical classification layer f (x) and also to the two multilayered perceptrons (MLP) output layers serving as the classification layer for the family h(x) and genus labels g(x) (equation 3).</p><formula xml:id="formula_2">out f eat = model(x) label tax = f (out f eat ) label gen = g(out f eat ) label f am = h(out f eat )<label>(3)</label></formula><p>where out f eat ? R 768 , label tax ? R 15501 , label gen ? R 2564 and label f am ? R 272 . Loss for this model is calculated as shown below (equation 4).</p><formula xml:id="formula_3">L = L CEtax + ? 1 L CEgen + ? 2 L CEfam<label>(4)</label></formula><p>In the next set of experiments, we added more regularization to the network. For this trial, we added triplet loss in addition to the cross entropy loss of the hierarchical labels (equation 5). We used two sampling techniques to draw samples at family or genus levels. For example, in genus-level sampling, positive samples are from the same genus labels as anchor labels, and negative is different from those. Similarly, we draw the same family label sample for a positive triplet at a family level, but it could have a different genus label than the anchor label. We added more output branches to the network in order to obtain the embedding features at the taxa ( emb tax ? R 512 ), genus (emb gen ? R 128 ) and family (emb f am ? R 128 ) level. These variables are considered in calculating loss during the training as mentioned in equation <ref type="bibr" target="#b5">6</ref>.</p><p>emb tax = f emb (out f eat ) emb gen = g emb (out f eat ) emb f am = h emb (out f eat )</p><formula xml:id="formula_4">(5) L = L CEtax + ? 1 L CEgen + ? 2 L CEfam + ? 3 L T riptax + ? 4 L T ripgen + ? 5 L T ripfam (6)</formula><p>Phylogenetic Distance Herbarium-2022 dataset also comes with phylogenetic distances (d phy ) between two genus. We used this distance while training the network to bring closer similar genera and send apart different genera. In this experiment, we took additional genera embedding head (? R 128 ) from the network while using the hierarchical labels of genera, family and category (equation 8).</p><formula xml:id="formula_5">L dist = n i=1 |x p i | 1 p ? d phy 2 (7) L = L CEcat + ? 1 L CEgen + ? 2 L CEfam + ? 3 L dist (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">iNaturalist 2019</head><p>In addition to the two datasets mentioned above, we evaluate the performance of our model on another fine-grained classification task, iNaturalist 2019. We compared the performance with other baselines involving Vision Transformers such as ConViT <ref type="bibr" target="#b15">[16]</ref>, Masked Auto Encoders (MAE) <ref type="bibr" target="#b52">[53]</ref>, Convolution-enhanced image Transformer (CeiT) <ref type="bibr" target="#b32">[33]</ref>, LeViT <ref type="bibr" target="#b33">[34]</ref>. For training Conviformer, we followed the default hyperparameter settings with CE loss for 300 epochs.</p><p>Compatibility This part explains how to load the base architecture pre-trained weights to our proposed model. Once we pass the input image through the convolutional block, we obtain feature vector x ? R tp?64 . In a general scenario, an input image is of resolution 224, and t p is 196. So we remove the patch embedding projection weight and bias from the state dictionary and load the weight. To go from Conviformer to ConViT or any other Vision transformer, we follow a similar step and remove the convolutional block weights from the patch embedding in the state dictionary and load the weights.</p><p>Hardware Accelerators All the experiments are performed on NVIDIA RTX A5000 or Nvidia Tesla V100 GPUs using Pytorch Data Distributed Parallel (DDP) framework. In our experimental setup, we used 8 GPUs in DDP mode for training each model with a batch size of 80 per GPU for the image of resolution 448?448.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Resolution This experiment shows the network's performance with increasing image resolutions. We find that as we go from 224-dimensional images to 448, there is an 8.7% gain in accuracy. As we further increased the image's resolution to 512 and 600, performance gain increased by 5.6% and .67%, respectively. We did not see much improvement in extending the image resolution from 512 to 600, so we stopped. This trend shows the vast possibility of improving the accuracy in a fine-grained classification task when the networks can handle higher-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Herbarium 2021</head><p>We trained the model for 300 epochs. During training, our model could achieve a validation accuracy of 98.3%. We evaluated the model trained on Herbarium 2021 with conviformer on the test images used for ranking on the Kaggle platform. Our model obtained an F1 score (public) of 0.729 using a single model training without any ensembling methods on images of resolution 448?448. This score is also the best uni-model score reported for the challenge <ref type="bibr" target="#b12">[13]</ref>. We also believe that other teams would have used some ensembling methods while submitting their prediction file; our model's performance still excels those. We have summarized the results in <ref type="table">Table 3</ref>.   <ref type="table">Table 3</ref>: Herbarium 2021: F1 score on the test set of Herbarium 2021 obtained with different architectures. We also mention the input image resolution used in training the network. ( * means scores are taken from the public leaderboard of the Kaggle competition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Herbarium 2022</head><p>Cross-entropy (CE) loss In the first setup of the experiment, we only used the category labels and trained the network with a CE loss. The validation accuracy of our trained model achieved nearly 1.0 F1 score. This dataset is taken as a subset of the training dataset using stratified sampling. Training the model using 90% of the data leads to a model with an F1 score of .814 on the test set. Later we merged the validation set to the training dataset to fine-tune the model using all available training samples for 10 more epochs. This increased our F1 score to .824 (summarized in <ref type="table">Table 4</ref>). We ran additional training where the dropout is set at 10% instead 0 as set by default and found a 1.5% gain in F1 score.</p><p>Steps F1 score Epochs (a) Trained on 90% dataset with CE loss 0.814 300 (b) Trained on full dataset with CE loss 0.824 +10 (c) Training on full dataset with Triplet regularization 0.827 +10 (c) Training on full dataset with ConViT <ref type="bibr" target="#b15">[16]</ref> and CE loss 0.802 300 <ref type="table">Table 4</ref>: Herbarium 2022: F1 score obtained on the test dataset following different strategies. We obtain a gain of 1.5% in F1 score when the model is trained on the full dataset with cross-entropy (CE) loss and further regularization using triplet loss.</p><p>At this stage, the F1 score on the dataset without using PreSizer is 0.813, which shows how our pre-processing technique helps get us an overall gain in the final score. Finally, we further fine-tuned this fully trained model with triplet regularization. Our triplet loss trained model gave us an F1 score of .827. We have summarized the results in <ref type="table">Table.</ref> 5.</p><p>Hierarchical loss For the next part of the experiment, we used hierarchical labels provided with the dataset. We followed a similar procedure as mentioned above to train the network. We trained the network for 300 epochs in their default training setup. When evaluated on the test dataset, it yielded a F1 score of .829. We also compared the network's performance when trained from scratch, i.e., with randomly initialized weights. F1 score when trained from scratch is 0.822. This score shows that a model using hierarchical labels, when trained from scratch, could reach</p><p>Steps F1 score Without PreSizer 0.813 With PreSizer 0.824  <ref type="table">Table 6</ref>: Herbarium 2022: F1 score obtained on the test dataset following regularization with hierarchical information. We found that the model trained using cross-entropy (CE) loss at hierarchical levels performs best, and there is no further need for triplet regularization.</p><p>the model's performance with a strong initialization, i.e., pre-trained on Herbarium data. While training, both ? 1 and ? 2 are set as 1.</p><p>Hierarchical + Triplet loss We also used triplet regularization at a hierarchical level. At this stage, we sampled images either at family level or at genus level. F1 score obtained on the test dataset at both these levels are 0.825 and 0.824, respectively. It shows that sampling at the genera level is more beneficial than at the family level because there is more similarity among genera than in the family, so triplet margin loss at this level performed better than at the family level. We have summarized these results in <ref type="table">Table 6</ref>. We set ? i = 1 ?i ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> and compute the euclidean distance between the anchor and positive/negative class in the triplet loss.</p><p>Phylogenetic Distance In our final experiment, we used the phylogenetic distance between different genera and constrained the network to process the embedding distance between those genera similarly. This loss term is added together in addition to the hierarchical training. At this stage, we obtained the F1 score on the test dataset as 0.824. We set ? 1 = ?2 = 1 and ? 3 = 0.1 and p = 2 to compute the euclidean distance between the two vectors.</p><p>Phylogenetic Distance In our final experiment, we used the phylogenetic distance between different genera and constrained the network to process the embedding distance between those genera similarly. This loss term is added together in addition to the hierarchical training. At this stage, we obtained the F1 score on the test dataset as 0.824. We set ? 1 = ?2 = 1 and ? 3 = 0.1 and p = 2 to compute the euclidean distance between the two vectors.</p><p>Final Recipe Finally, we want to share the final ingredients we learned after training different models for several hours on the computing clusters. First and foremost is to go higher in terms of image resolution. Our analysis shows that a 512-dimensional image could give the best performance and is a good tradeoff with the computational cost associated with training. Next, we suggest making use of hierarchical labels if available. We found these hierarchical labels to eliminate the effect of initialization. Our experimentation also found that training with the hierarchical labels performed equally to a model trained with contrastive learning. Additionally, we found a positive effect of using a dropout of 10%, which was otherwise 0 in the default parameters. In the end, we recommend using the smart resizing technique where the images do not have a symmetric dimension.</p><p>Top entry model We used ensembling techniques like balancing predicted classes according to the probability score obtained across multiple random cropped images. We also added a dropout of 10% while training. With these methods, we obtained the final F1 score of 0.861 and 0.868 on the public and private test datasets, respectively. This score is amongst the top 5 entries on the Kaggle leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">iNaturalist 2019</head><p>In <ref type="table">Table 7</ref> we have summarized the results obtained by the Conviformer and other baselines. Our model could surpass other baselines reported on the dataset and obtain an accuracy of 82.85%. To our knowledge, this is the best accuracy reported so far with similar parameter models. The other two better performing transformer architectures are MixMIM-L and MAE (ViT-H), which have ?300M and ?600M parameters and perform at 83.9% and 88.3% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Resolution Accuracy ConViT <ref type="bibr" target="#b15">[16]</ref> 224 77.84 MAE (ViT-B) <ref type="bibr" target="#b52">[53]</ref> 224 80.50 MaxMIM (B) <ref type="bibr" target="#b54">[55]</ref> 224 82.60 CeiT-S <ref type="bibr" target="#b32">[33]</ref> 384 82.70 LeViT <ref type="bibr" target="#b33">[34]</ref> 384 74.30 Conviformer (Ours) 448 82.85 <ref type="table">Table 7</ref>: iNaturalist 2019: Accuracy obtained with different baselines. We also mention the input image resolution used in training the network.</p><p>Compatibility We evaluated the compatibility of the weights trained on the baseline architecture (in this case, Con-ViT) with Conviformer. To check this, we compared the model trained with randomly initialized weights and ImageNet pre-trained weights available on the PyTorch library. We obtained a considerable performance difference of 6.86% in accuracy between the two <ref type="table">(Table 8</ref>). It shows that when training the Conviformer initialized with the pre-trained weight of the baseline model, it even outperforms the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random ImageNet</head><p>ConViT 69.92 77.84 Conviformer 75.99 82.85 <ref type="table">Table 8</ref>: iNaturalist 2019: Accuracy obtained when the Conviformer is initialized with random weights and ImageNet pre-trained weights of ConViT. There is a gain of 9% in the accuracy when we load the ImageNet weights to the network representing compatibility with weights trained on the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper put forward an elegant method helpful for fine-grained classification challenges such as Herbarium-202x and iNaturalist 2019. We first demonstrated the importance of resolution for fine-grained classification tasks. When increasing the image resolution from 224 to 512, we found an improvement of 14.72% in accuracy on the iNaturalist 2019 dataset. Our pre-processing method, PreSizer, resize the images without changing their aspect ratio while infusing more meaningful information using reflective padding and removing the artifacts around bordering pixels. This augmentation method helped to improve the F1 score by 1.4% on the Herbarium 2022 test dataset. We introduced an architecture Conviformer, which enables a vision transformer to use higher dimensional images. Our evaluation with ConViT as base architecture exhibited an improvement of 2.7% on test F1 score. We also demonstrated its compatibility with the base architecture to initialize with the pre-trained weights and use the default hyper-parameter setup. This initialization is critical in training any transformer architecture. We did a robust analysis using meta-information provided with the Herbarium-2022 dataset, like hierarchical labels and phylogenetic distances. We found that hierarchical labels are essential to achieving higher accuracy in a fine-grained classification task. Overall, we proposed a generic solution that can be used for several computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgement</head><p>This work was funded by NSF (IIS-1912280) and ONR (N00014-19-1-2029) to TS and ANR (OSCI-DEEP grant ANR-19-NEUC-0004) to RV. Additional support was provided by the ANR-3IA Artificial and Natural Intelligence Toulouse Institute (ANR-19-PI3A-0004), the Center for Computation and Visualization (CCV) and High-Performance Computing (HPC) resources from CALMIP (Grant 2016-p20019, 2016-p22041). We acknowledge the Cloud TPU hardware resources that Google made available via the TensorFlow Research Cloud (TFRC) program and computing hardware supported by NIH Office of the Director grant S10OD025181.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Herbarium 2021 Dataset: (a) Three representative examples from different classes showing the visual similarity. (b) Three representative examples from the same classes display intra-class variability. Respective identifiers are mentioned below each example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two representative examples showing resizing-preserving. (a) Initial image from the Herbarium dataset. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) on top of ConViT architecture, enabling it to attend to images of larger dimensions. This convolutional block take the input image (x) of dimension h?w?3 (usually h is equal to w)and downsampling if by a factor of (d s ) where d s = h/224 = w/224 and convert the image to h ? w ?64 h = h/d s and w = w/d s total patches (t p ) = h 16 ? w 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Conviformer: Convolutionally block proposed in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Architecture</cell><cell cols="3">Image Resolution 224 448 512 600</cell></row><row><cell>ConViT</cell><cell cols="3">196 784 1024 1369</cell></row><row><cell>Conviformer</cell><cell>-</cell><cell>196 256</cell><cell>324</cell></row></table><note>Number of patches (Sequence length) for different input image resolutions. Sequence length is directly proportional to the memory demand by the self-attention module. Similar to other Vision transformer architectures where different models are defined as tiny, small and base, we also use the same nomenclature. Unless otherwise specified we have refer Conviformer to architecture built on base model of ConViT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>iNaturalist 2019: Accuracy obtained for different input image resolutions while training the model with random initializing.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Resolution F1 score</cell></row><row><cell>SE ResNeXt-101  *  [54]</cell><cell>448</cell><cell>.726</cell></row><row><cell>SE ResNeXt-50  *  [54]</cell><cell>448</cell><cell>.708</cell></row><row><cell>DeIT (B) [48]</cell><cell>224</cell><cell>.706</cell></row><row><cell>Conviformer (Ours)</cell><cell>448</cell><cell>.729</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Herbarium 2022: F1 score obtained on the test dataset with and without using our novel augmentation technique PreSizer. The model is trained on a full dataset using cross-entropy loss.</figDesc><table><row><cell>Steps</cell><cell>F1 score</cell></row><row><cell>(a) Trained with CE loss at Family, Genus and Category level (pre-trained weights)</cell><cell>0.829</cell></row><row><cell>(b) Trained with CE loss at Family, Genus and Category level (from scratch)</cell><cell>0.822</cell></row><row><cell>(c) Training with Triplet regularization at</cell><cell></cell></row><row><cell>Genus level and CE loss at Family, Genus</cell><cell>0.825</cell></row><row><cell>and Category level</cell><cell></cell></row><row><cell>(d) Training with Triplet regularization at</cell><cell></cell></row><row><cell>Family level and CE loss at Family, Genus</cell><cell>0.824</cell></row><row><cell>and Category level</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.kaggle.com/c/herbarium-2022-fgvc9/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/convit</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A leaf recognition algorithm for plant classification using probabilistic neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">You</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE international symposium on signal processing and information technology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Plant classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hulya</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salar</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Md Hafizur Rahman Arfin, and Syed Akhter Hossain. Patanet: A convolutional neural networks to identify plant from leaf images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akm Shahahriar Azad</forename><surname>Md Majedul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Plant classification using artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Luciano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valmir</forename><surname>Pacifico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao Fl</forename><surname>Macario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An approach for iris plant classification using neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhusmita</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjit</forename><surname>Kumar Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sweta</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayeskanta</forename><surname>Mohapatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Soft Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning with taxonomic loss for plant identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danzi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongping</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Feature fusion vision transformer for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02341</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A novel plug-in module for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yung</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Chung</forename><surname>Kao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03822</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transfg: A transformer architecture for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Neng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A free lunch from vit: adaptive attention multi-scale fusion transformer for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqian</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The herbarium 2021 half-earth challenge dataset and machine learning competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>De Lutio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Aronco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Wieringa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Tulig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Pyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gillian</forename><surname>Gallaher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Guymer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhahara</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumiko</forename><surname>Ranatunga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Baba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabi?n</forename><forename type="middle">A</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">A</forename><surname>Michelangeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><forename type="middle">P</forename><surname>Ambrose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Plant Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021. 1, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>PMLR, 2021. 1, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7262" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computer vision cracks the leaf code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wilf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharat</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3305" to="3310" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A?2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3531" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding the Computational Demands Underlying Visual Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Vaishnav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Alamia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufin</forename><surname>Vanrullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
		<idno>02 2022. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cmt: Convolutional neural networks meet vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="12175" to="12185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conformer: Local features coupling global representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Elsa: Enhanced local self-attention for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12786</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evolving attention with residual convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10971" to="10980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rams-trans: Recurrent attention multi-scale transformer for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4239" to="4248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Re-rank coarse classification with local region enhanced features for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09875</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Channel interaction networks for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10818" to="10825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Training dataefficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6509" to="6518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10154</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Measuring dataset granularity. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Similarity-Based Pattern Recognition</title>
		<editor>Aasa Feragen, Marcello Pelillo, and Marco Loog</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mixmim: Mixed and masked image modeling for efficient visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.13137,2022.11</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
