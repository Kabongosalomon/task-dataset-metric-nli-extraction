<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SeMask: Semantically Masked Transformers for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitesh</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon &amp; UIUC</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">IIT Roorkee</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anukriti</forename><surname>Singh</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon &amp; UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Orlov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon &amp; UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon &amp; UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon &amp; UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon &amp; UIUC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SeMask: Semantically Masked Transformers for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finetuning a pretrained backbone in the encoder part of an image transformer network has been the traditional approach for the semantic segmentation task. However, such an approach leaves out the semantic context that an image provides during the encoding stage. This paper argues that incorporating semantic information of the image into pretrained hierarchical transformer-based backbones while finetuning improves the performance considerably. To achieve this, we propose SeMask, a simple and effective framework that incorporates semantic information into the encoder with the help of a semantic attention operation. In addition, we use a lightweight semantic decoder during training to provide supervision to the intermediate semantic prior maps at every stage. Our experiments demonstrate that incorporating semantic priors enhances the performance of the established hierarchical encoders with a slight increase in the number of FLOPs. We provide empirical proof by integrating SeMask into Swin Transformer and Mix Transformer backbones as our encoder paired with different decoders. Our framework achieves a new state-of-the-art of 58.25% mIoU on the ADE20K dataset and improvements of over 3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are publicly available at https://github.com/Picsart-AI-Research/SeMask-Segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic Segmentation aims to perform dense prediction for labeling each pixel in an image corresponding to the class that the pixel represents. Transformer-based vision networks <ref type="bibr" target="#b20">[17,</ref><ref type="bibr" target="#b50">46]</ref> have outperformed Convolutional Neural Networks on the image-classification task <ref type="bibr" target="#b35">[31]</ref>. In modern times, transformer backbones have shown impressive performance when transferred to downstream tasks like semantic segmentation <ref type="bibr" target="#b5">[2,</ref><ref type="bibr" target="#b27">24,</ref><ref type="bibr" target="#b41">37]</ref>.</p><p>Most of the architectural designs in vision transformers * Work done during an internship at Picsart AI Research (PAIR).  <ref type="figure" target="#fig_4">Figure 1</ref>. Comparison between popular transformer-based network for segmentation (left) and SeMask (right). In contrast to most existing methods ( <ref type="bibr" target="#b41">[37]</ref> in above figure) that directly use the pretrained backbones without any changes, SeMask uses semantic priors in the encoder backbones by adding an additional semantic layer; this simple change significantly improves performance.</p><p>approach the problem in either of the two ways: (i) Use an existing pretrained backbone as an encoder and transfer it to downstream tasks using pre-existing standard decoders such as, Semantic FPN <ref type="bibr" target="#b34">[30]</ref> or UperNet <ref type="bibr" target="#b55">[51]</ref>; OR (ii) design a new encoder-decoder network where the encoder is pretrained on ImageNet for the semantic segmentation task. Both of these ways, as mentioned earlier, involve finetuning the encoder backbone on the segmentation task. Finetuning from a large-scale dataset help early attention layers to incorporate local information at lower layers of the transformers <ref type="bibr" target="#b45">[41]</ref>. However, it can still not harness the semantic context during finetuning due to the relatively smaller size of the dataset and a change in the number and nature of semantic classes from classification to the segmentation task. Hierarchical vision transformers <ref type="bibr" target="#b41">[37,</ref><ref type="bibr" target="#b56">52]</ref> tackle the problem with progressive downsampling of features along the stages, although they still lack the semantic context of the image. Liu et al. <ref type="bibr" target="#b41">[37]</ref> introduced the Swin Transformer, which constructs hierarchical feature maps making it compatible as a general-purpose backbone for major downstream vision tasks. <ref type="bibr" target="#b13">[10]</ref> proposed to use two attention: globally subsampled and locally sub-samples on top of PVT <ref type="bibr" target="#b52">[48]</ref> and CPVT <ref type="bibr" target="#b14">[11]</ref> for effective segmentation. Xie et al. <ref type="bibr" target="#b56">[52]</ref> further modified the hierarchical transformer encoder by making it free from positional-encoding and thus robust to different resolutions as generally found in the segmentation task. All these works modified the encoders to make them work better for downstream tasks like segmentation and achieved success to an impressive extent. Still, they did not pay attention to capturing the semantic-level contextual information of the whole image. A lack of semantic contextual information leads to sub-optimal segmentation performance, especially in the case of small objects where those get merged with the boundaries of the larger categories, leading to wrong predictions. Recently, <ref type="bibr" target="#b48">[44]</ref> tried to tackle this issue by designing a pure transformer-based decoder that jointly processes the patch and class embedding. However, it does not perform efficiently for tiny variants and fails with hierarchical architectures leading to sub-optimal performance when used with major transformer backbones like Swin <ref type="bibr" target="#b41">[37]</ref>, and Twins <ref type="bibr" target="#b13">[10]</ref> transformers.</p><p>Jin et al. in <ref type="bibr" target="#b33">[29]</ref> proposed ISNet to model the image level contextual information along with semantic level contextual information by introducing the SLCM and ILCM modules in the decoder structure. However there is still a caveat: IS-Net is a CNN based method and only focuses on the decoder part of the network, leaving out the encoder unchanged.</p><p>To address the issues mentioned above, we propose the SeMask framework that incorporates semantic information into hierarchical vision transformer architectures and augments the global feature information captured by the transformers with the semantic context. The existing frameworks formulate the architecture as an encoder-decoder structure with transformers pretrained on ImageNet <ref type="bibr" target="#b35">[31]</ref> acting as the encoders and using a specialized decoder for semantic segmentation. In contrast to directly using the hierarchical transformers as a backbone, we insert a Semantic Layer after the Transformer Layer at each stage in the backbone, giving us the SeMask version of the backbone as illustrated in <ref type="figure" target="#fig_4">Fig. 1</ref>. We use a lightweight semantic decoder to accumulate the semantic maps from all the stages, and a standard decoder like Semantic-FPN <ref type="bibr" target="#b34">[30]</ref> for the main perpixel prediction. The added semantic modeling with feature modeling throughout the encoder helps us improve the performance of the semantic segmentation task. In Sec. 4, we integrate the proposed SeMask block into the Swin Transformer <ref type="bibr" target="#b41">[37]</ref> and Mix Transformer <ref type="bibr" target="#b56">[52]</ref> backbones. Our experimental results show considerable improvement in semantic segmentation for both backbones on two different datasets. To summarize, our contributions are three fold:</p><p>? To the best of our knowledge, we are the first to study the effect of adding semantic context to pretrained transformer backbones for the semantic segmentation task. Furthermore, we introduce a SeMask Block which can be plugged into any existing hierarchical vision transformer. We provide empirical evidence by integrating SeMask into Swin-transformer <ref type="bibr" target="#b41">[37]</ref> and Mix-Transformer <ref type="bibr" target="#b56">[52]</ref>, and achieving considerable performance improvement.</p><p>? We also propose to use a simple semantic decoder for aggregating the semantic priors from different stages of the encoder. The semantic priors receive supervision from the ground truth using a per-pixel crossentropy loss.</p><p>? Lastly, we provide an in-depth analysis of the SeMask Block's effect on two different datasets: ADE20K and Cityscapes. We achieve the new state-of-the-art performance on the ADE20K dataset and an improvement above 3% on the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>Semantic segmentation broadly formulates to a dense per-pixel classification task. The seminal work of FCN <ref type="bibr" target="#b42">[38]</ref> introduced the use of deep CNNs, removing fully connected layers to tackle the segmentation task. Several following works <ref type="bibr" target="#b4">[1,</ref><ref type="bibr" target="#b37">33,</ref><ref type="bibr" target="#b46">42]</ref> were built upon the same idea of using the encoder-decoder architecture. <ref type="bibr" target="#b7">[4]</ref> introduced the use of atrous convolutions inside the DCNN to tackle the signal downsampling issue. Later, various works focused on the aggregating long-range context in the final feature map: ASPP <ref type="bibr" target="#b8">[5]</ref><ref type="bibr" target="#b9">[6]</ref><ref type="bibr" target="#b10">[7]</ref> uses atrous convolutions with different dilation rates; PPM <ref type="bibr" target="#b58">[54]</ref> uses pooling operators with different kernel sizes.</p><p>The recent DCNN based models focus on efficiently aggregating the hierarchical features from a pretrained backbone based encoder with specially designed modules: <ref type="bibr" target="#b47">[43,</ref><ref type="bibr" target="#b49">45,</ref><ref type="bibr" target="#b54">50]</ref> introduce attention modules in the decoder; <ref type="bibr" target="#b21">[18,</ref><ref type="bibr" target="#b28">25]</ref> use different forms of non-local blocks <ref type="bibr" target="#b53">[49]</ref>; <ref type="bibr" target="#b36">[32]</ref> proposes a novel FAM module to solve the misalignment issue using semantic flow; AlignSeg <ref type="bibr" target="#b30">[26]</ref> proposes aligned feature aggregation module and aligned context modeling module to make contextual features be better aligned. <ref type="bibr" target="#b61">[57]</ref> uses a segmentation shelf for better information flow. In this work, we also follow the established direction to use a pretrained backbone and aggregating the hierarchical features <ref type="bibr" target="#b41">[37]</ref> using the Semantic-FPN <ref type="bibr" target="#b34">[30]</ref> decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformers for Segmentation</head><p>After being heavily used in Natural Language Processing field, transformer <ref type="bibr" target="#b51">[47]</ref> based models have gained popularity for various computer vision tasks since the introduction of ViT <ref type="bibr" target="#b20">[17]</ref> for image classification <ref type="bibr" target="#b20">[17,</ref><ref type="bibr" target="#b23">20,</ref><ref type="bibr" target="#b31">27,</ref><ref type="bibr" target="#b50">46]</ref>. SETR used ViT <ref type="bibr" target="#b20">[17]</ref> as an encoder and two decoders based upon progressive upsampling and multi-level feature aggregation.  <ref type="figure" target="#fig_4">Figure 2</ref>. SeMask Swin Semantic FPN Framework: We add a Semantic Layer with NS SeMask Blocks <ref type="figure" target="#fig_1">(Fig. 3b</ref>) after the Swin Transformer Layer to capture the semantic context in the encoder network. The Semantic Maps from the Semantic Layers at each stage are aggregated using a simple Upsample + Sum operation and passed through a weighted CE Loss to supervise the semantic context.</p><p>SegFormer <ref type="bibr" target="#b56">[52]</ref> proposed to use a hierarchical pyramid vision transformer network as an encoder with an MLP based decoder to obtain the segmentation mask. Segmenter <ref type="bibr" target="#b48">[44]</ref> designed mask transformer as a decoder, which uses learnable class-map tokens to enhance decoding performance. MaskFormer <ref type="bibr" target="#b12">[9]</ref> defines the problem of per-pixel classification from a mask classification point of view, creating an allin-one module for all segmentation tasks. Mask2Former <ref type="bibr" target="#b11">[8]</ref> further evolves masked attention to solve panoptic, instance and semantic segmentation tasks in one framework. Most recent transformer-based segmentation frameworks <ref type="bibr" target="#b19">[16,</ref><ref type="bibr" target="#b41">37]</ref> are based on finetuning a pretrained hierarchical backbone as an encoder, and standard decoders like Semantic-FPN and UperNet <ref type="bibr" target="#b34">[30,</ref><ref type="bibr" target="#b55">51]</ref> to the segmentation task. In this work, we follow the same paradigm and, in addition, propose a framework to enhance the finetuning ability of the pretrained vision transformer backbone. Note that there is also recent concurrent work like SwinV2 <ref type="bibr" target="#b40">[36]</ref> that reaches new state-of-the-art performance on ADE20k benchmark by using improved and giant backbones (e.g. SwinV2-G with 3.0 billion parameters). That is out of the scope of this work and we follow the current practice mainly based on Swin-L backbone. Theoretically, we can get even better performance if we apply our approach to such giant models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Semantic Context in Segmentation</head><p>Zhang et al. proposed the Context Encoding Module in <ref type="bibr" target="#b57">[53]</ref> which captures the global semantic context along with a feedback loop to balance the importance of classes in the features extracted by a ResNet backbone <ref type="bibr" target="#b24">[21]</ref>. More recently, <ref type="bibr" target="#b32">[28,</ref><ref type="bibr" target="#b33">29]</ref> focus on capturing and integrating the semantic-level contextual information along with the image-level context with specially designed decoders which shows significant improvement in DCNN based methods. Each of these works captures the semantic context after the encoding stage based on the extracted features and not the encoder's ability to capture the semantic features.</p><p>In this work, we argue that semantic information is lost during the encoding stage and hence, propose a framework to capture semantic information which can be plugged into any pretrained vision transformer backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>An overview of our architecture with Swin-Transformer <ref type="bibr" target="#b41">[37]</ref> backbone is shown in <ref type="figure" target="#fig_4">Fig. 2</ref>. The RGB input image, size H ? W ? 3, is first split into non-overlapping patches of size 4 ? 4. The smaller size of the patch supports dense prediction in segmentation. These patches act as tokens and are given as input to the hierarchical vision transformer encoder, which is the Swin-Transformer <ref type="bibr" target="#b41">[37]</ref> in our architecture. The encoding step consists of four different stages of hierarchical feature modeling. Every stage during the encoding step consists of two layers: The transformer layer, which is N A number of Swin Transformer blocks ( <ref type="figure" target="#fig_1">Fig. 3a</ref>) stacked together and Semantic Layer with N S number of SeMask Attention blocks <ref type="figure" target="#fig_1">(Fig. 3b</ref>). We collectively refer to the Transformer Layer and Semantic Layer at each stage as our SeMask Block. The patch tokens pass through each stage at { 1 4 , 1 8 , 1 16 , 1 32 } of the original image resolution for the feature maps and intermediate semantic-prior maps extraction.</p><p>In the encoder part of the network, the Semantic Layer takes in features from the Transformer Layer as inputs and returns the intermediate semantic-prior maps and semantically masked features <ref type="figure" target="#fig_1">(Fig. 3b</ref>). When we plug the Se-Mask Attention Block into other hierarchical vision transformers, the Transformer Layer consists of attention blocks corresponding to the specific backbone, like Efficient-Self Attention-based Transformer Layer for the Mix Transformer <ref type="bibr" target="#b56">[52]</ref> backbone. The semantically masked features from each stage are aggregated using the semantic-FPN <ref type="bibr" target="#b34">[30]</ref> decoder for producing the final dense-pixel prediction. Moreover, the semantic-prior maps from all the stages are aggregated using a lightweight upsample &amp; sum operationbased semantic decoder to predict the semantic-prior for the network during training. Both decoders' outputs are supervised using a weighted per-pixel cross-entropy loss. These additional semantic-prior maps greatly assist the feature extraction and eventually improve the performance on the semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SeMask Encoder</head><p>Each stage in our encoder consists of two layers: the Transformer Layer and the Semantic Layer. The transformer layer is composed of N A Swin Transformer blocks stacked to extract image-level context information from the image. The semantic layer contains N S SeMask Attention blocks stacked together to decouple semantic information from the features, producing semantic-priors and then updating the features with guidance from these semantic-prior maps.</p><p>Transformer layer. For the transformer layer, we adapt the hierarchical structure of Swin Transformer <ref type="bibr" target="#b41">[37]</ref> which constructs hierarchical feature maps and has linear computational complexity to the image resolution. Before feed-ing the RGB image into the transformer layer in the first stage, we split it into non-overlapping patches of size is 4 ? 4 ? 3 = 48. The first stage in the encoder has a linear embedding layer to change the feature dimension of the patch tokens. Inside each transformer layer, there are N A shifted window attention blocks <ref type="figure" target="#fig_1">(Fig. 3a</ref>) that have linear computation complexity along with cross-window connections to handle non-overlapping regions, making the design effective for image-level feature modeling. For a hierarchical representation, we shrink our feature maps from H 4 ? W 4 to H 8 ? W 8 by patch merging layers for the next stage. This patch merging is iterated for the next stages to obtain a hierarchical feature map, with a resolution of</p><formula xml:id="formula_0">H 2 i+1 + W 2 i+1 ? C i where i ? {1, 2, 3, 4}</formula><p>. X represents the input features inside the transformer layer block. And for computing selfattention in the transformer layer, X is transformed into: Q, K, V which are query, key and value matrices with same dimension of N ? C. Based on swin transformer, we also follow <ref type="bibr" target="#b6">[3,</ref><ref type="bibr" target="#b25">22,</ref><ref type="bibr" target="#b26">23,</ref><ref type="bibr" target="#b41">37,</ref><ref type="bibr" target="#b44">40]</ref> to include a relative position embedding (RPE) where RP E ? R N ?N and N = M ? M is the length of the sequence with M = window size. The attention inside the Transformer Layer is calculated as:  The resulting feature Y from the Transformer Layer after the last Swin Transformer block then acts as an input to the subsequent semantic layer in the same stage as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><formula xml:id="formula_1">Attention(Q, K, V ) = SoftMax QK T ? C + RP E V (1) N C N C N C N C</formula><p>Semantic Layer. The Semantic Layer follows the Transformer Layer at each stage of our hierarchical vision transformer. Unlike the Transformer Layer, the Semantic Layer's significance is in modeling the semantic context, which is used as a prior for calculating a segmentation score to update the feature maps based on guidance from the semantic nature present in the image. Inside each semantic layer, there are N S SeMask attention blocks <ref type="figure" target="#fig_1">(Fig. 3b</ref>). Inspired by the shifted window-based division of the tokens for efficient computation cost, we also divide the input to our SeMask blocks into windows with cross-window connections before calculating the segmentation score using a single-head self-attention operation. The SeMask block is responsible for capturing the semantic context in our encoder. It updates the features from the transformer layer from the segmentation score providing guidance and giving a semantic-prior map for efficient supervision of the semantic modeling during training. SeMask attention block divides the features Y from the preceding transformer layer into three entities: Semantic Query (S Q ), Semantic Key (S K ), and Feature Value (Y V ). We get S K and S Q by projecting the features onto the semantic space. The dimension of both S Q and S K is N ? K where K is equal to the number of classes, and the dimension of Y V is N ? C where C is the embedding dimension, N = M ? M is the length of the sequence with M = window size which we set as equal to that used inside the transformer layer. S Q returns the semantic map, and a segmentation score is calculated using S K and S Q . The score is passed through a softmax and is used to update Y V as shown in <ref type="figure" target="#fig_1">Fig. 3b</ref>. This SeMask attention equation is expressed as follows:</p><formula xml:id="formula_2">Score(S Q , S K , Y V ) = SoftMax(S Q S T K )Y V<label>(2)</label></formula><p>We perform a matrix multiplication between the feature values and the segmentation score. The matrix product is later passed through a linear layer and multiplied with a learnable scalar constant ?, used for smooth finetuning. After a residual connection <ref type="bibr" target="#b24">[21]</ref>, we finally get the modified features, rich with semantic information which we call the Semantically Masked features. The semantic queries S Q are later used to predict the semantic-prior map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoder</head><p>We use two decoders to aggregate the features and the semantic-prior maps respectively from the different stages in the encoder.</p><p>For aggregating the semantically masked features, we employ the popular Semantic-FPN decoder <ref type="bibr" target="#b34">[30]</ref>. The Semantic-FPN fuses the features from different stages with a series of convolution, bilinear upsampling, and sum operations, making it efficient and straightforward as a segmentation decoder for our purpose. In addition, we use a lightweight semantic decoder during training to provide ground truth supervision to the semantic-prior maps at every stage of the encoder. As the semantic-prior maps have the channel dimension of K in each stage, we only employ a series of upsampling and sum operations to aggregate the maps with K being equal to the number of classes in the dataset. Lastly, the output from both the decoders is upscaled ?4 to the resolution of the original image for the final predictions as shown in <ref type="figure" target="#fig_4">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>To train our model's parameters, we calculate the total loss L T as a summation of two per-pixel cross-entropy losses: L 1 and L 2 . The loss L 1 is calculated on the main prediction from the Semantic-FPN decoder and loss L 2 is calculated on the semantic-prior prediction from our lightweight decoder. F contains the main prediction of the network and S denotes the semantic-prior prediction. We define our losses on F and S as follows:</p><formula xml:id="formula_3">L 1 = 1 H ? W i,j L ce F [ * ,i,j] , ? GT [ij] .</formula><p>(3)</p><formula xml:id="formula_4">L 2 = 1 H ? W i,j L ce S [ * ,i,j] , ? GT [ij] .<label>(4)</label></formula><formula xml:id="formula_5">L T = L 1 + ?L 2<label>(5)</label></formula><p>Here, ? denotes for converting the ground truth class label stored in GT into one-hot format, i,j denotes that the summation is carried out over all the pixels of the GT , and L ce is the cross-entropy loss. We empirically set ? = 0.4 (check appendix for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare our approach with Swin Transformer <ref type="bibr" target="#b41">[37]</ref>, and Mix-Transformer <ref type="bibr" target="#b56">[52]</ref> with extensive experiments to demonstrate the effectiveness of the SeMask framework. We also ablate the SeMask structure and confirm that providing a semantic-prior to mask out the features improves semantic segmentation performance. The experiments are performed on two widely used datasets: ADE20K <ref type="bibr" target="#b17">[14]</ref> and Cityscapes <ref type="bibr" target="#b16">[13]</ref>. We include more experimental results in the appendix proving that our method is dataset agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and metrics</head><p>ADE20K. <ref type="bibr" target="#b17">[14]</ref> ADE20K is a scene parsing dataset covering 150 fine-grained semantic concepts and it is one of the most challenging semantic segmentation datasets. The training set contains 20,210 images with 150 semantic classes. The validation and test set contain 2,000 and 3,352 images respectively. Cityscapes. <ref type="bibr" target="#b16">[13]</ref> Cityscapes is an urban street driving dataset for semantic segmentation consisting of 5,000 images from 50 cities with 19 semantic classes. There are 2,975 images in the training set, 500 images in the validation set and 1,525 images in the test set. Metrics. We report mean Intersection-over-Union (mIoU ) over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Transformer models. For the encoder, we build upon the Swin Transformer <ref type="bibr" target="#b41">[37]</ref> and consider the Tiny, Small, Base and Large variants as described in Tab. 1. The variation in number of parameters among the baselines is due to the number of transformer blocks (N T B ) ( <ref type="figure" target="#fig_1">Fig. 3a</ref>) and the embedding dimension (C) for each stage of the model. The number of heads (N T H ) of a shifted window based multi-headed self-attention (SW-MSA) or Swin Transformer block varies from stage to stage. The hidden size of the MLP following SW-MSA is four times the embedding dimension at the corresponding stage. We also experiment with the MiT-B4 backbone variant of the Mix-Transformer [52] on the ADE20K <ref type="bibr" target="#b17">[14]</ref> dataset.</p><p>In the following sections, we use an abbreviation to describe the model variant. For example, Swin-T denotes the Tiny variant. The backbones pretrained on ImageNet-22k <ref type="bibr" target="#b35">[31]</ref> and with 384?384 resolution are denoted with a ?: Swin-B ? . All the other models are pretrained on ImageNet-1k and with 224?224 resolution. Network Initialization. Our SeMask models are initialized with publicly available models. The Tiny and Small variants are pre-trained on ImageNet-1k with an image resolution of 224 ? 224. The Base and Large variants are pretrained on ImageNet-22k with a resolution of 384?384. We keep the window size (M ) fixed as in the pretrained models and finetune the models for the semantic segmentation task at higher resolution depending on the dataset. Following <ref type="bibr" target="#b41">[37]</ref>, we include relative position bias while calculating the attention scores. The decoders, described in Sec. 3.2 are initialized with random weights from a normal distribution <ref type="bibr" target="#b22">[19]</ref>. Data augmentation. During training, we perform mean subtraction, scaling the image to a ratio randomly sampled from (0.5, 0.75, 1.0, 1.25, 1.5, 1.75), random left-right flipping, and color jittering. We randomly crop large images and pad small images to a fixed size of 512 ? 512 for ADE20K and 768 ?768 for Cityscapes. On ADE20K, we train our largest model Semask-L ? FPN with a 640 ? 640 resolution, matching the resolution used by the Swin-Transformer <ref type="bibr" target="#b41">[37]</ref>. Training Settings. To fine-tune the pre-trained models on the semantic segmentation task, we employ the AdamW <ref type="bibr" target="#b43">[39]</ref> optimizer with a base learning rate ? 0 . Following the seminal work of DeepLab <ref type="bibr" target="#b7">[4]</ref> we adopt the poly learning rate decay ? = ? 0 (1 ? Niter N total ) 0.9 where N iter and N total represent the current iteration number and the total iteration number. We use a linear warmup strategy for 1,500 iterations.</p><p>For ADE20K, we set the base learning rate ? 0 to 10 ?4 , weight decay to 10 ?4 and train for 80K iterations with a batch size of 16.</p><p>For Cityscapes, we set ? 0 to 10 ?3 , a weight decay of 5 ? 10 ?2 and train for 80K iterations with a batch size of 8. Inference. To handle varying image sizes during inference, we keep the aspect ratio intact and resize the image to a resolution with the smaller edge resized to the training resolution and consequently rescaled to the original dimensions before calculating the metric score. For multi-scale inference, following standard practice <ref type="bibr" target="#b10">[7]</ref> we use rescaled versions of the image with scaling factors of (0.5, 0.75, 1.0, 1.25, 1.5, 1.75).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this section, we ablate different variants of our Se-Mask framework. We investigate the model size, semantic attention, number of SeMask blocks (N S ), effect of the learnable scalar constant (?) inside the SeMask block and the pretraining dataset as well as image resolution. Unless stated otherwise, we use the Semantic-FPN <ref type="bibr" target="#b34">[30]</ref> as our decoder for the main prediction and report results using singlescale (s.s.) inference on the ADE20K <ref type="bibr" target="#b17">[14]</ref> val dataset. Transformer size. We study the impact of transformers size on performance in Tab. 2 by experimenting with the four different Swin variants: Tiny, Small, Base and Large with N S <ref type="figure">= [1, 1, 1, 1</ref>] for all the experiments. Our method gives improvement consistently over all the baseline variants with the improvement on the Cityscapes dataset being more impressive due to the fewer number of classes in the segmentation dataset creating a stronger prior.</p><p>We evaluate and record the mIoU scores for the baseline Swin models by training our networks using their publicly released code based on the MMSegmentation Library <ref type="bibr" target="#b15">[12]</ref>. Semantic Attention. We study the impact of the semantic attention operation calculated inside the SeMask Block on performance in Tab. 3 by replacing the SeMask Block with a simple single-head self-attention block on the Swin-Tiny variant. It is evident that simple attention does not help improve the results proving the validity and effectiveness of our SeMask Block.  <ref type="table">Table 4</ref>. Ablation on ?. We support the critical claim of the learnable scalar constant: ? inside the SeMask Block by removing and recording the mIoU (?).</p><p>Learnable Constant (?). We study the impact of ? on performance in Tab. 4, by removing it for the Tiny and Small variants. We observe that the inclusion of ? is potent to the success of the SeMask block as it acts as a tuning factor for the modified features, keeping the noise from weights' initialization in check. We also observe that ? ? [0.05, 0.3] during inference for different stages in the encoder. Number of SeMask Blocks (N S ). In Tab. 5 we study the impact of number of SeMask attention blocks on performance by changing the values of N S inside each semantic layer on the Swin-Tiny variant. We observe that N S <ref type="figure">= [1, 1, 1, 1</ref>] is the best setting. Interestingly, when stacking multiple blocks in a layer, we observe that inputting the S Q from the previous SeMask block into the later one gives better performance than obtaining S Q from the features. This shows that extracting semantic features using a single semantic attention operation is the optimum setting.</p><p>Pretraining Dataset. We study the impact of the pretraining dataset (ImageNet-1k v/s ImageNet-22k) on performance in Tab. 6 by training and evaluating the Base variant pretrained on various settings. Our framework is agnos-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Main Results</head><p>ADE20K. Using SeMask Swin-L ? as the encoder and Mask2Former-MSFaPN as our decoder for the main prediction, we achieve a new state-of-the-art performance with scores of 57.00% and 58.25% on the single-scale and multiscale mIoU metric, respectively. Following <ref type="bibr" target="#b41">[37]</ref>, our models were trained on 640?640 images. We also achieve competitive results with our SeMask Swin-L ? backbone with Semantic-FPN to the Swin-L ? based UPerNet model as shown in Tab. 8.</p><p>We also integrate our SeMask into the MiT-B4 based SegFormer model <ref type="bibr" target="#b56">[52]</ref> as shown in Tab. 8 and achieve an improvement of 1.55% on the single scale mIoU and 1.31% improvememt on the multi-scale mIoU metric scores. This  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Backbones</head><p>PSANet <ref type="bibr" target="#b59">[55]</ref> ResNet-101 77.94 79.05 DeepLabV3+ <ref type="bibr" target="#b10">[7]</ref> Xception-71 -79.55 CCNet <ref type="bibr" target="#b28">[25]</ref> ResNet-101 80.50 81.30</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Backbones</head><p>Seg-L-Mask/16 <ref type="bibr" target="#b48">[44]</ref> ViT   <ref type="bibr" target="#b11">[8]</ref>. Qualitative results. <ref type="figure" target="#fig_3">Fig. 4</ref> shows a qualitative comparison of Swin-T FPN and SeMask-T FPN on the Cityscapes dataset generated using the MMSegmentation library <ref type="bibr" target="#b15">[12]</ref>. It is evident that SeMask-T FPN is able to generate better class-wise predictions than the Swin-T FPN. As shown in the second row in <ref type="figure" target="#fig_3">Fig. 4</ref>, we are able to segment the pole with out SeMask-T FPN, while Swin-T FPN fails to do so. Similarly in the third row, we are better able to segment the boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper argues that directly finetuning off-the-shelf pretrained transformer backbone networks as encoders for semantic segmentation does not consider the semantic context tied up with the images. We claim that adding a semantic prior to guide the encoder's feature modeling enhances the finetuning process for semantic segmentation. Furthermore, to support our claim, we propose the SeMask Block, which can be plugged into any existing hierarchical vision transformer and uses a semantic attention operation to capture the semantic context and augment the semantic representation of the feature maps. We train and evaluate the proposed framework building on the Swin-Transformer <ref type="bibr" target="#b41">[37]</ref> and Mix-Transformer <ref type="bibr" target="#b56">[52]</ref> backbones based networks and show a considerable improvement in the semantic segmentation performance on the Cityscapes and ADE20K dataset, with improvements above 3% on the Cityscapes dataset. We provide a comprehensive experimental analysis applying  <ref type="bibr" target="#b27">[24]</ref> to the BasePixelDecoder <ref type="bibr" target="#b12">[9]</ref>. We add similar feature alignment modules based on deformable convolutions <ref type="bibr" target="#b18">[15]</ref> to the MSDeformAttnPixelDecoder proposed in <ref type="bibr" target="#b11">[8]</ref> to obtain the MSFaPN design. * Note that we follow the convention and compare methods based on the Swin-L backbone and we currently do not consider giant models like SwinV2-G that have billions of parameters.</p><p>SeMask to different backbone variants and achieving considerable performance improvement in every setting. Our method also achieves the new state-of-the-art performance on the ADE20K dataset. As a direction for future research, it will be interesting to observe the effect of adding similar priors for other vision downstream tasks like object detection and instance segmentation. Using weighted supervision for the semantic-prior maps is critical so that the model treats the semantic context as an additional signal for feature modeling and not as the main prediction.</p><formula xml:id="formula_6">L T = L 1 + ?L 2<label>(6)</label></formula><p>We study the impact of ? on performance in Tab. I by changing the values of ? on the Swin-Tiny variant. ? = 0.4 is the optimum setting for modeling the network's image feature level and semantic level context.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on COCO-Stuff 10k</head><p>COCO-Stuff 10k comprises of a total of 10k images with dense pixel-level annotations, selected from the COCO <ref type="bibr" target="#b38">[34]</ref> dataset. The training set contains 9k images with 171 semantic classes and the test set contains 1k images.</p><p>We set the base learning rate ? 0 to 10 ?4 , weight decay to 10 ?4 and train for 80K iterations with a batch size of 16.</p><p>We provide our experimental results in Tab. II. Our Se-Mask framework shows impressive improvement on the COCO-Stuff 10k dataset proving its dataset-agnostic ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis on SeMask</head><p>In order to confirm our hypothesis that adding semantic context inside the encoder with the help of the semantic attention operation helps in improving the semantic quality of the features, we analyze the pixel-wise  <ref type="figure" target="#fig_4">Figure II</ref>. SeMask Block. The semantic attention outputs the semask features (Ysemask) using the features from the transformer layer (Ypre). We use a residual connection from Ypre to obtain the final output (Ypost). S is the semantic-prior map used to semantically mask the features (Ypre).</p><p>Specifically, we analyze pixel-wise attention for the pre-SeMask (Y pre ) and post-SeMask (Y post ) features <ref type="figure" target="#fig_4">(Fig. II)</ref> for Stage-3 and Stage-4 which are downsampled by ?16 and ?32, respectively. We calculate the pixel-wise attention maps corresponding to the target pixel (red cross sign), and we observe that post-SeMask features have more similar features for the same semantic category region with better boundaries than the pre-SeMask features. It reflects that the semantic prior maps help increase similarity between the pixels belonging to the same semantic category and improve the semantic segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative results</head><p>We provide qualitative results on the COCO-Stuff 10k test set in <ref type="figure" target="#fig_4">Fig. III</ref> where SeMask-L FPN produces better per-pixel predictions compared to Swin-L FPN. It is evident in (b) as the Swin-L FPN network fails to label the pole correctly and completely mislabels the sky region in (c).</p><p>We show more qualitative results on the ADE20K validation set in <ref type="figure" target="#fig_4">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swin-L FPN Image</head><p>Ours Ground Truth <ref type="figure" target="#fig_4">Figure IV</ref>. Qualitative results on the ADE20K validation set. Our SeMask-L FPN can correctly classify the mirror region in (b), whereas Swin-L FPN mislabels a significant part of the mirror as curtain owing to the reflection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Attention Blocks. NA Shifted Window Self Attention Blocks, shown inFig. 3a, are stacked inside each Transformer Layer and NS SeMask Attention Blocks, shown inFig. 3b, are stacked inside each Semantic Layer at every stage(Fig. 2). The output, Y , from the last Swin Attention Block, is fed to the first SeMask block in the Semantic Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on the Cityscapes validation set. The dot-bordered boxes at the top show zoomed-in regions from the images for a more detailed look at the improvement using our SeMask-T FPN.MethodBackbone mIoU (%) MS mIoU (%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure I .</head><label>I</label><figDesc>Analysis of features on the Cityscapes val set. We analyze pixel-wise attention maps for the Ypre and Ypost features from Stage-3 and Stage-4 of our SeMask-T FPN network. The post-SeMask (Ypost) features are richer in clear boundaries and pixel similarity than pre-SeMask (Ypre) features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>attention quality of the intermediate features of our SeMask-T FPN model on the Cityscapes [13] val dataset as shown in Fig. I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure III .</head><label>III</label><figDesc>. IV. Swin-L FPN mislabels mirror as curtain in (b) due to the reflection of the curtain. On the other hand, SeMask-L FPN classifies the regions accurately. Qualitative results on the COCO-Stuff 10k test set. Swin-L FPN completely mislabels the sky region and a significant part of the ground in (c), and our SeMask-L FPN shows better accuracy in classifying the regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Details of Swin Transformer variants. The Tiny and Small variants are trained on ImageNet-1k and with 224?224 resolution. ? stands for ImageNet-22k pre-training on 384?384 resolution images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablation on Swin-Transformer varaints. We provide a comparison of using SeMask Swin with Semantic-FPN<ref type="bibr" target="#b34">[30]</ref> decoder on all 4 varaints on the ADE20K-Val and Cityscapes-Val dataset. We evaluate the models using both, the single scale (s.s) and multi-scale (m.s.) mIoU (?). All models are trained for 80k iterations. The FLOPs are calculated for the given crop sizes using the script provided by the MMSegmentation<ref type="bibr" target="#b15">[12]</ref> library. Ablation on Semantic Attention. We prove the effectiveness of the SeMask Block by replacing it with a simple Single-Head Self Attention block which harms the performance on the Tiny variant.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell></cell><cell></cell><cell></cell><cell>ADE20K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cityscapes</cell><cell></cell></row><row><cell cols="2">(encoder + decoder) (pretrained)</cell><cell></cell><cell>crop size</cell><cell cols="4">#param. (M) FLOPs (G) s.s. mIoU (%) m.s. mIoU (%)</cell><cell>crop size</cell><cell cols="4">#param. (M) FLOPs (G) s.s. mIoU (%) m.s. mIoU (%)</cell></row><row><cell>Swin-T FPN</cell><cell>Swin-T</cell><cell></cell><cell>512 ? 512</cell><cell>33</cell><cell>38</cell><cell>41.48</cell><cell>42.89</cell><cell>768 ? 768</cell><cell>33</cell><cell>81</cell><cell>71.81</cell><cell>73.74</cell></row><row><cell>SeMask-T FPN</cell><cell cols="2">SeMask Swin-T</cell><cell>512 ? 512</cell><cell>35</cell><cell>40</cell><cell>42.06 (+0.58)</cell><cell>43.36 (+0.47)</cell><cell>768 ? 768</cell><cell>34</cell><cell>84</cell><cell>74.92 (+3.11)</cell><cell>76.56 (+2.82)</cell></row><row><cell>Swin-S FPN</cell><cell>Swin-S</cell><cell></cell><cell>512 ? 512</cell><cell>54</cell><cell>61</cell><cell>45.20</cell><cell>46.96</cell><cell>768 ? 768</cell><cell>54</cell><cell>130</cell><cell>75.19</cell><cell>77.68</cell></row><row><cell>SeMask-S FPN</cell><cell cols="2">SeMask Swin-S</cell><cell>512 ? 512</cell><cell>56</cell><cell>63</cell><cell>45.92 (+0.72)</cell><cell>47.63 (+0.67)</cell><cell>768 ? 768</cell><cell>56</cell><cell>134</cell><cell>77.13 (+1.94)</cell><cell>79.14 (+1.46)</cell></row><row><cell>Swin-B FPN</cell><cell>Swin-B  ?</cell><cell></cell><cell>512 ? 512</cell><cell>93</cell><cell>103</cell><cell>48.80</cell><cell>50.28</cell><cell>768 ? 768</cell><cell>93</cell><cell>211</cell><cell>76.54</cell><cell>79.05</cell></row><row><cell>SeMask-B FPN</cell><cell cols="3">SeMask Swin-B  ? 512 ? 512</cell><cell>96</cell><cell>107</cell><cell>49.35 (+0.55)</cell><cell>50.98 (+0.70)</cell><cell>768 ? 768</cell><cell>96</cell><cell>217</cell><cell>77.70 (+1.16)</cell><cell>79.73 (+0.68)</cell></row><row><cell>Swin-L FPN</cell><cell>Swin-L  ?</cell><cell></cell><cell>640 ? 640</cell><cell>204</cell><cell>343</cell><cell>50.85</cell><cell>52.95</cell><cell>768 ? 768</cell><cell>204</cell><cell>444</cell><cell>78.03</cell><cell>79.53</cell></row><row><cell>SeMask-L FPN</cell><cell cols="3">SeMask Swin-L  ? 640 ? 640</cell><cell>212</cell><cell>356</cell><cell>51.89 (+1.04)</cell><cell>53.52 (+0.57)</cell><cell>768 ? 768</cell><cell>211</cell><cell>455</cell><cell>78.53 (+0.50)</cell><cell>80.39 (+0.86)</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="5">SA Block SeMask Block mIoU (%) #Param (M)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-T FPN</cell><cell>Swin-T</cell><cell></cell><cell></cell><cell>41.48</cell><cell cols="2">33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Trans Swin-T FPN Trans Swin-T</cell><cell></cell><cell></cell><cell>41.42</cell><cell cols="2">36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SeMask-T FPN</cell><cell>SeMask Swin-T</cell><cell></cell><cell></cell><cell>42.06</cell><cell cols="2">35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Backbone</cell><cell cols="4">? mIoU (%) #Param (M)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SeMask-T FPN SeMask Swin-T</cell><cell>42.06</cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SeMask-T FPN SeMask Swin-T ?</cell><cell>41.11</cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SeMask-S FPN SeMask Swin-S</cell><cell>45.92</cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SeMask-S FPN SeMask Swin-S ?</cell><cell>45.00</cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Ablation on NS. We experiment with different combinations of NS on the SeMask-Tiny variant and report mIoU (?). NS = [1, 1, 1, 1] is the best setting. Ablation on Pretraining dataset. We compare the improvement when using the SeMask-Base variant with different pretraining settings: ImageNet-1k v/s ImageNet-22k and 224?224 v/s 384?384 and show that it is agnostic to the pretraining setting.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Pre Res mIoU (%) #Param (M)</cell></row><row><cell>Swin-B FPN</cell><cell>Swin-B</cell><cell>1k 224</cell><cell>45.47</cell><cell>93</cell></row><row><cell cols="3">SeMask-B FPN SeMask Swin-B 1k 224</cell><cell>45.63</cell><cell>96</cell></row><row><cell>Swin-B FPN</cell><cell>Swin-B</cell><cell>22k 224</cell><cell>47.65</cell><cell>93</cell></row><row><cell cols="3">SeMask-B FPN SeMask Swin-B 22k 224</cell><cell>48.29</cell><cell>96</cell></row><row><cell>Swin-B FPN</cell><cell>Swin-B</cell><cell>22k 384</cell><cell>48.80</cell><cell>93</cell></row><row><cell cols="3">SeMask-B FPN SeMask Swin-B 22k 384</cell><cell>49.06</cell><cell>96</cell></row><row><cell cols="5">tic to the pretraining setting showing improvement for all</cell></row><row><cell cols="5">combinations mainly used for the ImageNet pretraining: (i)</cell></row><row><cell cols="5">ImageNet-1k and 224?224 image resolution; (ii) ImageNet-</cell></row><row><cell cols="5">22k and 224?224 image resolution; and (iii) ImageNet-22k</cell></row><row><cell cols="3">and 384?384 image resolution.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>SOTA Comparison on Cityscapes-Validation. Semask Swin-L ? is competitive with other state-of-the-art methods with SeMask Swin-L ? Mask2Former achieving 84.98% mIoU. We train our SeMask-L Mask2Former on 512 ? 1024 images following Mask2Former</figDesc><table><row><cell>We re-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table I .</head><label>I</label><figDesc>Ablation on ?. We experiment with different values of ? on the SeMask-Tiny variant and report single-scale mIoU (?). ? = 0.4 is the best setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table II .</head><label>II</label><figDesc>Experiments with COCO-Stuff 10k. We provide a comparison of using SeMask Swin with Semantic-FPN<ref type="bibr" target="#b34">[30]</ref> decoder on the COCO Stuff-10k test set. We evaluate the models using both, the single scale (s.s) and multi-scale (m.s.) mIoU (?).</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">Crop Size #Param. (M) s.s. mIoU (%) m.s. mIoU (%)</cell></row><row><cell>Swin-T FPN</cell><cell>Swin-T</cell><cell>512?512</cell><cell>33</cell><cell>37.14</cell><cell>38.37</cell></row><row><cell cols="2">SeMask-T FPN SeMask Swin-T</cell><cell>512?512</cell><cell>35</cell><cell>37.53 (+0.39)</cell><cell>38.88 (+0.55)</cell></row><row><cell>Swin-S FPN</cell><cell>Swin-S</cell><cell>512?512</cell><cell>54</cell><cell>40.53</cell><cell>41.91</cell></row><row><cell cols="2">SeMask-S FPN SeMask Swin-S</cell><cell>512?512</cell><cell>56</cell><cell>40.72 (+0.19)</cell><cell>42.27 (+0.36)</cell></row><row><cell>Swin-B FPN</cell><cell>Swin-B  ?</cell><cell>512?512</cell><cell>54</cell><cell>44.18</cell><cell>45.79</cell></row><row><cell cols="2">SeMask-B FPN SeMask Swin-B  ?</cell><cell>512?512</cell><cell>56</cell><cell>44.68 (+0.50)</cell><cell>46.30 (+0.51)</cell></row><row><cell>Swin-L FPN</cell><cell>Swin-L  ?</cell><cell>640?640</cell><cell>204</cell><cell>46.42</cell><cell>48.13</cell></row><row><cell cols="2">SeMask-L FPN SeMask Swin-L  ?</cell><cell>640?640</cell><cell>211</cell><cell>47.47 (+1.05)</cell><cell>48.54 (+0.41)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we present an additional ablation study for the value of the weight, ? in Appendix A. Then, we share our experimental results with SeMask on the COCO-Stuff 10k dataset in Appendix B. We also provide an analysis on the SeMask's effect on the feature maps in Appendix C. Appendix D provides a qualitative comparison of SeMask-L FPN to Swin-L FPN on the COCO-Stuff 10k <ref type="bibr" target="#b38">[34]</ref> and ADE20K <ref type="bibr" target="#b17">[14]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tuning the hyperparameter ?</head><p>We weigh the loss (L 2 ) calculated on the semantic-prior prediction with a hyperparameter ? as formulated in Eq. <ref type="bibr" target="#b9">(6)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Backbone Window Size Embedding Dim (C) Blocks (NT B ) Heads (NT H ) #Params (M) Swin-T</title>
		<imprint>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note>96, 192, 384, 768. 2, 2, 6, 2] [3, 6, 12, 24</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-S</forename><surname>Swin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
	<note>2, 2, 18, 2] [3, 6, 12, 24</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B ?</forename><surname>Swin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>12</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1024" />
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
	<note>2, 2, 18, 2] [4, 8, 16, 32</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-L ?</forename><surname>Swin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>12</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">197</biblScope>
		</imprint>
	</monogr>
	<note>192, 384, 768, 1536. 2, 2, 18, 2] [6, 12, 24, 48</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Beit: Bert pretraining of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>arXiv, 2021. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Maskedattention mask transformer for universal image segmentation. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<title level="m">Conditional positional encodings for vision transformers. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020.6" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu Weiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<idno>arxiv:preprint, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<title level="m">Escaping the big data paradigm with compact transformers. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FaPN: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Alignseg: Feature-aligned segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining contextual information beyond image for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenchao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Isnet: Integrate image-level and semantic-level context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenchao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Polarized self-attention: Towards high-quality pixel-wise regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<title level="m">Swin transformer v2: Scaling up capacity and resolution</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>arXiv, 2021. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attanet: Attention-augmented network for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangfu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. Training data-efficient image transformers &amp; distillation through attention. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lednet: A lightweight encoder-decoder network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequenceto-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Shelfnet for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicha</forename><surname>Dvornek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
