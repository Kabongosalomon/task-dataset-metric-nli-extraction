<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hling@cs.stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking multiple objects in videos relies on modeling the spatial-temporal interactions of the objects. In this paper, we propose a solution named TransMOT, which leverages powerful graph transformers to efficiently model the spatial and temporal interactions among the objects. Trans-MOT effectively models the interactions of a large number of objects by arranging the trajectories of the tracked objects as a set of sparse weighted graphs, and constructing a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial graph transformer decoder layer based on the graphs. TransMOT is not only more computationally efficient than the traditional Transformer, but it also achieves better tracking accuracy. To further improve the tracking speed and accuracy, we propose a cascade association framework to handle low-score detections and long-term occlusions that require large computational resources to model in TransMOT. The proposed method is evaluated on multiple benchmark datasets including MOT15, MOT16, MOT17, and MOT20, and it achieves state-of-the-art performance on all the datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Robust tracking of multiple objects in video is critical for many real-world applications, ranging from visionbased surveillance to autonomous driving vehicles. Most of the recent state-of-the-art Multiple Object Tracking (MOT) methods use the tracking-by-detection strategy, where target candidates proposed by an object detector on each frame are associated and connected to form target trajectories <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref>. There are two core tasks in this framework: accurate object detection and robust target association. In this paper, we focus on building models for robust target association, where successfully modeling the temporal history and appearance of the targets, as well as their spatial-temporal relationships plays an important role.</p><p>Traditional models of spatial-temporal relationships usu-ally rely on manually designed association rules, such as social interaction models or spatial exclusion models <ref type="bibr" target="#b27">[28]</ref>. The recent advances in deep learning inspire us to explore to learn the spatial-temporal relationships using deep learning. In particular, the success of Transformer suggests a new paradigm of modeling temporal dependencies through the powerful self-attention mechanism. Recent studies in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b43">44]</ref> have proved the feasibility of directly modeling spatial-temporal relationships with transformers. However, the tracking performance of transformer-based tracker is not state-of-the-art for several reasons. First, a video contains a large number of objects. Modeling the spatial temporal relationships of these objects with a general Transformer is ineffective, because it does not take the spatial-temporal structure of the objects into consideration. Second, it requires a lot of computation resources and data to learn a transformer to model long term temporal dependencies. Third, DETR-based object detector used in these works is still not the state-of-the-art for MOT. In this paper, we propose a novel spatial-temporal graph Transformer for MOT (TransMOT) to resolve all these issues. In TransMOT, the trajectories of all the tracked targets are arranged as a series of sparse weighted graphs that are constructed using spatial relationships of the targets. Based on these sparse graphs, TransMOT builds a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial transformer decoder layer to model the spatial temporal relationships of the objects. It is more computationally efficient during training and inference because of the sparsity of the weighted graph representation. It is also a more effective model than regular transformer because it exploits the structure of the objects. We also propose a cascade association framework to handle low-score detections and long-term occlusions. By incorporating TransMOT into the cascade association framework, we do not need to learn to associate a large number of low-score detections or model long-term temporal relationships. TransMOT can also be combined with different object detectors or visual feature extraction sub-networks to form a unified end-to-end solution so that we can exploit the  <ref type="figure" target="#fig_0">Figure 1</ref>. Overview of the proposed TransMOT pipeline for online MOT. The trajectories graph series ? t?1 till frame t ? 1 and detection candidates graph ? t at frame t serve as the source and target inputs, respectively, to the spatial-temporal graph transformer. state-of-the-art object detectors for MOT. Extensive experiments on MOT15, MOT16, MOT17, and MOT20 challenge datasets demonstrate that the proposed approach achieves the best overall performance and establishes new state-ofthe-art in comparison with other published works.</p><p>In summary, we make following contributions:</p><p>? We propose a spatial-temporal graph Transformer (TransMOT) for effective modeling of the spatialtemporal relationship of the objects for MOT.</p><p>? We design a cascade association framework that can improve TransMOT and other transformer-based trackers by handling low-score detections and longterm occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Most of the recent Multiple Object Tracking (MOT) trackers are based on the tracking-by-detection framework. Tracking-by-detection framework generates tracklets by associating object detections in all the frames using matching algorithms such as Hungarian algorithm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>, network flow <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, and multiple hypotheses tracking <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. Many works solve the association problem by building graphs of object detections across all the frames, such as multi-cuts <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b45">46]</ref> and lifting edges <ref type="bibr" target="#b46">[47]</ref>. However, these methods need to perform computationally expensive global optimization on large graphs, which limits their application to online tracking.</p><p>Recently, deep learning-based association algorithm is gaining popularity in MOT <ref type="bibr" target="#b61">[62]</ref>. In <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b31">[32]</ref>, recurrent neural networks (RNN) is explored to solve the association problem using only the motion information. In <ref type="bibr" target="#b10">[11]</ref>, a power iteration layer is introduced in the rank-1 tensor approximation framework <ref type="bibr" target="#b42">[43]</ref> to solve the multi-dimension assignment in MOT. <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b60">[61]</ref> combine object detection and target association that directly predict target locations in the current frame. In <ref type="bibr" target="#b52">[53]</ref>, a differentiable MOT loss is proposed to learn deep Hungarian Net for association. In <ref type="bibr" target="#b7">[8]</ref>, graph convolutional neural network is adopted as a neural solver for MOT, where a dense graph connecting every pair of nodes in different frames is constructed to infer the association. The proposed TransMOT also constructs a spatial graph for the objects within the same frame, but it exploits the Transformer networking architecture to jointly learn the spatial and temporal relationship of the tracklets and candidates for efficient association. Transformer has achieved great success in various computer vision tasks, such as detection <ref type="bibr" target="#b8">[9]</ref> and segmentation <ref type="bibr" target="#b25">[26]</ref>. In <ref type="bibr" target="#b54">[55]</ref>, Transformer is adopted for trajectory prediction. The studies in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b43">44]</ref> are the pioneer investigations in applying Transformer in MOT. Both methods use DETR for detection and feature extraction, and model the spatialtemporal relationship of the tracklets and detections using Transformer. The proposed TransMOT framework utilizes spatial graph transformer to model spatial relationship of the tracklets and detections, and it factorizes the spatial and temporal transformer encoder for model efficient modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>We aim at joint detection and tracking multiple objects in videos in an online fashion. t =t?T on the previous T image frames. Given a new image frame I t , the online tracking algorithm eliminates the tracklets whose tracked object exits the scene, determines whether any tracked objects are occluded, computes new locations for the existing tracklet? X t = x t i Nt i=1 , and generates new tracklets for new objects that enter the scene.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our framework contains two major parts: the detection and feature extraction sub-networks, and spatial temporal graph transformer association subnetwork. At each frame, the detection and feature extraction sub-networks generate M t candidate object detection</p><formula xml:id="formula_0">proposals O t = o t j</formula><p>Mt j=1 , as well as visual features for each proposal. The spatial-temporal graph transformer finds the best candidate proposal for each tracklet and models the special events, such as entering, exiting, or occlusion.</p><p>For each tracklet L t?1 i , the best matching is obtained through selecting the o j t maximizing the affinity</p><formula xml:id="formula_1">?(L t?1 i , o t j ), where ?(?)</formula><p>is a scoring function that computes the affinity of the tracklet state and the candidate. Taking all tracklets into consideration, the problem can be formulated as a constrained optimization problem as</p><formula xml:id="formula_2">max A t =(a t ij ) Nt?1 i=1 Mt j=1 a t ij ?(L t?1 i , o t j ),<label>(1)</label></formula><formula xml:id="formula_3">s.t. ? ? ? i a t ij = 1, ?i = 1, . . . , N t?1 j a t ij = 1, ?j = 1, . . . , M t a t ij ? {0, 1}, ?i = 1, . . . , N t?1 ; j = 1, . . . , M t (2) where A t = (a t ij ) indicates the association between track- lets L t?1 = {L t?1 i } Nt i=1</formula><p>and detected candidates O t . Eq. 2 is used to enforce the assignment constraints.</p><p>In order to more effectively model the spatial-temporal relationship between all the tracklets and candidates, the proposed framework rewrites Eq. 1 and Eq. 2 into a single</p><formula xml:id="formula_4">function A t = ?(L t?1 , O t ),</formula><p>where L t?1 and O t consist of all the tracklets and candidates, respectively.</p><p>To model the spatial-temporal object correlation, we build a weighted spatial graph ? t for the proposals at the current frame, and a set of weighted spatial graphs ? t?1 = {? t?T , ? 2 , . . . , ? t?1 } of the tracked objects at the previous T frames. The spatial-temporal graph neural network utilizes these graphs to build an efficient spatial-temporal graph transformer that models the relationship between the tracked objects and newly generated proposals. It generates an assignment matrix? t to track the objects and model the special events, such as entering, exiting, or occlusion, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The assignment matrix is used to update the tracked target while the special events are handled by the post-processing module, which will be explained in Sec. 4.4.</p><p>The details of the spatial-temporal graph Transformer will be explained in Sec. 4.1 and Sec. 4.2. The two types of training losses to train TransMOT will be elaborated in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TransMOT</head><p>Spatial-temporal graph Transformer for MOT (Trans-MOT) uses the graphs ? t?1 and ? t to learn a mapping ?(?) that models the spatial-temporal correlations, and generates an assignment/mapping matrix? t . It contains three parts: a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial graph transformer decoder layer. We propose graph multi-head attention to Graph Multi-Head Attention <ref type="figure">Figure 2</ref>. The spatial graph transformer encoder layer.</p><p>model spatial relationship of the tracklets and candidates using the self-attention mechanism. It is crucial for both the spatial graph transformer encoder layer and the spatial graph transformer decoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spatial-Temporal Graph Transformer Encoder</head><p>The spatial-temporal graph encoder consists of a spatialtemporal graph transformer encoder layer to model the spatial correlation among tracklets, and a temporal transformer encoder layer to further fuse and encode the spatial and temporal information of the tracklets. We find that by factoring the transformer into spatial and temporal transformers, it makes the model both more accurate and computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Spatial Graph Transformer Encoder Layer</head><p>The input of spatial-temporal graph encoder layer is the states of the tracklets for the past T frames. The tracklet state features are arranged using a sequence of tracklet graphs</p><formula xml:id="formula_5">? t?1 = {? t?T , ? t?T +1 , . . . , ? t?1 } , where ? t?1 = G({x t?1 i }, E t?1 X , w t?1 X ) is the spatial graph 1 of the tracklets at frame t ? 1. At frame t ? 1, the graph node x t?1 i</formula><p>represents the status of i-th tracklet at this frame, two nodes are connected by an edge in E t?1 X if their corresponding bounding boxes have IoU larger than 0, and the edge weight in w t?1 X is set to the IoU. The weight matrix w t?1 X ? R Nt?1?Nt?1 is a sparse matrix, whose (i, j) entry is the weight of the edge connecting node i and node j, or 0 if they are not connected.</p><p>The node features for the tracklets are first embedded through a source embedding layer (a linear layer) independently for each node. All the node features are arranged into a feature tensor F s ? R Nt?1?T ?D , where D is the dimension of the source embedding layer. It is passed into the spatial graph transformer encoder layer together with the graph series as shown in <ref type="figure">Fig. 2</ref>. Inside the layer, a multi-head graph attention module is utilized to generate self-attention for the input graph series. This module takes feature tensor F s and the graph weights w t?1 X to generate self-attention weights for the i-th head:</p><formula xml:id="formula_6">F AW i = softmax ?(F s , W Q i , W K i ) ? w t?1 X ,<label>(3)</label></formula><p>where ?(?) is the regular scaled dot-product to obtain attention weights as in <ref type="bibr" target="#b47">[48]</ref>, and ? is the element-wise product. It can be understood as computing the spatial graph selfattention for each timestamp independently. The multi-head graph attention utilizes the graph weights w t?1 X to generate non-zero attention weights only for the tracklets that have spatial interactions, because the tracklets that are far way from each other usually have very little interaction in practice. By focusing its attention on a much smaller subset, the spatial graph transformer encoder layer models the interactions more effectively, and runs faster during training and inference.</p><p>We also apply graph convolution instead of the linear layer to aggregate information from neighboring nodes. After the graph convolution layer, the node features are collected to form a value tensor F V i . Combined with the attention weights in Eq. 3, the graph multi-head attention weighted feature tensor can be written as</p><formula xml:id="formula_7">F en att = Concate({F AW i ? F V i }) ? W O ,</formula><p>where {?} iterates and aggregates the outputs from all the attention heads, ? is the tensor mode product <ref type="bibr" target="#b1">2</ref> The attention weighted feature tensor is projected through a linear feed forward and a normalization layer to get the final output of the spatial graph transformer encoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Temporal Transformer Encoder Layer</head><p>The features of the tracklets are further encoded by a temporal transformer encoder layer. The temporal transformer encoder layer transposes the first two dimension of the output tensor from the spatial graph transformer encoder, resulting in a tensor F en tmp ? R T ?Nt?1?D . The temporal transformer encoder layer employs a standard Transformer encoder layer over the temporal dimension for each tracklets independently. It calculates the self-attention weights along the temporal dimension, and computes the temporal attention-weighted feature tensor for the tracklets.</p><p>The output of the temporal transformer encoder layer is the final output of the spatial-temporal graph transformer encoder F en out . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatial Graph Transformer Decoder</head><p>The spatial graph transformer decoder produces extended assignment matrix? t from the candidate graph</p><formula xml:id="formula_8">? t = G({o t j }, E t O , w t O )</formula><p>and the output of the spatialtemporal transformer encoder F en out . The candidate graph is constructed similarly to the tracklet graphs in Sec. 4.1. Each node o t j represents a candidate in frame t. Two nodes are connected only if their bounding box's IoU is larger than zero, and the weight of the edge is set to the IoU. Besides the nodes representing the real candidates, a virtual sink node is added to the graph. The virtual sink node is responsible for exiting or occlusion events of any tracklet in the current frame. In particular, a node with a set of learnable embedding f snk ? R D is added to the ? t . The virtual sink node is connected to all the other nodes with weight 0.5.</p><p>Similar as the encoder in Sec. 4.1, the candidate node features of input graph are embedded and collected. The f snk is appended to the embedded feature set such that F de tgt ? R (Mt+1)?1?D . The spatial graph decoder first uses graph multi-head attention to encode the node features that is similar to the one Sec. 4.1, shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We denote the attention weighted candidate node features as</p><formula xml:id="formula_9">F de att ? R (Mt+1)?1?D .</formula><p>For the tracklet embedding F en out generated by the spatial-temporal graph transformer encoder, we add a virtual source to handle the candidates that initiate a new tracklet in the current frame t to form an extended tracklet embedding F en out ? R T ?(Nt?1+1)?D . The embedding of the virtual source is a learnable parameter. Note that we only add one virtual source node compared to multi-ple virtual source nodes in Transform-based MOT trackers, because we find adding one virtual source node yields comparable performance as adding multiple virtual source nodes while achieving better computational efficiency. F de att is duplicated N t?1 + 1 times such that F de att ? F de att ? R (Mt+1)?(Nt?1+1)?D . Multi-head cross attention is calculated for F de att and F en out to generate unnormalized attention weights. The output is passed through a feed forward layer and a normalization layer to generate the output tensor R (Mt+1)?(Nt?1+1)?D that corresponds to the matching between the tracklets and the candidates.</p><p>The output of the spatial graph decoder can be passed through a linear layer and a Softmax layer to generate the assignment matrix? t ? R (Mt+1)?(Nt?1+1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>The TransMOT is trained end-to-end with the guidance of the groundtruth extended assignment matrix. The constraints in Eq. 2 need to be relaxed to allow efficient optimization. We relax the constraints so that a detection candidate is always associated with a tracklet or a virtual source, while a tracklet can be associated with multiple candidates. In this way, Eq. 2 can be relaxed as:</p><formula xml:id="formula_10">s.t. Nt?1+1 j? t ij = 1, i ? [1, M t ],? t ij ? {0, 1}.</formula><p>As a result, a row of the assignment matrix can be treated as a probability distribution over a total of N t?1 +1 categories, and we use the cross-entropy loss to optimize the network.</p><p>In each training iteration, a continuous sequence of T +1 frames are randomly sampled from the training set. The bounding boxes and their corresponding IDs are collected from each frame. The groundtruth bounding boxes are then replaced by the bounding boxes generated from the object detector by matching their IoUs. In this way, the TransMOT will be more robust to detection noise. For all bounding boxes in a frame, their IDs are remapped to {0, 1, ? ? ? , N t?1 } indicating whether they are matched to a tracklet or a virtual source.</p><p>For the rows that correspond to actual tracklets, a crossentropy loss is utilized, as mentioned above. The last row of? t represents the virtual sink, and it may be matched to multiple tracklets. Thus, a multi-label soft margin loss is employed to optimize this part separately.</p><p>In summary, the overall training loss can be written as (1 ? y snk n )log e ?a n 1 + e ?a n ,  where y m and y snk n are IDs of the detection candidates and the virtual sink respectively,? m is the row element of? t , a Mt+1 = {a n }, and ? is a weighting coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cascade Association Framework</head><p>Although the spatial-temporal graph transformer network can effectively model the spatial-temporal relationship between tracklets and object candidates, we can achieve better inference speed and tracking accuracy by incorporating it in a three-stage cascade association framework. The illustration of the cascade association framework is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>The first stage matches and filters the low confidence candidate boxes using motion information. In particular, a Kalman Filter predicts the bounding boxes of the robustly tracked tracklets in the current frames. These tracklets must be successfully and continuously associated on the past K r frames. The IoU between the predicted bounding boxes and the candidate boxes are utilized as the association score. We match the predicted boxes and the candidate boxes with Hungarian algorithm. Only the matched pairs with IoU larger than ? M are associated. The rest of the candidate boxes whose confidence scores are lower than a threshold is filtered. This stage is mainly a speed optimization, because it removes the candidate boxes and tracklets that can be easily matched or filtered, and leaves the more challenging tracklets and candidate boxes for TransMOT for further association.</p><p>In the second stage, TransMOT calculates the extended assignment matrix for the rest of the tracklets and candidates. The upper left part of the extended assignment matrix A t denotes as? ? R Mt?Nt?1 determines the matching of actual tracklets and candidate boxes. Since the elements of A ? [0, 1] is a soft assignment, we apply bipartite matching algorithm to generate the actual matching. Similarly, only the pairs with assignment scores larger than a threshold will be matched at this stage. After the matching, the unresolved candidate boxes will be matched again to recover occlusion or initiate new targets in the third stage.</p><p>The third stage is the Long-Term Occlusion and Duplicated detection handling module in <ref type="figure" target="#fig_4">Fig. 4</ref> the tracklets that are occluded in the previous T frames are not matched. For these tracklets, we store their visual features and the bounding box coordinates at the latest frame when they were visible, and use them to calculate the association cost of a tracklet and a candidate detection. The association cost is defined as the addition of the Euclidean distance of the visual features and the normalized top distance of the occluded tracklets and candidate boxes.</p><formula xml:id="formula_11">D top = u i + w i 2 ? u j ? w j 2 , v i ? v j h i , where [u, v]</formula><p>indicates the left upper corner of the bounding box and [w, h] indicates its size <ref type="bibr" target="#b2">3</ref> . Duplicate detection handling removes the unmatched detection candidates that might be duplicate of the detections that are already matched to a tracklet. In this step, the unassociated candidates are matched to all the associated candidates. The association cost for this step is the bounding box intersection between the associated box and unassociated box over the area of the un-associated candidates. This cost is chosen so that a candidate box that is a sub-box of a matched candidate is removed, because it is very likely that this box is a duplicate detection. All the matched candidate detection at this step will be removed directly.</p><p>Finally, each of the remaining candidates is initialized as a new tracklet, and the unresolved tracklets that have not been updated for more than K p frames are removed. The tracklets that are not updated for less than K p frames are set to "occluded" state. <ref type="bibr" target="#b2">3</ref> The notation w should not be confused with the weight w in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct extensive experiments on four standard MOT challenge datasets for pedestrian tracking: MOT15 <ref type="bibr" target="#b23">[24]</ref>, MOT16, MOT17, and MOT20 <ref type="bibr" target="#b30">[31]</ref>. The proposed TransMOT based tracking framework is evaluated on both public and private detection tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setting and Implementation Details</head><p>The proposed approach is implemented in PyTorch, and the training and inference are performed on a machine with a 10 cores CPU@3.60GHz and an Nvidia Tesla V100 GPU. We set the number of frames for tracklets T = 5, the feature embedding dimension D = 1024, and the number of heads for all the multi-headed attention in spatial and temporal transformers to 8. For graph multi-head attention module, a single layer of ChebConv from <ref type="bibr" target="#b12">[13]</ref> with neighboring distance of 2 is adopted. The node features for an object at a frame are the concatenation of its visual features and normalized bounding box coordinates. During training, we use vanilla SGD with an initial learning rate of 0.0015. For all the experiments in Sec. 5.2, we use the training dataset from <ref type="bibr" target="#b26">[27]</ref> to train our TransMOT model. During inference, ? M is set to 0.75 for selecting confident associations. K r and K p are set to 15 and 50 respectively.</p><p>We trained a YOLOv5 <ref type="bibr" target="#b0">[1]</ref> detector model with 407 layers for 300 epochs on the combination of CrowdHuman dataset <ref type="bibr" target="#b41">[42]</ref> and the training sets of MOT17/MOT20. The SiamFC network <ref type="bibr" target="#b39">[40]</ref> pretrained on the ILSVRC15 dataset is adopted as our visual feature extraction sub-network. The maximum input image dimension of the tracking pipeline is set to 1920. The detector runs at 15.4 fps on our machine, while the TransMOT and visual feature extraction subnetwork run at 24.5 fps. The whole tracking pipeline runs at 9.6 fps. We also experimented with using TransTrack <ref type="bibr" target="#b43">[44]</ref> as our detection and feature extraction sub-network, as well as other visual features. These comparisons will be compared in the MOT16/17 and ablation parts of Sec. 5.2.</p><p>To evaluate the performance of the proposed method, the standard ID score metrics <ref type="bibr" target="#b38">[39]</ref> and CLEAR MOT metrics <ref type="bibr" target="#b4">[5]</ref> are reported. ID score metrics calculate the trajectory level ID precision (IDP), ID recall (IDR), and the IDF1 scores. CLEAR MOT metrics include multiple object tracking precision (MOTP) and multiple object tracking accuracy (MOTA) that combine false positives (FP), false negatives (FN) and the identity switches (IDS). The percentage of mostly tracked targets (MT) and the percentage of mostly lost targets (ML) are also reported.   <ref type="table">Table 4</ref>. Tracking Performance on the MOT20 benchmark test set. Best in bold. Method marked with * in public detection track does not use public detection filtering mechanism. It might achieve better tracking accuracy if the mechanism is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Results</head><p>were recorded with different camera motion, camera angles and imaging conditions. The dataset is equally split for training and testing. We report the quantitative results of the proposed method on the private detection track in Tab. 1, and the visualizations of the tracking results on selected videos are shown in <ref type="figure">Fig. 5</ref>. TransMOT achieves state-ofthe-art performance in metrics IDF1, MT, FN, and IDS. The relatively lower MOTA score on this dataset is caused by the high FP rate, because not all the objects are exhaustively annotated for some testing sequences. MOT16/17. MOT16 and MOT17 <ref type="bibr" target="#b30">[31]</ref> contain the same 14 videos for pedestrians tracking. MOT17 has more accurate ground truth annotations compared to MOT16 dataset. MOT17 also evaluates the effect of object detection quality on trackers, by providing three pretrained object detectors using DPM <ref type="bibr" target="#b16">[17]</ref>, Faster-RCNN <ref type="bibr" target="#b37">[38]</ref> and SDP <ref type="bibr" target="#b53">[54]</ref>. We report the performance and comparisons with the state-ofthe-art methods on the private detection track of MOT16 in Tab. 2. Our approach outperforms all other published trackers using the private detector in both IDF1 and MOTA metrics.</p><p>In MOT17, for a more complete comparison, we config-ure TransMOT as two additional settings: TransMOT-P and TransMOT-D. TransMOT-P uses the public detection results, and it follows the filtering mechanism adopted by <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b60">[61]</ref>. A new trajectory is initialized only if its bounding box at the current frame overlaps a public detection with IoU larger than 0.5. We compare TransMOT-P with other trackers adopting the same filtering mechanism on the public detection track of MOT17 in Tab. 3. Compared with regular Transformer-based tracker <ref type="bibr" target="#b29">[30]</ref>, TransMOT outperforms it in IDF1, MOTA, and IDS by a large margin. Trans-MOT also achieves the best IDF1 and MOTA scores among all published trackers, which demonstrates the robustness of TransMOT against detection quality variations.</p><p>TransMOT-D adopts the DETR framework as detection and visual feature extraction sub-networks. TransMOT-D takes the detection outputs of pretrained TransTrack <ref type="bibr" target="#b43">[44]</ref> and their Transformer embedding as visual features. For a fair comparison, the pretrained model of TransTrack is not fine-tuned in TransMOT-D. We compare TransMOT-D and TransMOT with state-of-the-art trackers on MOT17 private detection track in Tab. 3. The performance of TransMOT-D is better than TransTrack [44] by 10.0% and 3.0% in IDF1 and MOTA metrics respectively. This shows that our Trans-MOT framework can better model the spatial-temporal relationship of the tracklets and detections than standard Transformer. TransMOT-D achieves the lowest IDS among all the methods, because it generates fewer tracklets than other methods. Our TransMOT framework achieves the best IDF1 and MOTA metrics. It is also ranked as the top tracker in terms of IDF1 and FN on the private detector track of MOT17 challenge leaderboard when the paper is submitted. MOT20. MOT20 consists of eight sequences for pedestrian tracking. MOT20 video sequences are more challenging because they have much higher object density, e.g. 170.9 vs 31.8 in the test set. We report the experimental results of the proposed TransMOT and the comparison with the other methods in Tab. 4. Our method establishes state-ofthe-art in most metrics among all submissions. In particular, TransMOT achieves 6.6% higher IDF1 than CStrack, which ranked second in the leaderboard. In public detection setting, TransMOT-P also demonstrates its robustness to detection noise. Compared with the private detector setting, the MOTA decreases 4.4%, but IDF1 score only drops 0.9%. The experiments on both public and private detection settings demonstrate that the capability of TransMOT of modeling a large number of tracklets and detections in crowd scenes as shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation</head><p>We study the significance of different modules in the proposed method through ablation study as shown in Tab. 5. The ablations are conducted on the MOT17 training set. To avoid overfitting, the object detector used in the ablation MOT15: Venice-1 MOT15: AVG-TownCentre MOT16-03 MOT16-07 MOT17-08 MOT17-14 MOT20-04 MOT20-06 <ref type="figure">Figure 5</ref>. Results visualization of selected sequences in MOT15, MOT16, MOT17, and MOT20. study is trained on only the CrowdHuman dataset.</p><p>We first evaluate the settings that remove the Match with Motion module or the Long-Term Occlusion Handling module from the cascade association framework, noted as TransMOT?MwM and TransMOT?LTOH in Tab. 5. The MOTA score decreases significantly without MwM because the TransMOT cannot handle low score detection candidates well. It also leads to a slower speed, because Trans-MOT needs to handle more tracklets and detection candidates that were handled by MwM. Dropping the LTOH module results in more fragmented tracklets because the tracklets that are occluded for more than T frames are not linked in this case. It dramatically increases the number of tracklets, reduces the IDF1 score, and increases the inference speed.</p><p>The major hyper-parameters of the tracker are also studied. We test TransMOT running at a reduced input image resolution at maximum width of 1280 (TransMOT@1280 in Tab. 5) (The default setting uses a maximum width of 1920). We find that lower resolution input causes the tracker to miss small targets at the far end of the scene. The choice of the temporal history length T is investigated by choosing T = 1, T = 10, and T = 20 (T is set to 5 in default setting). Compared with T = 1 where no temporal history is included, T &gt; 1 can improve the association accuracy. However, since increasing T also adds more tracklets for association, it increases the complexity of the association task, and makes the learning harder under a limited number of training data for MOT. In addition, a lot of the tracklets that can be matched by increasing T are already handled by the long-term occlusion handling module. Thus increasing L beyond 5 has no performance gain and makes inference slower. T = 5 is used in all the other experiments.</p><p>Finally, in addition to SiamFC and DETR features, we evaluate other shallow and deep visual features, including color histogram and ReID feature DGNet <ref type="bibr" target="#b59">[60]</ref>. Benefiting from the fully trainable Transformer, even using sim-  ple color histogram features, TransMOT can achieve similar performance with the one using deep ReID features and runs at a much faster inference speed. On the other hand, SiamcFC features perform better than both color histogram and ReID features, because it is trained on a large scale video dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a novel Spatial-Temporal Graph Transformer for multi-object tracking (TransMOT) with Transformers. By formulating the tracklets and candidate detections as a series of weighted graphs, the spatial and temporal relationships of the tracklets and candidates are explicitly modeled and leveraged. The proposed TransMOT not only achieves higher tracking accuracy, but also is more computationally efficient than the transitional Transformer-based methods. Additionally, we developed the cascade association framework to further optimize the speed and accuracy of TransMOT by filtering low score candidates, recovering the tracklets that are occluded for a long time, and remove duplicate detections. Experiments on MOT15, MOT16, MOT17, and MOT20 challenge datasets show that the proposed approach achieves state-of-the-art performance on all the benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>illustrates our framework built upon tracking-by-detection framework. The framework maintains a set of N t?1 tracklets, each of which represents a tracked object. Each tracklet L t?1 i maintains a set of states, such as its past locations x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the spatial graph transformer decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the cascade association framework based tracking system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2104.00194v2 [cs.CV] 3 Apr 2021</figDesc><table><row><cell cols="4">Tracked Targets Graph Series</cell><cell>Candidates Graph</cell><cell>TransMOT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Spatial-Temporal Graph Transformer Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Embedding Embedding</cell><cell>Spatial Graph Transformer Encoder Layer Spatial Graph Transformer Encoder Temporal Layer Transformer Decoder Layer Spatial Graph Transformer Decoder Virtual Sink Virtual Source</cell><cell>Extended</cell><cell>Occlusion/Entering Handling</cell></row><row><cell>t-4</cell><cell>t-3</cell><cell>t-2</cell><cell>t-1</cell><cell>t</cell><cell></cell><cell>Assignment Matrix</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>. Since Trans-MOT only models the tracklets of the previous T frames,<ref type="bibr" target="#b43">44</ref>.5 34.7% 22.1% 8,088 25,335 684 TubeTK [35] 53.1 58.4 39.3% 18.0% 5,756 18,961 854 CDADDAL [3] 54.1 51.3 36.3% 22.2% 7,110 22,271 544 TRID [29] 61.0 55.7 40.6% 25.8% 6,273 20,611 351 RAR15 [16] 61.3 56.5 45.1% 14.6% 9,386 16,921 428 GSDT [50] 64.6 60.7 47.0% 10.5% 7,334 16,358 477 Fair [59] 64.7 60.6 47.6% 11.0% 7,854 15,785 591 TransMOT 66.0 57.0 64.5% 17.8% 12,454 13,725 244 Tracking Performance on the MOT15 benchmark test set private detection track. Best in bold. TransMOT 76.8 76.7 56.5% 19.2% 14,999 26,967 517 Tracking Performance on the MOT16 benchmark test set private detection track. Best in bold.</figDesc><table><row><cell>Method</cell><cell>IDF1 MOTA MT</cell><cell>ML? FP?</cell><cell>FN? IDS?</cell></row><row><cell cols="3">DMT [23] 49.2 Method IDF1 MOTA MT ML? FP?</cell><cell>FN? IDS?</cell></row><row><cell>IoU [7]</cell><cell cols="3">46.9 57.1 23.6% 32.9% 5,702 70,278 2,167</cell></row><row><cell cols="4">CTracker [36] 57.2 67.6 32.9% 23.1% 8,934 48,305 1,897</cell></row><row><cell cols="4">LMCNN [2] 61.2 67.4 38.2% 19.2% 10,109 48,435 931</cell></row><row><cell cols="4">DeepSort [51] 62.2 61.4 32.8% 18.2% 12,852 56,668 781</cell></row><row><cell cols="4">FUFET [41] 68.6 76.5 52.8% 12.3% 12,878 28,982 1,026</cell></row><row><cell>LMP [47]</cell><cell cols="3">70.1 71.0 46.9% 21.9% 7,880 44,564 434</cell></row><row><cell cols="4">CSTrack [25] 73.3 75.6 42.8% 16.5% 9,646 33,777 1,121</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MOT15 [24] contains 22 different indoor and outdoor scenes for pedestrian tracking. The 22 sequences are collected from several public and private datasets, and they ] 53.8 53.7 19.4% 36.6% 11,731 247,447 1,947 Tracktor [4] 55.1 56.3 21.1% 35.3% 8,866 235,449 1,987 CTTrack [61] 59.6 61.5 26.4% 31.9% 14,076 200,672 2,583 TrackFormer [30] 59.8 61.8 35.4% 21.1% 35,226 177,270 2,982 MPNTrack [8] 61.7 58.8 28.8% 33.5% 17,413 213,594 1,185</figDesc><table><row><cell></cell><cell>Method</cell><cell>IDF1 MOTA MT</cell><cell>ML? FP?</cell><cell>FN? IDS?</cell></row><row><cell>Public Detection</cell><cell>TrctrD [53LifT [19] MAT [18]</cell><cell cols="3">65.6 60.5 27.0% 33.6% 14,966 206,619 1,189 69.2 67.1 38.9% 26.4% 22,756 161,547 1,279</cell></row><row><cell></cell><cell cols="4">TransMOT-P 72.2 68.7 33.5% 31.0% 8,078 167,602 1,014</cell></row><row><cell>Private Detection</cell><cell cols="4">DAN [45] TransTrack [44] 56.9 65.8 32.2% 21.8% 24,000 163,683 5,355 49.5 52.4 21.4% 30.7% 25,423 234,592 8,431 TubeTK [35] 58.6 63.0 31.2% 19.9% 27,060 177,483 5,727 CTTrack [61] 64.7 67.8 34.6% 24.6% 18,498 160,332 6,102 Fair [59] 72.3 73.7 43.2% 17.3% 27,507 117,477 8,073 TransMOT-D 66.9 68.8 35.8% 31.7% 26,670 147,690 1,797 TransMOT 75.1 76.7 51.0% 16.4% 36,231 93,150 2,346</cell></row></table><note>MOT15.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Tracking Performance on the MOT17 benchmark test set. Best in bold.</figDesc><table><row><cell></cell><cell>Method IDF1 MOTA MT</cell><cell>ML?</cell><cell>FP?</cell><cell>FN? IDS?</cell></row><row><cell>Public Det.</cell><cell cols="4">SORT* [6] 45.1 42.7 16.7% 26.2% 27,521 264,694 4,470 Tracktor [4] 52.7 52.6 29.4% 26.7% 6,930 236,680 1,648 MPNTrack [8] 59.1 57.6 38.2% 22.5% 16,953 201,384 1,210 LPCMOT [12] 62.5 56.3 34.1% 25.2% 11,726 213,056 1,562 TransMOT-P 74.3 73.1 54.3% 14.6% 12,366 125,665 1,042</cell></row><row><cell>Private Det.</cell><cell cols="4">MLT [58] 54.6 48.9 30.9% 22.1% 45,660 216,803 2,187 GSDT [50] 67.5 67.1 53.1% 13.2% 31,913 135,409 3,131 Fair [59] 67.3 61.8 68.8% 7.6% 103,440 88,901 5,243 CSTrack [25] 68.6 66.6 50.4% 15.5% 25,404 144,358 3,196 TransMOT 75.2 77.5 70.7% 9.1% 34,201 80,788 1,615</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablations on the MOT17 benchmark training set. FPS indicates the inference speed of the full tracker that includes the detection and feature sub-net.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use G(?) to denote a graph.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It performs matrix product of each slice of right and left tensors along the dimension sharing the same length.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yolov5</forename></persName>
		</author>
		<ptr target="https://github.com/ultralytics/yolov5.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A dual cnnrnn for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="69" to="83" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Confidence-based data association and discriminative deep appearance learning for robust online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Seung-Hwan Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIVP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Highspeed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Volker Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancing detection model for multiple hypothesis tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRw</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6172" to="6181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a proposal classifier for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangping</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Target identity-aware network flow for online multiple target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving multi-frame data association with sparse representations for robust near-online multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Fagot-Bouquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoann</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Lerasle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent autoregressive networks for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mat: Motion-aware multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghaisheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Swoboda</surname></persName>
		</author>
		<idno>PMLR, 2020. 7</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4364" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust object tracking by hierarchical association of detection responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cdt: Cooperative detection and tracking for tracing multiple objects in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Ul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="851" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Motchallenge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking the competition between detection and reid in multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiao</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12138</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9131" to="9140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Human in events: A large-scale benchmark for humancentric video analysis in complex events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04490</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiple object tracking: A literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">103448</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pathtrack: Fast trajectory annotation with path supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Trackformer: Multi-object tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online multi-target tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multitarget tracking by discrete-continuous energy minimization. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep tracking: Seeing beyond seeing using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01909</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaobing</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbo</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09015</idno>
		<title level="m">Xian-Sheng Hua, Xiaoliang Cheng, and Kewei Liang. Fgagt: Flow-guided adaptive graph tracking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-target tracking by rank-1 tensor approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep affinity network for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huansheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="104" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-person tracking by multicut and deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVw</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="100" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tracking interacting objects using intertwined flows. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>T?retken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Joint detection and multi-object tracking with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13164</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How to train your deep multi-object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Alameda-Pineda</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph transformer networks for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Gmcp-tracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>ECCV. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multiplex labeling graph for near-online tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7892" to="7902" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
