<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 What and When to Look?: Temporal Span Proposal Network for Video Relation Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Sangmin</forename><surname>Woo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Junhyug</forename><surname>Noh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Kangil</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 What and When to Look?: Temporal Span Proposal Network for Video Relation Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Visual Relationship Detection (VidVRD)</term>
					<term>Video Understanding</term>
					<term>Temporal Span Proposal Network (TSPN)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying relations between objects is central to understanding the scene. While several works have been proposed for relation modeling in the image domain, there have been many constraints in the video domain due to challenging dynamics of spatio-temporal interactions (e.g., between which objects are there an interaction? when do relations start and end?). To date, two representative methods have been proposed to tackle Video Visual Relation Detection (VidVRD): segmentbased and window-based. We first point out limitations of these methods and propose a novel approach named Temporal Span Proposal Network (TSPN). TSPN tells what to look: it sparsifies relation search space by scoring relationness of object pair, i.e., measuring how probable a relation exist. TSPN tells when to look: it simultaneously predicts start-end timestamps (i.e., temporal spans) and categories of the all possible relations by utilizing full video context. These two designs enable a win-win scenario: it accelerates training by 2? or more than existing methods and achieves competitive performance on two VidVRD benchmarks (ImageNet-VidVDR and VidOR). Moreover, comprehensive ablative experiments demonstrate the effectiveness of our approach. Codes are available at https://github.com/sangminwoo/ Temporal-Span-Proposal-Network-VidVRD. . His research interests include artificial intelligence, evolutionary computation, machine learning, and natural language processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C APTURING the semantics of the visual scene has long been a concern in computer vision. Despite the remarkable progress of computer vision, understanding visual scenes remains a challenging task. On the way to leap forward, visual relations serve as the stepping stone for narrowing the gap between perceptive and cognitive tasks. A number of works leverages the visual relations including the tasks of image retrieval <ref type="bibr" target="#b0">[1]</ref>, dynamics prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, image captioning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, visual question answering <ref type="bibr" target="#b5">[6]</ref>, image generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and video understanding <ref type="bibr" target="#b8">[9]</ref>. The Visual Relation Detection (VRD) task Sangmin Woo is with the School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon 34141, Korea. This work was done when he was an M.S. student at GIST (email: smwoo95@kaist.ac.kr).</p><p>Junhyug Noh is with the Computational Engineering Division, Lawrence Livermore National Laboratory, CA 94550, United States (email: noh1@llnl.gov).</p><p>Kangil Kim is with the School of Electrical Engineering and Computer Science and the AI Graduate School, Gwangju Institute of Science and Technology, Gwangju 61005, Korea (email: kangil.kim.01@gmail.com). <ref type="figure">Fig. 1</ref>: Who is handing over the bear to whom? When we try to answer the question with only a single image (leftmost), the answer can be both: left or right man. While guessing relations in short-term video segment (middle) is still questionable, the answer becomes clear in full video (rightmost) thanks to the spatio-temporal contexts. The time sequence is from top to bottom. Answer: right man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>requires modeling both visible information about what and</head><p>where entities are and underlying information of what interactions are happening between objects in the scene. Relation reasoning is essential in the high-level understanding of the scene. Since a pioneer work of VRD <ref type="bibr" target="#b9">[10]</ref> was proposed, several interesting relational reasoning approaches have been studied on the image domain <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Recently, a Video Visual Relation Detection (VidVRD) task has been proposed <ref type="bibr" target="#b14">[15]</ref>, but due to the difficulty of spatio-temporal relationship modeling, it has not yet been received much attention, and just a few of works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> have been proposed.</p><p>What makes detecting relations in video difficult? At first glance, we consider the common difficulties of the task. In the view of the object, new objects may appear or disappear due to viewpoint variation or occlusion over time and may also contain motion blur. Therefore, we need to adopt proper  <ref type="table" target="#tab_1">Table II</ref>, III, IV). (a) Segment-based approach first chunks a video into multiple segments, predict the short-term relations within each segment, and then greedily associate the relations of adjacent segments into the long-term relations. (b) Window-based approach generates a set of sub-tracklet pairs via a size-varying sliding window, and then predict all relations with different temporal span. (c) TSPN (ours) jointly predicts relation categories and its temporal span with a single video-level object trajectory pair. Oi stands for i-th object trajectory of all object trajectories in the video, and Rj denotes j-th relation category. We assume the relations are predicted only for temporal span in which two object trajectories appear simultaneously in the video. We note that illustration of each method may not contain all the detailed procedures. object detection <ref type="bibr" target="#b24">[25]</ref> and tracking <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> methods. In the view of the relation, it requires modeling long-term temporal dependency and complex dynamics. Relational reasoning in the video requires contextual information over time, unlike static images. For example, in <ref type="figure">Fig. 1</ref>, the relation between two men and bear is ambiguous and hard to define with a static image but can be inferred from longer spatio-temporal contexts. Suppose there are n object and m relation categories, the number of possible combinations of pair-wise relationships is O(n 2 ? m). A naive approach to this problem is to learn a distribution over the n ? n ? m lattice space -the complete combinations of objects and relations. However, the model would be highly prone to bias in this case, due to the sparsity of existing relation types and data limits. A simple sidestep is to learn low-rank matrices, factorized by object and relations. To put it more simply, the strategy is to separate pipelines of predicting objects and relations, reducing the complexity to O(n + m). To this end, we first detect objects and then predict relations conditionally. Here we further factorize the relation prediction process into two subprocesses: relationness scoring and temporal span proposal. We detail the steps in Sec. III.</p><p>To investigate the more complicated issues of the task, we compare typical VidVRD approaches in <ref type="figure" target="#fig_1">Fig. 2</ref>. They can be divided into two categories: (a) segment-based and (b) windowbased approach. Segment-based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref> first split a video into several segments -typically a segment contains 30 frames, and then predict relations at the segment-level. If the neighboring segments share the same relation triplets, they are merged into a single relation with an extended range. This approach predicts segment-wise relations with only corresponding 30 frames, assuming that the basic relations can always be found in a short duration and can indirectly build the video-level predictions via the association method. However, we argue that 30 frames are not enough to model long-term interactions such as 'dog-past-person'. Because the relation 'past' can only be inferred based on the overall process by which a dog is behind a person and overtakes a person, continuous monitoring of interactions between objects is required during successive events. More recently, a windowbased approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref> have been proposed in order to incorporate longer temporal contexts of video. They adopt a sliding window scheme over the whole object trajectories and obtains all possible sizes of object tracklets. With the object tracklets of various lengths, they can predict both short-term and long-term interactions. However, we see that this is hardly scalable for extremely long videos such as movies because it requires an exhaustive search to sample all combinations of size-varying tracklet pairs to cover various durations of relations, resulting in a cubic complexity.</p><p>This work aims to bridge the gap between segment-based methods and window-based methods while improving the shortcomings and exploiting the advantages of the methods with proposed Temporal Span Proposal Network (TSPN) (see <ref type="figure" target="#fig_1">Fig. 2</ref>(c)). Given a pair of object trajectories, TSPN first finds what pairs of objects are probable to have relations inbetween 1 , then predicts when the pair-wise relations begin and end. We call the first step as relationness scoring and the second step as temporal span proposal. Since it is possible that an object pair can have multiple relations (e.g., A-standing next to-B; A-watch-B), we formalize the second step of TSPN as a multi-label classification in practice. By leveraging both temporal locality as in the segment-based approach and temporal globality as in the window-based approach, TSPN can effectively predict pair-wise object relationships. It is also clear that TSPN is more efficient than both methods because it only uses each object pair once to predict all relationships across the video. In particular, it prevents duplicate use of each object tracklet by dividing it into different window lengths, such as the window-based method. By design, the proposed TSPN can coarsen relation search space by capturing the regularity of pair-wise object interaction. Also, TSPN leverages the global video context features, making it strong in both the short and long-term relationship modeling.</p><p>We validate TSPN against existing approaches on two video visual relation detection datasets: ImageNet-VidVRD <ref type="bibr" target="#b14">[15]</ref> and VidOR <ref type="bibr" target="#b29">[30]</ref>. We observe that our method achieves new stateof-the-art on both benchmarks with a significant performance gain. Also, we examine the theoretical computation complexity of three methods (segment, window, and TSPN) and see that TSPN is approximately 2? and 4 orders of magnitude efficient than the segment-based method and window-based method, respectively. Moreover, comprehensive quantitative and qualitative analyses demonstrate the efficacy of building blocks of TSPN.</p><p>Our contributions can be summarized as follows:</p><p>? We investigate the problem of Video Visual Relationship Detection (VidVRD), and identify the challenges that the task itself naturally entails and the challenges that the current (segment-based and window-based) methods have.</p><p>To the best of our knowledge, TSPN is the first proposalbased approach for VidVRD. ? We propose a novel approach named Temporal Span Proposal Network (TSPN) that tackles the aforementioned issues. TSPN has two key components: (i) relationness scoring module that sparsifies relation search space and (ii) temporal span proposal module that predicts relations and its duration upon global video context. ? We show that TSPN is not only effective but also efficient: TSPN achieves state-of-the-art performances in two benchmark datasets (ImageNet-VidVRD and VidOR) and theoretically faster than conventional VidVRD methods by 2? or more. ? Comprehensive ablation studies demonstrates the efficacy of model components and several design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our work intersects the following three research topics: multi object tracking, relation detection in images &amp; videos, and proposal networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi Object Tracking</head><p>Multiple Object Tracking (MOT) aims to identify and track objects in videos without prior knowledge of the appearance and number of targets. Unlike object detection algorithms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, where the output is a set of rectangular bounding boxes with four coordinates (or two coordinates with height and width), MOT methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> assign an additional target ID to each box in order to differentiate objects within the same class. Standard MOT algorithms follow a tracking-by-detection strategy. First, a collection of bounding boxes are extracted from the video frames (detection), and then they are utilized to guide the tracking process (tracking). The tracking is typically done by bounding box association across the frames: bounding boxes containing the same target are assigned with the same ID.</p><p>As VidVRD operates upon object trajectory proposals, we employ the tracking-by-detection strategy for MOT. In line with other VidVRD works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>, we utilize Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> for object detection and DeepSORT <ref type="bibr" target="#b26">[27]</ref> for multi object tracking from detected bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relation Detection in Images</head><p>In an effort to detect relations in images, numerous studies have been explicitly modeled and adopted deep neural networks.</p><p>The challenging and open-ended nature of the task lends itself to various forms.</p><p>Visual Relation Detection (VRD) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> aims to recognize relations between objects in an image, which is the generic form of visual relational reasoning tasks. Lu et al. <ref type="bibr" target="#b9">[10]</ref> present the first VRD work, in which they employed RCNN <ref type="bibr" target="#b39">[40]</ref> to recognize objects and predicates in an image, and leverage language priors from semantic word embeddings in order to finetune the likelihood of predicted relationships. Since then, a variety of methods has been proposed: visual translation embedding (VTransE) <ref type="bibr" target="#b11">[12]</ref>; reinforcement learning (RL)-based framework <ref type="bibr" target="#b36">[37]</ref>; linguistic knowledge distillation <ref type="bibr" target="#b37">[38]</ref>; prior and posterior statistics <ref type="bibr" target="#b10">[11]</ref>; context-aware attention models <ref type="bibr" target="#b38">[39]</ref>.</p><p>Scene Graph Generation (SGG) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46</ref>] is a graphical form of VRD, or a structured scene descriptor, where nodes correspond to objects and edges correspond to pairwise relationships between objects. The challenges in SGG yield diverse methods : Xu et al. <ref type="bibr" target="#b40">[41]</ref> design an RNN-based model to exploit contextual cues in order to improve the scene graph predictions via iterative message passing between objects and predicates; Yang et al. <ref type="bibr" target="#b41">[42]</ref> utilizes graph convolution network <ref type="bibr" target="#b46">[47]</ref> to model contextual information between objects and relations; Zellers et al. <ref type="bibr" target="#b42">[43]</ref> analyzes the role of motifs: regularly appearing substructures in scene graphs and provide RNN-based model that captures higher order motifs; Tang et al. <ref type="bibr" target="#b45">[46]</ref> present unbiased SGG framework that is robust to skewed data distribution using causal inference; Woo et al. <ref type="bibr" target="#b13">[14]</ref> identifies underlying challenges in SGG (ambiguity, asymmetry, higher-order contexts) and tackle the problem with local-to-global interaction network.</p><p>Human-Object Interaction (HOI) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> more focuses on human and considers them as the subject. Gkioxario et al. <ref type="bibr" target="#b10">[11]</ref> build their model on Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> with a humancentric branch that performs target object localization and action classification, and an interaction branch that combines human features with target object features; Qi et al. <ref type="bibr" target="#b50">[51]</ref> apply a graph neural network (GNN)-based model on the humanobject graph containing all potential human-object interactions to remove unlikely edges to be connected; Zhou et al. <ref type="bibr" target="#b51">[52]</ref> propose a multi-stream network which incorporates language priors, geometric features, and visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relation Detection in Videos</head><p>Most of the relation detection works are mainly conducted in the static image domain. As a breakthrough, the pioneer work <ref type="bibr" target="#b14">[15]</ref> has introduced the first dataset and the baseline of VRD in the dynamic video domain. Since then, a few works have been proposed due to the difficulty of spatio-temporal modeling.</p><p>A majority of existing methods follow the segment-based approach <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> which was firstly introduced in <ref type="bibr" target="#b14">[15]</ref>. It first breaks a video into several segments, predicts per-segment relations, and finally associates relations in a greedy manner. This line of methods achieved sound performance, but they are inherently incapable of using the long-term temporal context in other segments: VidVRD <ref type="bibr" target="#b14">[15]</ref> detects relations in the short-term and greedily associates them; MHRA <ref type="bibr" target="#b27">[28]</ref> generates multiple hypotheses for video relation instances for more robust long-term relation prediction; GSTEG <ref type="bibr" target="#b15">[16]</ref> constructs a conditional random field on a spatio-temporal graph exploiting the statistical dependency of relational entities; VRD-GCN <ref type="bibr" target="#b16">[17]</ref> passes the message through fully-connected spatial-temporal graphs and conducts reasoning in the 3D graphs using Graph Convolution Network (GCN) <ref type="bibr" target="#b46">[47]</ref>; MMFF <ref type="bibr" target="#b17">[18]</ref> predicts relations by jointly using spatial-temporal visual feature and language context feature; MHA <ref type="bibr" target="#b18">[19]</ref> maintains multiple possible relation hypotheses during the association process to handle the inaccuracy of the former steps; VSRN <ref type="bibr" target="#b53">[54]</ref> utilizes 3D CNN to encode spatiotemporal information, and to use semantic collocations between objects for comprehensive relation representations; IVRD <ref type="bibr" target="#b20">[21]</ref> adopts a causal intervention on the input subject and object that leads the model to incorporate each potential predicate prototype, which is a set of relation references with the same predicate.</p><p>More recently, lie et al. <ref type="bibr" target="#b19">[20]</ref> proposed a window-based approach to handle long-term temporal context. It utilizes sliding window of varying sizes to generate a large number of tracklets then predicts their pairwise relations. Since then, a handful of works have been proposed that follow window-based approach <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref>: PPN-STGCN <ref type="bibr" target="#b19">[20]</ref> employs a sliding-window scheme to predict both short-term and long-term relationships and utilizes GCN to calculate the compatibility of tracklet proposal pair; Gao et al. <ref type="bibr" target="#b28">[29]</ref> construct a VidVRD model based on the Transformer encoder-decoder <ref type="bibr" target="#b54">[55]</ref> that models interactions between tracklets and learnable predicate queries; VidVRD-II <ref type="bibr" target="#b21">[22]</ref> leverages inter-dependency among subjectobject-predicate classification and fine-tunes its predictions based on the learnt dependency.</p><p>However, the above-mentioned methods either lack temporal globality or scale poorly for extremely long videos such as movies. To move one step forward, we introduce a novel TSPN to directly predict relations in an end-to-end manner without the need for many heuristics such as segment length and window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proposal Networks</head><p>There is a large body of literature on object proposal techniques. Standard object proposal approaches fall into two categories: (i) unsupervised approaches using super-pixel grouping <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> or sliding windows <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>. These techniques are mostly offline algorithms that operate independently of detectors (e.g., RCNN <ref type="bibr" target="#b39">[40]</ref>, and Fast R-CNN <ref type="bibr" target="#b59">[60]</ref>). (ii) supervised approaches based on learnt deep representations from CNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b60">61]</ref>. In these approaches, proposal generation is done by a single forward pass inside an end-to-end trainable detection system.</p><p>Among them, Region Proposal Network (RPN) <ref type="bibr" target="#b24">[25]</ref> used for the object detection strongly inspired our TSPN. RPN simultaneously predicts object bounding boxes and 'objectness' score (we named the term 'relationness' after this) to detect the region of interest. We extend the concept of proposal from the object to relation as well as the space domain to the time domain.</p><p>Several works also share a similar spirit with TSPN that of reducing relation search space <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b61">62]</ref>: Zhang et al. <ref type="bibr" target="#b61">[62]</ref> propose a Relation Proposal Network that selects a set of subject-object pairs among every possible pairs by evaluating visual and spatial compatibility; Yang et al. <ref type="bibr" target="#b41">[42]</ref> present a network that learns to estimate the relatedness of an object pair and prunes unlikely relations while preserving likely ones; Liu et al. <ref type="bibr" target="#b19">[20]</ref> construct two graphs to independently formulate spatial and temporal interactions between tracklet proposal features to filter out incompatible proposal pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TEMPORAL SPAN PROPOSAL NETWORK (TSPN)</head><p>In this section, we present an end-to-end trainable model Temporal Span Proposal Network (TSPN) for VidVRD. TSPN is built on top of the object trajectory proposal module, which is a combination of object detector <ref type="bibr" target="#b24">[25]</ref> and multi-object tracker <ref type="bibr" target="#b26">[27]</ref>. In order to ease the optimization, we pre-train object detector and jointly train the whole video relation detection network. TSPN shares the video-level convolutional features, enabling itself to make cost-efficient proposals. An overview of TSPN is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, and <ref type="table" target="#tab_1">Table I</ref>    , resulting in H. The relationness S between an object pair is first calculated by feeding H into a FC layer. After then, a set of pairs with high relationness scores (colored in red in the figure) is only considered in the subsequent process. Note that the relationness score is computed differently for S(O1 ? O2) = 0.97 and S(O2 ? O1) = 0.53 since the pair-wise relationship can vary when subject and object are switched (Sec. III-D). (d) Finally, joint features are concatenated and fed to another FC layer to predict output Z which is deemed as an outer product of relation labels R and their temporal spans T , i.e., start-end time (Sec. III-E). Our TSPN can be trained in an end-to-end manner. See texts for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>We design a video relation detector with the notion of object trajectories, relationness, temporal span, and relation labels. Formally, let V denote an input video, O be a set of object trajectories and S, T and R denote the relationness score, temporal span, and relation, respectively. The goal is to build a model for P (? VidVRD = (O, S, T , R)|V). The VidVRD can be factorized into three processes:</p><formula xml:id="formula_0">P (? VidVRD |V) = P (O|V) Object Trajectory Proposal P (S|O, V) Relationness Scoring P (T , R|S, O, V).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation &amp; Temporal Span Prediction</head><p>The most fundamental yet essential part of relation reasoning is the reliable object trajectory generation, since relationships are established between two objects. Thus, we first detect locations and predict categories of objects appearing within the video. The object trajectory proposal P (O|V) is typically modeled using an off-the-shelf object detector <ref type="bibr" target="#b24">[25]</ref> and multi-object tracker <ref type="bibr" target="#b26">[27]</ref>. The subsequent steps can be break down into 1) relationness scoring and 2) relation &amp; temporal span prediction subprocesses. Given a set of object trajectory pairs, TSPN first calculates the probability that each pair will have a relation (i.e., relationness), and sample pairs of object trajectories with high relationness (i.e., pair-of-interest). We note that even for the same pair of objects, relationness can be different when the semantic identity of the subject and the object is reversed: </p><formula xml:id="formula_2">S(O 1 ? O 2 ) = S(O 2 ?<label>O</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Trajectory Proposal</head><p>As all the subsequent processes heavily rely on the object trajectory proposals, it is essential to assure them to be highquality. The purpose of this step is to find trajectories O for all n objects present in the video V with f frames. We adopt the Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> object detector equipped with ResNet-101 backbone <ref type="bibr" target="#b62">[63]</ref> to detect what the objects are (classification) and where they are located (localization) at every frame. The Non-Maximum Suppression (NMS) algorithm <ref type="bibr" target="#b64">[65]</ref> is performed with Intersection over Union (IoU) threshold of 0.5 to reduce redundant bounding boxes. The object bounding boxes have variations in scale, aspect ratio, and position, hence we use Region-of-Interest (RoI) Align operation <ref type="bibr" target="#b65">[66]</ref> to generate fixedlength feature representations from variable-size bounding box proposals, easing the subsequent relation computations. From object detection, we can obtain object bounding boxes B 1 , ? ? ? , B n , which are defined by two points on the lower left and upper right (x min , y min , x max , y max ), and classification probability distributions C 1 , ? ? ? , C n , which are predicted from RoI appearance features A 1 , ? ? ? , A n for each frame.</p><p>To ease the optimization of TSPN, we pre-train the object detector and then jointly train the whole TSPN model (including the object detector). Let the number of objects in a video as n and the possible number of relations as m. Considering all pair-wise relations of n objects, direct optimization of object detector and relation detector costs O(n 2 ? m), which is prohibitively expensive; however, the sequential optimization of object and relation detectors lowers the costs to O(n + m).</p><p>We also utilize a DeepSORT tracker <ref type="bibr" target="#b26">[27]</ref> to link framelevel bounding boxes with the same identity into the videolevel object trajectory proposals O 1 , ? ? ? , O n . The DeepSORT tracker integrates visual features as matching descriptors to improve tracking performance. We use RoI appearance features obtained from the object detection phase as visual features for DeepSORT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object-level Feature Extraction</head><p>For a pair of objects O s , O o , we obtain a set of RoI appearance features A s , A o , A u by applying RoI Align operation, where subscript s and o denote the semantic identity (e.g., subject and object) of objects, and A u denotes the union RoI feature. The object class distributions C s , C o , C u are predicted via a linear predictor, e.g., FC layer, using RoI features A s , A o , A u , where C u is defined as the mean of C s and C o . We define the union RoI as a rectangular area that tightly encompasses the subject RoI and object RoI (see <ref type="figure" target="#fig_2">Fig. 3</ref> (b) for better understanding). The frame-wise features A, B, C are averaged across the frames. We use these obtained features for the subsequent processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Relationness Scoring</head><p>The computational cost to learn relation distributions for every object pair is prohibitively expensive. Only a handful of objects have meaningful relations due to the sparse nature of real-world interactions. In order to model such regularities, we start by scoring relationness between object trajectory proposals. More formally, from n object trajectory proposals O, the possible pairs are in n 2 2 . Our goal here is to sample p n 2 pairs. To this end, we filter out less correlated pairs based on the relationness score. We first jointly model visual, geometric and semantic relationness by concatenating RoI appearance features A i along with corresponding bounding box coordinates B i and classification probability distributions C i , resulting in joint representation J i .</p><formula xml:id="formula_3">J i = A i B i C i , where, i ? {s,o,u}<label>(2)</label></formula><p>and is a concatenation operation. Then, each J i is projected into its respective embedding space via the FC layer, and fused by the Hadamard product (i.e., element-wise multiplication).</p><formula xml:id="formula_4">H = (W T s J s + b s ) (W T o J o + b o ) (W T u J u + b u ),<label>(3)</label></formula><p>where W and b respectively denotes weight matrix and bias matrix of each FC layer (subscripts s, o, u denote subject, object, and union, respectively). The Hadamard product has shown to be effective as an attention mechanism for highdimensional representations <ref type="bibr" target="#b66">[67]</ref>. Also, unlike concatenation that increases the input dimension of the fusion unit (FC layer), the Hadamard product is efficient fusion operation since it maintains the same dimension. The fused representation H is passed to another FC layer to project them into the same space where the relationness score S is calculated. Formally, the relationness score S is computed as follows:</p><formula xml:id="formula_5">S = ? W T H + b ,<label>(4)</label></formula><p>where W and b are weight matrix and bias matrix of FC layer, indicates Hadamard product, and ? is a non-linear activation, which maps any real values into a finite interval (e.g., sigmoid). For a pair of objects, subject (s) and object (o) can be switched. As such, the relationness score is calculated twice since it can vary depending on direction (</p><formula xml:id="formula_6">S(O s ? O o ) = S(O o ? O s )).</formula><p>After iterating the process over every object trajectory pairs existing in the video 2 , we sort the output relationness scores in descending order and maintain only the top-p pair-of-interests per video. Only these are considered in subsequent processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Relation &amp; Temporal Span Prediction</head><p>Due to the dynamic nature of video, numerous relations may exist between a single pair of object trajectories. These relations can be exist in the same time span or can be placed over multiple time spans. For example, think of a scenario where two racehorses appear: two racehorses stand side by side, move ahead when they hear a gunshot, and continue to compete during the race. Even to simplify the situation, it would be horse1stand next to -horse2 in the early timespan, and horse1chase -horse2 in the late timespan. Therefore, we need to simultaneously predict what </p><formula xml:id="formula_7">s ? L 2 2l 2 s ? L 2ls + 2L 2 2L 2 l ? L 2 l 3 ? L l 2 + 2L 2 O(L 4 ) TSPN (ours) L l L l L l O(L)</formula><p>TABLE II: Comparison of estimated relation prediction time with typical approaches. L denotes the intersection of subject trajectory and object trajectory (i.e., pair-of-interest), l denotes segment length (in segment-based) or minimum window size (in window-based) or coverage of each temporal sector (in TSPN), and s denotes the stride of segment (in segment-based) or window (in window-based), where s &lt; l L. Note that l is set the same for all methods for a more equitable comparison. We conclude that our TSPN is 2? or more faster than conventional VidVRD methods in typical setting, where s = l 2 .</p><p>relations exist and when the relations occur (i.e., the range of time), given the object trajectory pair.</p><p>There is an open choice in predicting the temporal span of relations. Early object detectors adopt RPN <ref type="bibr" target="#b24">[25]</ref> that uses a sliding window scheme with anchors in different scales to learn region proposals by reducing the localization error between anchors and actual object regions. However, when it comes to temporal span proposal, it becomes unnecessary since relation detection does not require fine-grained temporal duration to match the ground-truth. We thus are not using the anchor or sliding-window scheme in this work. The bruteforce approach to find temporal span is to learn a probability distribution over the m?f lattice space -the number of relation categories is m, and the total number of frames is f -which is highly costly. To conform with the parsimonious property, we simplify the process by quantizing the video-level temporal span into several sectors; we then predict the likelihood of each relation category's presence within the sectors. This strategy is distinguished from the segment-based approaches in terms of the feature level because they are not possible to encode the video-level temporal contexts by extracting features from segments. In contrast, we can utilize the video-level features and predict relations with just a single glance at a video, i.e., there is no need to chunk video into overlapping short segments or repeat computation over the same part. That is, TSPN can directly specify relationship ranges and categories across the whole video without redundancy.</p><p>Let the time span over which two object trajectories intersect (i.e., O s ? O o ) as T . After T breaks down into k temporal sectors, one sector is responsible for the range l of the entire time span. In other words, each temporal sector per pair-ofinterest covers a length of l, which is calculated as:</p><formula xml:id="formula_8">l = 1 k (len (O s ? O o )) ,<label>(5)</label></formula><p>where len(?) stands for the length of a given trajectory.</p><p>We can now discretely predict which temporal sectors the relations exist in, instead of directly predicting the temporal span in continuous video space. We concatenate joint features J s , J o , J u and feed into a fully-connected layer followed by a non-linearity (e.g., sigmoid).</p><formula xml:id="formula_9">Z = ? W T z (J s J o J u ) + b z ,<label>(6)</label></formula><p>where the output matrix Z ? R m?k represents the outer product (denoted as ? the equation below) of the probability distribution of relation categories R ? R m and that of temporal span T ? R k , enabling multi-label classification within multiple time spans.</p><formula xml:id="formula_10">Z = R ? T .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Loss Function</head><p>The whole network can be trained in an end-to-end manner. Total loss is sum of relationness (L R ) and temporal span (L T ) loss. Both are binary cross entropy loss.</p><formula xml:id="formula_11">L total = L R + L T .<label>(8)</label></formula><p>L R measures the loss between p pair of ground truth binary values r * (pair-of-interest as 1, otherwise 0) and the predicted relationness (r in Eq. (4)).</p><formula xml:id="formula_12">L R = ? 1 p p i=1 r * ? log r + (1 ? r * ) ? log(1 ? r).<label>(9)</label></formula><p>L T measures the loss between m ? k pair of ground truth temporal spans t * and predicted temporal spans t.</p><formula xml:id="formula_13">L T = ? 1 m ? k m?k i t * ? log t + (1 ? t * ) ? log(1 ? t). (10)</formula><p>We construct a ground truth temporal span into a set of temporal sectors with values of 0 or 1, each of which is set to 1 if the relation lasts more than half within that temporal sector, otherwise set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EFFICIENCY OF TSPN</head><p>To see how efficient TSPN is, we compare TSPN with two representative VidVRD approaches (vs.segment-based and window-based) in terms of estimated computation time. The segment-based approach predicts video-level relations by dividing a video into several short-term segments and connecting to longer relations if adjacent segments share the same relations. The window-based approach first obtains multiple tracklets by sliding varying-size windows over the trajectories, finds tracklet pairs with high similarity via a correlation embedding module, and finally predicts the relations for the valid pairs. TSPN first finds out what object trajectory pairs are highly likely to have relations by calculating relationness score, then TSPN simultaneously predicts relation categories and their temporal span with sampled pairs.</p><p>Note that direct comparison of total computing time is difficult as the greedy association in the segment-based approach is an offline algorithm, and the source codes of the windowbased approach are not publicly available. We thus provide the asymptotic computational complexity of each method for relation prediction. Here, we do not consider additional features or modules (e.g., greedy association, pair correlation embedding module, and relationness scoring) other than the main components for simplicity. Also, we consider that the segment length of the segment-based approach, a minimum window size of the window-based approach, and coverage per temporal sector in TSPN are all on the same scale for fair comparison. Now the time complexity can be represented by the number of object pairs to consider, assuming that predicting relations for each pair of objects takes the same amount of time.</p><p>Let the intersecting length of subject and object be L, segment or minimum-window or sector length be l, and stride be s. Intuitively, the computation time of segmentbased approach is proportional to the number of segments. The number of segments can be calculated as:</p><formula xml:id="formula_14">N s = L ? l + s s .<label>(11)</label></formula><p>Similarly, the window-based method depends on the number of windows. The window-based method exhaustively uses window of all sizes, from smallest (i.e., l) to largest (i.e., L), to capture both the short-term and long-term object relations. The number of all windows can be calculated as:</p><formula xml:id="formula_15">N w = L l k=1 L ? kl + s s .<label>(12)</label></formula><p>Since relation detection requires two windows each responsible for subject and object, the overall computation shows a quadratic growth rate with respect to the number of windows.</p><p>In contrast, TSPN directly predicts the relationship categories and their temporal span using the entire video context only once. Thus, it shows a linear growth rate with respect to the number of temporal sectors. Since there is no overlap between sectors, the number of sectors can be simplified as:</p><formula xml:id="formula_16">N t = L l .<label>(13)</label></formula><p>Under the condition of s &lt; l, we can obtain the following conclusions <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_17">N t N w &lt; N s ,<label>(14)</label></formula><p>which means that TSPN is always more efficient than the segment and window-based approaches. In practice, we follow the typical settings of stride/segment length ratio (s = 15, l = 30; thus, s = l/2), so the estimated computation time of segment and TSPN-based approaches are proportional to L s and L l , respectively. Therefore, relation prediction of TSPN is approximately 2? more efficient than the segment-based <ref type="bibr" target="#b2">3</ref> Here is a simple derivation: 1) Nw can be rewritten as Ns + L l k=2 L?kl+s s ; therefore, Ns &lt; Nw.</p><p>2) The lower bound of Ns is L?l s + 1; since s &lt; l, it satisfies: L?l s + 1 &gt; L?l l + 1 = L l = Nt; therefore, Nt &lt; L?l s + 1 ? Ns.</p><p>approach and even 4 orders of magnitude efficient than windowbased approach. The overall estimated computation times for relation prediction are summarized in <ref type="table" target="#tab_1">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we conduct comprehensive studies to validate the effectiveness of the proposed TSPN. Here, we report results on two challenging datasets: ImageNet-VidVRD <ref type="bibr" target="#b14">[15]</ref> and VidOR <ref type="bibr" target="#b29">[30]</ref>. To understand the behavior of TSPN, we provide extensive quantitative analyses. Lastly, we present qualitative results to examine how our model can benefit from temporal globality concretely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Following <ref type="bibr" target="#b19">[20]</ref>, we adopt Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> and Deep-SORT <ref type="bibr" target="#b26">[27]</ref> for object trajectory detection. We first generate bounding boxes at each frame using Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> equipped with ResNet-101 <ref type="bibr" target="#b62">[63]</ref> backbone. The Faster R-CNN is pre-trained on MS-COCO <ref type="bibr" target="#b71">[72]</ref> and ILSVRC2016-DET <ref type="bibr" target="#b72">[73]</ref> datasets with 35 object categories. We perform NMS in the detection phase with an IoU threshold of 0.5 to eliminate redundant bounding boxes since NMS in the tracking phase may wrongly remove overlapping trajectories with different classes because it is class-agnostic. As a multi object tracker, we adopt DeepSORT <ref type="bibr" target="#b26">[27]</ref>, which integrates visual appearance information to gain robustness against identity switching while associating the objects with the same identity in neighboring frames. We extract fixed-size features of video using ResNet 101 backbone <ref type="bibr" target="#b62">[63]</ref> and RoI Align operation <ref type="bibr" target="#b65">[66]</ref> to get fixedlength representations from varying-size object proposals. For TSPN training, we use Adam optimizer <ref type="bibr" target="#b73">[74]</ref> with a learning rate of 10 ?3 and weight decay of 10 ?4 for a batch size of 32. We set the number of pair-of-interests as 64 (i.e., p = 64), and the number of sectors is set to 16 (i.e., k = 16). We keep top-100 relation triplet predictions per video. The hyperparameter configurations are set to be the same on both ImageNet-VidVRD <ref type="bibr" target="#b14">[15]</ref> and VidOR <ref type="bibr" target="#b29">[30]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Configuration</head><p>In both datasets, objects and relation triplets (with temporal span) are annotated in the form of {trajectory id, category, bounding box coordinates} and {(subject id -predicate category -object id), (start frame -end frame)}, respectively. We exclude several unannotated videos in the experiment. a) ImageNet-VidVRD: ImageNet-VidVRD <ref type="bibr" target="#b14">[15]</ref> is a subset of ILSVRC2016-VID <ref type="bibr" target="#b72">[73]</ref> train and validation set. Videos are selected with the criteria of whether they contain clear visual relations. It contains 1,000 videos (train:test split is 800 : 200) with the manually labeled object categories, corresponding bounding box trajectories, and relation triplets. It covers 35 and 132 categories for objects and predicates, respectively. Note that the relations in the training set are all annotated in the segment-level (e.g., 30 frames). Thus, we link them into the video-level relations in order to directly optimize long-term relations.   <ref type="bibr" target="#b67">[68]</ref>, ResNet101 <ref type="bibr" target="#b62">[63]</ref>, and I3D <ref type="bibr" target="#b68">[69]</ref>. Also, some of the methods used language embeddings: word2vec <ref type="bibr" target="#b69">[70]</ref> and GloVe <ref type="bibr" target="#b70">[71]</ref>.   <ref type="bibr" target="#b74">[75]</ref> collection. The average video length in train/val set is ?36 seconds, totaling ?84 hours. It contains 80 categories of objects with trajectories to indicate their spatio-temporal location in the videos, 50 categories of relation predicates, and carefully annotated the relation triplets. This results in around 50,000 objects and 380,000 relation instances annotated. Note that only the training and validation set are used for the experiment because the test set is not yet publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Protocol</head><p>We follow the standard evaluation task settings and metrics <ref type="bibr" target="#b14">[15]</ref> listed below: a) Tasks: In Visual Relation Detection (VRDet) task, given a video, the objective is to jointly predict object trajectories, categories, and existing relations with its duration. The object trajectories are considered matched only if vIoU 4 with the ground truth trajectory is greater than half.</p><p>In Visual Relation Tagging (VRTag) task, given video with a set of ground truth object trajectories, the objective is to find all object categories and existing relations. In other words, it needs to detect the relation triplet &lt; subject ? predicate ? object &gt; correctly.</p><p>In both tasks, detected visual relation instances are treated as correct only if the trajectories of subject and object that form the relation both have sufficiently high vIoU (i.e., vIoU &gt; 0.5) with the ground truth. <ref type="bibr" target="#b3">4</ref> vIoU denotes the volumetric intersection over the union of two tubes. b) Evaluation metrics: For Visual Relation Detection (VRDet) task, we adopt mean Average Precision (mAP ) and Recall@K to evaluate the detection performance. Specifically, we use video-wise Recall@50 and Recall@100, which measures the fraction of the top-K predictions among the groundtruth triplets. Following <ref type="bibr" target="#b14">[15]</ref>, we also report the mAP metric to evaluate the overall precision performance at different recall values.</p><p>For Visual Relation Tagging (VRTag) task, P recision@K as the evaluation metric to emphasize the ability to tag accurate visual relations. It measures the fraction of groundtruth among the top-K triplets. Specifically, we use video-wise P recision@1, P recision@5, and P recision@10 since the number of visual relation instances per segment is 9.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-Art</head><p>We compare TSPN with state-of-the-art approaches on the two VidVRD benchmarks: ImageNet-VidVRD <ref type="bibr" target="#b14">[15]</ref> and VidOR <ref type="bibr" target="#b29">[30]</ref>. For a comparison, we consider the following methods: VidVRD <ref type="bibr" target="#b14">[15]</ref>, MHRA <ref type="bibr" target="#b27">[28]</ref>, GSTEG <ref type="bibr" target="#b15">[16]</ref>, VRD-GCN <ref type="bibr" target="#b16">[17]</ref>, MMFF <ref type="bibr" target="#b17">[18]</ref>, MHA <ref type="bibr" target="#b18">[19]</ref>, VSRN <ref type="bibr" target="#b53">[54]</ref>, PPN-STGCN <ref type="bibr" target="#b19">[20]</ref>, and gao et al. <ref type="bibr" target="#b28">[29]</ref>. Existing VidVRD approaches can be divided into two families: segment-based <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref> and window-based <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref> approaches. They adopt different methods to generate object trajectory representations including improved dense trajectory (iDT) features <ref type="bibr" target="#b67">[68]</ref>, ResNet101 <ref type="bibr" target="#b62">[63]</ref>, and I3D <ref type="bibr" target="#b68">[69]</ref>. Also, some of them used language embeddings such as word2vec <ref type="bibr" target="#b69">[70]</ref> or GloVe <ref type="bibr" target="#b70">[71]</ref>. The results are presented in <ref type="table" target="#tab_1">Table III and Table IV</ref>. In general, the VidOR dataset has longer video lengths than the ImageNet-VidVRD dataset, hence the performance tends to be lower in the same metric. Overall, we observe that TSPN achieves competitive results on both   ImageNet-VidVRD and VidOR benchmarks, demonstrating its effectiveness while having lower computational cost than counterparts. In ImageNet-VidVRD benchmark, when evaluated with VRDet mAP metric, MHA <ref type="bibr" target="#b18">[19]</ref> slightly performs better (+0.13) than TSPN, albeit our TSPN outperforms all methods in all other metrics without the need of any external resources (e.g., language embeddings). In VidOR benchmark, our TSPN mostly outperforms its counterparts and performs on par with Gao et al. <ref type="bibr" target="#b28">[29]</ref>. Generally, our TSPN performs better than Gao et al. <ref type="bibr" target="#b28">[29]</ref> in VRDet task which requires both object and relation detection at the same time. TSPN leverages the complementariness of multiple information (visual, geometric, and semantic), and is designed to utilize the spatio-temporal contexts of the entire video, making it strong not only for the short-term but also for the long-term relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative Analysis</head><p>All analytic experiments are conducted on the ImageNet-VidVRD dataset.</p><p>a) Contribution of different information for relationness scoring: In <ref type="table" target="#tab_8">Table V</ref>, we ablate the feature combinations in relationness scoring module. We used RoI appearance features (A), bounding box coordinates (B), and classification probability distribution (C) as visual, geometric, and semantic information for relationness scoring. By default, we only used visual information (Exp 1) to measure relationness score. We then progressively fuse geometrical information (Exp 2) and semantic information (Exp 3). The bounding box coordinates of object trajectories (B) can provide cues about the spatial relations (e.g., next to, in front of, etc.), and the classification probability distribution (C) can be used to leverage inter-object bias. It is known that inter-object bias can help determining relationship between them <ref type="bibr" target="#b42">[43]</ref>. When all information are combined (Exp 4), the model achieves the best performance in both VRDet and VRTag tasks, implying that each information contributes to the performance in different aspect. From the results, we confirm the importance of the complementary feature fusion.    <ref type="table" target="#tab_1">Table VI</ref>. Without both relationness scoring (R) and temporal span proposal (T ) (equivalent to Exp 1), we have to optimize m ? l distribution for n 2 pairs, but with both R and T (equivalent to Exp 4), we only need to optimize m ? k distribution for p pairs, where p n 2 and k l. In this experiment, we can see that the R and T are complementary to each other in constructing a strong VidVRD model. c) Optimal Size of Relation Search Space: We further explore the optimal number of pair-of-interest (p) per video. Considering that VidVRD contains 4,835 video-level relation instances in 200 test set (?24 instances per video on average), we set the number of pair-of-interest as 32 by default. We examine several variations (p = 16 or 64 or 128) in <ref type="table" target="#tab_1">Table VII</ref>. Although 16 shows the highest precision since it has a low false-positive rate, 64 holds the best overall results. The results indicate that balancing positives and negatives labels is crucial since the TSPN learns from penalizing the false positives. The number of PoIs should be appropriately set because the resulting sampled object pairs affect the subsequent processing. If it is too large (e.g., 128), the probability of the noisy object pairs being included in the PoIs becomes higher and result in degraded performance. We empirically found that 64 works well, and use this number for the rest of the experiments. d) Trade-off in Temporal Span Quantization: We investigate the effect of quantization of temporal spans by varying the number of sectors. We divide the intersecting temporal span of subject and object trajectories into k sectors (k of Eq. (5)). Since the intersection of each object trajectory pair varies, each object pair may have a different quantization impact. Sparse quantization narrows the search space thus easy to optimize but lowers the matching rate at the same time. On the other hand, dense quantization increases the matching rate, but it also broadens the search space thus expensive to optimize. We conduct an experiment in <ref type="table" target="#tab_1">Table VIII</ref> to assess the optimal degree of quantization. A limited number of sectors (k = 4, 8) exhibit sub-optimal performances. On the other hand, the model achieves good performances when videos are divided into an <ref type="figure">Fig. 4</ref>: Qualitative examples of visual relation detection results. For comparison, we contrast the predicted relation triplets (i.e., subject-relation-object) of VidVRD with those of TSPN for each given video. The same color means the same object instance. The arrows represent the time axes, providing an approximation of the temporal span of the predicted relation triplets. We highlight relations that TSPN correctly predicted, while VidVRD did not. The predicted relations are considered correct only if the pair of object trajectories have sufficiently high vIoU (i.e., vIoU &gt; 0.5) with ground truth trajectories, and only the correct relations of the top-20 predictions are shown in the figure.</p><p>adequate number of sectors (k = 16, 32). Between k = 16 and k = 32, we observe that the model performs slightly better when k is set to 16 than when k is set to 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Analysis</head><p>To better see how TSPN understands the dynamics of spatiotemrpoal interactions, we provide qualitative examples in <ref type="figure">Fig. 4</ref>. Here, we compare TSPN with VidVRD <ref type="bibr" target="#b14">[15]</ref>, which first proposed a popular segment-based approach. The results show that the VidVRD often fails to capture long-term relations since it relies on temporarily local features. On the other hand, TSPN successfully identifies them by leveraging global video context. For example, the relation person-past-bicycle (105?150) <ref type="bibr" target="#b4">5</ref> in the first row can only be detected by understanding the longterm relationship since the model should capture the entire process of person being behind bicycle and catching up with bicycle as it moves forward. It goes the same for the relation red panda-walk past-red panda (0?75) in the second row. TSPN also detects more active or sophisticated relations such as elephant-kick-ball (0?60) in the third row and skateboard-move beneath-dog (0?360) in the fourth row, while VidVRD simply detects trivial relations (e.g., elephant-taller-person, person-taller-dog), which can also be predicted in a still image. This also reveals the limitation of segment-based methods. That is, they cannot leverage the video-level context. VidVRD is unable to employ temporarily global features directly since it predicts segment-level relations and aggregates them into video-level relations. In contrast, TSPN are designed to utilize video-level context to predict long-term and shortterm relations from the proposed temporal span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This work introduces a novel Temporal Span Proposal Network (TSPN) for Video Visual Relation Detection (Vid-VRD). TSPN guides what and when to look in order to detect relationships, making it efficient yet effective. TSPN effectively reduces the relational search space by learning which object pairs should be considered based on the relationness score between object trajectory pairs (relationness scoring module). TSPN simultaneously predicts the temporal span and categories of the entire relations with a single global video feature, which is not only efficient but also effective in predicting both short and long-term relations (relation &amp; temporal span prediction module). We validate the TSPN through comprehensive experiments. TSPN establishes a strong baseline for VidVRD by leveraging the complementariness of the two key modules. In particular, TSPN achieves state-ofthe-art performance on two VidVRD benchmarks (ImageNet-VidVRD and VidOR) without many external algorithms or heuristics (e.g., greedy association, window sliding) commonly seen in previous approaches while 2? or more faster than conventional approaches (segment-based and window-based approaches).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2019R1A2C109107712), and the Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-01842, Artificial Intelligence Graduate School Program (GIST)). (Corresponding author: Kangil Kim)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Conceptual Comparison of typical VidVRD approaches (empirical comparisons are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of TSPN. (a) TSPN is built upon the object trajectory proposal head which comprises object detection and tracking stages (Sec. III-B). (b) We first extract video visual features via a CNN backbone [63]. With detection results (RoIs), we then extract RoI features [64] of subject, object, and union area. Their corresponding bounding box coordinates and class distribution can be naturally obtained from object detection phase (Sec. III-C). (c) The concatenation of RoI features with bounding box coordinates and class distribution (J) are linearly transformed and then fused via a Hadamard product (denoted as in the figure)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1 ). TSPN finally predicts when relations start and end (i.e., temporal span) for all relation categories within a pair-of-interest. Note that a pair-of-interest can have multiple relations: O 1 R1,??? ,Rm ? ?????? ? O 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>summarizes the relevant notations.</figDesc><table><row><cell>Step</cell><cell>Notation</cell><cell>Description</cell></row><row><cell></cell><cell>V</cell><cell>input video</cell></row><row><cell>Object Trajectory</cell><cell>O</cell><cell>object trajectories</cell></row><row><cell>Proposal</cell><cell>f</cell><cell>number of frames in input video</cell></row><row><cell></cell><cell>n</cell><cell>number of object trajectories</cell></row><row><cell>Object-level Feature Extraction</cell><cell>A B C</cell><cell>RoI appearance features bounding box coordinates classification probability distributions</cell></row><row><cell>Relationness Scoring</cell><cell>J S p</cell><cell>joint features relationness scores number of pair of interests</cell></row><row><cell></cell><cell>R</cell><cell>relation categories</cell></row><row><cell></cell><cell>T</cell><cell>temporal spans</cell></row><row><cell>Relation &amp;</cell><cell>Z</cell><cell>outer product of R and T</cell></row><row><cell>Temporal Span Prediction</cell><cell>m</cell><cell>number of relation categories</cell></row><row><cell></cell><cell>k</cell><cell>number of temporal sectors</cell></row><row><cell></cell><cell>l</cell><cell>length of each sector</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Summary of notations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Comparison with recent approaches on ImageNet-VidVRD<ref type="bibr" target="#b14">[15]</ref> dataset. We list the features used in each model if available. They used different features to extract object trajectory representation: improved dense trajectory (iDT) features</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison with recent approaches on VidOR<ref type="bibr" target="#b29">[30]</ref> validation dataset.</figDesc><table><row><cell>b) VidOR: VidOR [30] is a large-scale video dataset</cell></row><row><cell>which contains 10,000 videos (train:val:test split is 7,000 :</cell></row><row><cell>835 : 2,165) from YFCC100M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Effect of feature combination in Relationness Scoring module. A refers to visual, B to geometric, and C to semantic features.</figDesc><table><row><cell></cell><cell cols="2">Ablations</cell><cell></cell><cell>VRDet</cell><cell></cell><cell></cell><cell>VRTag</cell><cell></cell></row><row><cell>Exp</cell><cell>R</cell><cell>T</cell><cell>R@50</cell><cell>R@100</cell><cell>mAP</cell><cell>P@1</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>6.14</cell><cell>7.52</cell><cell>9.38</cell><cell>40.50</cell><cell>28.90</cell><cell>21.95</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>9.62</cell><cell>11.69</cell><cell>16.29</cell><cell>57.00</cell><cell>41.90</cell><cell>30.45</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>9.95</cell><cell>12.35</cell><cell>15.93</cell><cell>54.50</cell><cell>40.20</cell><cell>29.50</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>11.56</cell><cell>14.13</cell><cell>18.90</cell><cell>60.50</cell><cell>43.80</cell><cell>33.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Model ablations. Here, R and T denote relationness scoring module and temporal span proposal module, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Optimal number of Pair-of-Interests (PoIs).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>VRDet</cell><cell></cell><cell></cell><cell>VRTag</cell></row><row><cell cols="4">Exp # of sectors (k) R@50 R@100</cell><cell>mAP</cell><cell>P@1</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell>1</cell><cell>4</cell><cell>5.19</cell><cell>6.25</cell><cell>9.79</cell><cell cols="2">33.50 23.10</cell><cell>15.44</cell></row><row><cell>2</cell><cell>8</cell><cell>10.02</cell><cell>11.49</cell><cell cols="3">14.74 49.00 36.50</cell><cell>24.04</cell></row><row><cell>3</cell><cell>16</cell><cell>11.56</cell><cell>14.13</cell><cell cols="3">18.90 60.50 43.80</cell><cell>33.73</cell></row><row><cell>4</cell><cell>32</cell><cell>11.32</cell><cell>14.50</cell><cell cols="3">18.69 59.50 42.60</cell><cell>33.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII :</head><label>VIII</label><figDesc>Effect of the number of temporal sectors. We consider several ablations on building blocks of TSPN to identify how each component contributes to the performance and verify its efficacy. Ablation results are summarized in</figDesc><table /><note>b) TSPN Ablations:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, we use the term 'relationness' to indicate the probability that a relation exists between a pair of objects.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In practice, n object trajectories are batched together to calculate pair-wise relationness scores in parallel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The numbers in parentheses mean ground truth "begin frame -end frame".</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image Retrieval Using Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Simple Neural Network Module for Relational Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Interaction Networks for Learning About Objects, Relations and Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00222</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring Visual Relationship for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="684" to="699" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auto-Encoding Scene Graphs for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="685" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph-Structured Representations for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image Generation From Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1219" to="1228" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Specifying Object Attributes and Relations in Interactive Scene Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attend and Interact: Higher-Order Object Interactions for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6790" to="6800" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual Relationship Detection With Language Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting Visual Relationships With Deep Relational Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visual Translation Embedding Network for Visual Relation Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tensorize, Factorize and Regularize: Robust Visual Relationship Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Jae</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1014" to="1023" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tackling the Challenges in Scene Graph Generation With Local-to-Global Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS, 2022</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video Visual Relation Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video Relationship Reasoning Using Gated Spatio-Temporal Energy Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>433. 1, 2, 4</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video Relation Detection With Spatio-Temporal Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video Visual Relation Detection via Multi-Modal Feature Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video Relation Detection via Multiple Hypothesis Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM, 2020</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>1, 2, 4, 9</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Beyond Short-Term Snippet: Video Relation Detection with Spatio-Temporal Global Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>in CVPR, 2020, pp. 10 840-10 849. 1, 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interventional video relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video visual relation detection via iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social fabric: Tubelet compositions for video relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="485" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vrdformer: End-to-end video visual relation detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2022</title>
		<imprint>
			<biblScope unit="volume">846</biblScope>
			<biblScope unit="page" from="18" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection With Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>2, 3, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple Online and Realtime Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple Online and Realtime Tracking With a Deep Association Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3645" to="3649" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multiple Hypothesis Video Relation Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<editor>BigMM. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video relation detection via tracklet based visual transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Annotating Objects and Relations in User-Generated Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="366" to="382" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gm-phd filter based online multiple human tracking using deep discriminative correlation matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Naqvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4299" to="4303" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Heterogeneous association graph fusion for target association in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3269" to="3280" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-object tracking with neural gating using bilinear lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="848" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Visual Relationship Detection With Internal and External Linguistic Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1974" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Towards context-aware interaction recognition for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Neural Motifs: Scene Graph Parsing With Global Context,&quot; in CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Linknet: Relational Embedding for Scene Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="560" to="570" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to Compose Dynamic Tree Structures for Visual Contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unbiased Scene Graph Generation From Biased Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification With Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning to Detect Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV. IEEE</publisher>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Detecting and Recognizing Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8359" to="8367" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="417" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cascaded parsing of human-object interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2827" to="2840" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video relation detection with trajectoryaware multi-modal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4590" to="4594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Vsrn: Visual-semantic relation network for video visual relation inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Stairnet: Top-down semantic aggregation for accurate one shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV. IEEE</publisher>
			<biblScope unit="page" from="1093" to="1102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Relationship Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5678" to="5686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Seq-nms for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">YFCC100M: The New Data in Multimedia Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">His research interests lie in computer vision and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Student Member, IEEE) is currently pursuing the Ph.D. degree in electrical engineering at Korea Advanced Institute of Science and Technology (KAIST)</title>
		<meeting><address><addrLine>Daejeon, Korea; Gwangju, Korea; Daegu, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Electrical Engineering and Computer Science from Gwangju Institute of Science and Technology (GIST) ; Electrical Engineering from Kyungpook National University</orgName>
		</respStmt>
	</monogr>
	<note>He received an M.S. degree in. especially in a high-level visual understanding</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">His research has focused on artificial intelligence, machine learning, and computer vision with a particular interest in object detection and its related high-level vision tasks such as semantic/instance segmentation</title>
	</analytic>
	<monogr>
		<title level="m">in 2013, and the M.S. and Ph.D. in Computer Science Engineering from Seoul National University in 2015 and 2020, respectively</title>
		<imprint/>
		<respStmt>
			<orgName>S. in Computer Science and Engineering &amp; Statistics from Seoul National University</orgName>
		</respStmt>
	</monogr>
	<note>He received the B. scene understanding, and image captioning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
